{
    "Human-Computer Interaction": {
        "http://arxiv.org/abs/1901.00715v6": {
            "Paper Title": "Wi-Fi Sensing: Applications and Challenges",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.09698v5": {
            "Paper Title": "Gift Contagion in Online Groups: Evidence From Virtual Red Packets",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.04446v2": {
            "Paper Title": "Let's Keep It Safe: Designing User Interfaces that Allow Everyone to\n  Contribute to AI Safety",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.06380v2": {
            "Paper Title": "Bridging Information Security and Environmental Criminology Research to\n  Better Mitigate Cybercrime",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.12879v2": {
            "Paper Title": "Visual Entropy and the Visualization of Uncertainty",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.03779v8": {
            "Paper Title": "Forecast Aggregation via Peer Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.08591v2": {
            "Paper Title": "Doubly Robust Crowdsourcing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.04853v3": {
            "Paper Title": "Embedding Comparator: Visualizing Differences in Global Structure and\n  Local Neighborhoods via Small Multiples",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": ". Our system\nexposes the objects that are most and least similar between two\nvector spaces via a reciprocal local neighborhood similarity metric,\nand local neighborhood based metrics have been shown to usefully\ncapture differences in embedding spaces ",
                    "Citation Text": "William L. Hamilton, Jure Leskovec, and Dan Jurafsky. 2016. Cultural Shift or Lin-\nguistic Drift? Comparing Two Computational Measures of Semantic Change. In\nProceedings of the Conference on Empirical Methods in Natural Language Processing\n(EMNLP) . ACL, Austin, USA, 2116\u20132121.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.02821",
                        "Citation Paper Title": "Title:Cultural Shift or Linguistic Drift? Comparing Two Computational Measures of Semantic Change",
                        "Citation Paper Abstract": "Abstract:Words shift in meaning for many reasons, including cultural factors like new technologies and regular linguistic processes like subjectification. Understanding the evolution of language and culture requires disentangling these underlying causes. Here we show how two different distributional measures can be used to detect two different types of semantic change. The first measure, which has been used in many previous works, analyzes global shifts in a word's distributional semantics, it is sensitive to changes due to regular processes of linguistic drift, such as the semantic generalization of promise (\"I promise.\" -> \"It promised to be exciting.\"). The second measure, which we develop here, focuses on local changes to a word's nearest semantic neighbors; it is more sensitive to cultural shifts, such as the change in the meaning of cell (\"prison cell\" -> \"cell phone\"). Comparing measurements made by these two methods allows researchers to determine whether changes are more cultural or linguistic in nature, a distinction that is essential for work in the digital humanities and historical linguistics.",
                        "Citation Paper Authors": "Authors:William L. Hamilton, Jure Leskovec, Dan Jurafsky"
                    }
                },
                {
                    "Sentence ID": 77,
                    "Sentence": "\u2014 a process that can require non-trivial ML\nexpertise. Prior work has demonstrated that the choice of dimen-\nsionality reduction technique can impact downstream data analy-\nsis ",
                    "Citation Text": "Jiazhi Xia, Yuchen Zhang, Jie Song, Yang Chen, Yunhai Wang, and Shixia Liu. 2022.\nRevisiting Dimensionality Reduction Techniques for Visual Cluster Analysis: An\nEmpirical Study. IEEE Transactions on Visualization and Computer Graphics 28, 1\n(2022), 529\u2013539.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.02894",
                        "Citation Paper Title": "Title:Revisiting Dimensionality Reduction Techniques for Visual Cluster Analysis: An Empirical Study",
                        "Citation Paper Abstract": "Abstract:Dimensionality Reduction (DR) techniques can generate 2D projections and enable visual exploration of cluster structures of high-dimensional datasets. However, different DR techniques would yield various patterns, which significantly affect the performance of visual cluster analysis tasks. We present the results of a user study that investigates the influence of different DR techniques on visual cluster analysis. Our study focuses on the most concerned property types, namely the linearity and locality, and evaluates twelve representative DR techniques that cover the concerned properties. Four controlled experiments were conducted to evaluate how the DR techniques facilitate the tasks of 1) cluster identification, 2) membership identification, 3) distance comparison, and 4) density comparison, respectively. We also evaluated users' subjective preference of the DR techniques regarding the quality of projected clusters. The results show that: 1) Non-linear and Local techniques are preferred in cluster identification and membership identification; 2) Linear techniques perform better than non-linear techniques in density comparison; 3) UMAP (Uniform Manifold Approximation and Projection) and t-SNE (t-Distributed Stochastic Neighbor Embedding) perform the best in cluster identification and membership identification; 4) NMF (Nonnegative Matrix Factorization) has competitive performance in distance comparison; 5) t-SNLE (t-Distributed Stochastic Neighbor Linear Embedding) has competitive performance in density comparison.",
                        "Citation Paper Authors": "Authors:Jiazhi Xia, Yuchen Zhang, Jie Song, Yang Chen, Yunhai Wang, Shixia Liu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1910.14084v2": {
            "Paper Title": "Building an Application Independent Natural Language Interface",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.07773v4": {
            "Paper Title": "MEDIRL: Predicting the Visual Attention of Drivers via Maximum Entropy\n  Deep Inverse Reinforcement Learning",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "are the most\nwell-known large-scale annotated datasets in naturalistic\nand in-lab driving settings, respectively. Importantly, the\nrecently-released annotated driving attention dataset with\nin-lab settings, DADA-2000 ",
                    "Citation Text": "Jianwu Fang, Dingxin Yan, Jiahuan Qiao, Jianru Xue, He\nWang, and Sen Li. Dada-2000: Can driving accident be pre-\ndicted by driver attention analyzed by a benchmark. In 2019\nIEEE Intelligent Transportation Systems Conference (ITSC) ,\npages 4303\u20134309. IEEE, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.12634",
                        "Citation Paper Title": "Title:DADA-2000: Can Driving Accident be Predicted by Driver Attention? Analyzed by A Benchmark",
                        "Citation Paper Abstract": "Abstract:Driver attention prediction is currently becoming the focus in safe driving research community, such as the DR(eye)VE project and newly emerged Berkeley DeepDrive Attention (BDD-A) database in critical situations. In safe driving, an essential task is to predict the incoming accidents as early as possible. BDD-A was aware of this problem and collected the driver attention in laboratory because of the rarity of such scenes. Nevertheless, BDD-A focuses the critical situations which do not encounter actual accidents, and just faces the driver attention prediction task, without a close step for accident prediction. In contrast to this, we explore the view of drivers' eyes for capturing multiple kinds of accidents, and construct a more diverse and larger video benchmark than ever before with the driver attention and the driving accident annotation simultaneously (named as DADA-2000), which has 2000 video clips owning about 658,476 frames on 54 kinds of accidents. These clips are crowd-sourced and captured in various occasions (highway, urban, rural, and tunnel), weather (sunny, rainy and snowy) and light conditions (daytime and nighttime). For the driver attention representation, we collect the maps of fixations, saccade scan path and focusing time. The accidents are annotated by their categories, the accident window in clips and spatial locations of the crash-objects. Based on the analysis, we obtain a quantitative and positive answer for the question in this paper.",
                        "Citation Paper Authors": "Authors:Jianwu Fang, Dingxin Yan, Jiahuan Qiao, Jianru Xue, He Wang, Sen Li"
                    }
                },
                {
                    "Sentence ID": 62,
                    "Sentence": "which can handle raw image inputs\nand enables the model to handle the often sub-optimal and\nseemingly stochastic behaviors of drivers ",
                    "Citation Text": "Markus Wulfmeier, Peter Ondruska, and Ingmar Posner.\nMaximum entropy deep inverse reinforcement learning.\narXiv preprint arXiv:1507.04888 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1507.04888",
                        "Citation Paper Title": "Title:Maximum Entropy Deep Inverse Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:This paper presents a general framework for exploiting the representational capacity of neural networks to approximate complex, nonlinear reward functions in the context of solving the inverse reinforcement learning (IRL) problem. We show in this context that the Maximum Entropy paradigm for IRL lends itself naturally to the efficient training of deep architectures. At test time, the approach leads to a computational complexity independent of the number of demonstrations, which makes it especially well-suited for applications in life-long learning scenarios. Our approach achieves performance commensurate to the state-of-the-art on existing benchmarks while exceeding on an alternative benchmark based on highly varying reward structures. Finally, we extend the basic architecture - which is equivalent to a simplified subclass of Fully Convolutional Neural Networks (FCNNs) with width one - to include larger convolutions in order to eliminate dependency on precomputed spatial features and work on raw input representations.",
                        "Citation Paper Authors": "Authors:Markus Wulfmeier, Peter Ondruska, Ingmar Posner"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.10920v3": {
            "Paper Title": "Driver perceptions of advanced driver assistance systems and safety",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.12686v4": {
            "Paper Title": "Learning Representations by Humans, for Humans",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.05248v3": {
            "Paper Title": "What Do Compressed Deep Neural Networks Forget?",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.03450v2": {
            "Paper Title": "Ubiquitous Acoustic Sensing on Commodity IoT Devices: A Survey",
            "Sentences": [
                {
                    "Sentence ID": 145,
                    "Sentence": ". It allows synthesizing audio sig-\nnals inaudible to humans to manipulate such devices ",
                    "Citation Text": "G. Zhang, C. Yan, X. Ji, T. Zhang, T. Zhang, and W. Xu. Dolphi-\nnAttack: Inaudible V oice Commands. In Proc. of the 24th ACM CCS ,\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.09537",
                        "Citation Paper Title": "Title:DolphinAtack: Inaudible Voice Commands",
                        "Citation Paper Abstract": "Abstract:Speech recognition (SR) systems such as Siri or Google Now have become an increasingly popular human-computer interaction method, and have turned various systems into voice controllable systems(VCS). Prior work on attacking VCS shows that the hidden voice commands that are incomprehensible to people can control the systems. Hidden voice commands, though hidden, are nonetheless audible. In this work, we design a completely inaudible attack, DolphinAttack, that modulates voice commands on ultrasonic carriers (e.g., f > 20 kHz) to achieve inaudibility. By leveraging the nonlinearity of the microphone circuits, the modulated low frequency audio commands can be successfully demodulated, recovered, and more importantly interpreted by the speech recognition systems. We validate DolphinAttack on popular speech recognition systems, including Siri, Google Now, Samsung S Voice, Huawei HiVoice, Cortana and Alexa. By injecting a sequence of inaudible voice commands, we show a few proof-of-concept attacks, which include activating Siri to initiate a FaceTime call on iPhone, activating Google Now to switch the phone to the airplane mode, and even manipulating the navigation system in an Audi automobile. We propose hardware and software defense solutions. We validate that it is feasible to detect DolphinAttack by classifying the audios using supported vector machine (SVM), and suggest to re-design voice controllable systems to be resilient to inaudible voice command attacks.",
                        "Citation Paper Authors": "Authors:Guoming Zhang, Chen Yan, Xiaoyu Ji, Taimin Zhang, Tianchen Zhang, Wenyuan Xu"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "only require limited\nlabels to quickly adapt on target environments. Some early\nattempts in this direction has been made. For example, the\nauthors in ",
                    "Citation Text": "S. Ding, Z. Chen, T. Zheng, and J. Luo. RF-Net: A Uni\ufb01ed\nMeta-Learning Framework for RF-Enabled One-Shot Human Activity\nRecognition. In Proc. of the 18th ACM SenSys , pages 517\u2013530, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.04566",
                        "Citation Paper Title": "Title:RF-Net: a Unified Meta-learning Framework for RF-enabled One-shot Human Activity Recognition",
                        "Citation Paper Abstract": "Abstract:Radio-Frequency (RF) based device-free Human Activity Recognition (HAR) rises as a promising solution for many applications. However, device-free (or contactless) sensing is often more sensitive to environment changes than device-based (or wearable) sensing. Also, RF datasets strictly require on-line labeling during collection, starkly different from image and text data collections where human interpretations can be leveraged to perform off-line labeling. Therefore, existing solutions to RF-HAR entail a laborious data collection process for adapting to new environments. To this end, we propose RF-Net as a meta-learning based approach to one-shot RF-HAR; it reduces the labeling efforts for environment adaptation to the minimum level. In particular, we first examine three representative RF sensing techniques and two major meta-learning approaches. The results motivate us to innovate in two designs: i) a dual-path base HAR network, where both time and frequency domains are dedicated to learning powerful RF features including spatial and attention-based temporal ones, and ii) a metric-based meta-learning framework to enhance the fast adaption capability of the base network, including an RF-specific metric module along with a residual classification module. We conduct extensive experiments based on all three RF sensing techniques in multiple real-world indoor environments; all results strongly demonstrate the efficacy of RF-Net compared with state-of-the-art baselines.",
                        "Citation Paper Authors": "Authors:Shuya Ding, Zhe Chen, Tianyue Zheng, Jun Luo"
                    }
                },
                {
                    "Sentence ID": 126,
                    "Sentence": "allow a model to generalize well on even unseen data, while\nfew-shot learning techniques ",
                    "Citation Text": "Y . Wang, Q. Yao, J. T. Kwok, and L. M. Ni. Generalizing from a\nFew Examples: A Survey on Few-Shot Learning. ACM Comput. Surv. ,\n53(3), June 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.05046",
                        "Citation Paper Title": "Title:Generalizing from a Few Examples: A Survey on Few-Shot Learning",
                        "Citation Paper Abstract": "Abstract:Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-Shot Learning (FSL) is proposed to tackle this problem. Using prior knowledge, FSL can rapidly generalize to new tasks containing only a few samples with supervised information. In this paper, we conduct a thorough survey to fully understand FSL. Starting from a formal definition of FSL, we distinguish FSL from several relevant machine learning problems. We then point out that the core issue in FSL is that the empirical risk minimized is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize FSL methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the FSL problem setups, techniques, applications and theories, are also proposed to provide insights for future research.",
                        "Citation Paper Authors": "Authors:Yaqing Wang, Quanming Yao, James Kwok, Lionel M. Ni"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1907.00062v2": {
            "Paper Title": "DIEL: Interactive Visualization Beyond the Here and Now",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.00719v3": {
            "Paper Title": "Stable Visual Summaries for Trajectory Collections",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.08228v4": {
            "Paper Title": "TED-On: A Total Error Framework for Digital Traces of Human Behavior on\n  Online Platforms",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01970v2": {
            "Paper Title": "The Effects of Information and Communication Technology Use on Human\n  Energy and Fatigue: A Review",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.03970v2": {
            "Paper Title": "A systematic review of smartphone-based human activity recognition for\n  health research",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.01652v2": {
            "Paper Title": "RadVR: A 6DOF Virtual Reality Daylighting Analysis Tool",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.03409v2": {
            "Paper Title": "Conditional Text Generation for Harmonious Human-Machine Interaction",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": "Conditional GAN (CGAN)CNN captures information in an image,\nand LSTM generates the relevant\ndescriptions; the discriminator evaluates\nthe quality of generated descriptions\nDaset al. ",
                    "Citation Text": "Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos\u00e9 MF Moura, Devi Parikh, and Dhruv Batra.\n2017. Visual dialog. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 326\u2013335.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.08669",
                        "Citation Paper Title": "Title:Visual Dialog",
                        "Citation Paper Abstract": "Abstract:We introduce the task of Visual Dialog, which requires an AI agent to hold a meaningful dialog with humans in natural, conversational language about visual content. Specifically, given an image, a dialog history, and a question about the image, the agent has to ground the question in image, infer context from history, and answer the question accurately. Visual Dialog is disentangled enough from a specific downstream task so as to serve as a general test of machine intelligence, while being grounded in vision enough to allow objective evaluation of individual responses and benchmark progress. We develop a novel two-person chat data-collection protocol to curate a large-scale Visual Dialog dataset (VisDial). VisDial v0.9 has been released and contains 1 dialog with 10 question-answer pairs on ~120k images from COCO, with a total of ~1.2M dialog question-answer pairs.\nWe introduce a family of neural encoder-decoder models for Visual Dialog with 3 encoders -- Late Fusion, Hierarchical Recurrent Encoder and Memory Network -- and 2 decoders (generative and discriminative), which outperform a number of sophisticated baselines. We propose a retrieval-based evaluation protocol for Visual Dialog where the AI agent is asked to sort a set of candidate answers and evaluated on metrics such as mean-reciprocal-rank of human response. We quantify gap between machine and human performance on the Visual Dialog task via human studies. Putting it all together, we demonstrate the first 'visual chatbot'! Our dataset, code, trained models and visual chatbot are available on this https URL",
                        "Citation Paper Authors": "Authors:Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos\u00e9 M. F. Moura, Devi Parikh, Dhruv Batra"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "is a document grounded conversation dataset\nwhere each conversation is followed by specified documents about popular movies extracted from\nWikipedia articles. Wizard of Wikipedia24 ",
                    "Citation Text": "Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2018. Wizard of Wikipedia:\nKnowledge-Powered Conversational Agents. In International Conference on Learning Representations .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.01241",
                        "Citation Paper Title": "Title:Wizard of Wikipedia: Knowledge-Powered Conversational agents",
                        "Citation Paper Abstract": "Abstract:In open-domain dialogue intelligent agents should exhibit the use of knowledge, however there are few convincing demonstrations of this to date. The most popular sequence to sequence models typically \"generate and hope\" generic utterances that can be memorized in the weights of the model when mapping from input utterance(s) to output, rather than employing recalled knowledge as context. Use of knowledge has so far proved difficult, in part because of the lack of a supervised learning benchmark task which exhibits knowledgeable open dialogue with clear grounding. To that end we collect and release a large dataset with conversations directly grounded with knowledge retrieved from Wikipedia. We then design architectures capable of retrieving knowledge, reading and conditioning on it, and finally generating natural responses. Our best performing dialogue models are able to conduct knowledgeable discussions on open-domain topics as evaluated by automatic metrics and human evaluations, while our new benchmark allows for measuring further improvements in this important research direction.",
                        "Citation Paper Authors": "Authors:Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, Jason Weston"
                    }
                },
                {
                    "Sentence ID": 164,
                    "Sentence": "publish another high-quality emotional dialogue dataset\ncollecting from telescripts and dialogues in Facebook, named EmotionLines22. All utterances in it\nare labeled with specific emotion according to textual contents to guide the emotional dialogue\nresponse generation.\nKnowledge-based datasets. CMU DoG23 ",
                    "Citation Text": "Kangyan Zhou, Shrimai Prabhumoye, and Alan W Black. 2018. A Dataset for Document Grounded Conversations. In\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing . 708\u2013713.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.07358",
                        "Citation Paper Title": "Title:A Dataset for Document Grounded Conversations",
                        "Citation Paper Abstract": "Abstract:This paper introduces a document grounded dataset for text conversations. We define \"Document Grounded Conversations\" as conversations that are about the contents of a specified document. In this dataset the specified documents were Wikipedia articles about popular movies. The dataset contains 4112 conversations with an average of 21.43 turns per conversation. This positions this dataset to not only provide a relevant chat history while generating responses but also provide a source of information that the models could use. We describe two neural architectures that provide benchmark performance on the task of generating the next response. We also evaluate our models for engagement and fluency, and find that the information from the document helps in generating more engaging and fluent responses.",
                        "Citation Paper Authors": "Authors:Kangyan Zhou, Shrimai Prabhumoye, Alan W Black"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "publish an emotional dialogue dataset called Empathetic\nDialogues21, including an extensive set of emotions and every speaker in it feels with a given emotion\nduring conversations. Chen et al. ",
                    "Citation Text": "Sheng Yeh Chen, Chao Chun Hsu, Chuan Chun Kuo, Kenneth Huang, and Lun Wei Ku. 2019. Emotionlines: An\nemotion corpus of multi-party conversations. In 11th International Conference on Language Resources and Evaluation,\nLREC 2018 . European Language Resources Association (ELRA), 1597\u20131601.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.08379",
                        "Citation Paper Title": "Title:EmotionLines: An Emotion Corpus of Multi-Party Conversations",
                        "Citation Paper Abstract": "Abstract:Feeling emotion is a critical characteristic to distinguish people from machines. Among all the multi-modal resources for emotion detection, textual datasets are those containing the least additional information in addition to semantics, and hence are adopted widely for testing the developed systems. However, most of the textual emotional datasets consist of emotion labels of only individual words, sentences or documents, which makes it challenging to discuss the contextual flow of emotions. In this paper, we introduce EmotionLines, the first dataset with emotions labeling on all utterances in each dialogue only based on their textual content. Dialogues in EmotionLines are collected from Friends TV scripts and private Facebook messenger dialogues. Then one of seven emotions, six Ekman's basic emotions plus the neutral emotion, is labeled on each utterance by 5 Amazon MTurkers. A total of 29,245 utterances from 2,000 dialogues are labeled in EmotionLines. We also provide several strong baselines for emotion detection models on EmotionLines in this paper.",
                        "Citation Paper Authors": "Authors:Sheng-Yeh Chen, Chao-Chun Hsu, Chuan-Chun Kuo, Ting-Hao (Kenneth)Huang, Lun-Wei Ku"
                    }
                },
                {
                    "Sentence ID": 92,
                    "Sentence": "present a high-quality personalized dialogue dataset\nnamed PERSONA CHAT18. In each dialogue, two parts of the conversation are given a group\nof profile information, and the whole dialogue process is conducted around these personalized\ncharacteristics. Humeau et al. ",
                    "Citation Text": "Pierre-Emmanuel Mazar\u00e9, Samuel Humeau, Martin Raison, and Antoine Bordes. 2018. Training Millions of Personal-\nized Dialogue Agents. In EMNLP .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.01984",
                        "Citation Paper Title": "Title:Training Millions of Personalized Dialogue Agents",
                        "Citation Paper Abstract": "Abstract:Current dialogue systems are not very engaging for users, especially when trained end-to-end without relying on proactive reengaging scripted strategies. Zhang et al. (2018) showed that the engagement level of end-to-end dialogue models increases when conditioning them on text personas providing some personalized back-story to the model. However, the dataset used in Zhang et al. (2018) is synthetic and of limited size as it contains around 1k different personas. In this paper we introduce a new dataset providing 5 million personas and 700 million persona-based dialogues. Our experiments show that, at this scale, training using personas still improves the performance of end-to-end systems. In addition, we show that other tasks benefit from the wide coverage of our dataset by fine-tuning our model on the data from Zhang et al. (2018) and achieving state-of-the-art results.",
                        "Citation Paper Authors": "Authors:Pierre-Emmanuel Mazar\u00e9, Samuel Humeau, Martin Raison, Antoine Bordes"
                    }
                },
                {
                    "Sentence ID": 151,
                    "Sentence": "is another large multi-turn dialogue dataset containing almost one million conversa-\ntions extracted from the Ubuntu chat logs that is a great help for training context-sensitive technical\ndialogue systems.\nPersonalized datasets. Zhang et al. ",
                    "Citation Text": "Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018. Personalizing\nDialogue Agents: I have a dog, do you have pets too?. In Proceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) . 2204\u20132213.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.07243",
                        "Citation Paper Title": "Title:Personalizing Dialogue Agents: I have a dog, do you have pets too?",
                        "Citation Paper Abstract": "Abstract:Chit-chat models are known to have several problems: they lack specificity, do not display a consistent personality and are often not very captivating. In this work we present the task of making chit-chat more engaging by conditioning on profile information. We collect data and train models to (i) condition on their given profile information; and (ii) information about the person they are talking to, resulting in improved dialogues, as measured by next utterance prediction. Since (ii) is initially unknown our model is trained to engage its partner with personal topics, and we show the resulting dialogue can be used to predict profile information about the interlocutors.",
                        "Citation Paper Authors": "Authors:Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, Jason Weston"
                    }
                },
                {
                    "Sentence ID": 82,
                    "Sentence": "is a large-scale multi-turn dialogue\ndataset providing contextual information during the conversation, which contains a large metadata-\nrich collection of fictional conversations extracted from raw movie scripts. Ubuntu Dialogue\nCorpus17 ",
                    "Citation Text": "Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. 2015. The ubuntu dialogue corpus: A large dataset for\nresearch in unstructured multi-turn dialogue systems. arXiv preprint arXiv:1506.08909 (2015).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.08909",
                        "Citation Paper Title": "Title:The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems",
                        "Citation Paper Abstract": "Abstract:This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words. This provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data. The dataset has both the multi-turn property of conversations in the Dialog State Tracking Challenge datasets, and the unstructured nature of interactions from microblog services such as Twitter. We also describe two neural learning architectures suitable for analyzing this dataset, and provide benchmark performance on the task of selecting the best next response.",
                        "Citation Paper Authors": "Authors:Ryan Lowe, Nissan Pow, Iulian Serban, Joelle Pineau"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": "samples from the next-token distribution after\nhaving filtered this distribution to keep only the top \ud835\udc58tokens, while the Top-p sampling ",
                    "Citation Text": "Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The Curious Case of Neural Text Degeneration.\nInInternational Conference on Learning Representations .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.09751",
                        "Citation Paper Title": "Title:The Curious Case of Neural Text Degeneration",
                        "Citation Paper Abstract": "Abstract:Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive.\nIn this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.",
                        "Citation Paper Authors": "Authors:Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi"
                    }
                },
                {
                    "Sentence ID": 73,
                    "Sentence": "propose the Group diverse Beam Search to increase the variety\nof generated texts, which divides the beam into groups and utilizes a group dissimilarity penalty to\nreduce the similarity between different search groups. Similarly, Li et al. ",
                    "Citation Text": "Jiwei Li, Will Monroe, and Dan Jurafsky. 2016. A simple, fast diverse decoding algorithm for neural generation. arXiv\npreprint arXiv:1611.08562 (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.08562",
                        "Citation Paper Title": "Title:A Simple, Fast Diverse Decoding Algorithm for Neural Generation",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose a simple, fast decoding algorithm that fosters diversity in neural generation. The algorithm modifies the standard beam search algorithm by adding an inter-sibling ranking penalty, favoring choosing hypotheses from diverse parents. We evaluate the proposed model on the tasks of dialogue response generation, abstractive summarization and machine translation. We find that diverse decoding helps across all tasks, especially those for which reranking is needed.\nWe further propose a variation that is capable of automatically adjusting its diversity decoding rates for different inputs using reinforcement learning (RL). We observe a further performance boost from this RL technique. This paper includes material from the unpublished script \"Mutual Information and Diverse Decoding Improve Neural Machine Translation\" (Li and Jurafsky, 2016).",
                        "Citation Paper Authors": "Authors:Jiwei Li, Will Monroe, Dan Jurafsky"
                    }
                },
                {
                    "Sentence ID": 154,
                    "Sentence": "propose the Adapter-Bot,\nwhich triggers different skills via different Adapters trained independently. The backbone of the\nAdapter-Bot is a pre-trained conversational model such as DialoGPT ",
                    "Citation Text": "Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill\nDolan. 2019. Dialogpt: Large-scale generative pre-training for conversational response generation. arXiv preprint\narXiv:1911.00536 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.00536",
                        "Citation Paper Title": "Title:DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation",
                        "Citation Paper Abstract": "Abstract:We present a large, tunable neural conversational response generation model, DialoGPT (dialogue generative pre-trained transformer). Trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning from 2005 through 2017, DialoGPT extends the Hugging Face PyTorch transformer to attain a performance close to human both in terms of automatic and human evaluation in single-turn dialogue settings. We show that conversational systems that leverage DialoGPT generate more relevant, contentful and context-consistent responses than strong baseline systems. The pre-trained model and training pipeline are publicly released to facilitate research into neural response generation and the development of more intelligent open-domain dialogue systems.",
                        "Citation Paper Authors": "Authors:Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan"
                    }
                },
                {
                    "Sentence ID": 94,
                    "Sentence": ". Using a large amount of\ndata to train the LSTM language model in an unsupervised way, the contextual word vector of\neach word can be obtained to demonstrate strong results across discriminative natural language\nunderstanding ( NLU ) tasks ",
                    "Citation Text": "Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Contextualized\nword vectors. In Advances in Neural Information Processing Systems . 6294\u20136305.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.00107",
                        "Citation Paper Title": "Title:Learned in Translation: Contextualized Word Vectors",
                        "Citation Paper Abstract": "Abstract:Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.",
                        "Citation Paper Authors": "Authors:Bryan McCann, James Bradbury, Caiming Xiong, Richard Socher"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "LSTM + CNNCNN and LSTM respectively encodes the\nimage and the question into vectors to\ncapture the semantic information, and\nthen another LSTM generates\ncorresponding answers\nDaiet al. ",
                    "Citation Text": "Bo Dai, Sanja Fidler, Raquel Urtasun, and Dahua Lin. 2017. Towards diverse and natural image descriptions via a\nconditional gan. In Proceedings of the IEEE International Conference on Computer Vision . 2970\u20132979.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.06029",
                        "Citation Paper Title": "Title:Towards Diverse and Natural Image Descriptions via a Conditional GAN",
                        "Citation Paper Abstract": "Abstract:Despite the substantial progress in recent years, the image captioning techniques are still far from being perfect.Sentences produced by existing methods, e.g. those based on RNNs, are often overly rigid and lacking in variability. This issue is related to a learning principle widely used in practice, that is, to maximize the likelihood of training samples. This principle encourages high resemblance to the \"ground-truth\" captions while suppressing other reasonable descriptions. Conventional evaluation metrics, e.g. BLEU and METEOR, also favor such restrictive methods. In this paper, we explore an alternative approach, with the aim to improve the naturalness and diversity -- two essential properties of human expression. Specifically, we propose a new framework based on Conditional Generative Adversarial Networks (CGAN), which jointly learns a generator to produce descriptions conditioned on images and an evaluator to assess how well a description fits the visual content. It is noteworthy that training a sequence generator is nontrivial. We overcome the difficulty by Policy Gradient, a strategy stemming from Reinforcement Learning, which allows the generator to receive early feedback along the way. We tested our method on two large datasets, where it performed competitively against real people in our user study and outperformed other methods on various tasks.",
                        "Citation Paper Authors": "Authors:Bo Dai, Sanja Fidler, Raquel Urtasun, Dahua Lin"
                    }
                },
                {
                    "Sentence ID": 89,
                    "Sentence": "RNN + CNNEncoder CNN captures information in\nimages, and decoder RNN generates\nneural language descriptions based on\ntheir features\nMalinowski et al. ",
                    "Citation Text": "Mateusz Malinowski, Marcus Rohrbach, and Mario Fritz. 2015. Ask your neurons: A neural-based approach to\nanswering questions about images. In Proceedings of the IEEE international conference on computer vision . 1\u20139.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1505.01121",
                        "Citation Paper Title": "Title:Ask Your Neurons: A Neural-based Approach to Answering Questions about Images",
                        "Citation Paper Abstract": "Abstract:We address a question answering task on real-world images that is set up as a Visual Turing Test. By combining latest advances in image representation and natural language processing, we propose Neural-Image-QA, an end-to-end formulation to this problem for which all parts are trained jointly. In contrast to previous efforts, we are facing a multi-modal problem where the language output (answer) is conditioned on visual and natural language input (image and question). Our approach Neural-Image-QA doubles the performance of the previous best approach on this problem. We provide additional insights into the problem by analyzing how much information is contained only in the language part for which we provide a new human baseline. To study human consensus, which is related to the ambiguities inherent in this challenging task, we propose two novel metrics and collect additional answers which extends the original DAQUAR dataset to DAQUAR-Consensus.",
                        "Citation Paper Authors": "Authors:Mateusz Malinowski, Marcus Rohrbach, Mario Fritz"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "is an extension of CNN in the graph domain, which can effectively learn the structural\ninformation of nodes and edges in the knowledge graph. De et al. ",
                    "Citation Text": "Nicola De Cao, Wilker Aziz, and Ivan Titov. 2019. Question Answering by Reasoning Across Documents with Graph\nConvolutional Networks. In Proceedings of NAACL-HLT . 2306\u20132317.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.09920",
                        "Citation Paper Title": "Title:Question Answering by Reasoning Across Documents with Graph Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:Most research in reading comprehension has focused on answering questions based on individual documents or even single paragraphs. We introduce a neural model which integrates and reasons relying on information spread within documents and across multiple documents. We frame it as an inference problem on a graph. Mentions of entities are nodes of this graph while edges encode relations between different mentions (e.g., within- and cross-document co-reference). Graph convolutional networks (GCNs) are applied to these graphs and trained to perform multi-step reasoning. Our Entity-GCN method is scalable and compact, and it achieves state-of-the-art results on a multi-document question answering dataset, WikiHop (Welbl et al., 2018).",
                        "Citation Paper Authors": "Authors:Nicola De Cao, Wilker Aziz, Ivan Titov"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1909.00855v2": {
            "Paper Title": "Defining and Adopting an End User Computing Policy: A Case Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.11936v4": {
            "Paper Title": "Smell Pittsburgh: Engaging Community Citizen Science for Air Quality",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.00310v4": {
            "Paper Title": "Towards Robust Deep Neural Networks for Affect and Depression\n  Recognition from Speech",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.02015v2": {
            "Paper Title": "Reach Out and Help: Assisted Remote Collaboration through a Handheld\n  Robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.04149v5": {
            "Paper Title": "A Survey on Deep Learning-based Non-Invasive Brain Signals:Recent\n  Advances and New Frontiers",
            "Sentences": [
                {
                    "Sentence ID": 130,
                    "Sentence": "investigated the performance of CNN-LSTM on seizure\ndetection after channel selection and the sensitivities range\nfrom 33% to 37% while false alarms ranges from 38%\nto 50%. Golmohammadi et al. ",
                    "Citation Text": "M. Golmohammadi, S. Ziyabari, V . Shah, S. L. de Diego, I. Obeid,\nand J. Picone, \u201cDeep architectures for automated seizure\ndetection in scalp eegs,\u201d arXiv preprint arXiv:1712.09776 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.09776",
                        "Citation Paper Title": "Title:Deep Architectures for Automated Seizure Detection in Scalp EEGs",
                        "Citation Paper Abstract": "Abstract:Automated seizure detection using clinical electroencephalograms is a challenging machine learning problem because the multichannel signal often has an extremely low signal to noise ratio. Events of interest such as seizures are easily confused with signal artifacts (e.g, eye movements) or benign variants (e.g., slowing). Commercially available systems suffer from unacceptably high false alarm rates. Deep learning algorithms that employ high dimensional models have not previously been effective due to the lack of big data resources. In this paper, we use the TUH EEG Seizure Corpus to evaluate a variety of hybrid deep structures including Convolutional Neural Networks and Long Short-Term Memory Networks. We introduce a novel recurrent convolutional architecture that delivers 30% sensitivity at 7 false alarms per 24 hours. We have also evaluated our system on a held-out evaluation set based on the Duke University Seizure Corpus and demonstrate that performance trends are similar to the TUH EEG Seizure Corpus. This is a significant finding because the Duke corpus was collected with different instrumentation and at different hospitals. Our work shows that deep learning architectures that integrate spatial and temporal contexts are critical to achieving state of the art performance and will enable a new generation of clinically-acceptable technology.",
                        "Citation Paper Authors": "Authors:Meysam Golmohammadi, Saeedeh Ziyabari, Vinit Shah, Silvia Lopez de Diego, Iyad Obeid, Joseph Picone"
                    }
                },
                {
                    "Sentence ID": 149,
                    "Sentence": "used\nCNN to extract the latent features and fed into a Random\nForest classi\ufb01er for the \ufb01nal seizure detection of neonatal\nbabies. Chu et al. ",
                    "Citation Text": "L. Chu, R. Qiu, H. Liu, Z. Ling, T. Zhang, and J. Wang, \u201cIndividual\nrecognition in schizophrenia using deep learning methods with\nrandom forest and voting classi\ufb01ers: Insights from resting state\neeg streams,\u201d arXiv preprint arXiv:1707.03467 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.03467",
                        "Citation Paper Title": "Title:Individual Recognition in Schizophrenia using Deep Learning Methods with Random Forest and Voting Classifiers: Insights from Resting State EEG Streams",
                        "Citation Paper Abstract": "Abstract:Recently, there has been a growing interest in monitoring brain activity for individual recognition system. So far these works are mainly focussing on single channel data or fragment data collected by some advanced brain monitoring modalities. In this study we propose new individual recognition schemes based on spatio-temporal resting state Electroencephalography (EEG) data. Besides, instead of using features derived from artificially-designed procedures, modified deep learning architectures which aim to automatically extract an individual's unique features are developed to conduct classification. Our designed deep learning frameworks are proved of a small but consistent advantage of replacing the $softmax$ layer with Random Forest. Additionally, a voting layer is added at the top of designed neural networks in order to tackle the classification problem arisen from EEG streams. Lastly, various experiments are implemented to evaluate the performance of the designed deep learning architectures; Results indicate that the proposed EEG-based individual recognition scheme yields a high degree of classification accuracy: $81.6\\%$ for characteristics in high risk (CHR) individuals, $96.7\\%$ for clinically stable first episode patients with schizophrenia (FES) and $99.2\\%$ for healthy controls (HC).",
                        "Citation Paper Authors": "Authors:Lei Chu, Robert Qiu, Haichun Liu, Zenan Ling, Tianhong Zhang, Jijun Wang"
                    }
                },
                {
                    "Sentence ID": 127,
                    "Sentence": "combined CNN and a traditional\nclassi\ufb01er for schizophrenia recognition.\n(ii)Representative models. For disease detection,\none commonly used method is adopting a representative\nmodel ( e.g., DBN) followed by a softmax layer for\nclassi\ufb01cation ",
                    "Citation Text": "J. Turner, A. Page, T. Mohsenin, and T. Oates, \u201cDeep belief net-\nworks used on high resolution multichannel electroencephalog-\nraphy data for seizure detection,\u201d in 2014 AAAI Spring Sympo-\nsium Series , 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.08430",
                        "Citation Paper Title": "Title:Deep Belief Networks used on High Resolution Multichannel Electroencephalography Data for Seizure Detection",
                        "Citation Paper Abstract": "Abstract:Ubiquitous bio-sensing for personalized health monitoring is slowly becoming a reality with the increasing availability of small, diverse, robust, high fidelity sensors. This oncoming flood of data begs the question of how we will extract useful information from it. In this paper we explore the use of a variety of representations and machine learning algorithms applied to the task of seizure detection in high resolution, multichannel EEG data. We explore classification accuracy, computational complexity and memory requirements with a view toward understanding which approaches are most suitable for such tasks as the number of people involved and the amount of data they produce grows to be quite large. In particular, we show that layered learning approaches such as Deep Belief Networks excel along these dimensions.",
                        "Citation Paper Authors": "Authors:JT Turner, Adam Page, Tinoosh Mohsenin, Tim Oates"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": "manually extracted\nthe time-frequency features and achieved a classi\ufb01cation\naccuracy of 86%. Others used RNN ",
                    "Citation Text": "S. Biswal, J. Kulas, H. Sun, B. Goparaju, M. B. Westover, M. T.\nBianchi, and J. Sun, \u201cSleepnet: automated sleep staging system\nvia deep learning,\u201d arXiv preprint arXiv:1707.08262 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.08262",
                        "Citation Paper Title": "Title:SLEEPNET: Automated Sleep Staging System via Deep Learning",
                        "Citation Paper Abstract": "Abstract:Sleep disorders, such as sleep apnea, parasomnias, and hypersomnia, affect 50-70 million adults in the United States (Hillman et al., 2006). Overnight polysomnography (PSG), including brain monitoring using electroencephalography (EEG), is a central component of the diagnostic evaluation for sleep disorders. While PSG is conventionally performed by trained technologists, the recent rise of powerful neural network learning algorithms combined with large physiological datasets offers the possibility of automation, potentially making expert-level sleep analysis more widely available. We propose SLEEPNET (Sleep EEG neural network), a deployed annotation tool for sleep staging. SLEEPNET uses a deep recurrent neural network trained on the largest sleep physiology database assembled to date, consisting of PSGs from over 10,000 patients from the Massachusetts General Hospital (MGH) Sleep Laboratory. SLEEPNET achieves human-level annotation performance on an independent test set of 1,000 EEGs, with an average accuracy of 85.76% and algorithm-expert inter-rater agreement (IRA) of kappa = 79.46%, comparable to expert-expert IRA.",
                        "Citation Paper Authors": "Authors:Siddharth Biswal, Joshua Kulas, Haoqi Sun, Balaji Goparaju, M Brandon Westover, Matt T Bianchi, Jimeng Sun"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "Partial EEG (ERD, P300, SSVEP, VEP, AEP) No 2007\n6 ",
                    "Citation Text": "G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi,\nM. Ghafoorian, J. A. Van Der Laak, B. Van Ginneken, and C. I.\nS\u00b4anchez, \u201cA survey on deep learning in medical image analysis,\u201d\nMedical image analysis , vol. 42, pp. 60\u201388, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.05747",
                        "Citation Paper Title": "Title:A Survey on Deep Learning in Medical Image Analysis",
                        "Citation Paper Abstract": "Abstract:Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks and provide concise overviews of studies per application area. Open challenges and directions for future research are discussed.",
                        "Citation Paper Authors": "Authors:Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud Arindra Adiyoso Setio, Francesco Ciompi, Mohsen Ghafoorian, Jeroen A.W.M. van der Laak, Bram van Ginneken, Clara I. S\u00e1nchez"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1905.03302v2": {
            "Paper Title": "PerceptNet: Learning Perceptual Similarity of Haptic Textures in\n  Presence of Unorderable Triplets",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": ". Our\nwork has similar goals, albeit in the haptic domain. However,\nnone of these prior works leverages unorderable/ambiguous\ntriplets. Pei et al. ",
                    "Citation Text": "Y . Pei, X. Z. Fern, R. Rosales, and T. V . Tjahja. Discriminative\nclustering with relative constraints. arXiv preprint arXiv:1501.00037 ,\n2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1501.00037",
                        "Citation Paper Title": "Title:Discriminative Clustering with Relative Constraints",
                        "Citation Paper Abstract": "Abstract:We study the problem of clustering with relative constraints, where each constraint specifies relative similarities among instances. In particular, each constraint $(x_i, x_j, x_k)$ is acquired by posing a query: is instance $x_i$ more similar to $x_j$ than to $x_k$? We consider the scenario where answers to such queries are based on an underlying (but unknown) class concept, which we aim to discover via clustering. Different from most existing methods that only consider constraints derived from yes and no answers, we also incorporate don't know responses. We introduce a Discriminative Clustering method with Relative Constraints (DCRC) which assumes a natural probabilistic relationship between instances, their underlying cluster memberships, and the observed constraints. The objective is to maximize the model likelihood given the constraints, and in the meantime enforce cluster separation and cluster balance by also making use of the unlabeled instances. We evaluated the proposed method using constraints generated from ground-truth class labels, and from (noisy) human judgments from a user study. Experimental results demonstrate: 1) the usefulness of relative constraints, in particular when don't know answers are considered; 2) the improved performance of the proposed method over state-of-the-art methods that utilize either relative or pairwise constraints; and 3) the robustness of our method in the presence of noisy constraints, such as those provided by human judgement.",
                        "Citation Paper Authors": "Authors:Yuanli Pei, Xiaoli Z. Fern, R\u00f3mer Rosales, Teresa Vania Tjahja"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1907.02872v3": {
            "Paper Title": "Anteater: Interactive Visualization of Program Execution Values in\n  Context",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.06814v4": {
            "Paper Title": "SoK: Three Facets of Privacy Policies",
            "Sentences": [
                {
                    "Sentence ID": 50,
                    "Sentence": ", which will later be\nInriaSoK: Three Facets of Privacy Policies 9\nnotably used by Polisis ",
                    "Citation Text": "Hamza Harkous, Kassem Fawaz, R ~A\u00a9mi Lebret, Florian Schaub, Kang G. Shin, and Karl\nAberer. Polisis: Automated Analysis and Presentation of Privacy Policies Using Deep\nLearning. 2018-02-07. URL http://arxiv :org/abs/1802 :02561 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.02561",
                        "Citation Paper Title": "Title:Polisis: Automated Analysis and Presentation of Privacy Policies Using Deep Learning",
                        "Citation Paper Abstract": "Abstract:Privacy policies are the primary channel through which companies inform users about their data collection and sharing practices. These policies are often long and difficult to comprehend. Short notices based on information extracted from privacy policies have been shown to be useful but face a significant scalability hurdle, given the number of policies and their evolution over time. Companies, users, researchers, and regulators still lack usable and scalable tools to cope with the breadth and depth of privacy policies. To address these hurdles, we propose an automated framework for privacy policy analysis (Polisis). It enables scalable, dynamic, and multi-dimensional queries on natural language privacy policies. At the core of Polisis is a privacy-centric language model, built with 130K privacy policies, and a novel hierarchy of neural-network classifiers that accounts for both high-level aspects and fine-grained details of privacy practices. We demonstrate Polisis' modularity and utility with two applications supporting structured and free-form querying. The structured querying application is the automated assignment of privacy icons from privacy policies. With Polisis, we can achieve an accuracy of 88.4% on this task. The second application, PriBot, is the first freeform question-answering system for privacy policies. We show that PriBot can produce a correct answer among its top-3 results for 82% of the test questions. Using an MTurk user study with 700 participants, we show that at least one of PriBot's top-3 answers is relevant to users for 89% of the test questions.",
                        "Citation Paper Authors": "Authors:Hamza Harkous, Kassem Fawaz, R\u00e9mi Lebret, Florian Schaub, Kang G. Shin, Karl Aberer"
                    }
                },
                {
                    "Sentence ID": 106,
                    "Sentence": "observed that\na signi\fcant part (16%) of websites added cookie consent notices after the GDPR, but these\nnotices do not always comply with transparency requirements according to Utz et al. ",
                    "Citation Text": "Christine Utz, Martin Degeling, Sascha Fahl, Florian Schaub, and Thorsten Holz.\n(Un)informed Consent: Studying GDPR Consent Notices in the Field. page 18, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.02638",
                        "Citation Paper Title": "Title:(Un)informed Consent: Studying GDPR Consent Notices in the Field",
                        "Citation Paper Abstract": "Abstract:Since the adoption of the General Data Protection Regulation (GDPR) in May 2018 more than 60 % of popular websites in Europe display cookie consent notices to their visitors. This has quickly led to users becoming fatigued with privacy notifications and contributed to the rise of both browser extensions that block these banners and demands for a solution that bundles consent across multiple websites or in the browser.\nIn this work, we identify common properties of the graphical user interface of consent notices and conduct three experiments with more than 80,000 unique users on a German website to investigate the influence of notice position, type of choice, and content framing on consent. We find that users are more likely to interact with a notice shown in the lower (left) part of the screen. Given a binary choice, more users are willing to accept tracking compared to mechanisms that require them to allow cookie use for each category or company individually. We also show that the wide-spread practice of nudging has a large effect on the choices users make. Our experiments show that seemingly small implementation decisions can substantially impact whether and how people interact with consent notices. Our findings demonstrate the importance for regulation to not just require consent, but also provide clear requirements or guidance for how this consent has to be obtained in order to ensure that users can make free and informed choices.",
                        "Citation Paper Authors": "Authors:Christine Utz, Martin Degeling, Sascha Fahl, Florian Schaub, Thorsten Holz"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1903.01977v2": {
            "Paper Title": "Crowdsourced Behavior-Driven Development: Implementing Microservices\n  through Microtasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.00124v2": {
            "Paper Title": "A Free Lunch in Generating Datasets: Building a VQG and VQA System with\n  Attention and Humans in the Loop",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": ", we hope that our work contributes to the\nexpansion of datasets and thus improves performance.\n3. Data\nWe used the Visual Genome dataset ",
                    "Citation Text": "R. Krishna, Y . Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz,\nS. Chen, Y . Kalantidis, L.-J. Li, D. A. Shamma, et al. Vi-\nsual genome: Connecting language and vision using crowd-\nsourced dense image annotations. International Journal of\nComputer Vision , 123(1):32\u201373, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.07332",
                        "Citation Paper Title": "Title:Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations",
                        "Citation Paper Abstract": "Abstract:Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked \"What vehicle is the person riding?\", computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) in order to answer correctly that \"the person is riding a horse-drawn carriage\".\nIn this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense annotations of objects, attributes, and relationships within each image to learn these models. Specifically, our dataset contains over 100K images where each image has an average of 21 objects, 18 attributes, and 18 pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases in region descriptions and questions answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answers.",
                        "Citation Paper Authors": "Authors:Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, Fei-Fei Li"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "and performance even then being at around 60\nto 70 percent ",
                    "Citation Text": "A. Agrawal, D. Batra, and D. Parikh. Analyzing the\nbehavior of visual question answering models. CoRR ,\nabs/1606.07356, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.07356",
                        "Citation Paper Title": "Title:Analyzing the Behavior of Visual Question Answering Models",
                        "Citation Paper Abstract": "Abstract:Recently, a number of deep-learning based models have been proposed for the task of Visual Question Answering (VQA). The performance of most models is clustered around 60-70%. In this paper we propose systematic methods to analyze the behavior of these models as a first step towards recognizing their strengths and weaknesses, and identifying the most fruitful directions for progress. We analyze two models, one each from two major classes of VQA models -- with-attention and without-attention and show the similarities and differences in the behavior of these models. We also analyze the winning entry of the VQA Challenge 2016.\nOur behavior analysis reveals that despite recent progress, today's VQA models are \"myopic\" (tend to fail on sufficiently novel instances), often \"jump to conclusions\" (converge on a predicted answer after 'listening' to just half the question), and are \"stubborn\" (do not change their answers across images).",
                        "Citation Paper Authors": "Authors:Aishwarya Agrawal, Dhruv Batra, Devi Parikh"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.04719v4": {
            "Paper Title": "PLIERS: A Process that Integrates User-Centered Methods into Programming\n  Language Design",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.11474v3": {
            "Paper Title": "SoundSpaces: Audio-Visual Navigation in 3D Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.07747v4": {
            "Paper Title": "Mutual Information-driven Subject-invariant and Class-relevant Deep\n  Representation Learning in BCI",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": "Fig. 3. Illustrative comparison of PSD maps and decision-relevance heatmaps ",
                    "Citation Text": "G. Montavon, S. Lapuschkin, A. Binder, W. Samek, and K.-R. M \u00a8uller,\n\u201cExplaining nonlinear classi\ufb01cation decisions with deep taylor decom-\nposition,\u201d Pattern Recognition , pp. 211\u2013222, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.02479",
                        "Citation Paper Title": "Title:Explaining NonLinear Classification Decisions with Deep Taylor Decomposition",
                        "Citation Paper Abstract": "Abstract:Nonlinear methods such as Deep Neural Networks (DNNs) are the gold standard for various challenging machine learning problems, e.g., image classification, natural language processing or human action recognition. Although these methods perform impressively well, they have a significant disadvantage, the lack of transparency, limiting the interpretability of the solution and thus the scope of application in practice. Especially DNNs act as black boxes due to their multilayer nonlinear structure. In this paper we introduce a novel methodology for interpreting generic multilayer neural networks by decomposing the network classification decision into contributions of its input elements. Although our focus is on image classification, the method is applicable to a broad set of input data, learning tasks and network architectures. Our method is based on deep Taylor decomposition and efficiently utilizes the structure of the network by backpropagating the explanations from the output to the input layer. We evaluate the proposed method empirically on the MNIST and ILSVRC data sets.",
                        "Citation Paper Authors": "Authors:Gr\u00e9goire Montavon, Sebastian Bach, Alexander Binder, Wojciech Samek, Klaus-Robert M\u00fcller"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": ".\nRegarding the domain shift problem, numerous studies\non machine learning, referring to domain adaptation, have\nbeen conducted ",
                    "Citation Text": "Y . Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Lavio-\nlette, M. Marchand, and V . Lempitsky, \u201cDomain-adversarial training of\nneural networks,\u201d The Journal of Machine Learning Research , vol. 17,\nno. 1, pp. 2096\u20132030, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1505.07818",
                        "Citation Paper Title": "Title:Domain-Adversarial Training of Neural Networks",
                        "Citation Paper Abstract": "Abstract:We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.",
                        "Citation Paper Authors": "Authors:Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario Marchand, Victor Lempitsky"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "with a rate of 0:5to\nprevent over-\ufb01tting. We trained models by using a RAdam ",
                    "Citation Text": "L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han, \u201cOn the\nvariance of the adaptive learning rate and beyond,\u201d in Proceedings of\nthe Eighth International Conference on Learning Representations (ICLR\n2020) , April 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.03265",
                        "Citation Paper Title": "Title:On the Variance of the Adaptive Learning Rate and Beyond",
                        "Citation Paper Abstract": "Abstract:The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Here, we study its mechanism in details. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate (i.e., it has problematically large variance in the early stage), suggest warmup works as a variance reduction technique, and provide both empirical and theoretical evidence to verify our hypothesis. We further propose RAdam, a new variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Extensive experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the effectiveness and robustness of our proposed method. All implementations are available at: this https URL.",
                        "Citation Paper Authors": "Authors:Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, Jiawei Han"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "takes advantage of the Jenshen-Shannon\n(JS) divergence as an alternative of Eq. (2) by following ",
                    "Citation Text": "S. Nowozin, B. Cseke, and R. Tomioka, \u201cf-gan: Training generative\nneural samplers using variational divergence minimization,\u201d in Advances\nin Neural Information Processing Systems , 2016, pp. 271\u2013279.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.00709",
                        "Citation Paper Title": "Title:f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization",
                        "Citation Paper Abstract": "Abstract:Generative neural samplers are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution defined by the network weights. These models are expressive and allow efficient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization. The generative-adversarial training method allows to train such models through the use of an auxiliary discriminative neural network. We show that the generative-adversarial approach is a special case of an existing more general variational divergence estimation approach. We show that any f-divergence can be used for training generative neural samplers. We discuss the benefits of various choices of divergence functions on training complexity and the quality of the obtained generative models.",
                        "Citation Paper Authors": "Authors:Sebastian Nowozin, Botond Cseke, Ryota Tomioka"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": ". In order to address the\nlimitation, previous studies exploited multiple subjects and/or\nsessions data simultaneously to train their respective models\nthrough transfer learning ",
                    "Citation Text": "V . Jayaram, M. Alamgir, Y . Altun, B. Scholkopf, and M. Grosse-\nWentrup, \u201cTransfer learning in brain-computer interfaces,\u201d IEEE Com-\nputational Intelligence Magazine , vol. 11, no. 1, pp. 20\u201331, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.00296",
                        "Citation Paper Title": "Title:Transfer Learning in Brain-Computer Interfaces",
                        "Citation Paper Abstract": "Abstract:The performance of brain-computer interfaces (BCIs) improves with the amount of available training data, the statistical distribution of this data, however, varies across subjects as well as across sessions within individual subjects, limiting the transferability of training data or trained models between them. In this article, we review current transfer learning techniques in BCIs that exploit shared structure between training data of multiple subjects and/or sessions to increase performance. We then present a framework for transfer learning in the context of BCIs that can be applied to any arbitrary feature space, as well as a novel regression estimation method that is specifically designed for the structure of a system based on the electroencephalogram (EEG). We demonstrate the utility of our framework and method on subject-to-subject transfer in a motor-imagery paradigm as well as on session-to-session transfer in one patient diagnosed with amyotrophic lateral sclerosis (ALS), showing that it is able to outperform other comparable methods on an identical dataset.",
                        "Citation Paper Authors": "Authors:Vinay Jayaram, Morteza Alamgir, Yasemin Altun, Bernhard Sch\u00f6lkopf, Moritz Grosse-Wentrup"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1909.05189v3": {
            "Paper Title": "ORES: Lowering Barriers with Participatory Machine Learning in Wikipedia",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.03863v5": {
            "Paper Title": "Exploring the Effectiveness of Face-to-face Mixed Reality for Teaching\n  with Chalktalk",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.04464v4": {
            "Paper Title": "Toward Personalized XAI: A Case Study in Intelligent Tutoring Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.10966v2": {
            "Paper Title": "Hate begets Hate: A Temporal Study of Hate Speech",
            "Sentences": [
                {
                    "Sentence ID": 72,
                    "Sentence": "presented a list of criteria based on\ncritical race theory to identify racist and sexist slurs.111:4 Mathew et al.\nMore recently, researchers have started using deep learning methods [ 6,89] and graph embedding\ntechniques ",
                    "Citation Text": "Manoel Horta Ribeiro, Pedro H Calais, Yuri A Santos, Virg\u00edlio AF Almeida, and Wagner Meira Jr. 2018. Characterizing\nand detecting hateful users on twitter. In Twelfth International AAAI Conference on Web and Social Media .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.08977",
                        "Citation Paper Title": "Title:Characterizing and Detecting Hateful Users on Twitter",
                        "Citation Paper Abstract": "Abstract:Most current approaches to characterize and detect hate speech focus on \\textit{content} posted in Online Social Networks. They face shortcomings to collect and annotate hateful speech due to the incompleteness and noisiness of OSN text and the subjectivity of hate speech. These limitations are often aided with constraints that oversimplify the problem, such as considering only tweets containing hate-related words. In this work we partially address these issues by shifting the focus towards \\textit{users}. We develop and employ a robust methodology to collect and annotate hateful users which does not depend directly on lexicon and where the users are annotated given their entire profile. This results in a sample of Twitter's retweet graph containing $100,386$ users, out of which $4,972$ were annotated. We also collect the users who were banned in the three months that followed the data collection. We show that hateful users differ from normal ones in terms of their activity patterns, word usage and as well as network structure. We obtain similar results comparing the neighbors of hateful vs. neighbors of normal users and also suspended users vs. active users, increasing the robustness of our analysis. We observe that hateful users are densely connected, and thus formulate the hate speech detection problem as a task of semi-supervised learning over a graph, exploiting the network of connections on Twitter. We find that a node embedding algorithm, which exploits the graph structure, outperforms content-based approaches for the detection of both hateful ($95\\%$ AUC vs $88\\%$ AUC) and suspended users ($93\\%$ AUC vs $88\\%$ AUC). Altogether, we present a user-centric view of hate speech, paving the way for better detection and understanding of this relevant and challenging issue.",
                        "Citation Paper Authors": "Authors:Manoel Horta Ribeiro, Pedro H. Calais, Yuri A. Santos, Virg\u00edlio A. F. Almeida, Wagner Meira Jr"
                    }
                },
                {
                    "Sentence ID": 58,
                    "Sentence": "study the use of emojis in white nationalist conversation on Twitter and found\nstriking difference between the \u2018pro\u2019 and \u2018anti\u2019 stance.\nThe work done by Mathew et al . ",
                    "Citation Text": "Binny Mathew, Ritam Dutt, Pawan Goyal, and Animesh Mukherjee. 2018. Spread of hate speech in online social media.\narXiv preprint arXiv:1812.01693 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.01693",
                        "Citation Paper Title": "Title:Spread of hate speech in online social media",
                        "Citation Paper Abstract": "Abstract:The present online social media platform is afflicted with several issues, with hate speech being on the predominant forefront. The prevalence of online hate speech has fueled horrific real-world hate-crime such as the mass-genocide of Rohingya Muslims, communal violence in Colombo and the recent massacre in the Pittsburgh synagogue. Consequently, It is imperative to understand the diffusion of such hateful content in an online setting. We conduct the first study that analyses the flow and dynamics of posts generated by hateful and non-hateful users on Gab (this http URL) over a massive dataset of 341K users and 21M posts. Our observations confirms that hateful content diffuse farther, wider and faster and have a greater outreach than those of non-hateful users. A deeper inspection into the profiles and network of hateful and non-hateful users reveals that the former are more influential, popular and cohesive. Thus, our research explores the interesting facets of diffusion dynamics of hateful users and broadens our understanding of hate speech in the online world.",
                        "Citation Paper Authors": "Authors:Binny Mathew, Ritam Dutt, Pawan Goyal, Animesh Mukherjee"
                    }
                },
                {
                    "Sentence ID": 86,
                    "Sentence": ", each individual has a fixed set of neighbours, and\nthe local interaction is captured by taking the convex combination of his/her own opinion and the\nopinions of his/her neighbours at each time step ",
                    "Citation Text": "Zhi Xu, Ji Liu, and Tamer Ba\u015far. 2015. On a modified DeGroot-Friedkin model of opinion dynamics. In 2015 American\nControl Conference (ACC) . IEEE, 1047\u20131052.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1503.04179",
                        "Citation Paper Title": "Title:On a Modified DeGroot-Friedkin Model of Opinion Dynamics",
                        "Citation Paper Abstract": "Abstract:This paper studies the opinion dynamics that result when individuals consecutively discuss a sequence of issues. Specifically, we study how individuals' self-confidence levels evolve via a reflected appraisal mechanism. Motivated by the DeGroot-Friedkin model, we propose a Modified DeGroot-Friedkin model which allows individuals to update their self-confidence levels by only interacting with their neighbors and in particular, the modified model allows the update of self-confidence levels to take place in finite time without waiting for the opinion process to reach a consensus on any particular issue. We study properties of this Modified DeGroot-Friedkin model and compare the associated equilibria and stability with those of the original DeGroot-Friedkin model. Specifically, for the case when the interaction matrix is doubly stochastic, we show that for the modified model, the vector of individuals' self-confidence levels asymptotically converges to a unique nontrivial equilibrium which for each individual is equal to 1/n, where n is the number of individuals. This implies that eventually, individuals reach a democratic state.",
                        "Citation Paper Authors": "Authors:Zhi Xu, Ji Liu, Tamer Basar"
                    }
                },
                {
                    "Sentence ID": 63,
                    "Sentence": ". These high-precision\nkeywords were selected from Hatebase8and Urban dictionary9. To further enhance the quality of the\nlexicon, we adopt the word embedding method, skip-gram ",
                    "Citation Text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words\nand phrases and their compositionality. In Advances in neural information processing systems . 3111\u20133119.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1310.4546",
                        "Citation Paper Title": "Title:Distributed Representations of Words and Phrases and their Compositionality",
                        "Citation Paper Abstract": "Abstract:The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
                        "Citation Paper Authors": "Authors:Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "found that several of the existing state-of-the-art hate speech\ndetection models work well only when tested on the same type of data they were trained on. Aluru\net al. ",
                    "Citation Text": "Sai Saket Aluru, Binny Mathew, Punyajoy Saha, and Animesh Mukherjee. 2020. Deep Learning Models for Multilingual\nHate Speech Detection. arXiv preprint arXiv:2004.06465 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.06465",
                        "Citation Paper Title": "Title:Deep Learning Models for Multilingual Hate Speech Detection",
                        "Citation Paper Abstract": "Abstract:Hate speech detection is a challenging problem with most of the datasets available in only one language: English. In this paper, we conduct a large scale analysis of multilingual hate speech in 9 languages from 16 different sources. We observe that in low resource setting, simple models such as LASER embedding with logistic regression performs the best, while in high resource setting BERT based models perform better. In case of zero-shot classification, languages such as Italian and Portuguese achieve good results. Our proposed framework could be used as an efficient solution for low-resource languages. These models could also act as good baselines for future multilingual hate speech detection tasks. We have made our code and experimental settings public for other researchers at this https URL.",
                        "Citation Paper Authors": "Authors:Sai Saketh Aluru, Binny Mathew, Punyajoy Saha, Animesh Mukherjee"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "utilized the context information accompanied with the text to develop hate speech\ndetection models. Grondahl et al. ",
                    "Citation Text": "Tommi Gr\u00f6ndahl, Luca Pajola, Mika Juuti, Mauro Conti, and N Asokan. 2018. All You Need is\" Love\": Evading\nHate-speech Detection. arXiv preprint arXiv:1808.09115 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.09115",
                        "Citation Paper Title": "Title:All You Need is \"Love\": Evading Hate-speech Detection",
                        "Citation Paper Abstract": "Abstract:With the spread of social networks and their unfortunate use for hate speech, automatic detection of the latter has become a pressing problem. In this paper, we reproduce seven state-of-the-art hate speech detection models from prior work, and show that they perform well only when tested on the same type of data they were trained on. Based on these results, we argue that for successful hate speech detection, model architecture is less important than the type of data and labeling criteria. We further show that all proposed detection techniques are brittle against adversaries who can (automatically) insert typos, change word boundaries or add innocuous words to the original hate speech. A combination of these methods is also effective against Google Perspective -- a cutting-edge solution from industry. Our experiments demonstrate that adversarial training does not completely mitigate the attacks, and using character-level features makes the models systematically more attack-resistant than using word-level features.",
                        "Citation Paper Authors": "Authors:Tommi Gr\u00f6ndahl, Luca Pajola, Mika Juuti, Mauro Conti, N. Asokan"
                    }
                },
                {
                    "Sentence ID": 74,
                    "Sentence": "incorporated LDA topic modelling for improving the\nperformance of the hate speech detection task. Saleem et al. ",
                    "Citation Text": "Haji Mohammad Saleem, Kelly P Dillon, Susan Benesch, and Derek Ruths. 2017. A web of hate: Tackling hateful\nspeech in online social spaces. arXiv preprint arXiv:1709.10159 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.10159",
                        "Citation Paper Title": "Title:A Web of Hate: Tackling Hateful Speech in Online Social Spaces",
                        "Citation Paper Abstract": "Abstract:Online social platforms are beset with hateful speech - content that expresses hatred for a person or group of people. Such content can frighten, intimidate, or silence platform users, and some of it can inspire other users to commit violence. Despite widespread recognition of the problems posed by such content, reliable solutions even for detecting hateful speech are lacking. In the present work, we establish why keyword-based methods are insufficient for detection. We then propose an approach to detecting hateful speech that uses content produced by self-identifying hateful communities as training data. Our approach bypasses the expensive annotation process often required to train keyword systems and performs well across several established platforms, making substantial improvements over current state-of-the-art approaches.",
                        "Citation Paper Authors": "Authors:Haji Mohammad Saleem, Kelly P Dillon, Susan Benesch, Derek Ruths"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1904.11608v2": {
            "Paper Title": "Gradient Descent for Sparse Rank-One Matrix Completion for Crowd-Sourced\n  Aggregation of Sparsely Interacting Workers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.05971v2": {
            "Paper Title": "Toward Better Understanding of Saliency Prediction in Augmented 360\n  Degree Videos",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.05154v3": {
            "Paper Title": "Text-based depression detection on sparse data",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": ".\nfastText Another popular text-embedding is fastText, which\ndifferent from Word2Vec is sub-word based (e.g., consecutive\nsequences of characters). A fastText ",
                    "Citation Text": "A. Joulin, E. Grave, P. Bojanowski, and T. Mikolov, \u201cBag of tricks for\nef\ufb01cient text classi\ufb01cation,\u201d arXiv preprint arXiv:1607.01759 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.01759",
                        "Citation Paper Title": "Title:Bag of Tricks for Efficient Text Classification",
                        "Citation Paper Abstract": "Abstract:This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute.",
                        "Citation Paper Authors": "Authors:Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1910.03061v3": {
            "Paper Title": "Keeping Designers in the Loop: Communicating Inherent Algorithmic\n  Trade-offs Across Multiple Objectives",
            "Sentences": [
                {
                    "Sentence ID": 46,
                    "Sentence": ". Visualization techniques have been\ndeveloped to explain different types of machine learning mod-\nels. Examples include traditional machine learning models\nsuch as linear models ",
                    "Citation Text": "Marco Tulio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2016b. Why should i trust you?: Explaining\nthe predictions of any classi\ufb01er. In Proceedings of the\n22nd ACM SIGKDD international conference on\nknowledge discovery and data mining . ACM,\n1135\u20131144.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.04938",
                        "Citation Paper Title": "Title:\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier",
                        "Citation Paper Abstract": "Abstract:Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.",
                        "Citation Paper Authors": "Authors:Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.01753v4": {
            "Paper Title": "Cognitive and motor compliance in intentional human-robot interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.06764v2": {
            "Paper Title": "Learning to Engage with Interactive Systems: A Field Study on Deep\n  Reinforcement Learning in a Public Museum",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.07816v3": {
            "Paper Title": "A Multi-Turn Emotionally Engaging Dialog Model",
            "Sentences": [
                {
                    "Sentence ID": 22,
                    "Sentence": "with a beam\nwidth of 256.\nWe have made the source code publicly available.3\nEvaluation Metrics\nThe evaluation of chatbots remains an open problem in the\nfield. Recent work ",
                    "Citation Text": "Chia-Wei Liu, Ryan Lowe, Iulian Serban, Michael Noseworthy, Laurent\nCharlin, and Joelle Pineau. 2016. How NOT To Evaluate Your Dialogue\nSystem: An Empirical Study of Unsupervised Evaluation Metrics for\nDialogue Response Generation. In Proceedings of EMNLP 2016 . 2122\u2013\n2132. http://aclweb.org/anthology/D/D16/D16-1230.pdf",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.08023",
                        "Citation Paper Title": "Title:How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation",
                        "Citation Paper Abstract": "Abstract:We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in response generation have adopted metrics from machine translation to compare a model's generated response to a single target response. We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.",
                        "Citation Paper Authors": "Authors:Chia-Wei Liu, Ryan Lowe, Iulian V. Serban, Michael Noseworthy, Laurent Charlin, Joelle Pineau"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "where models are only trained and tested on\nsingle-turn dialogs. While using a hierarchical mechanism\nto track the conversation history in multi-turn dialogs is\nnot new (e.g., HRAN by Xing et al. ",
                    "Citation Text": "Chen Xing, Yu Wu, Wei Wu, Yalou Huang, and Ming Zhou. 2018.\nHierarchical Recurrent Attention Network for Response Generation.\nInProceedings of AAAI 2018 . 5610\u20135617. https://www.aaai.org/ocs/\nindex.php/AAAI/AAAI18/paper/view/16510",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1701.07149",
                        "Citation Paper Title": "Title:Hierarchical Recurrent Attention Network for Response Generation",
                        "Citation Paper Abstract": "Abstract:We study multi-turn response generation in chatbots where a response is generated according to a conversation context. Existing work has modeled the hierarchy of the context, but does not pay enough attention to the fact that words and utterances in the context are differentially important. As a result, they may lose important information in context and generate irrelevant responses. We propose a hierarchical recurrent attention network (HRAN) to model both aspects in a unified framework. In HRAN, a hierarchical attention mechanism attends to important parts within and among utterances with word level attention and utterance level attention respectively. With the word level attention, hidden vectors of a word level encoder are synthesized as utterance vectors and fed to an utterance level encoder to construct hidden representations of the context. The hidden vectors of the context are then processed by the utterance level attention and formed as context vectors for decoding the response. Empirical studies on both automatic evaluation and human judgment show that HRAN can significantly outperform state-of-the-art models for multi-turn response generation.",
                        "Citation Paper Authors": "Authors:Chen Xing, Wei Wu, Yu Wu, Ming Zhou, Yalou Huang, Wei-Ying Ma"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "extended\nthe standard seq2seq model to a conditional variational au-\ntoencoder combined with policy gradient techniques. The\nmodel takes a post and an emoji as input, and generates the\nresponse with target emotion specified by the emoji. Hu et\nal. ",
                    "Citation Text": "Tianran Hu, Anbang Xu, Zhe Liu, Quanzeng You, Yufan Guo, Vibha\nSinha, Jiebo Luo, and Rama Akkiraju. 2018. Touch Your Heart: A Tone-\naware Chatbot for Customer Care on Social Media. In Proceedings of\nCHI 2018 . 415. https://doi.org/10.1145/3173574.3173989",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.02952",
                        "Citation Paper Title": "Title:Touch Your Heart: A Tone-aware Chatbot for Customer Care on Social Media",
                        "Citation Paper Abstract": "Abstract:Chatbot has become an important solution to rapidly increasing customer care demands on social media in recent years. However, current work on chatbot for customer care ignores a key to impact user experience - tones. In this work, we create a novel tone-aware chatbot that generates toned responses to user requests on social media. We first conduct a formative research, in which the effects of tones are studied. Significant and various influences of different tones on user experience are uncovered in the study. With the knowledge of effects of tones, we design a deep learning based chatbot that takes tone information into account. We train our system on over 1.5 million real customer care conversations collected from Twitter. The evaluation reveals that our tone-aware chatbot generates as appropriate responses to user requests as human agents. More importantly, our chatbot is perceived to be even more empathetic than human agents.",
                        "Citation Paper Authors": "Authors:Tianran Hu, Anbang Xu, Zhe Liu, Quanzeng You, Yufan Guo, Vibha Sinha, Jiebo Luo, Rama Akkiraju"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.10637v3": {
            "Paper Title": "GrabAR: Occlusion-aware Grabbing Virtual Objects in AR",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": "formulate network learning as an ordinal regression problem\nand develop a spacing-increasing discretization strategy to\ndiscretize depth, whereas Nicodemou et al. ",
                    "Citation Text": "Vassilis C Nicodemou, Iason Oikonomidis, Georgios\nTzimiropoulos, and Antonis Argyros. 2018. Learning to\nInfer the Depth Map of a Hand from its Color Image.\narXiv preprint arXiv:1812.02486 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.02486",
                        "Citation Paper Title": "Title:Learning to Infer the Depth Map of a Hand from its Color Image",
                        "Citation Paper Abstract": "Abstract:We propose the first approach to the problem of inferring the depth map of a human hand based on a single RGB image. We achieve this with a Convolutional Neural Network (CNN) that employs a stacked hourglass model as its main building block. Intermediate supervision is used in several outputs of the proposed architecture in a staged approach. To aid the process of training and inference, hand segmentation masks are also estimated in such an intermediate supervision step, and used to guide the subsequent depth estimation process. In order to train and evaluate the proposed method we compile and make publicly available HandRGBD, a new dataset of 20,601 views of hands, each consisting of an RGB image and an aligned depth map. Based on HandRGBD, we explore variants of the proposed approach in an ablative study and determine the best performing one. The results of an extensive experimental evaluation demonstrate that hand depth estimation from a single RGB frame can be achieved with an accuracy of 22mm, which is comparable to the accuracy achieved by contemporary low-cost depth cameras. Such a 3D reconstruction of hands based on RGB information is valuable as a final result on its own right, but also as an input to several other hand analysis and perception algorithms that require depth input. Essentially, in such a context, the proposed approach bridges the gap between RGB and RGBD, by making all existing RGBD-based methods applicable to RGB input.",
                        "Citation Paper Authors": "Authors:Vassilis C. Nicodemou, Iason Oikonomidis, Georgios Tzimiropoulos, Antonis Argyros"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "develop a deep residual network and leverage\nthe reverse Huber loss to predict depth. Recently, Fu et al. ",
                    "Citation Text": "Huan Fu, Mingming Gong, Chaohui Wang, Kayhan\nBatmanghelich, and Dacheng Tao. 2018. Deep ordinal\nregression network for monocular depth estimation. In\nCVPR . 2002\u20132011.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.02446",
                        "Citation Paper Title": "Title:Deep Ordinal Regression Network for Monocular Depth Estimation",
                        "Citation Paper Abstract": "Abstract:Monocular depth estimation, which plays a crucial role in understanding 3D scene geometry, is an ill-posed problem. Recent methods have gained significant improvement by exploring image-level information and hierarchical features from deep convolutional neural networks (DCNNs). These methods model depth estimation as a regression problem and train the regression networks by minimizing mean squared error, which suffers from slow convergence and unsatisfactory local solutions. Besides, existing depth estimation networks employ repeated spatial pooling operations, resulting in undesirable low-resolution feature maps. To obtain high-resolution depth maps, skip-connections or multi-layer deconvolution networks are required, which complicates network training and consumes much more computations. To eliminate or at least largely reduce these problems, we introduce a spacing-increasing discretization (SID) strategy to discretize depth and recast depth network learning as an ordinal regression problem. By training the network using an ordinary regression loss, our method achieves much higher accuracy and \\dd{faster convergence in synch}. Furthermore, we adopt a multi-scale network structure which avoids unnecessary spatial pooling and captures multi-scale information in parallel.\nThe method described in this paper achieves state-of-the-art results on four challenging benchmarks, i.e., KITTI [17], ScanNet [9], Make3D [50], and NYU Depth v2 [42], and win the 1st prize in Robust Vision Challenge 2018. Code has been made available at: this https URL.",
                        "Citation Paper Authors": "Authors:Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, Dacheng Tao"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "adopt a CNN to predict a\ncoarse depth map then apply another CNN to re\ufb01ne the result.\nLaina et al. ",
                    "Citation Text": "Iro Laina, Christian Rupprecht, Vasileios Belagiannis,\nFederico Tombari, and Nassir Navab. 2016. Deeper\ndepth prediction with fully convolutional residual\nnetworks. In 3DV. 239\u2013248.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.00373",
                        "Citation Paper Title": "Title:Deeper Depth Prediction with Fully Convolutional Residual Networks",
                        "Citation Paper Abstract": "Abstract:This paper addresses the problem of estimating the depth map of a scene given a single RGB image. We propose a fully convolutional architecture, encompassing residual learning, to model the ambiguous mapping between monocular images and depth maps. In order to improve the output resolution, we present a novel way to efficiently learn feature map up-sampling within the network. For optimization, we introduce the reverse Huber loss that is particularly suited for the task at hand and driven by the value distributions commonly present in depth maps. Our model is composed of a single architecture that is trained end-to-end and does not rely on post-processing techniques, such as CRFs or other additional refinement steps. As a result, it runs in real-time on images or videos. In the evaluation, we show that the proposed model contains fewer parameters and requires fewer training data than the current state of the art, while outperforming all approaches on depth estimation. Code and models are publicly available.",
                        "Citation Paper Authors": "Authors:Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Federico Tombari, Nassir Navab"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.00581v3": {
            "Paper Title": "Optimality and limitations of audio-visual integration for cognitive\n  systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.01517v2": {
            "Paper Title": "Real Differences between OT and CRDT in Building Co-Editing Systems and\n  Real World Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.01518v2": {
            "Paper Title": "Real Differences between OT and CRDT under a General Transformation\n  Framework for Consistency Maintenance in Co-Editors",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.11111v3": {
            "Paper Title": "Face Behavior a la carte: Expressions, Affect and Action Units in a\n  Single Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.02982v2": {
            "Paper Title": "DRLViz: Understanding Decisions and Memory in Deep Reinforcement\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.03107v3": {
            "Paper Title": "CoAug-MR: An MR-based Interactive Office Workstation Design System via\n  Augmented Multi-Person Collaboration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.07835v4": {
            "Paper Title": "EEG-Based Emotion Recognition Using Regularized Graph Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.03598v2": {
            "Paper Title": "Interactive Classification by Asking Informative Questions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.09457v2": {
            "Paper Title": "TopoLines: Topological Smoothing for Line Charts",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.07766v3": {
            "Paper Title": "Lemotif: An Affective Visual Journal Using Deep Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.08562v3": {
            "Paper Title": "Privacy, Altruism, and Experience: Estimating the Perceived Value of\n  Internet Data for Medical Uses",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.07123v3": {
            "Paper Title": "Estimating Attention Flow in Online Video Networks",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": "revealed that the\nlifecycles of online videos exhibit a multi-phase pattern. Figueiredo et al . ",
                    "Citation Text": "Flavio Figueiredo, Jussara M Almeida, Marcos A Gon\u00e7alves, and Fabricio Benevenuto. 2016. Trendlearner: Early\nprediction of popularity trends of user generated content. Information Sciences (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1402.2351",
                        "Citation Paper Title": "Title:TrendLearner: Early Prediction of Popularity Trends of User Generated Content",
                        "Citation Paper Abstract": "Abstract:We here focus on the problem of predicting the popularity trend of user generated content (UGC) as early as possible. Taking YouTube videos as case study, we propose a novel two-step learning approach that: (1) extracts popularity trends from previously uploaded objects, and (2) predicts trends for new content. Unlike previous work, our solution explicitly addresses the inherent tradeoff between prediction accuracy and remaining interest in the content after prediction, solving it on a per-object basis. Our experimental results show great improvements of our solution over alternatives, and its applicability to improve the accuracy of state-of-the-art popularity prediction methods.",
                        "Citation Paper Authors": "Authors:Flavio Figueiredo, Jussara M. Almeida, Marcos Andr\u00e9 Gon\u00e7alves, Fabr\u00edcio Benevenuto"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "stressed the necessity\nof predicting content popularity before the user interests exhaust.\nFor engagement studies on YouTube, we refer to our previous paper ",
                    "Citation Text": "Siqi Wu, Marian-Andrei Rizoiu, and Lexing Xie. 2018. Beyond views: Measuring and predicting engagement in online\nvideos. In Proceedings of ICWSM .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.02541",
                        "Citation Paper Title": "Title:Beyond Views: Measuring and Predicting Engagement in Online Videos",
                        "Citation Paper Abstract": "Abstract:The share of videos in the internet traffic has been growing, therefore understanding how videos capture attention on a global scale is also of growing importance. Most current research focus on modeling the number of views, but we argue that video engagement, or time spent watching is a more appropriate measure for resource allocation problems in attention, networking, and promotion activities. In this paper, we present a first large-scale measurement of video-level aggregate engagement from publicly available data streams, on a collection of 5.3 million YouTube videos published over two months in 2016. We study a set of metrics including time and the average percentage of a video watched. We define a new metric, relative engagement, that is calibrated against video properties and strongly correlate with recognized notions of quality. Moreover, we find that engagement measures of a video are stable over time, thus separating the concerns for modeling engagement and those for popularity -- the latter is known to be unstable over time and driven by external promotions. We also find engagement metrics predictable from a cold-start setup, having most of its variance explained by video context, topics and channel information -- R2=0.77. Our observations imply several prospective uses of engagement metrics -- choosing engaging topics for video production, or promoting engaging videos in recommender systems.",
                        "Citation Paper Authors": "Authors:Siqi Wu, Marian-Andrei Rizoiu, Lexing Xie"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "stressed the difficulty of inferring causal\nrelations based on observational data in recommender systems.\nCheng et al . ",
                    "Citation Text": "Xu Cheng, Cameron Dale, and Jiangchuan Liu. 2008. Statistics and social network of YouTube videos. In Proceedings of\nInternational Workshop on Quality of Service .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:0707.3670",
                        "Citation Paper Title": "Title:Understanding the Characteristics of Internet Short Video Sharing: YouTube as a Case Study",
                        "Citation Paper Abstract": "Abstract:  Established in 2005, YouTube has become the most successful Internet site providing a new generation of short video sharing service. Today, YouTube alone comprises approximately 20% of all HTTP traffic, or nearly 10% of all traffic on the Internet. Understanding the features of YouTube and similar video sharing sites is thus crucial to their sustainable development and to network traffic engineering. In this paper, using traces crawled in a 3-month period, we present an in-depth and systematic measurement study on the characteristics of YouTube videos. We find that YouTube videos have noticeably different statistics compared to traditional streaming videos, ranging from length and access pattern, to their active life span, ratings, and comments. The series of datasets also allows us to identify the growth trend of this fast evolving Internet site in various aspects, which has seldom been explored before. We also look closely at the social networking aspect of YouTube, as this is a key driving force toward its success. In particular, we find that the links to related videos generated by uploaders' choices form a small-world network. This suggests that the videos have strong correlations with each other, and creates opportunities for developing novel caching or peer-to-peer distribution schemes to efficiently deliver videos to end users.",
                        "Citation Paper Authors": "Authors:Xu Cheng, Cameron Dale, Jiangchuan Liu"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "linked the aggregate\neffects of recommendations and network structure, and found that popular items profit substantially\nmore than the average ones. However, Sharma et al . ",
                    "Citation Text": "Amit Sharma, Jake M Hofman, and Duncan J Watts. 2015. Estimating the causal impact of recommendation systems\nfrom observational data. In Proceedings of EC .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1510.05569",
                        "Citation Paper Title": "Title:Estimating the Causal Impact of Recommendation Systems from Observational Data",
                        "Citation Paper Abstract": "Abstract:Recommendation systems are an increasingly prominent part of the web, accounting for up to a third of all traffic on several of the world's most popular sites. Nevertheless, little is known about how much activity such systems actually cause over and above activity that would have occurred via other means (e.g., search) if recommendations were absent. Although the ideal way to estimate the causal impact of recommendations is via randomized experiments, such experiments are costly and may inconvenience users. In this paper, therefore, we present a method for estimating causal effects from purely observational data. Specifically, we show that causal identification through an instrumental variable is possible when a product experiences an instantaneous shock in direct traffic and the products recommended next to it do not. We then apply our method to browsing logs containing anonymized activity for 2.1 million users on this http URL over a 9 month period and analyze over 4,000 unique products that experience such shocks. We find that although recommendation click-throughs do account for a large fraction of traffic among these products, at least 75% of this activity would likely occur in the absence of recommendations. We conclude with a discussion about the assumptions under which the method is appropriate and caveats around extrapolating results to other products, sites, or settings.",
                        "Citation Paper Authors": "Authors:Amit Sharma, Jake M. Hofman, Duncan J. Watts"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1905.01950v2": {
            "Paper Title": "Digitally Capturing Physical Prototypes During Early-Stage Engineering\n  Design Projects for Initial Analysis of Project Output and Progression",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.01769v2": {
            "Paper Title": "Tendrils of Crime: Visualizing the Diffusion of Stolen Bitcoins",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.07208v2": {
            "Paper Title": "MFCC-based Recurrent Neural Network for Automatic Clinical Depression\n  Recognition and Assessment from Speech",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": ".\n\u000fTheProsodic features: describe the speech into-\nnation, rate, and rhythm, like the Fundamental Fre-\nquency F0 (the \ufb01rst signal harmonic) and the Loud-\nness ",
                    "Citation Text": "M. Valstar, J. Gratch, B. Schuller, F. Ringeval, D. Lalanne,\nM. Torres Torres, S. Scherer, G. Stratou, R. Cowie, M. Pan-\ntic, Avec 2016: Depression, mood, and emotion recognition\nworkshop and challenge, in: Proceedings of the 6th international\nworkshop on audio /visual emotion challenge, ACM, 2016, pp.\n3\u201310.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.01600",
                        "Citation Paper Title": "Title:AVEC 2016 - Depression, Mood, and Emotion Recognition Workshop and Challenge",
                        "Citation Paper Abstract": "Abstract:The Audio/Visual Emotion Challenge and Workshop (AVEC 2016) \"Depression, Mood and Emotion\" will be the sixth competition event aimed at comparison of multimedia processing and machine learning methods for automatic audio, visual and physiological depression and emotion analysis, with all participants competing under strictly the same conditions. The goal of the Challenge is to provide a common benchmark test set for multi-modal information processing and to bring together the depression and emotion recognition communities, as well as the audio, video and physiological processing communities, to compare the relative merits of the various approaches to depression and emotion recognition under well-defined and strictly comparable conditions and establish to what extent fusion of the approaches is possible and beneficial. This paper presents the challenge guidelines, the common data used, and the performance of the baseline system on the two tasks.",
                        "Citation Paper Authors": "Authors:Michel Valstar, Jonathan Gratch, Bjorn Schuller, Fabien Ringeval, Denis Lalanne, Mercedes Torres Torres, Stefan Scherer, Guiota Stratou, Roddy Cowie, Maja Pantic"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.01166v4": {
            "Paper Title": "Different Set Domain Adaptation for Brain-Computer Interfaces: A Label\n  Alignment Approach",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": "is a state-of-the-art DA approach for BCIs,\nwhich reduces the individual differences by aligning the EE G\ncovariance matrices.\nSome DA approaches ",
                    "Citation Text": "D. Wu, V . J. Lawhern, W. D. Hairston, and B. J. Lance, \u201cSwi tching\nEEG headsets made easy: Reducing of\ufb02ine calibration effort using active\nweighted adaptation regularization,\u201d IEEE Trans. on Neural Systems and\nRehabilitation Engineering , vol. 24, no. 11, pp. 1125\u20131137, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.02906",
                        "Citation Paper Title": "Title:Switching EEG Headsets Made Easy: Reducing Offline Calibration Effort Using Active Weighted Adaptation Regularization",
                        "Citation Paper Abstract": "Abstract:Electroencephalography (EEG) headsets are the most commonly used sensing devices for Brain-Computer Interface. In real-world applications, there are advantages to extrapolating data from one user session to another. However, these advantages are limited if the data arise from different hardware systems, which often vary between application spaces. Currently, this creates a need to recalibrate classifiers, which negatively affects people's interest in using such systems. In this paper, we employ active weighted adaptation regularization (AwAR), which integrates weighted adaptation regularization (wAR) and active learning, to expedite the calibration process. wAR makes use of labeled data from the previous headset and handles class-imbalance, and active learning selects the most informative samples from the new headset to label. Experiments on single-trial event-related potential classification show that AwAR can significantly increase the classification accuracy, given the same number of labeled samples from the new headset. In other words, AwAR can effectively reduce the number of labeled samples required from the new headset, given a desired classification accuracy, suggesting value in collating data for use in wide scale transfer-learning applications.",
                        "Citation Paper Authors": "Authors:Dongrui Wu, Vernon J. Lawhern, W. David Hairston, Brent J. Lance"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1909.00045v2": {
            "Paper Title": "An active smartphone authentication method based on daily cyclical\n  activity",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.00903v3": {
            "Paper Title": "ZuCo 2.0: A Dataset of Physiological Recordings During Natural Reading\n  and Annotation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.05878v2": {
            "Paper Title": "Manifold Embedded Knowledge Transfer for Brain-Computer Interfaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.07076v2": {
            "Paper Title": "TGView3D System Description: 3-Dimensional Visualization of Theory\n  Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.07720v2": {
            "Paper Title": "On-Body Visualization of Patient Data for Cooperative Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.06327v3": {
            "Paper Title": "FastV2C-HandNet: Fast Voxel to Coordinate Hand Pose Estimation with 3D\n  Convolutional Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.01947v2": {
            "Paper Title": "The Plausibility Paradox for Scaled-Down Users in Virtual Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06675v3": {
            "Paper Title": "MAD-TN: A Tool for Measuring Fluency in Human-Robot Collaboration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.01281v2": {
            "Paper Title": "rIoT: Enabling Seamless Context-Aware Automation in the Internet of\n  Things",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.09919v2": {
            "Paper Title": "Speech, Head, and Eye-based Cues for Continuous Affect Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.11006v2": {
            "Paper Title": "Word-level Deep Sign Language Recognition from Video: A New Large-scale\n  Dataset and Methods Comparison",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.04711v2": {
            "Paper Title": "End-to-end facial and physiological model for Affective Computing and\n  applications",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": "and their availability on all\ncompared studies. In addition, to investigate the importance\nof physiological signal for affect estimations, we also add\nstate of the art, deep learning-based affect networks from\nKollias et al. ",
                    "Citation Text": "D. Kollias, P. Tzirakis, M. A. Nicolaou, A. Papaioannou, G. Zhao,\nB. Schuller, I. Kotsia, and S. Zafeiriou. Deep affect prediction in-the-\nwild: Aff-wild database and challenge, deep architectures, and beyond.\nIJCV , pages 1\u201323, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.10938",
                        "Citation Paper Title": "Title:Deep Affect Prediction in-the-wild: Aff-Wild Database and Challenge, Deep Architectures, and Beyond",
                        "Citation Paper Abstract": "Abstract:Automatic understanding of human affect using visual signals is of great importance in everyday human-machine interactions. Appraising human emotional states, behaviors and reactions displayed in real-world settings, can be accomplished using latent continuous dimensions (e.g., the circumplex model of affect). Valence (i.e., how positive or negative is an emotion) & arousal (i.e., power of the activation of the emotion) constitute popular and effective affect representations. Nevertheless, the majority of collected datasets this far, although containing naturalistic emotional states, have been captured in highly controlled recording conditions. In this paper, we introduce the Aff-Wild benchmark for training and evaluating affect recognition algorithms. We also report on the results of the First Affect-in-the-wild Challenge that was organized in conjunction with CVPR 2017 on the Aff-Wild database and was the first ever challenge on the estimation of valence and arousal in-the-wild. Furthermore, we design and extensively train an end-to-end deep neural architecture which performs prediction of continuous emotion dimensions based on visual cues. The proposed deep learning architecture, AffWildNet, includes convolutional & recurrent neural network layers, exploiting the invariant properties of convolutional features, while also modeling temporal dynamics that arise in human behavior via the recurrent layers. The AffWildNet produced state-of-the-art results on the Aff-Wild Challenge. We then exploit the AffWild database for learning features, which can be used as priors for achieving best performances both for dimensional, as well as categorical emotion recognition, using the RECOLA, AFEW-VA and EmotiW datasets, compared to all other methods designed for the same goal. The database and emotion recognition models are available at this http URL.",
                        "Citation Paper Authors": "Authors:Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A. Nicolaou, Athanasios Papaioannou, Guoying Zhao, Bj\u00f6rn Schuller, Irene Kotsia, Stefanos Zafeiriou"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1906.12175v2": {
            "Paper Title": "Are you really looking at me? A Feature-Extraction Framework for\n  Estimating Interpersonal Eye Gaze from Conventional Video",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.10528v2": {
            "Paper Title": "Emotion Recognition Using Wearables: A Systematic Literature Review Work\n  in progress",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.05789v2": {
            "Paper Title": "On the Utility of Learning about Humans for Human-AI Coordination",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.00031v2": {
            "Paper Title": "Interactive Task and Concept Learning from Natural Language Instructions\n  and GUI Demonstrations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.10410v3": {
            "Paper Title": "Visualization of Very Large High-Dimensional Data Sets as Minimum\n  Spanning Trees",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.11729v2": {
            "Paper Title": "Discussion of Intelligent Electric Wheelchairs for Caregivers and Care\n  Recipients",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.08097v2": {
            "Paper Title": "Conflict Detection and Resolution in Table Top Scenarios for Human-Robot\n  Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.00453v1": {
            "Paper Title": "An Approach Towards Intelligent Accident Detection, Location Tracking\n  and Notification System",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.12526v1": {
            "Paper Title": "Real World Longitudinal iOS App Usage Study at Scale",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.06972v2": {
            "Paper Title": "Utilizing Players' Playtime Records for Churn Prediction: Mining\n  Playtime Regularity",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.11371v1": {
            "Paper Title": "Comparison of the P300 detection accuracy related to the BCI speller and\n  image recognition scenarios",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.07938v2": {
            "Paper Title": "How Personal is Machine Learning Personalization?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.00062v1": {
            "Paper Title": "Visual Evaluation of Generative Adversarial Networks for Time Series\n  Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.10311v1": {
            "Paper Title": "Do Facial Expressions Predict Ad Sharing? A Large-Scale Observational\n  Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.00470v1": {
            "Paper Title": "The Mobile AR Sensor Logger for Android and iOS Devices",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.00471v1": {
            "Paper Title": "A Voice Interactive Multilingual Student Support System using IBM Watson",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.09589v1": {
            "Paper Title": "Smart Home Appliances: Chat with Your Fridge",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.02736v2": {
            "Paper Title": "Insights from BB-MAS -- A Large Dataset for Typing, Gait and Swipes of\n  the Same Person on Desktop, Tablet and Phone",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.10576v4": {
            "Paper Title": "Efficient Object Annotation via Speaking and Pointing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.08609v1": {
            "Paper Title": "Psychoacoustic Sonification as User Interface for Human-Machine\n  Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.08101v2": {
            "Paper Title": "Visualizing and Analyzing Entity Activity on the Bitcoin Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.08809v1": {
            "Paper Title": "Field Label Prediction for Autofill in Web Browsers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.06979v1": {
            "Paper Title": "Breaking Speech Recognizers to Imagine Lyrics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.05376v2": {
            "Paper Title": "AffWild Net and Aff-Wild Database",
            "Sentences": [
                {
                    "Sentence ID": 44,
                    "Sentence": "2.4.2 ResNet\nDeep convolutional neural networks have shown great results for image clas-\nsi\ufb01cation in the past years. They have the ability to recognize low/mid/high level\nfeatures, according to the number of stacked layers existing in the network. As ",
                    "Citation Text": "K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recog-\nnition,\u201d CoRR , vol. abs/1512.03385, 2015. pages 18",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.03385",
                        "Citation Paper Title": "Title:Deep Residual Learning for Image Recognition",
                        "Citation Paper Abstract": "Abstract:Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
                        "Citation Paper Authors": "Authors:Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.06602v1": {
            "Paper Title": "That and There: Judging the Intent of Pointing Actions with Robotic Arms",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.07456v1": {
            "Paper Title": "Organizing Family Support Services at ACM Conferences",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.13607v2": {
            "Paper Title": "Mathematical decisions and non-causal elements of explainable AI",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.01987v2": {
            "Paper Title": "Scalable Bayesian Preference Learning for Crowds",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.05047v1": {
            "Paper Title": "Form + Function: Optimizing Aesthetic Product Design via Adaptive,\n  Geometrized Preference Elicitation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.04786v1": {
            "Paper Title": "edBB: Biometrics and Behavior for Assessing Remote Education",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.04444v1": {
            "Paper Title": "Practice of Efficient Data Collection via Crowdsourcing at Large-Scale",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.04235v1": {
            "Paper Title": "An Interactive Indoor Drone Assistant",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": "and is trained end-to-end. The\ninput is the dialogue history and the current utterance of the\nuser. We use byte pair encoding ",
                    "Citation Text": "R. Sennrich, B. Haddow, and A. Birch, \u201cNeural machine translation\nof rare words with subword units,\u201d in Proceedings of the 54th Annual\nMeeting of the Association for Computational Linguistics (ACL) , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1508.07909",
                        "Citation Paper Title": "Title:Neural Machine Translation of Rare Words with Subword Units",
                        "Citation Paper Abstract": "Abstract:Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.",
                        "Citation Paper Authors": "Authors:Rico Sennrich, Barry Haddow, Alexandra Birch"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1906.05996v2": {
            "Paper Title": "Multi-level tree based approach for interactive graph visualization with\n  semantic zoom",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.02985v2": {
            "Paper Title": "Sonovortex: Aerial Haptic Layer Rendering by Aerodynamic Vortex and\n  Focused Ultrasound",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": ". Nanosecond lasers applied to the skin\ninduce a tactile sensation ",
                    "Citation Text": "Yoichi Ochiai, Kota Kumagai, Takayuki Hoshi,\nJun Rekimoto, Satoshi Hasegawa, and Yoshio\nHayasaki. Fairy lights in femtoseconds: Aerial\nand volumetric graphics rendered by focused\nfemtosecond laser combined with computa-\ntional holographic \felds. ACM Trans. Graph. ,\n35(2):17:1{17:14, February 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.06668",
                        "Citation Paper Title": "Title:Fairy Lights in Femtoseconds: Aerial and Volumetric Graphics Rendered by Focused Femtosecond Laser Combined with Computational Holographic Fields",
                        "Citation Paper Abstract": "Abstract:We present a method of rendering aerial and volumetric graphics using femtosecond lasers. A high-intensity laser excites a physical matter to emit light at an arbitrary 3D position. Popular applications can then be explored especially since plasma induced by a femtosecond laser is safer than that generated by a nanosecond laser. There are two methods of rendering graphics with a femtosecond laser in air: Producing holograms using spatial light modulation technology, and scanning of a laser beam by a galvano mirror. The holograms and workspace of the system proposed here occupy a volume of up to 1 cm^3; however, this size is scalable depending on the optical devices and their setup. This paper provides details of the principles, system setup, and experimental evaluation, and discussions on scalability, design space, and applications of this system. We tested two laser sources: an adjustable (30-100 fs) laser which projects up to 1,000 pulses per second at energy up to 7 mJ per pulse, and a 269-fs laser which projects up to 200,000 pulses per second at an energy up to 50 uJ per pulse. We confirmed that the spatiotemporal resolution of volumetric displays, implemented with these laser sources, is 4,000 and 200,000 dots per second. Although we focus on laser-induced plasma in air, the discussion presented here is also applicable to other rendering principles such as fluorescence and microbubble in solid/liquid materials.",
                        "Citation Paper Authors": "Authors:Yoichi Ochiai, Kota Kumagai, Takayuki Hoshi, Jun Rekimoto, Satoshi Hasegawa, Yoshio Hayasaki"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1909.08758v3": {
            "Paper Title": "Extracting Super-resolution Structures inside a Single Molecule or\n  Overlapped Molecules from One Blurred Image",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.08089v2": {
            "Paper Title": "\"The Human Body is a Black Box\": Supporting Clinical Decision-Making\n  with Deep Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.02943v1": {
            "Paper Title": "An Algorithmic Equity Toolkit for Technology Audits by Community\n  Advocates and Activists",
            "Sentences": [
                {
                    "Sentence ID": 48,
                    "Sentence": "; counterfactual explanations of what set of cir-\ncumstances would result in a different algorithmic decision ",
                    "Citation Text": "Sandra Wachter, Brent Mittelstadt, and Chris Russell. 2017. Coun-\nterfactual Explanations without Opening the Black Box: Automated\nDecisions and the GPDR. Harv. JL & Tech. 31 (2017), 841.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.00399",
                        "Citation Paper Title": "Title:Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR",
                        "Citation Paper Abstract": "Abstract:There has been much discussion of the right to explanation in the EU General Data Protection Regulation, and its existence, merits, and disadvantages. Implementing a right to explanation that opens the black box of algorithmic decision-making faces major legal and technical barriers. Explaining the functionality of complex algorithmic decision-making systems and their rationale in specific cases is a technically challenging problem. Some explanations may offer little meaningful information to data subjects, raising questions around their value. Explanations of automated decisions need not hinge on the general public understanding how algorithmic systems function. Even though such interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the black box. Looking at explanations as a means to help a data subject act rather than merely understand, one could gauge the scope and content of explanations according to the specific goal or action they are intended to support. From the perspective of individuals affected by automated decision-making, we propose three aims for explanations: (1) to inform and help the individual understand why a particular decision was reached, (2) to provide grounds to contest the decision if the outcome is undesired, and (3) to understand what would need to change in order to receive a desired result in the future, based on the current decision-making model. We assess how each of these goals finds support in the GDPR. We suggest data controllers should offer a particular type of explanation, unconditional counterfactual explanations, to support these three aims. These counterfactual explanations describe the smallest change to the world that can be made to obtain a desirable outcome, or to arrive at the closest possible world, without needing to explain the internal logic of the system.",
                        "Citation Paper Authors": "Authors:Sandra Wachter, Brent Mittelstadt, Chris Russell"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.02913v1": {
            "Paper Title": "A Comparative Analysis of Virtual Reality Head-Mounted Display Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.02241v1": {
            "Paper Title": "Learning from Interventions using Hierarchical Policies for Safe\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.02083v1": {
            "Paper Title": "Evaluating the Data Quality of Eye Tracking Signals from a Virtual\n  Reality System: Case Study using SMI's Eye-Tracking HTC Vive",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.09386v2": {
            "Paper Title": "The Power and Pitfalls of Transparent Privacy Policies in Social\n  Networking Service Platforms",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.01218v1": {
            "Paper Title": "Writing Across the World's Languages: Deep Internationalization for\n  Gboard, the Google Keyboard",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.01177v1": {
            "Paper Title": "A New Terrain in HCI: Emotion Recognition Interface using Biometric Data\n  for an Immersive VR Experience",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.01130v1": {
            "Paper Title": "Addict Free -- A Smart and Connected Relapse Intervention Mobile App",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.05662v1": {
            "Paper Title": "Computa\u00e7\u00e3o Urbana da Teoria \u00e0 Pr\u00e1tica: Fundamentos,\n  Aplica\u00e7\u00f5es e Desafios",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.05284v1": {
            "Paper Title": "Interactive AI with a Theory of Mind",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.00317v1": {
            "Paper Title": "An Observational Investigation of Reverse Engineers' Processes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.09713v2": {
            "Paper Title": "Using Socially Expressive Mixed Reality Arms for Enhancing\n  Low-Expressivity Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.00019v1": {
            "Paper Title": "Long Short-Term Network Based Unobtrusive Perceived Workload Monitoring\n  with Consumer Grade Smartwatches in the Wild",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.00045v1": {
            "Paper Title": "Mechanism for Embossing Braille Characters on Paper: Conceptual Design",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.13032v1": {
            "Paper Title": "Safe Walking In VR using Augmented Virtuality",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.12119v2": {
            "Paper Title": "Interactivity and Transparency in Medical Risk Assessment with\n  Supersparse Linear Integer Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.12482v1": {
            "Paper Title": "Designing the Next Generation of Intelligent Personal Robotic Assistants\n  for the Physically Impaired",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.13248v1": {
            "Paper Title": "To Trust, or Not to Trust? A Study of Human Bias in Automated Video\n  Interview Assessments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.11920v1": {
            "Paper Title": "Warning Signs in Communicating the Machine Learning Detection Results of\n  Misinformation with Individuals",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.11751v1": {
            "Paper Title": "Multi-person Spatial Interaction in a Large Immersive Display Using\n  Smartphones as Touchpads",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.11743v1": {
            "Paper Title": "Device-Free User Authentication, Activity Classification and Tracking\n  using Passive Wi-Fi Sensing: A Deep Learning Based Approach",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": "1)User Authentication :Majority of the wireless aided user\nauthentication systems in the literature require the user t o carry\nor wear a device to facilitate the authentication process ",
                    "Citation Text": "E. R. Isaac, S. Elias, S. Rajagopalan, and K. Easwarakuma r, \u201cTemplate-\nbased gait authentication through bayesian thresholding, \u201dIEEE/CAA\nJournal of Automatica Sinica , vol. 6, no. 1, pp. 209\u2013219, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.05273",
                        "Citation Paper Title": "Title:View-invariant Gait Recognition through Genetic Template Segmentation",
                        "Citation Paper Abstract": "Abstract:Template-based model-free approach provides by far the most successful solution to the gait recognition problem in literature. Recent work discusses how isolating the head and leg portion of the template increase the performance of a gait recognition system making it robust against covariates like clothing and carrying conditions. However, most involve a manual definition of the boundaries. The method we propose, the genetic template segmentation (GTS), employs the genetic algorithm to automate the boundary selection process. This method was tested on the GEI, GEnI and AEI templates. GEI seems to exhibit the best result when segmented with our approach. Experimental results depict that our approach significantly outperforms the existing implementations of view-invariant gait recognition.",
                        "Citation Paper Authors": "Authors:Ebenezer Isaac, Susan Elias, Srinivasan Rajagopalan, K.S. Easwarakumar"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.11356v1": {
            "Paper Title": "Semantic Interior Mapology: A Toolbox For Indoor Scene Description From\n  Architectural Floor Plans",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": ". Bottom row: segmentation of individual objects using the web toolkit\ndescribed in ",
                    "Citation Text": "Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber,\nThomas Funkhouser, and Matthias Nie\u00dfner. 2017. Scannet:\nRichly-annotated 3D reconstructions of indoor scenes. In\nProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition , 5828\u20135839.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.04405",
                        "Citation Paper Title": "Title:ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes",
                        "Citation Paper Abstract": "Abstract:A key requirement for leveraging supervised deep learning methods is the availability of large, labeled datasets. Unfortunately, in the context of RGB-D scene understanding, very little data is available -- current datasets cover a small range of scene views and have limited semantic annotations. To address this issue, we introduce ScanNet, an RGB-D video dataset containing 2.5M views in 1513 scenes annotated with 3D camera poses, surface reconstructions, and semantic segmentations. To collect this data, we designed an easy-to-use and scalable RGB-D capture system that includes automated surface reconstruction and crowdsourced semantic annotation. We show that using this data helps achieve state-of-the-art performance on several 3D scene understanding tasks, including 3D object classification, semantic voxel labeling, and CAD model retrieval. The dataset is freely available at this http URL.",
                        "Citation Paper Authors": "Authors:Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, Matthias Nie\u00dfner"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "leveraged crowd-sensed\ndata from mobile users to obtain the spatial relationship between\nadjacent objects to complete a \uffffoor plan reconstruction. Liu et al. ",
                    "Citation Text": "Chen Liu, Jiaye Wu, and Yasutaka Furukawa. 2018. Floornet:\nA uni\uffffed framework for \uffffoorplan reconstruction from 3D\nscans. In Proceedings of the European Conference on Computer\nVision (ECCV) , 201\u2013217.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.00090",
                        "Citation Paper Title": "Title:FloorNet: A Unified Framework for Floorplan Reconstruction from 3D Scans",
                        "Citation Paper Abstract": "Abstract:The ultimate goal of this indoor mapping research is to automatically reconstruct a floorplan simply by walking through a house with a smartphone in a pocket. This paper tackles this problem by proposing FloorNet, a novel deep neural architecture. The challenge lies in the processing of RGBD streams spanning a large 3D space. FloorNet effectively processes the data through three neural network branches: 1) PointNet with 3D points, exploiting the 3D information; 2) CNN with a 2D point density image in a top-down view, enhancing the local spatial reasoning; and 3) CNN with RGB images, utilizing the full image information. FloorNet exchanges intermediate features across the branches to exploit the best of all the architectures. We have created a benchmark for floorplan reconstruction by acquiring RGBD video streams for 155 residential houses or apartments with Google Tango phones and annotating complete floorplan information. Our qualitative and quantitative evaluations demonstrate that the fusion of three branches effectively improves the reconstruction quality. We hope that the paper together with the benchmark will be an important step towards solving a challenging vector-graphics reconstruction problem. Code and data are available at this https URL.",
                        "Citation Paper Authors": "Authors:Chen Liu, Jiaye Wu, Yasutaka Furukawa"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.10742v1": {
            "Paper Title": "End-to-End Trainable Non-Collaborative Dialog System",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.10629v1": {
            "Paper Title": "Fatigue Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.05011v1": {
            "Paper Title": "A psychophysics approach for quantitative comparison of interpretable\n  computer vision models",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": "\u2022Guided backpropagation : applies ReLU in gradient computa-\ntion in addition to the gradient of a ReLU ",
                    "Citation Text": "Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Ried-\nmiller. 2014. Striving for Simplicity: The All Convolutional Net. (dec 2014).\narXiv:1412.6806 http://arxiv.org/abs/1412.6806",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.6806",
                        "Citation Paper Title": "Title:Striving for Simplicity: The All Convolutional Net",
                        "Citation Paper Abstract": "Abstract:Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the \"deconvolution approach\" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.",
                        "Citation Paper Authors": "Authors:Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, Martin Riedmiller"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "For all methods we used the implementation in the iNNvestigate!\npackage ",
                    "Citation Text": "Maximilian Alber, Sebastian Lapuschkin, Philipp Seegerer, Miriam H\u00e4gele,\nKristof T. Sch\u00fctt, Gr\u00e9goire Montavon, Wojciech Samek, Klaus-Robert M\u00fcller,\nSven D\u00e4hne, and Pieter-Jan Kindermans. 2018. iNNvestigate neural networks!\n(aug 2018). arXiv:1808.04260 http://arxiv.org/abs/1808.04260",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.04260",
                        "Citation Paper Title": "Title:iNNvestigate neural networks!",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have revolutionized many application domains of machine learning and are key components of many critical decision or predictive processes. Therefore, it is crucial that domain specialists can understand and analyze actions and pre- dictions, even of the most complex neural network architectures. Despite these arguments neural networks are often treated as black boxes. In the attempt to alleviate this short- coming many analysis methods were proposed, yet the lack of reference implementations often makes a systematic comparison between the methods a major effort. The presented library iNNvestigate addresses this by providing a common interface and out-of-the- box implementation for many analysis methods, including the reference implementation for PatternNet and PatternAttribution as well as for LRP-methods. To demonstrate the versatility of iNNvestigate, we provide an analysis of image classifications for variety of state-of-the-art neural network architectures.",
                        "Citation Paper Authors": "Authors:Maximilian Alber, Sebastian Lapuschkin, Philipp Seegerer, Miriam H\u00e4gele, Kristof T. Sch\u00fctt, Gr\u00e9goire Montavon, Wojciech Samek, Klaus-Robert M\u00fcller, Sven D\u00e4hne, Pieter-Jan Kindermans"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "or interpretability for\nneural network models [ 24,29,33]. Second there are interpretabil-\nity approaches that aim at rendering anymodel interpretable, a\npopular example are the Local Interpretable Model-Agnostic Expla-\nnations (LIME) ",
                    "Citation Text": "Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \"Why Should I\nTrust You?\": Explaining the Predictions of Any Classifier. In SIGKDD . 1135\u20131144.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.04938",
                        "Citation Paper Title": "Title:\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier",
                        "Citation Paper Abstract": "Abstract:Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.",
                        "Citation Paper Authors": "Authors:Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1907.06535v2": {
            "Paper Title": "Characterizing Developer Use of Automatically Generated Patches",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.01871v6": {
            "Paper Title": "Help, Anna! Visual Navigation with Natural Multimodal Assistance via\n  Retrospective Curiosity-Encouraging Imitation Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.09958v1": {
            "Paper Title": "PhotoTwinVR: An Immersive System for Manipulation, Inspection and\n  Dimension Measurements of the 3D Photogrammetric Models of Real-Life\n  Structures in Virtual Reality",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.09668v1": {
            "Paper Title": "Visualization by Example",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.09284v1": {
            "Paper Title": "EnergyScout: A Consumer Oriented Dashboard for Smart Meter Data\n  Analytics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.09219v1": {
            "Paper Title": "Integrating Automated Play in Level Co-Creation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.07179v2": {
            "Paper Title": "NeckSense: A Multi-Sensor Necklace for Detecting Eating Activities in\n  Free-Living Conditions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.10958v2": {
            "Paper Title": "Explainable Reinforcement Learning Through a Causal Lens",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.06541v2": {
            "Paper Title": "The Markup Language for Designing Gaze Controlled Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.08608v1": {
            "Paper Title": "Seq2Seq RNN based Gait Anomaly Detection from Smartphone Acquired\n  Multimodal Motion Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.07983v1": {
            "Paper Title": "Task-Based Hybrid Shared Control for Training Through Forceful\n  Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.07701v1": {
            "Paper Title": "Making Privacy Graspable: Can we Nudge Users to use Privacy Enhancing\n  Techniques?",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.07692v1": {
            "Paper Title": "\"Please enter your PIN\" -- On the Risk of Bypass Attacks on Biometric\n  Authentication on Mobile Devices",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.07624v1": {
            "Paper Title": "Designing Accessible Visual Programming Tools for Children with Autism\n  Spectrum Condition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.13875v2": {
            "Paper Title": "Expressive Inverse Kinematics Solving in Real-time for Virtual and\n  Robotic Interactive Characters",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.07447v1": {
            "Paper Title": "Subspace Shapes: Enhancing High-Dimensional Subspace Structures via\n  Ambient Occlusion Shading",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.07380v1": {
            "Paper Title": "Developing a Scenario-Based Video Game Generation Framework: Preliminary\n  Results",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.06877v1": {
            "Paper Title": "Exploring Configurations for Multi-user Communication in Virtual Reality",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.08277v1": {
            "Paper Title": "Exploring the added value of blockchain technology for the healthcare\n  domain",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.06747v1": {
            "Paper Title": "Towards Personalized Dialog Policies for Conversational Skill Discovery",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.04176v3": {
            "Paper Title": "Human-Machine Collaboration for Fast Land Cover Mapping",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": ". Unfortunately, the algorithms often suffer from \u201cunknown unknowns\u201d: self-inspection\ndoes not reliably reveal what is not modeled well. This is the case in most ML algorithms, including\ndeep neural networks ",
                    "Citation Text": "Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, and Balaji Lakshminarayanan. Do deep\ngenerative models know what they don\u2019t know? In International Conference on Learning Representations ,\n2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.09136",
                        "Citation Paper Title": "Title:Do Deep Generative Models Know What They Don't Know?",
                        "Citation Paper Abstract": "Abstract:A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data. A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong. Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel, out-of-distribution inputs. In this paper we challenge this assumption. We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former. Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN. To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood. We find such behavior persists even when we restrict the flows to constant-volume transformations. These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.",
                        "Citation Paper Authors": "Authors:Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, Balaji Lakshminarayanan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.05996v1": {
            "Paper Title": "Privacy and Utility Preserving Sensor-Data Transformations",
            "Sentences": [
                {
                    "Sentence ID": 49,
                    "Sentence": ", or\ntraining the required model through a federated learning ",
                    "Citation Text": "K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov, C. Kiddon,\nJ. Konecny, S. Mazzocchi, H. B. McMahan, T. Van Overveldt, D. Petrou, D. Ramage,\nJ. Roselander, Towards federated learning at scale: System design, in: Proceedings of the\n2nd SysML Conference, Palo Alto, CA, USA, 2019.\n20",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.01046",
                        "Citation Paper Title": "Title:Towards Federated Learning at Scale: System Design",
                        "Citation Paper Abstract": "Abstract:Federated Learning is a distributed machine learning approach which enables model training on a large corpus of decentralized data. We have built a scalable production system for Federated Learning in the domain of mobile devices, based on TensorFlow. In this paper, we describe the resulting high-level design, sketch some of the challenges and their solutions, and touch upon the open problems and future directions.",
                        "Citation Paper Authors": "Authors:Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Kone\u010dn\u00fd, Stefano Mazzocchi, H. Brendan McMahan, Timon Van Overveldt, David Petrou, Daniel Ramage, Jason Roselander"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": ".\nTransformations can reduce the amount of sensitive information in the data\nby reconstruction ",
                    "Citation Text": "C. Huang, P. Kairouz, X. Chen, L. Sankar, R. Rajagopal, Context-aware generative adver-\nsarial privacy, Entropy 19 (12) (2017) 656.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.09549",
                        "Citation Paper Title": "Title:Context-Aware Generative Adversarial Privacy",
                        "Citation Paper Abstract": "Abstract:Preserving the utility of published datasets while simultaneously providing provable privacy guarantees is a well-known challenge. On the one hand, context-free privacy solutions, such as differential privacy, provide strong privacy guarantees, but often lead to a significant reduction in utility. On the other hand, context-aware privacy solutions, such as information theoretic privacy, achieve an improved privacy-utility tradeoff, but assume that the data holder has access to dataset statistics. We circumvent these limitations by introducing a novel context-aware privacy framework called generative adversarial privacy (GAP). GAP leverages recent advancements in generative adversarial networks (GANs) to allow the data holder to learn privatization schemes from the dataset itself. Under GAP, learning the privacy mechanism is formulated as a constrained minimax game between two players: a privatizer that sanitizes the dataset in a way that limits the risk of inference attacks on the individuals' private variables, and an adversary that tries to infer the private variables from the sanitized dataset. To evaluate GAP's performance, we investigate two simple (yet canonical) statistical dataset models: (a) the binary data model, and (b) the binary Gaussian mixture model. For both models, we derive game-theoretically optimal minimax privacy mechanisms, and show that the privacy mechanisms learned from data (in a generative adversarial fashion) match the theoretically optimal ones. This demonstrates that our framework can be easily applied in practice, even in the absence of dataset statistics.",
                        "Citation Paper Authors": "Authors:Chong Huang, Peter Kairouz, Xiao Chen, Lalitha Sankar, Ram Rajagopal"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.05824v1": {
            "Paper Title": "A Discreet Wearable IoT Sensor for Continuous Transdermal Alcohol\n  Monitoring -- Challenges and Opportunities",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.05683v1": {
            "Paper Title": "Modeling patterns of smartphone usage and their relationship to\n  cognitive health",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.05305v1": {
            "Paper Title": "Emotion Recognition with Forearm-based Electromyography",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.04794v2": {
            "Paper Title": "Emerging Natural User Interfaces in Mobile Computing: A Bottoms-Up\n  Survey",
            "Sentences": [
                {
                    "Sentence ID": 165,
                    "Sentence": "EMG HCI CNN: 2 CL, 1 FCL, 3 LSTM, softmax 97% ",
                    "Citation Text": "Zhang, X., Yao, L., Sheng, Q. Z., Kanhere, S. S., Gu, T., and Zhang, D. Converting your thoughts to texts: Enabling\nbraintyping viadeep featurelearning ofeeg signals. In 2018IEEE InternationalConference onPervasive Computing\nand Communications (PerCom) (2018), IEEE, pp. 1\u201310.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.08820",
                        "Citation Paper Title": "Title:Converting Your Thoughts to Texts: Enabling Brain Typing via Deep Feature Learning of EEG Signals",
                        "Citation Paper Abstract": "Abstract:An electroencephalography (EEG) based Brain Computer Interface (BCI) enables people to communicate with the outside world by interpreting the EEG signals of their brains to interact with devices such as wheelchairs and intelligent robots. More specifically, motor imagery EEG (MI-EEG), which reflects a subjects active intent, is attracting increasing attention for a variety of BCI applications. Accurate classification of MI-EEG signals while essential for effective operation of BCI systems, is challenging due to the significant noise inherent in the signals and the lack of informative correlation between the signals and brain activities. In this paper, we propose a novel deep neural network based learning framework that affords perceptive insights into the relationship between the MI-EEG data and brain activities. We design a joint convolutional recurrent neural network that simultaneously learns robust high-level feature presentations through low-dimensional dense embeddings from raw MI-EEG signals. We also employ an Autoencoder layer to eliminate various artifacts such as background activities. The proposed approach has been evaluated extensively on a large- scale public MI-EEG dataset and a limited but easy-to-deploy dataset collected in our lab. The results show that our approach outperforms a series of baselines and the competitive state-of-the- art methods, yielding a classification accuracy of 95.53%. The applicability of our proposed approach is further demonstrated with a practical BCI system for typing.",
                        "Citation Paper Authors": "Authors:Xiang Zhang, Lina Yao, Quan Z. Sheng, Salil S. Kanhere, Tao Gu, Dalin Zhang"
                    }
                },
                {
                    "Sentence ID": 166,
                    "Sentence": "BCI, SSVEPs Speller 1D CNN: 4 CL, 1 FCL, softmax 99% ",
                    "Citation Text": "Zhang,X.,Yao,L.,Zhang,S.,Kanhere,S.,Sheng,M.,andLiu,Y. Internetofthingsmeetsbrain\u2013computerinterface:\nA unified deep learning framework for enabling human-thing cognitive interactivity. IEEE Internet of Things Journal\n6, 2 (2018), 2084\u20132092.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.00789",
                        "Citation Paper Title": "Title:Internet of Things Meets Brain-Computer Interface: A Unified Deep Learning Framework for Enabling Human-Thing Cognitive Interactivity",
                        "Citation Paper Abstract": "Abstract:A Brain-Computer Interface (BCI) acquires brain signals, analyzes and translates them into commands that are relayed to actuation devices for carrying out desired actions. With the widespread connectivity of everyday devices realized by the advent of the Internet of Things (IoT), BCI can empower individuals to directly control objects such as smart home appliances or assistive robots, directly via their thoughts. However, realization of this vision is faced with a number of challenges, most importantly being the issue of accurately interpreting the intent of the individual from the raw brain signals that are often of low fidelity and subject to noise. Moreover, pre-processing brain signals and the subsequent feature engineering are both time-consuming and highly reliant on human domain expertise. To address the aforementioned issues, in this paper, we propose a unified deep learning based framework that enables effective human-thing cognitive interactivity in order to bridge individuals and IoT objects. We design a reinforcement learning based Selective Attention Mechanism (SAM) to discover the distinctive features from the input brain signals. In addition, we propose a modified Long Short-Term Memory (LSTM) to distinguish the inter-dimensional information forwarded from the SAM. To evaluate the efficiency of the proposed framework, we conduct extensive real-world experiments and demonstrate that our model outperforms a number of competitive state-of-the-art baselines. Two practical real-time human-thing cognitive interaction applications are presented to validate the feasibility of our approach.",
                        "Citation Paper Authors": "Authors:Xiang Zhang, Lina Yao, Shuai Zhang, Salil S. Kanhere, Quan Z. Sheng, Yunhao Liu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.04930v1": {
            "Paper Title": "HMTNet:3D Hand Pose Estimation from Single Depth Image Based on Hand\n  Morphological Topology",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": "in\nthese publications. In the following, we will review the re-\nlated works using hand model or hand kinematic constraints.\nA. HAND KINEMATIC CONSTRAINTS\nDeep-Prior ",
                    "Citation Text": "M. Oberweger, P. Wohlhart, and V . Lepetit, \u201cHands deep in deep learning\nfor hand pose estimation,\u201d arXiv preprint arXiv:1502.06807, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1502.06807",
                        "Citation Paper Title": "Title:Hands Deep in Deep Learning for Hand Pose Estimation",
                        "Citation Paper Abstract": "Abstract:We introduce and evaluate several architectures for Convolutional Neural Networks to predict the 3D joint locations of a hand given a depth map. We first show that a prior on the 3D pose can be easily introduced and significantly improves the accuracy and reliability of the predictions. We also show how to use context efficiently to deal with ambiguities between fingers. These two contributions allow us to significantly outperform the state-of-the-art on several challenging benchmarks, both in terms of accuracy and computation times.",
                        "Citation Paper Authors": "Authors:Markus Oberweger, Paul Wohlhart, Vincent Lepetit"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.04787v1": {
            "Paper Title": "Effects of data ambiguity and cognitive biases on the interpretability\n  of machine learning models in humanitarian decision making",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.04643v1": {
            "Paper Title": "Framing Effects on Privacy Concerns about a Home Telepresence Robot",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": ". We use a privacy taxonomy compiled from\nthe literature ",
                    "Citation Text": "M. Rueben, C. M. Grimm, F. J. Bernieri, and W. D.\nSmart. A taxonomy of privacy constructs for\nprivacy-sensitive robotics. arXiv:1701.00841v1 [cs.CY].",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1701.00841",
                        "Citation Paper Title": "Title:A Taxonomy of Privacy Constructs for Privacy-Sensitive Robotics",
                        "Citation Paper Abstract": "Abstract:The introduction of robots into our society will also introduce new concerns about personal privacy. In order to study these concerns, we must do human-subject experiments that involve measuring privacy-relevant constructs. This paper presents a taxonomy of privacy constructs based on a review of the privacy literature. Future work in operationalizing privacy constructs for HRI studies is also discussed.",
                        "Citation Paper Authors": "Authors:Matthew Rueben, Cindy M. Grimm, Frank J. Bernieri, William D. Smart"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.08293v1": {
            "Paper Title": "Experiences with Improving the Transparency of AI Models and Services",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.04446v1": {
            "Paper Title": "Enhancing User Experience in Virtual Reality with Radial Basis Function\n  Interpolation Based Stereoscopic Camera Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.04052v1": {
            "Paper Title": "Scaling Robot Supervision to Hundreds of Hours with RoboTurk: Robotic\n  Manipulation Dataset through Human Reasoning and Dexterity",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.04338v1": {
            "Paper Title": "Active Learning for Black-Box Adversarial Attacks in EEG-Based\n  Brain-Computer Interfaces",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": ". Transferabilit y-based\nattacks were \ufb01rst proposed by Papernot et al. ",
                    "Citation Text": "N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Ce lik, and\nA. Swami, \u201cPractical black-box attacks against machine lea rning,\u201d in\nProc. ACM Asia Conf. on Computer and Communications Securit y. Abu\nDhabi, UAE: ACM, Apr. 2017, pp. 506\u2013519.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.02697",
                        "Citation Paper Title": "Title:Practical Black-Box Attacks against Machine Learning",
                        "Citation Paper Abstract": "Abstract:Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19% and 88.94%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.",
                        "Citation Paper Authors": "Authors:Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik, Ananthram Swami"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.02789v1": {
            "Paper Title": "Active Multi-Label Crowd Consensus",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.02726v1": {
            "Paper Title": "An Agent-Based Intelligent HCI Information System in Mixed Reality",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.02725v1": {
            "Paper Title": "Benchmark for Skill Learning from Demonstration: Impact of User\n  Experience, Task Complexity, and Start Configuration on Performance",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.02524v1": {
            "Paper Title": "A Spoken Dialogue System for Spatial Question Answering in a Physical\n  Blocks World",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.02320v1": {
            "Paper Title": "Nonverbal Robot Feedback for Human Teachers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06560v2": {
            "Paper Title": "Unclogging Our Arteries: Using Human-Inspired Signals to Disambiguate\n  Navigational Intentions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.01072v2": {
            "Paper Title": "Wearable Affective Life-Log System for Understanding Emotion Dynamics in\n  Daily Life",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.01158v2": {
            "Paper Title": "An Affective Situation Labeling System from Psychological Behaviors in\n  Emotion Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.06607v1": {
            "Paper Title": "Context Aware Family Dynamics based Internet of Things Access Control\n  Towards Better Child Safety",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.11637v2": {
            "Paper Title": "Mixing realities for sketch retrieval in Virtual Reality",
            "Sentences": [
                {
                    "Sentence ID": 40,
                    "Sentence": ", where they developed a learning model\nbased on triplets to describe the images considering fine-grained\nsimilarities. This approach was extensively used in recent works\nlike in Wang et al. ",
                    "Citation Text": "Fang Wang, Le Kang, and Yi Li. 2015. Sketch-based 3D Shape Retrieval using\nConvolutional Neural Networks. CoRR abs/1504.03504 (2015). arXiv:1504.03504http://arxiv.org/abs/1504.03504",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1504.03504",
                        "Citation Paper Title": "Title:Sketch-based 3D Shape Retrieval using Convolutional Neural Networks",
                        "Citation Paper Abstract": "Abstract:Retrieving 3D models from 2D human sketches has received considerable attention in the areas of graphics, image retrieval, and computer vision. Almost always in state of the art approaches a large amount of \"best views\" are computed for 3D models, with the hope that the query sketch matches one of these 2D projections of 3D models using predefined features.\nWe argue that this two stage approach (view selection -- matching) is pragmatic but also problematic because the \"best views\" are subjective and ambiguous, which makes the matching inputs obscure. This imprecise nature of matching further makes it challenging to choose features manually. Instead of relying on the elusive concept of \"best views\" and the hand-crafted features, we propose to define our views using a minimalism approach and learn features for both sketches and views. Specifically, we drastically reduce the number of views to only two predefined directions for the whole dataset. Then, we learn two Siamese Convolutional Neural Networks (CNNs), one for the views and one for the sketches. The loss function is defined on the within-domain as well as the cross-domain similarities. Our experiments on three benchmark datasets demonstrate that our method is significantly better than state of the art approaches, and outperforms them in all conventional metrics.",
                        "Citation Paper Authors": "Authors:Fang Wang, Le Kang, Yi Li"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": ". Improving the image similarity metric is a\nmain challenge as these triplet architectures are used to measure\nsimilarities between images and sketches ",
                    "Citation Text": "Tu Bui, Leonardo Ribeiro, Moacir Ponti, and John P. Collomosse. 2016. General-\nisation and Sharing in Triplet Convnets for Sketch based Visual Search. CoRR\nabs/1611.05301 (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.05301",
                        "Citation Paper Title": "Title:Generalisation and Sharing in Triplet Convnets for Sketch based Visual Search",
                        "Citation Paper Abstract": "Abstract:We propose and evaluate several triplet CNN architectures for measuring the similarity between sketches and photographs, within the context of the sketch based image retrieval (SBIR) task. In contrast to recent fine-grained SBIR work, we study the ability of our networks to generalise across diverse object categories from limited training data, and explore in detail strategies for weight sharing, pre-processing, data augmentation and dimensionality reduction. We exceed the performance of pre-existing techniques on both the Flickr15k category level SBIR benchmark by $18\\%$, and the TU-Berlin SBIR benchmark by $\\sim10 \\mathcal{T}_b$, when trained on the 250 category TU-Berlin classification dataset augmented with 25k corresponding photographs harvested from the Internet.",
                        "Citation Paper Authors": "Authors:Tu Bui, Leonardo Ribeiro, Moacir Ponti, John Collomosse"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.01474v1": {
            "Paper Title": "VASTA: A Vision and Language-assisted Smartphone Task Automation System",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.01318v1": {
            "Paper Title": "Sequential/Spatial, a Survey of Interactive Information Retrieval\n  Methods for Controlled Experimentation and Evaluation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.11386v4": {
            "Paper Title": "Detecting gender differences in perception of emotion in crowdsourced\n  data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.00988v1": {
            "Paper Title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.00879v1": {
            "Paper Title": "A low-cost real-time 3D imaging system for contactless asthma\n  observation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06527v3": {
            "Paper Title": "Towards Effective Human-AI Teams: The Case of Collaborative Packing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.00737v1": {
            "Paper Title": "The Effect of Video Playback Speed on Surgeon Technical Skill Perception",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.00568v1": {
            "Paper Title": "Goals, Process, and Challenges of Exploratory Data Analysis: An\n  Interview Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.00523v1": {
            "Paper Title": "What Gets Echoed? Understanding the \"Pointers\" in Explanations of\n  Persuasive Arguments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.03245v2": {
            "Paper Title": "Ask Not What AI Can Do, But What AI Should Do: Towards a Framework of\n  Task Delegability",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.00466v1": {
            "Paper Title": "VR with Older Adults: Participatory Design of a Virtual ATM Training\n  Simulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.01121v4": {
            "Paper Title": "HYPE: A Benchmark for Human eYe Perceptual Evaluation of Generative\n  Models",
            "Sentences": [
                {
                    "Sentence ID": 51,
                    "Sentence": ", we suggest that diversity can be incorporated using the automated recall score\nmeasures diversity independently from precision F1=8 ",
                    "Citation Text": "Mehdi SM Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly. Assess-\ning generative models via precision and recall. In Advances in Neural Information Processing\nSystems , pp. 5228\u20135237, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.00035",
                        "Citation Paper Title": "Title:Assessing Generative Models via Precision and Recall",
                        "Citation Paper Abstract": "Abstract:Recent advances in generative modeling have led to an increased interest in the study of statistical divergences as means of model comparison. Commonly used evaluation methods, such as the Frechet Inception Distance (FID), correlate well with the perceived quality of samples and are sensitive to mode dropping. However, these metrics are unable to distinguish between different failure cases since they only yield one-dimensional scores. We propose a novel definition of precision and recall for distributions which disentangles the divergence into two separate dimensions. The proposed notion is intuitive, retains desirable properties, and naturally leads to an efficient algorithm that can be used to evaluate generative models. We relate this notion to total variation as well as to recent evaluation metrics such as Inception Score and FID. To demonstrate the practical utility of the proposed approach we perform an empirical study on several variants of Generative Adversarial Networks and Variational Autoencoders. In an extensive set of experiments we show that the proposed metric is able to disentangle the quality of generated samples from the coverage of the target distribution.",
                        "Citation Paper Authors": "Authors:Mehdi S. M. Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, Sylvain Gelly"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": ".\nThis suggests that different datasets will have different levels of complexity for crossing realistic or\nhyper-realistic thresholds. The closest recent work to ours compares models using a tournament of\ndiscriminators ",
                    "Citation Text": "Catherine Olsson, Surya Bhupatiraju, Tom Brown, Augustus Odena, and Ian Goodfellow. Skill\nrating for generative models. arXiv preprint arXiv:1808.04888 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.04888",
                        "Citation Paper Title": "Title:Skill Rating for Generative Models",
                        "Citation Paper Abstract": "Abstract:We explore a new way to evaluate generative models using insights from evaluation of competitive games between human players. We show experimentally that tournaments between generators and discriminators provide an effective way to evaluate generative models. We introduce two methods for summarizing tournament outcomes: tournament win rate and skill rating. Evaluations are useful in different contexts, including monitoring the progress of a single model as it learns during the training process, and comparing the capabilities of two different fully trained models. We show that a tournament consisting of a single model playing against past and future versions of itself produces a useful measure of training progress. A tournament containing multiple separate models (using different seeds, hyperparameters, and architectures) provides a useful relative comparison between different trained GANs. Tournament-based rating methods are conceptually distinct from numerous previous categories of approaches to evaluation of generative models, and have complementary advantages and disadvantages.",
                        "Citation Paper Authors": "Authors:Catherine Olsson, Surya Bhupatiraju, Tom Brown, Augustus Odena, Ian Goodfellow"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": ", among\nothers. These tasks often resort to automatic metrics like the Inception Score (IS) ",
                    "Citation Text": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\nImproved techniques for training gans. In Advances in Neural Information Processing Systems ,\npp. 2234\u20132242, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.03498",
                        "Citation Paper Title": "Title:Improved Techniques for Training GANs",
                        "Citation Paper Abstract": "Abstract:We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.",
                        "Citation Paper Authors": "Authors:Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1910.14112v1": {
            "Paper Title": "Alexa, Who Am I Speaking To? Understanding Users' Ability to Identify\n  Third-Party Apps on Amazon Alexa",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.13872v1": {
            "Paper Title": "The Game Performance Index for Mobile Phones",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.09078v2": {
            "Paper Title": "Open Challenges of Blind People using Smartphones",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.13656v1": {
            "Paper Title": "Outliagnostics: Visualizing Temporal Discrepancy in Outlying Signatures\n  of Data Entries",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.13166v2": {
            "Paper Title": "Towards a Model for Spoken Conversational Search",
            "Sentences": [
                {
                    "Sentence ID": 56,
                    "Sentence": ".\nIn recent years, interest in SCS has grown, as speech technology ",
                    "Citation Text": "W. Xiong, L. Wu, F. Alleva, J. Droppo, X. Huang, and A. Stolcke. The\nmicrosoft 2017 conversational speech recognition system. In Proc. of the\nIEEE Int. Conf. on Acoustics, Speech and Signal Processing , ICASSP '18,\npages 5934{5938. IEEE, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.06073",
                        "Citation Paper Title": "Title:The Microsoft 2017 Conversational Speech Recognition System",
                        "Citation Paper Abstract": "Abstract:We describe the 2017 version of Microsoft's conversational speech recognition system, in which we update our 2016 system with recent developments in neural-network-based acoustic and language modeling to further advance the state of the art on the Switchboard speech recognition task. The system adds a CNN-BLSTM acoustic model to the set of model architectures we combined previously, and includes character-based and dialog session aware LSTM language models in rescoring. For system combination we adopt a two-stage approach, whereby subsets of acoustic models are first combined at the senone/frame level, followed by a word-level voting via confusion networks. We also added a confusion network rescoring step after system combination. The resulting system yields a 5.1\\% word error rate on the 2000 Switchboard evaluation set.",
                        "Citation Paper Authors": "Authors:W. Xiong, L. Wu, F. Alleva, J. Droppo, X. Huang, A. Stolcke"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1910.13249v1": {
            "Paper Title": "Navigation Agents for the Visually Impaired: A Sidewalk Simulator and\n  Experiments",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": "provides agents with 21,567 human-generated visually-grounded natural language\ninstructions for reaching target locations in the Matterport3D environment ",
                    "Citation Text": "A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner, M. Savva, S. Song, A. Zeng, and\nY . Zhang. Matterport3d: Learning from RGB-D data in indoor environments. International\nConference on 3D Vision (3DV) , 2017.\n9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.06158",
                        "Citation Paper Title": "Title:Matterport3D: Learning from RGB-D Data in Indoor Environments",
                        "Citation Paper Abstract": "Abstract:Access to large, diverse RGB-D datasets is critical for training RGB-D scene understanding algorithms. However, existing datasets still cover only a limited number of views or a restricted scale of spaces. In this paper, we introduce Matterport3D, a large-scale RGB-D dataset containing 10,800 panoramic views from 194,400 RGB-D images of 90 building-scale scenes. Annotations are provided with surface reconstructions, camera poses, and 2D and 3D semantic segmentations. The precise global alignment and comprehensive, diverse panoramic set of views over entire buildings enable a variety of supervised and self-supervised computer vision tasks, including keypoint matching, view overlap prediction, normal prediction from color, semantic segmentation, and region classification.",
                        "Citation Paper Authors": "Authors:Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Nie\u00dfner, Manolis Savva, Shuran Song, Andy Zeng, Yinda Zhang"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ". Many environments focus on navigating apartment\ninteriors, with continuous ",
                    "Citation Text": "F. Xia, A. R. Zamir, Z.-Y . He, A. Sax, J. Malik, and S. Savarese. Gibson env: real-world\nperception for embodied agents. In Computer Vision and Pattern Recognition (CVPR), 2018\nIEEE Conference on .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.10654",
                        "Citation Paper Title": "Title:Gibson Env: Real-World Perception for Embodied Agents",
                        "Citation Paper Abstract": "Abstract:Developing visual perception models for active agents and sensorimotor control are cumbersome to be done in the physical world, as existing algorithms are too slow to efficiently learn in real-time and robots are fragile and costly. This has given rise to learning-in-simulation which consequently casts a question on whether the results transfer to real-world. In this paper, we are concerned with the problem of developing real-world perception for active agents, propose Gibson Virtual Environment for this purpose, and showcase sample perceptual tasks learned therein. Gibson is based on virtualizing real spaces, rather than using artificially designed ones, and currently includes over 1400 floor spaces from 572 full buildings. The main characteristics of Gibson are: I. being from the real-world and reflecting its semantic complexity, II. having an internal synthesis mechanism, \"Goggles\", enabling deploying the trained models in real-world without needing further domain adaptation, III. embodiment of agents and making them subject to constraints of physics and space.",
                        "Citation Paper Authors": "Authors:Fei Xia, Amir Zamir, Zhi-Yang He, Alexander Sax, Jitendra Malik, Silvio Savarese"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "has also motivated the development of systems to generate nat-\nural language instructions for navigation ",
                    "Citation Text": "D. Fried, R. Hu, V . Cirik, A. Rohrbach, J. Andreas, L.-P. Morency, T. Berg-Kirkpatrick,\nK. Saenko, D. Klein, and T. Darrell. Speaker-follower models for vision-and-language navi-\ngation. In Advances in Neural Information Processing Systems , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.02724",
                        "Citation Paper Title": "Title:Speaker-Follower Models for Vision-and-Language Navigation",
                        "Citation Paper Abstract": "Abstract:Navigation guided by natural language instructions presents a challenging reasoning problem for instruction followers. Natural language instructions typically identify only a few high-level decisions and landmarks rather than complete low-level motor behaviors; much of the missing information must be inferred based on perceptual context. In machine learning settings, this is doubly challenging: it is difficult to collect enough annotated data to enable learning of this reasoning process from scratch, and also difficult to implement the reasoning process using generic sequence models. Here we describe an approach to vision-and-language navigation that addresses both these issues with an embedded speaker model. We use this speaker model to (1) synthesize new instructions for data augmentation and to (2) implement pragmatic reasoning, which evaluates how well candidate action sequences explain an instruction. Both steps are supported by a panoramic action space that reflects the granularity of human-generated instructions. Experiments show that all three components of this approach---speaker-driven data augmentation, pragmatic reasoning and panoramic action space---dramatically improve the performance of a baseline instruction follower, more than doubling the success rate over the best existing approach on a standard benchmark.",
                        "Citation Paper Authors": "Authors:Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, Trevor Darrell"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1910.12544v1": {
            "Paper Title": "Human-AI Co-Learning for Data-Driven AI",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.11734v1": {
            "Paper Title": "What Smartphones, Ethnomethodology, and Bystander Inaccessibility Can\n  Teach Us About Better Design?",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.11459v1": {
            "Paper Title": "A Robot's Expressive Language Affects Human Strategy and Perceptions in\n  a Competitive Game",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.11437v1": {
            "Paper Title": "Development and Implementation of a Dashboard for Diabetes Care\n  Management in OpenMRS",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.11059v1": {
            "Paper Title": "Interactive Image Restoration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.10481v2": {
            "Paper Title": "The Task Analysis Cell Assembly Perspective",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.08595v2": {
            "Paper Title": "Identifying the Most Explainable Classifier",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.10140v1": {
            "Paper Title": "Gesture Agreement Assessment Using Description Vectors",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.09911v1": {
            "Paper Title": "Designing Security and Privacy Requirements in Internet of Things: A\n  Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.09725v1": {
            "Paper Title": "Let's Gamble: Uncovering the Impact of Visualization on Risk Perception\n  and Decision-Making",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.09719v1": {
            "Paper Title": "Spatiotemporal Emotion Recognition using Deep CNN Based on EEG during\n  Music Listening",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.09636v1": {
            "Paper Title": "Real-Time Multi-Diver Tracking and Re-identification for Underwater\n  Human-Robot Collaboration",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": "which increase tracker accuracy at the cost of\nprocessing speed. This phenomenon is illustrated by the\nleaderboard for the Conference on Computer Vision and\nPattern Recognition (CVPR) 2019 MOT Challenge ",
                    "Citation Text": "P. Dendorfer, H. Rezato\ufb01ghi, A. Milan, J. Shi,\nD. Cremers, I. Reid, S. Roth, K. Schindler, andL. Leal-Taixe, \u201cCVPR19 tracking and detection chal-\nlenge: How crowded can it get?,\u201d arXiv preprint\narXiv:1906.04567 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.04567",
                        "Citation Paper Title": "Title:CVPR19 Tracking and Detection Challenge: How crowded can it get?",
                        "Citation Paper Abstract": "Abstract:Standardized benchmarks are crucial for the majority of computer vision applications. Although leaderboards and ranking tables should not be over-claimed, benchmarks often provide the most objective measure of performance and are therefore important guides for research.\nThe benchmark for Multiple Object Tracking, MOTChallenge, was launched with the goal to establish a standardized evaluation of multiple object tracking methods. The challenge focuses on multiple people tracking, since pedestrians are well studied in the tracking community, and precise tracking and detection has high practical relevance. Since the first release, MOT15, MOT16 and MOT17 have tremendously contributed to the community by introducing a clean dataset and precise framework to benchmark multi-object trackers. In this paper, we present our CVPR19 benchmark, consisting of 8 new sequences depicting very crowded challenging scenes. The benchmark will be presented at the 4th BMTT MOT Challenge Workshop at the Computer Vision and Pattern Recognition Conference (CVPR) 2019, and will evaluate the state-of-the-art in multiple object tracking whend handling extremely crowded scenarios.",
                        "Citation Paper Authors": "Authors:Patrick Dendorfer, Hamid Rezatofighi, Anton Milan, Javen Shi, Daniel Cremers, Ian Reid, Stefan Roth, Konrad Schindler, Laura Leal-Taixe"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1910.09382v1": {
            "Paper Title": "Danse-doigts, a Fine Motor Game",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.08865v1": {
            "Paper Title": "Deck.gl: Large-scale Web-based Visual Analytics Made Easy",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.08814v1": {
            "Paper Title": "On Using Chatbots to Promote Smoking Cessation Among Adolescents of Low\n  Socioeconomic Status",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.10489v1": {
            "Paper Title": "Continuous Emotion Recognition during Music Listening Using EEG Signals:\n  A Fuzzy Parallel Cascades Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.08685v1": {
            "Paper Title": "Real-Time Lip Sync for Live 2D Animation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.08534v1": {
            "Paper Title": "Many Faces of Feature Importance: Comparing Built-in and Post-hoc\n  Feature Importance in Text Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.08549v1": {
            "Paper Title": "Towards Learning Cross-Modal Perception-Trace Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.07809v1": {
            "Paper Title": "Do you see what I see? Taking perspective of others using facial images",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.07799v1": {
            "Paper Title": "Exploring Semi-Automatic Map Labeling",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.07784v1": {
            "Paper Title": "Indoor Information Retrieval using Lifelog Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.07563v1": {
            "Paper Title": "Explainable AI for Intelligence Augmentation in Multi-Domain Operations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.07269v1": {
            "Paper Title": "\"Get a Free Item Pack with Every Activation!\" -- Do Incentives Increase\n  the Adoption Rates of Two-Factor Authentication?",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.02445v3": {
            "Paper Title": "Enhanced Human-Machine Interaction by Combining Proximity Sensing with\n  Global Perception",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": "combine RGB-D cameras and\ninertial measurement units to detect human gestures for robot\nlearning. Zimmermann et al. ",
                    "Citation Text": "Christian Zimmermann, Tim Welschehold, Christian Dornhege, Wol-\nfram Burgard, and Thomas Brox. 3D human pose estimation in RGB-\nD images for robotic task learning. In IEEE International Conference\non Robotics and Automation (ICRA) , pages 1986\u20131992. IEEE, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.02622",
                        "Citation Paper Title": "Title:3D Human Pose Estimation in RGBD Images for Robotic Task Learning",
                        "Citation Paper Abstract": "Abstract:We propose an approach to estimate 3D human pose in real world units from a single RGBD image and show that it exceeds performance of monocular 3D pose estimation approaches from color as well as pose estimation exclusively from depth. Our approach builds on robust human keypoint detectors for color images and incorporates depth for lifting into 3D. We combine the system with our learning from demonstration framework to instruct a service robot without the need of markers. Experiments in real world settings demonstrate that our approach enables a PR2 robot to imitate manipulation actions observed from a human teacher.",
                        "Citation Paper Authors": "Authors:Christian Zimmermann, Tim Welschehold, Christian Dornhege, Wolfram Burgard, Thomas Brox"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1910.07058v1": {
            "Paper Title": "Health Monitoring in Smart Homes Utilizing Internet of Things",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.12581v1": {
            "Paper Title": "A Multivariate Elo-based Learner Model for Adaptive Educational Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.06234v1": {
            "Paper Title": "The Theory behind Controllable Expressive Speech Synthesis: a\n  Cross-disciplinary Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.05624v1": {
            "Paper Title": "A Research Platform for Multi-Robot Dialogue with Humans",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.05522v1": {
            "Paper Title": "RiPPLE: A Crowdsourced Adaptive Platform for Recommendation of Learning\n  Activities",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.05327v1": {
            "Paper Title": "Gamification of In-Classroom Diagram Design for Science Students",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.05265v1": {
            "Paper Title": "Automating dynamic consent decisions for the processing of social media\n  data in health research",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.04357v1": {
            "Paper Title": "Visual Understanding of Multiple Attributes Learning Model of X-Ray\n  Scattering Images",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": "system helps designers\nin their understanding and diagnosis of CNNs by explor-\ning the learned representations in the graph layout of the\nneural networks. ActiVis ",
                    "Citation Text": "M. Kahng, P. Y . Andrews, A. Kalro, and D. H. P. Chau.\nActiVis: Visual Exploration of Industry-Scale Deep Neural\nNetwork Models. IEEE Transactions on Visualization and\nComputer Graphics , 24(1):88\u201397, 2018. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.01942",
                        "Citation Paper Title": "Title:ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models",
                        "Citation Paper Abstract": "Abstract:While deep learning models have achieved state-of-the-art accuracies for many prediction tasks, understanding these models remains a challenge. Despite the recent interest in developing visual tools to help users interpret deep learning models, the complexity and wide variety of models deployed in industry, and the large-scale datasets that they used, pose unique design challenges that are inadequately addressed by existing work. Through participatory design sessions with over 15 researchers and engineers at Facebook, we have developed, deployed, and iteratively improved ActiVis, an interactive visualization system for interpreting large-scale deep learning models and results. By tightly integrating multiple coordinated views, such as a computation graph overview of the model architecture, and a neuron activation view for pattern discovery and comparison, users can explore complex deep neural network models at both the instance- and subset-level. ActiVis has been deployed on Facebook's machine learning platform. We present case studies with Facebook researchers and engineers, and usage scenarios of how ActiVis may work with different models.",
                        "Citation Paper Authors": "Authors:Minsuk Kahng, Pierre Y. Andrews, Aditya Kalro, Duen Horng Chau"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "shows that optimizing synthetic im-\nages with better natural image priors produces more recog-\nnizable visualizations. CNNVis ",
                    "Citation Text": "M. Liu, J. Shi, Z. Li, C. Li, J. Zhu, and S. Liu. Towards\nBetter Analysis of Deep Convolutional Neural Networks.\nIEEE Transactions on Visualization and Computer Graph-\nics, 23(1):91\u2013100, 2017. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1604.07043",
                        "Citation Paper Title": "Title:Towards Better Analysis of Deep Convolutional Neural Networks",
                        "Citation Paper Abstract": "Abstract:Deep convolutional neural networks (CNNs) have achieved breakthrough performance in many pattern recognition tasks such as image classification. However, the development of high-quality deep models typically relies on a substantial amount of trial-and-error, as there is still no clear understanding of when and why a deep model works. In this paper, we present a visual analytics approach for better understanding, diagnosing, and refining deep CNNs. We formulate a deep CNN as a directed acyclic graph. Based on this formulation, a hybrid visualization is developed to disclose the multiple facets of each neuron and the interactions between them. In particular, we introduce a hierarchical rectangle packing algorithm and a matrix reordering algorithm to show the derived features of a neuron cluster. We also propose a biclustering-based edge bundling method to reduce visual clutter caused by a large number of connections between neurons. We evaluated our method on a set of CNNs and the results are generally favorable.",
                        "Citation Paper Authors": "Authors:Mengchen Liu, Jiaxin Shi, Zhen Li, Chongxuan Li, Jun Zhu, Shixia Liu"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "allow users to visualize and\ninteract with the activation maps and network structures,\ntogether with line graphs and histograms of characteristic\nstatistics. DeepVis ",
                    "Citation Text": "J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, and H. Lipson.\nUnderstanding Neural Networks Through Deep Visualiza-\ntion. 2015. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.06579",
                        "Citation Paper Title": "Title:Understanding Neural Networks Through Deep Visualization",
                        "Citation Paper Abstract": "Abstract:Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.",
                        "Citation Paper Authors": "Authors:Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, Hod Lipson"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1907.11040v3": {
            "Paper Title": "DeepDrawing: A Deep Learning Approach to Graph Drawing",
            "Sentences": [
                {
                    "Sentence ID": 80,
                    "Sentence": "extended tree-LSTM by distinguishing different edge types in the\ngraph and applied the model to the relation extraction problem in the\nNatural Language Processing (NLP) \ufb01eld. You et al. ",
                    "Citation Text": "J. You, R. Ying, X. Ren, W. L. Hamilton, and J. Leskovec. Graphrnn: a\ndeep generative model for graphs. In Proceedings of the 35th International\nConference on Machine Learning , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.08773",
                        "Citation Paper Title": "Title:GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models",
                        "Citation Paper Abstract": "Abstract:Modeling and generating graphs is fundamental for studying networks in biology, engineering, and social sciences. However, modeling complex distributions over graphs and then efficiently sampling from these distributions is challenging due to the non-unique, high-dimensional nature of graphs and the complex, non-local dependencies that exist between edges in a given graph. Here we propose GraphRNN, a deep autoregressive model that addresses the above challenges and approximates any distribution of graphs with minimal assumptions about their structure. GraphRNN learns to generate graphs by training on a representative set of graphs and decomposes the graph generation process into a sequence of node and edge formations, conditioned on the graph structure generated so far.\nIn order to quantitatively evaluate the performance of GraphRNN, we introduce a benchmark suite of datasets, baselines and novel evaluation metrics based on Maximum Mean Discrepancy, which measure distances between sets of graphs. Our experiments show that GraphRNN significantly outperforms all baselines, learning to generate diverse graphs that match the structural characteristics of a target set, while also scaling to graphs 50 times larger than previous deep models.",
                        "Citation Paper Authors": "Authors:Jiaxuan You, Rex Ying, Xiang Ren, William L. Hamilton, Jure Leskovec"
                    }
                },
                {
                    "Sentence ID": 65,
                    "Sentence": "proposed\ntwo types of tree-LSTM, generalizing the basic LSTM to tree-structure\ntypologies, to predict the semantic relatedness of sentences. Peng et al. ",
                    "Citation Text": "N. Peng, H. Poon, C. Quirk, K. Toutanova, and W.-t. Yih. Cross-sentence n-\nary relation extraction with graph lstms. arXiv preprint arXiv:1708.03743 ,\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.03743",
                        "Citation Paper Title": "Title:Cross-Sentence N-ary Relation Extraction with Graph LSTMs",
                        "Citation Paper Abstract": "Abstract:Past work in relation extraction has focused on binary relations in single sentences. Recent NLP inroads in high-value domains have sparked interest in the more general setting of extracting n-ary relations that span multiple sentences. In this paper, we explore a general relation extraction framework based on graph long short-term memory networks (graph LSTMs) that can be easily extended to cross-sentence n-ary relation extraction. The graph formulation provides a unified way of exploring different LSTM approaches and incorporating various intra-sentential and inter-sentential dependencies, such as sequential, syntactic, and discourse relations. A robust contextual representation is learned for the entities, which serves as input to the relation classifier. This simplifies handling of relations with arbitrary arity, and enables multi-task learning with related relations. We evaluate this framework in two important precision medicine settings, demonstrating its effectiveness with both conventional supervised learning and distant supervision. Cross-sentence extraction produced larger knowledge bases. and multi-task learning significantly improved extraction accuracy. A thorough analysis of various LSTM approaches yielded useful insight the impact of linguistic analysis on extraction accuracy.",
                        "Citation Paper Authors": "Authors:Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina Toutanova, Wen-tau Yih"
                    }
                },
                {
                    "Sentence ID": 70,
                    "Sentence": "modi\ufb01ed the Gate Recurrent Units (GRU) and proposed\na gated GNN to learn node representations. Tai et al. ",
                    "Citation Text": "K. S. Tai, R. Socher, and C. D. Manning. Improved semantic represen-\ntations from tree-structured long short-term memory networks. In Pro-\nceedings of the 7th International Joint Conference on Natural Language\nProcessing , vol. 1, pp. 1556\u20131566, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1503.00075",
                        "Citation Paper Title": "Title:Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks",
                        "Citation Paper Abstract": "Abstract:Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).",
                        "Citation Paper Authors": "Authors:Kai Sheng Tai, Richard Socher, Christopher D. Manning"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": ". A closely related research direction explores\nusing RNNs for graph-structured data [57, 65, 70, 82]. For example,\nLi et al. ",
                    "Citation Text": "Y . Li, D. Tarlow, M. Brockschmidt, and R. Zemel. Gated graph sequence\nneural networks. arXiv preprint arXiv:1511.05493 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.05493",
                        "Citation Paper Title": "Title:Gated Graph Sequence Neural Networks",
                        "Citation Paper Abstract": "Abstract:Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.",
                        "Citation Paper Authors": "Authors:Yujia Li, Daniel Tarlow, Marc Brockschmidt, Richard Zemel"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "conducted convolution in the Fourier\ndomain using eigen decomposition of the graph Laplacian. Defferrard\net al. ",
                    "Citation Text": "M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural\nnetworks on graphs with fast localized spectral \ufb01ltering. In Proceedings\nof Advances in Neural Information Processing Systems , pp. 3844\u20133852,\n2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.09375",
                        "Citation Paper Title": "Title:Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering",
                        "Citation Paper Abstract": "Abstract:In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.",
                        "Citation Paper Authors": "Authors:Micha\u00ebl Defferrard, Xavier Bresson, Pierre Vandergheynst"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": ". Spectral approaches\napply convolution to the spectral representation of graphs [9, 12, 15, 40,\n51]. For example, Bruna et al. ",
                    "Citation Text": "J. Bruna, W. Zaremba, A. Szlam, and Y . Lecun. Spectral networks and\nlocally connected networks on graphs. In Proceedings of International\nConference on Learning Representations , 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1312.6203",
                        "Citation Paper Title": "Title:Spectral Networks and Locally Connected Networks on Graphs",
                        "Citation Paper Abstract": "Abstract:Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.",
                        "Citation Paper Authors": "Authors:Joan Bruna, Wojciech Zaremba, Arthur Szlam, Yann LeCun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1910.03641v1": {
            "Paper Title": "Linking emotions to behaviors through deep transfer learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.03622v1": {
            "Paper Title": "Mobile App Privacy in Software Engineering Research: A Systematic\n  Mapping Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.07088v2": {
            "Paper Title": "BasketballGAN: Generating Basketball Play Simulation Through Sketching",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "presented\nWassestein GAN (WGAN), which applies the Earth Moving dis-\ntance to measure the similarity of real and fake samples. Later\non, Gulragini et al. ",
                    "Citation Text": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and\nAaron C Courville. 2017. Improved training of wasserstein gans. In Advances in\nNeural Information Processing Systems . 5767\u20135777.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.00028",
                        "Citation Paper Title": "Title:Improved Training of Wasserstein GANs",
                        "Citation Paper Abstract": "Abstract:Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.",
                        "Citation Paper Authors": "Authors:Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron Courville"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1910.03042v1": {
            "Paper Title": "Gunrock: A Social Bot for Complex and Engaging Long Conversations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.08796v2": {
            "Paper Title": "Learning Vis Tools: Teaching Data Visualization Tutorials",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.07665v2": {
            "Paper Title": "Knowledge Transferring via Model Aggregation for Online Social Care",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": ". Geyer et al. proposed differential privacy preserving techniques on the client side to\nbalance performance and privacy ",
                    "Citation Text": "Robin C Geyer, Tassilo Klein, and Moin Nabi. Differentially private federated learning: A client level\nperspective. arXiv preprint arXiv:1712.07557 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.07557",
                        "Citation Paper Title": "Title:Differentially Private Federated Learning: A Client Level Perspective",
                        "Citation Paper Abstract": "Abstract:Federated learning is a recent advance in privacy protection. In this context, a trusted curator aggregates parameters optimized in decentralized fashion by multiple clients. The resulting model is then distributed back to all clients, ultimately converging to a joint representative model without explicitly having to share the data. However, the protocol is vulnerable to differential attacks, which could originate from any party contributing during federated optimization. In such an attack, a client's contribution during training and information about their data set is revealed through analyzing the distributed model. We tackle this problem and propose an algorithm for client sided differential privacy preserving federated optimization. The aim is to hide clients' contributions during training, balancing the trade-off between privacy loss and model performance. Empirical studies suggest that given a sufficiently large number of participating clients, our proposed procedure can maintain client-level differential privacy at only a minor cost in model performance.",
                        "Citation Paper Authors": "Authors:Robin C. Geyer, Tassilo Klein, Moin Nabi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1910.02201v1": {
            "Paper Title": "Early Estimation of User's Intention of Tele-Operation Using Object\n  Affordance and Hand Motion in a Dual First-Person Vision",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": "with a sequence of hand im-\nages as an input. We concatenate both outputs and then sup-\nply it to a variant of U-Net ",
                    "Citation Text": "O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolu-\ntional networks for biomedical image segmentation. arXiv\npreprint arXiv:1505.04597 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1505.04597",
                        "Citation Paper Title": "Title:U-Net: Convolutional Networks for Biomedical Image Segmentation",
                        "Citation Paper Abstract": "Abstract:There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at this http URL .",
                        "Citation Paper Authors": "Authors:Olaf Ronneberger, Philipp Fischer, Thomas Brox"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1910.04703v1": {
            "Paper Title": "Predictive Simulation: Using Regression and Artificial Neural Networks\n  to Negate Latency in Networked Interactive Virtual Reality",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01546v1": {
            "Paper Title": "iVRNote: Design, Creation and Evaluation of an Interactive Note-Taking\n  Interface for Study and Reflection in VR Learning Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.11248v2": {
            "Paper Title": "When to Intervene: Detecting Abnormal Mood using Everyday Smartphone\n  Conversations",
            "Sentences": [
                {
                    "Sentence ID": 49,
                    "Sentence": ". Generative adversarial networks, or GANs, can\nlearn a latent space where anomalous data can then be more\nclearly distinguished ",
                    "Citation Text": "H. Zenati, C. S. Foo, B. Lecouat, G. Manek, and V . R. Chan-\ndrasekhar, \u201cEf\ufb01cient gan-based anomaly detection,\u201d arXiv preprint\narXiv:1802.06222 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.06222",
                        "Citation Paper Title": "Title:Efficient GAN-Based Anomaly Detection",
                        "Citation Paper Abstract": "Abstract:Generative adversarial networks (GANs) are able to model the complex highdimensional distributions of real-world data, which suggests they could be effective for anomaly detection. However, few works have explored the use of GANs for the anomaly detection task. We leverage recently developed GAN models for anomaly detection, and achieve state-of-the-art performance on image and network intrusion datasets, while being several hundred-fold faster at test time than the only published GAN-based method.",
                        "Citation Paper Authors": "Authors:Houssam Zenati, Chuan Sheng Foo, Bruno Lecouat, Gaurav Manek, Vijay Ramaseshan Chandrasekhar"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1910.01191v1": {
            "Paper Title": "Adaptive Generation of Phantom Limbs Using Visible Hierarchical\n  Autoencoders",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.04695v1": {
            "Paper Title": "GLADAS: Gesture Learning for Advanced Driver Assistance Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.04697v1": {
            "Paper Title": "Immersive virtual worlds: Multi-sensory virtual environments for health\n  and safety training",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.00340v1": {
            "Paper Title": "VOnDA: A Framework for Ontology-Based Dialogue Management",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.00106v1": {
            "Paper Title": "Development of a Game with a Purpose for Acquisition of Brain-Computer\n  Interface Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.13443v1": {
            "Paper Title": "Using Conversational Agents To Support Learning By Teaching",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.07083v1": {
            "Paper Title": "Occurence of A Cyber Security Eco-System: A Nature Oriented Project and\n  Evaluation of An Indirect Social Experiment",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.12969v1": {
            "Paper Title": "Counterfactual States for Atari Agents via Generative Deep Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.04847v2": {
            "Paper Title": "RecSim: A Configurable Simulation Platform for Recommender Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.11233v1": {
            "Paper Title": "Teacher-Student Learning Paradigm for Tri-training: An Efficient Method\n  for Unlabeled Data Exploitation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.10823v1": {
            "Paper Title": "Software architecture for YOLO, a creativity-stimulating robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.10567v1": {
            "Paper Title": "Tangent space spatial filters for interpretable and efficient Riemannian\n  classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06561v2": {
            "Paper Title": "Commitments in Human-Robot Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.10513v1": {
            "Paper Title": "Visualization and Travel Time Extraction System for the Statistics of\n  TDCS Travel using MapReduce Framework",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.10173v1": {
            "Paper Title": "Route Packing: Geospatially-Accurate Visualization of Route Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.07502v2": {
            "Paper Title": "Automatic Detection and Classification of Cognitive Distortions in\n  Mental Health Text",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.09942v1": {
            "Paper Title": "From Data Disclosure to Privacy Nudges: A Privacy-aware and User-centric\n  Personal Data Management Framework",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.03285v4": {
            "Paper Title": "Can You Explain That? Lucid Explanations Help Human-AI Collaborative\n  Image Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.09738v1": {
            "Paper Title": "How Players Play Games: Observing the Influences of Game Mechanics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.06725v3": {
            "Paper Title": "Mutual Reinforcement Learning",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": "described intelligent robot systems acquiring human-like writing style and then exploiting it to\nteach children. Fan et al. ",
                    "Citation Text": "Yang Fan, Fei Tian, Tao Qin, Xiang-Yang Li, and Tie-Yan Liu. 2018. Learning to teach. arXiv preprint arXiv:1805.03643\n(2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.03643",
                        "Citation Paper Title": "Title:Learning to Teach",
                        "Citation Paper Abstract": "Abstract:Teaching plays a very important role in our society, by spreading human knowledge and educating our next generations. A good teacher will select appropriate teaching materials, impact suitable methodologies, and set up targeted examinations, according to the learning behaviors of the students. In the field of artificial intelligence, however, one has not fully explored the role of teaching, and pays most attention to machine \\emph{learning}. In this paper, we argue that equal attention, if not more, should be paid to teaching, and furthermore, an optimization framework (instead of heuristics) should be used to obtain good teaching strategies. We call this approach `learning to teach'. In the approach, two intelligent agents interact with each other: a student model (which corresponds to the learner in traditional machine learning algorithms), and a teacher model (which determines the appropriate data, loss function, and hypothesis space to facilitate the training of the student model). The teacher model leverages the feedback from the student model to optimize its own teaching strategies by means of reinforcement learning, so as to achieve teacher-student co-evolution. To demonstrate the practical value of our proposed approach, we take the training of deep neural networks (DNN) as an example, and show that by using the learning to teach techniques, we are able to use much less training data and fewer iterations to achieve almost the same accuracy for different kinds of DNN models (e.g., multi-layer perceptron, convolutional neural networks and recurrent neural networks) under various machine learning tasks (e.g., image classification and text understanding).",
                        "Citation Paper Authors": "Authors:Yang Fan, Fei Tian, Tao Qin, Xiang-Yang Li, Tie-Yan Liu"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ". These researchers are also using multi-arm bandit\ntechniques (MAB) to address the problems with assistive agents who can help human participants\nto select appropriate channels to maximize the cumulative reward ",
                    "Citation Text": "Lawrence Chan, Dylan Hadfield-Menell, Siddhartha Srinivasa, and Anca Dragan. 2019. The Assistive Multi-Armed\nBandit. In 2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI) . IEEE, 354\u2013363.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.08654",
                        "Citation Paper Title": "Title:The Assistive Multi-Armed Bandit",
                        "Citation Paper Abstract": "Abstract:Learning preferences implicit in the choices humans make is a well studied problem in both economics and computer science. However, most work makes the assumption that humans are acting (noisily) optimally with respect to their preferences. Such approaches can fail when people are themselves learning about what they want. In this work, we introduce the assistive multi-armed bandit, where a robot assists a human playing a bandit task to maximize cumulative reward. In this problem, the human does not know the reward function but can learn it through the rewards received from arm pulls; the robot only observes which arms the human pulls but not the reward associated with each pull. We offer sufficient and necessary conditions for successfully assisting the human in this framework. Surprisingly, better human performance in isolation does not necessarily lead to better performance when assisted by the robot: a human policy can do better by effectively communicating its observed rewards to the robot. We conduct proof-of-concept experiments that support these results. We see this work as contributing towards a theory behind algorithms for human-robot interaction.",
                        "Citation Paper Authors": "Authors:Lawrence Chan, Dylan Hadfield-Menell, Siddhartha Srinivasa, Anca Dragan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1909.09633v1": {
            "Paper Title": "Quantifying the Impact of Cognitive Biases in Question-Answering Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.09351v1": {
            "Paper Title": "An Experimental Comparison of Map-like Visualisations and Treemaps",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.09156v1": {
            "Paper Title": "Learning to Conceal: A Deep Learning Based Method for Preserving Privacy\n  and Avoiding Prejudice",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.08500v1": {
            "Paper Title": "Emotion Filtering at the Edge",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.08997v1": {
            "Paper Title": "Real-time Recognition of Smartphone User Behavior Based on Prophet\n  Algorithms",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.08172v1": {
            "Paper Title": "RTTD-ID: Tracked Captions with Multiple Speakers for Deaf Students",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.07471v1": {
            "Paper Title": "Multimodal Dataset of Human-Robot Hugging Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.04492v2": {
            "Paper Title": "Pluggable Social Artificial Intelligence for Enabling Human-Agent\n  Teaming",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06976v1": {
            "Paper Title": "Virtual Guide Dog: Next Generation Pedestrian Signal for the Visually\n  Impaired",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06907v1": {
            "Paper Title": "X-ToM: Explaining with Theory-of-Mind for Gaining Justified Human Trust",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": "proposed counterfactual visual explanations that identify how the input\ncould change such that the underlying vision system would make a different decision.\nMore recently, few methods have been developed for building models which are in-\ntrinsically interpretable ",
                    "Citation Text": "Q. Zhang, Y . N. Wu, S.-C. Zhu, Interpretable convolutional neural networks, The\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2018)\n8827\u20138836.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.00935",
                        "Citation Paper Title": "Title:Interpretable Convolutional Neural Networks",
                        "Citation Paper Abstract": "Abstract:This paper proposes a method to modify traditional convolutional neural networks (CNNs) into interpretable CNNs, in order to clarify knowledge representations in high conv-layers of CNNs. In an interpretable CNN, each filter in a high conv-layer represents a certain object part. We do not need any annotations of object parts or textures to supervise the learning process. Instead, the interpretable CNN automatically assigns each filter in a high conv-layer with an object part during the learning process. Our method can be applied to different types of CNNs with different structures. The clear knowledge representation in an interpretable CNN can help people understand the logics inside a CNN, i.e., based on which patterns the CNN makes the decision. Experiments showed that filters in an interpretable CNN were more semantically meaningful than those in traditional CNNs.",
                        "Citation Paper Authors": "Authors:Quanshi Zhang, Ying Nian Wu, Song-Chun Zhu"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "to identify minimal and suf\ufb01cient features to justify the classi\ufb01cation\nresult. ",
                    "Citation Text": "Y . Goyal, Z. Wu, J. Ernst, D. Batra, D. Parikh, S. Lee, Counterfactual visual\nexplanations, in: ICML 2019, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.07451",
                        "Citation Paper Title": "Title:Counterfactual Visual Explanations",
                        "Citation Paper Abstract": "Abstract:In this work, we develop a technique to produce counterfactual visual explanations. Given a 'query' image $I$ for which a vision system predicts class $c$, a counterfactual visual explanation identifies how $I$ could change such that the system would output a different specified class $c'$. To do this, we select a 'distractor' image $I'$ that the system predicts as class $c'$ and identify spatial regions in $I$ and $I'$ such that replacing the identified region in $I$ with the identified region in $I'$ would push the system towards classifying $I$ as $c'$. We apply our approach to multiple image classification datasets generating qualitative results showcasing the interpretability and discriminativeness of our counterfactual explanations. To explore the effectiveness of our explanations in teaching humans, we present machine teaching experiments for the task of fine-grained bird classification. We find that users trained to distinguish bird species fare better when given access to counterfactual explanations in addition to training examples.",
                        "Citation Paper Authors": "Authors:Yash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh, Stefan Lee"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "aims to generate\nexplanations based on high-level user de\ufb01ned concepts. Contrastive explanations are\nproposed by ",
                    "Citation Text": "A. Dhurandhar, P.-Y . Chen, R. Luss, C.-C. Tu, P. Ting, K. Shanmugam, P. Das,\nExplanations based on the missing: Towards contrastive explanations with per-\ntinent negatives, in: Advances in Neural Information Processing Systems, 2018,\npp. 592\u2013603.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.07623",
                        "Citation Paper Title": "Title:Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives",
                        "Citation Paper Abstract": "Abstract:In this paper we propose a novel method that provides contrastive explanations justifying the classification of an input by a black box classifier such as a deep neural network. Given an input we find what should be %necessarily and minimally and sufficiently present (viz. important object pixels in an image) to justify its classification and analogously what should be minimally and necessarily \\emph{absent} (viz. certain background pixels). We argue that such explanations are natural for humans and are used commonly in domains such as health care and criminology. What is minimally but critically \\emph{absent} is an important part of an explanation, which to the best of our knowledge, has not been explicitly identified by current explanation methods that explain predictions of neural networks. We validate our approach on three real datasets obtained from diverse domains; namely, a handwritten digits dataset MNIST, a large procurement fraud dataset and a brain activity strength dataset. In all three cases, we witness the power of our approach in generating precise explanations that are also easy for human experts to understand and evaluate.",
                        "Citation Paper Authors": "Authors:Amit Dhurandhar, Pin-Yu Chen, Ronny Luss, Chun-Chen Tu, Paishun Ting, Karthikeyan Shanmugam, Payel Das"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "explains predictions of any classi\ufb01er by ap-\nproximating it locally with an interpretable model. In\ufb02uence measures ",
                    "Citation Text": "A. Datta, A. Datta, A. D. Procaccia, Y . Zick, In\ufb02uence in classi\ufb01cation via coop-\nerative game theory., in: IJCAI, 2015, pp. 511\u2013517.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1505.00036",
                        "Citation Paper Title": "Title:Influence in Classification via Cooperative Game Theory",
                        "Citation Paper Abstract": "Abstract:A dataset has been classified by some unknown classifier into two types of points. What were the most important factors in determining the classification outcome? In this work, we employ an axiomatic approach in order to uniquely characterize an influence measure: a function that, given a set of classified points, outputs a value for each feature corresponding to its influence in determining the classification outcome. We show that our influence measure takes on an intuitive form when the unknown classifier is linear. Finally, we employ our influence measure in order to analyze the effects of user profiling on Google's online display advertising.",
                        "Citation Paper Authors": "Authors:Amit Datta, Anupam Datta, Ariel D. Procaccia, Yair Zick"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": ", and an interactive visualization for facilitating\nanalysis of RNN hidden states ",
                    "Citation Text": "H. Strobelt, S. Gehrmann, B. Huber, H. P\ufb01ster, A. M. Rush, Visual anal-\nysis of hidden state dynamics in recurrent neural networks, arXiv preprint\narXiv:1606.07461.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.07461",
                        "Citation Paper Title": "Title:LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:Recurrent neural networks, and in particular long short-term memory (LSTM) networks, are a remarkably effective tool for sequence modeling that learn a dense black-box hidden representation of their sequential input. Researchers interested in better understanding these models have studied the changes in hidden state representations over time and noticed some interpretable patterns but also significant noise. In this work, we present LSTMVIS, a visual analysis tool for recurrent neural networks with a focus on understanding these hidden state dynamics. The tool allows users to select a hypothesis input range to focus on local state changes, to match these states changes to similar patterns in a large data set, and to align these results with structural annotations from their domain. We show several use cases of the tool for analyzing specific hidden state properties on dataset containing nesting, phrase structure, and chord progressions, and demonstrate how the tool can be used to isolate patterns for further statistical analysis. We characterize the domain, the different stakeholders, and their goals and tasks.",
                        "Citation Paper Authors": "Authors:Hendrik Strobelt, Sebastian Gehrmann, Hanspeter Pfister, Alexander M. Rush"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": ", an interpretable character-level language models for an-\nalyzing the predictions in RNNs ",
                    "Citation Text": "A. Karpathy, J. Johnson, L. Fei-Fei, Visualizing and understanding recurrent net-\nworks, arXiv preprint arXiv:1506.02078.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.02078",
                        "Citation Paper Title": "Title:Visualizing and Understanding Recurrent Networks",
                        "Citation Paper Abstract": "Abstract:Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.",
                        "Citation Paper Authors": "Authors:Andrej Karpathy, Justin Johnson, Li Fei-Fei"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": ". Recent visual explanation models seek to jointly\nclassify the image and explain why the predicted class label is appropriate for the im-\nage ",
                    "Citation Text": "L. A. Hendricks, Z. Akata, M. Rohrbach, J. Donahue, B. Schiele, T. Darrell,\nGenerating visual explanations, in: European Conference on Computer Vision,\nSpringer, 2016, pp. 3\u201319.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.08507",
                        "Citation Paper Title": "Title:Generating Visual Explanations",
                        "Citation Paper Abstract": "Abstract:Clearly explaining a rationale for a classification decision to an end-user can be as important as the decision itself. Existing approaches for deep visual recognition are generally opaque and do not output any justification text; contemporary vision-language models can describe image content but fail to take into account class-discriminative image aspects which justify visual predictions. We propose a new model that focuses on the discriminating properties of the visible object, jointly predicts a class label, and explains why the predicted label is appropriate for the image. We propose a novel loss function based on sampling and reinforcement learning that learns to generate sentences that realize a global sentence property, such as class specificity. Our results on a fine-grained bird species classification dataset show that our model is able to generate explanations which are not only consistent with an image but also more discriminative than descriptions produced by existing captioning methods.",
                        "Citation Paper Authors": "Authors:Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt Schiele, Trevor Darrell"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": ". Also, convolutional layers are visualized by reconstructing the in-\nput of each layer from its output ",
                    "Citation Text": "M. D. Zeiler, R. Fergus, Visualizing and understanding convolutional networks,\nin: European conference on computer vision, Springer, 2014, pp. 818\u2013833.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1311.2901",
                        "Citation Paper Title": "Title:Visualizing and Understanding Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \\etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.",
                        "Citation Paper Authors": "Authors:Matthew D Zeiler, Rob Fergus"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1909.03012v2": {
            "Paper Title": "One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI\n  Explainability Techniques",
            "Sentences": [
                {
                    "Sentence ID": 47,
                    "Sentence": "has directly interpretable models such as rule-based models and\ndecision trees. These can also be used for global post-hoc inter-\npretation. It also has LIME-like ",
                    "Citation Text": "M. Ribeiro, S. Singh, and C. Guestrin. \u201cWhy should I trust you?\u201d Explaining\nthe predictions of any classifier. In ACM SIGKDD Intl. Conference on Knowledge\nDiscovery and Data Mining , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.04938",
                        "Citation Paper Title": "Title:\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier",
                        "Citation Paper Abstract": "Abstract:Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.",
                        "Citation Paper Authors": "Authors:Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1909.11004v1": {
            "Paper Title": "Fuzzy Knowledge-Based Architecture for Learning and Interaction in\n  Social Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.08052v1": {
            "Paper Title": "Towards an Adaptive Robot for Sports and Rehabilitation Coaching",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.08398v1": {
            "Paper Title": "Design research, eHealth, and the convergence revolution",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06174v1": {
            "Paper Title": "Petri Net Machines for Human-Agent Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06077v1": {
            "Paper Title": "Enabling Humans to Plan Inspection Paths Using a Virtual Reality\n  Interface",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.05897v1": {
            "Paper Title": "Efficient 2.5D Hand Pose Estimation via Auxiliary Multi-Task Training\n  for Embedded Devices",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.05160v1": {
            "Paper Title": "Trust and Cognitive Load During Human-Robot Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.11766v1": {
            "Paper Title": "An Automated Vehicle (AV) like Me? The Impact of Personality\n  Similarities and Differences between Humans and AVs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.04773v1": {
            "Paper Title": "Understanding user search processes across varying cognitive levels",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.02800v2": {
            "Paper Title": "CrowdHub: Extending crowdsourcing platforms for the controlled\n  evaluation of tasks designs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.04387v1": {
            "Paper Title": "A Crowd-based Evaluation of Abuse Response Strategies in Conversational\n  Agents",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.04230v1": {
            "Paper Title": "Investigating Crowdsourcing to Generate Distractors for Multiple-Choice\n  Assessments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.03847v1": {
            "Paper Title": "Aligning Daily Activities with Personality: Towards A Recommender System\n  for Improving Wellbeing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.04747v1": {
            "Paper Title": "Recognizing Human Internal States: A Conceptor-Based Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.02764v2": {
            "Paper Title": "Towards Multimodal Emotion Recognition in German Speech Events in Cars\n  using Transfer Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.03596v1": {
            "Paper Title": "Lessons Learned from Developing a Microservice Based Mobile\n  Location-Based Crowdsourcing Platform",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.03582v1": {
            "Paper Title": "Clickbait? Sensational Headline Generation with Auto-tuned Reinforcement\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.03567v1": {
            "Paper Title": "What You See Is What You Get? The Impact of Representation Criteria on\n  Human Bias in Hiring",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.03466v1": {
            "Paper Title": "Multi-Modal Three-Stream Network for Action Recognition",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "motion and to make motion concentrated on\nactors, similar as a visual saliency map. Pre-trained spatial\nand temporal models on videos of UCF101 ",
                    "Citation Text": "K. Soomro, A. R. Zamir, and M. Shah, \u201cUcf101: A dataset of 101 human\nactions classes from videos in the wild,\u201d arXiv preprint arXiv:1212.0402 ,\n2012.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1212.0402",
                        "Citation Paper Title": "Title:UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild",
                        "Citation Paper Abstract": "Abstract:We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5%. To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips.",
                        "Citation Paper Authors": "Authors:Khurram Soomro, Amir Roshan Zamir, Mubarak Shah"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1909.05667v1": {
            "Paper Title": "Explainable Deep Learning for Video Recognition Tasks: A Framework &\n  Recommendations",
            "Sentences": [
                {
                    "Sentence ID": 52,
                    "Sentence": "in machine learning or computer vision. This problem is addr essed in ",
                    "Citation Text": "Richard Tomsett, Dave Braines, Dan Harborne, Alun Pree ce, and Supriyo Chakraborty.\nInterpretable to whom? a role-based model for analyzing int erpretable machine learn-\ning systems. arXiv preprint arXiv:1806.07552 , 2018.HILEY ET AL.: EXPLAINABLE DEEP LEARNING FOR VIDEO RECOGNITI ON TASKS 15",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.07552",
                        "Citation Paper Title": "Title:Interpretable to Whom? A Role-based Model for Analyzing Interpretable Machine Learning Systems",
                        "Citation Paper Abstract": "Abstract:Several researchers have argued that a machine learning system's interpretability should be defined in relation to a specific agent or task: we should not ask if the system is interpretable, but to whom is it interpretable. We describe a model intended to help answer this question, by identifying different roles that agents can fulfill in relation to the machine learning system. We illustrate the use of our model in a variety of scenarios, exploring how an agent's role influences its goals, and the implications for defining interpretability. Finally, we make suggestions for how our model could be useful to interpretability researchers, system developers, and regulatory bodies auditing machine learning systems.",
                        "Citation Paper Authors": "Authors:Richard Tomsett, Dave Braines, Dan Harborne, Alun Preece, Supriyo Chakraborty"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1909.03054v1": {
            "Paper Title": "Calibrating Wayfinding Decisions in Pedestrian Simulation Models: The\n  Entropy Map",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.02780v1": {
            "Paper Title": "Understanding the Impact of Text Highlighting in Crowdsourcing Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.02507v1": {
            "Paper Title": "A Generalized Web Component for Domain-Independent Smart Assistants",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.02043v1": {
            "Paper Title": "PARQR: Augmenting the Piazza Online Forum to Better Support Degree\n  Seeking Online Masters Students",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.05655v1": {
            "Paper Title": "Assessment of Shift-Invariant CNN Gaze Mappings for PS-OG Eye Movement\n  Sensors",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": ".\nRigas et al. demonstrated a hybrid technique in which\na low speed (5 Hz) video sensor was used to correct er-\nrors in the PS-OG gaze estimate resulting from sensor shifts ",
                    "Citation Text": "Ioannis Rigas, Hayes Raf\ufb02e, and Oleg V . Komogortsev. Hy-\nbrid PS-V Technique: A Novel Sensor Fusion Approach for\nFast Mobile Eye-Tracking with Sensor-Shift Aware Correc-\ntion. IEEE Sensors Journal , 17(24):8356\u20138366, 2017. 3,\n4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.05411",
                        "Citation Paper Title": "Title:Hybrid PS-V Technique: A Novel Sensor Fusion Approach for Fast Mobile Eye-Tracking with Sensor-Shift Aware Correction",
                        "Citation Paper Abstract": "Abstract:This paper introduces and evaluates a hybrid technique that fuses efficiently the eye-tracking principles of photosensor oculography (PSOG) and video oculography (VOG). The main concept of this novel approach is to use a few fast and power-economic photosensors as the core mechanism for performing high speed eye-tracking, whereas in parallel, use a video sensor operating at low sampling-rate (snapshot mode) to perform dead-reckoning error correction when sensor movements occur. In order to evaluate the proposed method, we simulate the functional components of the technique and present our results in experimental scenarios involving various combinations of horizontal and vertical eye and sensor movements. Our evaluation shows that the developed technique can be used to provide robustness to sensor shifts that otherwise could induce error larger than 5 deg. Our analysis suggests that the technique can potentially enable high speed eye-tracking at low power profiles, making it suitable to be used in emerging head-mounted devices, e.g. AR/VR headsets.",
                        "Citation Paper Authors": "Authors:Ioannis Rigas, Hayes Raffle, Oleg V. Komogortsev"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1902.09159v2": {
            "Paper Title": "A Survey of Crowdsourcing in Medical Image Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.01428v1": {
            "Paper Title": "Map plasticity",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.01362v1": {
            "Paper Title": "Trouble on the Horizon: Forecasting the Derailment of Online\n  Conversations as they Develop",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.01322v1": {
            "Paper Title": "CMU GetGoing: An Understandable and Memorable Dialog System for Seniors",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.01206v1": {
            "Paper Title": "Efficient Real-Time Camera Based Estimation of Heart Rate and Its\n  Variability",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.05644v1": {
            "Paper Title": "Illuminated Decision Trees with Lucid",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.05329v1": {
            "Paper Title": "Robot Capability and Intention in Trust-based Decisions across Tasks",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": ". Recent work has explored the\nrole of the robot\u2019s intention, e.g., its policy ",
                    "Citation Text": "S. H. Huang, K. Bhatia, P. Abbeel, and A. D. Dragan, \u201cEstablishing\nAppropriate Trust via Critical States,\u201d IROS 2018 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.08174",
                        "Citation Paper Title": "Title:Establishing Appropriate Trust via Critical States",
                        "Citation Paper Abstract": "Abstract:In order to effectively interact with or supervise a robot, humans need to have an accurate mental model of its capabilities and how it acts. Learned neural network policies make that particularly challenging. We propose an approach for helping end-users build a mental model of such policies. Our key observation is that for most tasks, the essence of the policy is captured in a few critical states: states in which it is very important to take a certain action. Our user studies show that if the robot shows a human what its understanding of the task's critical states is, then the human can make a more informed decision about whether to deploy the policy, and if she does deploy it, when she needs to take control from it at execution time.",
                        "Citation Paper Authors": "Authors:Sandy H. Huang, Kush Bhatia, Pieter Abbeel, Anca D. Dragan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1909.00865v1": {
            "Paper Title": "Are digital natives spreadsheet natives?",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.02323v3": {
            "Paper Title": "Summit: Scaling Deep Learning Interpretability by Visualizing Activation\n  and Attribution Summarizations",
            "Sentences": [
                {
                    "Sentence ID": 56,
                    "Sentence": "In this work, we demonstrate our approach on INCEPTION V1 ",
                    "Citation Text": "C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,\nV . Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition , pp. 1\u20139, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.4842",
                        "Citation Paper Title": "Title:Going Deeper with Convolutions",
                        "Citation Paper Abstract": "Abstract:We propose a deep convolutional neural network architecture codenamed \"Inception\", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.",
                        "Citation Paper Authors": "Authors:Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1909.00699v1": {
            "Paper Title": "Experiences from Using Gamification and IoT-based Educational Tools in\n  High Schools towards Energy Savings",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.00608v1": {
            "Paper Title": "Collecting and Structuring Information in the Information Collage",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.00482v1": {
            "Paper Title": "A Semi-Automated Usability Evaluation Framework for Interactive Image\n  Segmentation Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.11765v1": {
            "Paper Title": "A Multimodal Alerting System for Online Class Quality Assurance",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.00193v1": {
            "Paper Title": "Functional advantages of an adaptive Theory of Mind for robotics: a\n  review of current architectures",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.00234v3": {
            "Paper Title": "Analysis of Contraction Effort Level in EMG-Based Gesture Recognition\n  Using Hyperdimensional Computing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.11706v1": {
            "Paper Title": "The OMG-Empathy Dataset: Evaluating the Impact of Affective Behavior in\n  Storytelling",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.04225v2": {
            "Paper Title": "Talking With Your Hands: Scaling Hand Gestures and Recognition With CNNs",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": ", this\nproblem was solved and successful 3D CNNs could be\ntrained from scratch without over\ufb01tting ",
                    "Citation Text": "K. Hara, H. Kataoka, and Y . Satoh. Can spatiotemporal 3d\ncnns retrace the history of 2d cnns and imagenet. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, Salt Lake City, UT, USA , pages 18\u201322,\n2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.09577",
                        "Citation Paper Title": "Title:Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?",
                        "Citation Paper Abstract": "Abstract:The purpose of this study is to determine whether current video datasets have sufficient data for training very deep convolutional neural networks (CNNs) with spatio-temporal three-dimensional (3D) kernels. Recently, the performance levels of 3D CNNs in the field of action recognition have improved significantly. However, to date, conventional research has only explored relatively shallow 3D architectures. We examine the architectures of various 3D CNNs from relatively shallow to very deep ones on current video datasets. Based on the results of those experiments, the following conclusions could be obtained: (i) ResNet-18 training resulted in significant overfitting for UCF-101, HMDB-51, and ActivityNet but not for Kinetics. (ii) The Kinetics dataset has sufficient data for training of deep 3D CNNs, and enables training of up to 152 ResNets layers, interestingly similar to 2D ResNets on ImageNet. ResNeXt-101 achieved 78.4% average accuracy on the Kinetics test set. (iii) Kinetics pretrained simple 3D architectures outperforms complex 2D architectures, and the pretrained ResNeXt-101 achieved 94.5% and 70.2% on UCF-101 and HMDB-51, respectively. The use of 2D CNNs trained on ImageNet has produced significant progress in various tasks in image. We believe that using deep 3D CNNs together with Kinetics will retrace the successful history of 2D CNNs and ImageNet, and stimulate advances in computer vision for videos. The codes and pretrained models used in this study are publicly available. this https URL",
                        "Citation Paper Authors": "Authors:Kensho Hara, Hirokatsu Kataoka, Yutaka Satoh"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": ". Afterwards, CNNs are also applied for video analysistasks. However, as the \ufb01rst video datasets were compar-\natively small such as UCF-101 ",
                    "Citation Text": "K. Soomro, A. R. Zamir, and M. Shah. Ucf101: A dataset\nof 101 human actions classes from videos in the wild. arXiv\npreprint arXiv:1212.0402 , 2012.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1212.0402",
                        "Citation Paper Title": "Title:UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild",
                        "Citation Paper Abstract": "Abstract:We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5%. To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips.",
                        "Citation Paper Authors": "Authors:Khurram Soomro, Amir Roshan Zamir, Mubarak Shah"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1909.05152v1": {
            "Paper Title": "Context Aware Road-user Importance Estimation (iCARE)",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": "model as feature extractor in\nconjunction with the road user proposal generator model.\nTo train the important road user proposal generator model,\nwe \ufb01rst initialize it with Faster R-CNN trained on COCO\nobject detection dataset ",
                    "Citation Text": "T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-\nmanan, P. Doll \u00b4ar, and C. L. Zitnick. Microsoft coco: Com-\nmon objects in context. In European conference on computer\nvision , pages 740\u2013755. Springer, 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1405.0312",
                        "Citation Paper Title": "Title:Microsoft COCO: Common Objects in Context",
                        "Citation Paper Abstract": "Abstract:We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
                        "Citation Paper Authors": "Authors:Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Doll\u00e1r"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "uses smart antennas and proposes\na distance awareness system for important object estima-\ntion. The model proposed by ",
                    "Citation Text": "X. Chen, H. Ma, J. Wan, B. Li, and T. Xia. Multi-view 3d\nobject detection network for autonomous driving. In IEEE\nCVPR , volume 1, page 3, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.07759",
                        "Citation Paper Title": "Title:Multi-View 3D Object Detection Network for Autonomous Driving",
                        "Citation Paper Abstract": "Abstract:This paper aims at high-accuracy 3D object detection in autonomous driving scenario. We propose Multi-View 3D networks (MV3D), a sensory-fusion framework that takes both LIDAR point cloud and RGB images as input and predicts oriented 3D bounding boxes. We encode the sparse 3D point cloud with a compact multi-view representation. The network is composed of two subnetworks: one for 3D object proposal generation and another for multi-view feature fusion. The proposal network generates 3D candidate boxes efficiently from the bird's eye view representation of 3D point cloud. We design a deep fusion scheme to combine region-wise features from multiple views and enable interactions between intermediate layers of different paths. Experiments on the challenging KITTI benchmark show that our approach outperforms the state-of-the-art by around 25% and 30% AP on the tasks of 3D localization and 3D detection. In addition, for 2D detection, our approach obtains 10.3% higher AP than the state-of-the-art on the hard data among the LIDAR-based methods.",
                        "Citation Paper Authors": "Authors:Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, Tian Xia"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "inspects the driver\u2019s attention\nspeci\ufb01cally towards pedestrians and motorbikes, and ex-\nploits object saliency. In ",
                    "Citation Text": "A. Palazzi, D. Abati, S. Calderara, F. Solera, and R. Cuc-\nchiara. Predicting the driver\u2019s focus of attention: the dr (eye)\nve project. arXiv preprint arXiv:1705.03854 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.03854",
                        "Citation Paper Title": "Title:Predicting the Driver's Focus of Attention: the DR(eye)VE Project",
                        "Citation Paper Abstract": "Abstract:In this work we aim to predict the driver's focus of attention. The goal is to estimate what a person would pay attention to while driving, and which part of the scene around the vehicle is more critical for the task. To this end we propose a new computer vision model based on a multi-branch deep architecture that integrates three sources of information: raw video, motion and scene semantics. We also introduce DR(eye)VE, the largest dataset of driving scenes for which eye-tracking annotations are available. This dataset features more than 500,000 registered frames, matching ego-centric views (from glasses worn by drivers) and car-centric views (from roof-mounted camera), further enriched by other sensors measurements. Results highlight that several attention patterns are shared across drivers and can be reproduced to some extent. The indication of which elements in the scene are likely to capture the driver's attention may benefit several applications in the context of human-vehicle interaction and driver attention analysis.",
                        "Citation Paper Authors": "Authors:Andrea Palazzi, Davide Abati, Simone Calderara, Francesco Solera, Rita Cucchiara"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1908.05203v2": {
            "Paper Title": "Why Should the Q-method be Integrated Into the Design Science Research?\n  A Systematic Mapping Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.10948v1": {
            "Paper Title": "A Large-Scale Empirical Study of Geotagging Behavior on Twitter",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.10585v1": {
            "Paper Title": "Attention-based Fusion for Outfit Recommendation",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "propose to learn\na compatibility space for different types of relatedness (e.g., color,\ntexture, brand) and weight these spaces according to their relevance\nfor a particular pair of items. ",
                    "Citation Text": "Mariya I. Vasileva, Bryan A. Plummer, Krishna Dusad, Shreya Rajpal, Ranjitha\nKumar, and David A. Forsyth. 2018. Learning Type-Aware Embeddings for\nFashion Compatibility. In ECCV .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.09196",
                        "Citation Paper Title": "Title:Learning Type-Aware Embeddings for Fashion Compatibility",
                        "Citation Paper Abstract": "Abstract:Outfits in online fashion data are composed of items of many different types (e.g. top, bottom, shoes) that share some stylistic relationship with one another. A representation for building outfits requires a method that can learn both notions of similarity (for example, when two tops are interchangeable) and compatibility (items of possibly different type that can go together in an outfit). This paper presents an approach to learning an image embedding that respects item type, and jointly learns notions of item similarity and compatibility in an end-to-end model. To evaluate the learned representation, we crawled 68,306 outfits created by users on the Polyvore website. Our approach obtains 3-5% improvement over the state-of-the-art on outfit compatibility prediction and fill-in-the-blank tasks using our dataset, as well as an established smaller dataset, while supporting a variety of useful queries.",
                        "Citation Paper Authors": "Authors:Mariya I. Vasileva, Bryan A. Plummer, Krishna Dusad, Shreya Rajpal, Ranjitha Kumar, David Forsyth"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "neural networks are used to acquire multimodal\nrepresentations of items based on the item image, category and title,\nto pool these into one outfit representation and to score the outfit\u2019sfashionability. Other approaches to outfit fashionability prediction\nalso exist. In ",
                    "Citation Text": "Xintong Han, Zuxuan Wu, Yu-Gang Jiang, and Larry S Davis. 2017. Learning\nFashion Compatibility with Bidirectional LSTMs. In ACM Multimedia .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.05691",
                        "Citation Paper Title": "Title:Learning Fashion Compatibility with Bidirectional LSTMs",
                        "Citation Paper Abstract": "Abstract:The ubiquity of online fashion shopping demands effective recommendation services for customers. In this paper, we study two types of fashion recommendation: (i) suggesting an item that matches existing components in a set to form a stylish outfit (a collection of fashion items), and (ii) generating an outfit with multimodal (images/text) specifications from a user. To this end, we propose to jointly learn a visual-semantic embedding and the compatibility relationships among fashion items in an end-to-end fashion. More specifically, we consider a fashion outfit to be a sequence (usually from top to bottom and then accessories) and each item in the outfit as a time step. Given the fashion items in an outfit, we train a bidirectional LSTM (Bi-LSTM) model to sequentially predict the next item conditioned on previous ones to learn their compatibility relationships. Further, we learn a visual-semantic space by regressing image features to their semantic representations aiming to inject attribute and category information as a regularization for training the LSTM. The trained network can not only perform the aforementioned recommendations effectively but also predict the compatibility of a given outfit. We conduct extensive experiments on our newly collected Polyvore dataset, and the results provide strong qualitative and quantitative evidence that our framework outperforms alternative methods.",
                        "Citation Paper Authors": "Authors:Xintong Han, Zuxuan Wu, Yu-Gang Jiang, Larry S. Davis"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "the visual compatibility of clothing items is captured with a\ncorrelated topic model to automatically create capsule wardrobes. ",
                    "Citation Text": "Yujie Lin, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Jun Ma, and Maarten de\nRijke. 2019. Improving Outfit Recommendation with Co-supervision of Fashion\nGeneration. In The World Wide Web Conference (WWW \u201919) . ACM, 1095\u20131105.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.09104",
                        "Citation Paper Title": "Title:Improving Outfit Recommendation with Co-supervision of Fashion Generation",
                        "Citation Paper Abstract": "Abstract:The task of fashion recommendation includes two main challenges: visual understanding and visual matching. Visual understanding aims to extract effective visual features. Visual matching aims to model a human notion of compatibility to compute a match between fashion items. Most previous studies rely on recommendation loss alone to guide visual understanding and matching. Although the features captured by these methods describe basic characteristics (e.g., color, texture, shape) of the input items, they are not directly related to the visual signals of the output items (to be recommended). This is problematic because the aesthetic characteristics (e.g., style, design), based on which we can directly infer the output items, are lacking. Features are learned under the recommendation loss alone, where the supervision signal is simply whether the given two items are matched or not. To address this problem, we propose a neural co-supervision learning framework, called the FAshion Recommendation Machine (FARM). FARM improves visual understanding by incorporating the supervision of generation loss, which we hypothesize to be able to better encode aesthetic information. FARM enhances visual matching by introducing a novel layer-to-layer matching mechanism to fuse aesthetic information more effectively, and meanwhile avoiding paying too much attention to the generation quality and ignoring the recommendation performance. Extensive experiments on two publicly available datasets show that FARM outperforms state-of-the-art models on outfit recommendation, in terms of AUC and MRR. Detailed analyses of generated and recommended items demonstrate that FARM can encode better features and generate high quality images as references to improve recommendation performance.",
                        "Citation Paper Authors": "Authors:Yujie Lin, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Jun Ma, Maarten de Rijke"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "use a Siamese convolutional neural network (CNN) architec-\nture to infer a compatibility space of clothing items. Instead of only\none feature space, multiple feature spaces can also be learned to\nfocus on certain compatibility relationships. ",
                    "Citation Text": "Ruining He, Charles Packer, and Julian McAuley. 2016. Learning Compatibility\nAcross Categories for Heterogeneous Item Recommendation. In International\nConference on Data Mining .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.09473",
                        "Citation Paper Title": "Title:Learning Compatibility Across Categories for Heterogeneous Item Recommendation",
                        "Citation Paper Abstract": "Abstract:Identifying relationships between items is a key task of an online recommender system, in order to help users discover items that are functionally complementary or visually compatible. In domains like clothing recommendation, this task is particularly challenging since a successful system should be capable of handling a large corpus of items, a huge amount of relationships among them, as well as the high-dimensional and semantically complicated features involved. Furthermore, the human notion of \"compatibility\" to capture goes beyond mere similarity: For two items to be compatible---whether jeans and a t-shirt, or a laptop and a charger---they should be similar in some ways, but systematically different in others.\nIn this paper we propose a novel method, Monomer, to learn complicated and heterogeneous relationships between items in product recommendation settings. Recently, scalable methods have been developed that address this task by learning similarity metrics on top of the content of the products involved. Here our method relaxes the metricity assumption inherent in previous work and models multiple localized notions of 'relatedness,' so as to uncover ways in which related items should be systematically similar, and systematically different. Quantitatively, we show that our system achieves state-of-the-art performance on large-scale compatibility prediction tasks, especially in cases where there is substantial heterogeneity between related items. Qualitatively, we demonstrate that richer notions of compatibility can be learned that go beyond similarity, and that our model can make effective recommendations of heterogeneous content.",
                        "Citation Paper Authors": "Authors:Ruining He, Charles Packer, Julian McAuley"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1904.07802v2": {
            "Paper Title": "Learning 3D Navigation Protocols on Touch Interfaces with Cooperative\n  Multi-Agent Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.10307v1": {
            "Paper Title": "Physiological and Affective Computing through Thermal Imaging: A Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.10127v1": {
            "Paper Title": "Learning-Based Video Game Development in MLP@UoM: An Overview",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.10125v1": {
            "Paper Title": "Proactive Intention Recognition for Joint Human-Robot Search and Rescue\n  Missions through Monte-Carlo Planning in POMDP Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.10013v1": {
            "Paper Title": "EmoSense: Computational Intelligence Driven Emotion Sensing via Wireless\n  Channel Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.09984v1": {
            "Paper Title": "Caring for Alzheimer's Disease Caregivers: A Qualitative Study\n  Investigating Opportunities for Exergame Innovation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.09348v1": {
            "Paper Title": "Color Blending in Outdoor Optical See-through AR: The Effect of\n  Real-world Backgrounds on User Interface Color",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.09165v1": {
            "Paper Title": "Augmented Unlocking Techniques for Smartphones Using Pre-Touch\n  Information",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.09082v1": {
            "Paper Title": "Kinesthetic Learning -- Haptic User Interfaces for Gyroscopic Precession\n  Simulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.08893v1": {
            "Paper Title": "You Can't Publish Replication Studies (and How to Anyways)",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.08597v1": {
            "Paper Title": "Sign Language Recognition, Generation, and Translation: An\n  Interdisciplinary Perspective",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.08412v1": {
            "Paper Title": "ChordLink: A New Hybrid Visualization Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.07894v1": {
            "Paper Title": "Visualization in the preprocessing phase: an interview study with\n  enterprise professionals",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.07792v1": {
            "Paper Title": "A Quality Metric for Visualization of Clusters in Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.07577v1": {
            "Paper Title": "Challenges of Designing HCI for Negative Emotions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.07519v1": {
            "Paper Title": "Multi-Modal Recognition of Worker Activity for Human-Centered\n  Intelligent Manufacturing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.07316v1": {
            "Paper Title": "Evaluating Alignment Approaches in Superimposed Time-Series and Temporal\n  Event-Sequence Visualizations",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": ". Performance-based payments,\ne.g. a correctness bonus, may increase work quality but also increase\nthe amount of time workers spend ",
                    "Citation Text": "C.-J. Ho, A. Slivkins, S. Suri, and J. W. Vaughan. Incentivizing high\nquality crowdwork. In Proc. 24th International Conference on World\nWide Web , WWW \u201915, pp. 419\u2013429, 2015. doi: 10.1145/2736277.\n2741102",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1503.05897",
                        "Citation Paper Title": "Title:Incentivizing High Quality Crowdwork",
                        "Citation Paper Abstract": "Abstract:We study the causal effects of financial incentives on the quality of crowdwork. We focus on performance-based payments (PBPs), bonus payments awarded to workers for producing high quality work. We design and run randomized behavioral experiments on the popular crowdsourcing platform Amazon Mechanical Turk with the goal of understanding when, where, and why PBPs help, identifying properties of the payment, payment structure, and the task itself that make them most effective. We provide examples of tasks for which PBPs do improve quality. For such tasks, the effectiveness of PBPs is not too sensitive to the threshold for quality required to receive the bonus, while the magnitude of the bonus must be large enough to make the reward salient. We also present examples of tasks for which PBPs do not improve quality. Our results suggest that for PBPs to improve quality, the task must be effort-responsive: the task must allow workers to produce higher quality work by exerting more effort. We also give a simple method to determine if a task is effort-responsive a priori. Furthermore, our experiments suggest that all payments on Mechanical Turk are, to some degree, implicitly performance-based in that workers believe their work may be rejected if their performance is sufficiently poor. Finally, we propose a new model of worker behavior that extends the standard principal-agent model from economics to include a worker's subjective beliefs about his likelihood of being paid, and show that the predictions of this model are in line with our experimental findings. This model may be useful as a foundation for theoretical studies of incentives in crowdsourcing markets.",
                        "Citation Paper Authors": "Authors:Chien-Ju Ho, Aleksandrs Slivkins, Siddharth Suri, Jennifer Wortman Vaughan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1908.08931v1": {
            "Paper Title": "Machine Teaching by Domain Experts: Towards More Humane,Inclusive, and\n  Intelligent Machine Learning Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.07047v1": {
            "Paper Title": "Mobile community sensing with smallholder farmers in a developing\n  nation; A scaled pilot for crop health monitoring",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.02486v2": {
            "Paper Title": "A Visual Programming Paradigm for Abstract Deep Learning Model\n  Development",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.06922v1": {
            "Paper Title": "Thumbnails for Data Stories: A Survey of Current Practices",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.06484v1": {
            "Paper Title": "A Software to Detect OCC Emotion, Big-Five Personality and Hofstede\n  Cultural Dimensions of Pedestrians from Video Sequences",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": ", present a system of automatic personality screening from video\npresentations in order to make a decision whether a person has to be invited to a job interview based on visual,\naudio and lexical cues. As we presented in ",
                    "Citation Text": "R. M. Favaretto, L. Dihl, S. R. Musse, F. Vilanova, and A. B. Costa. Using big \ufb01ve personality model to\ndetect cultural aspects in crowds. In 2017 30th SIBGRAPI Conference on Graphics, Patterns and Images\n(SIBGRAPI) , pages 223\u2013229, Oct 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.01688",
                        "Citation Paper Title": "Title:Using Big Five Personality Model to Detect Cultural Aspects in Crowds",
                        "Citation Paper Abstract": "Abstract:The use of information technology in the study of human behavior is a subject of great scientific interest. Cultural and personality aspects are factors that influence how people interact with one another in a crowd. This paper presents a methodology to detect cultural characteristics of crowds in video sequences. Based on filmed sequences, pedestrians are detected, tracked and characterized. Such information is then used to find out cultural differences in those videos, based on the Big-five personality model. Regarding cultural differences of each country, results indicate that this model generates coherent information when compared to data provided in literature.",
                        "Citation Paper Authors": "Authors:Rodolfo Migon Favaretto, Leandro Dihl, Soraia Raupp Musse, Felipe Vilanova, Angelo Brandelli Costa"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1908.06404v1": {
            "Paper Title": "Sensors and Game Synchronization for Data Analysis in eSports",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": "Mohammad Ali Sarvghadi and Tat-Chee Wan. Overview of time\nsynchronization protocols in wireless sensor networks. In Electronic\nDesign (ICED), 2014 2nd International Conference on , pages 204\u2013209.\nIEEE, 2014. ",
                    "Citation Text": "Salim el Khediri, Nejah Nasri, Mounir Samet, Anne Wei, and Abden-\nnaceur Kachouri. Analysis study of time synchronization protocols in\nwireless sensor networks. arXiv preprint arXiv:1206.1419 , 2012.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1206.1419",
                        "Citation Paper Title": "Title:Analysis study of time synchronization protocols in wireless sensor networks",
                        "Citation Paper Abstract": "Abstract:One of the main pervasive problems Wireless Sensor Networks (WSN) encounter is to maintain flawless communication sharing and cooperative processing between sensors via radio links to ensure a reliable treatment of information. Many applications based on these WSNs consider local clocks at each sensor node that need to be synchronized to a common notion of time. In this context, the majority of previous researches were focused on the study of protocols, and algorithms that address these issues in order to resolve synchronization problems. Previous fforts and empirical studies in wireless sensor network (WSN) proposed several solutions (algorithms). The focus of this this paper is to examine and evaluate the most important synchronization algorithms based on the positions of various quantitative and qualitative synchronization protocols for energy-efficient information processing and routing in WSNs.",
                        "Citation Paper Authors": "Authors:Salim el Khediri, Nejah Nasri, Mounir Samet, Anne Wei, Abdennaceur Kachouri"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1908.06403v1": {
            "Paper Title": "Towards Understanding of eSports Athletes' Potentialities: The Sensing\n  System for Data Collection and Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.07333v1": {
            "Paper Title": "Fairness Issues in AI Systems that Augment Sensory Abilities",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.05913v1": {
            "Paper Title": "Context-Aware Emotion Recognition Networks",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": "has\nbeen built as a static subset of the AFEW. FER-Wild ",
                    "Citation Text": "Ali Mollahosseini, Behzad Hasani, Michelle J Salvador, Ho-\njjat Abdollahi, David Chan, and Mohammad H Mahoor. Fa-\ncial expression recognition from world wild web. In: CVPR\nWork. , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.03639",
                        "Citation Paper Title": "Title:Facial Expression Recognition from World Wild Web",
                        "Citation Paper Abstract": "Abstract:Recognizing facial expression in a wild setting has remained a challenging task in computer vision. The World Wide Web is a good source of facial images which most of them are captured in uncontrolled conditions. In fact, the Internet is a Word Wild Web of facial images with expressions. This paper presents the results of a new study on collecting, annotating, and analyzing wild facial expressions from the web. Three search engines were queried using 1250 emotion related keywords in six different languages and the retrieved images were mapped by two annotators to six basic expressions and neutral. Deep neural networks and noise modeling were used in three different training scenarios to find how accurately facial expressions can be recognized when trained on noisy images collected from the web using query terms (e.g. happy face, laughing man, etc)? The results of our experiments show that deep neural networks can recognize wild facial expressions with an accuracy of 82.12%.",
                        "Citation Paper Authors": "Authors:Ali Mollahosseini, Behzad Hassani, Michelle J. Salvador, Hojjat Abdollahi, David Chan, Mohammad H. Mahoor"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1908.05902v1": {
            "Paper Title": "MFA is a Waste of Time! Understanding Negative Connotation Towards MFA\n  Applications via User Generated Content",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.05901v1": {
            "Paper Title": "Evaluating User Perception of Multi-Factor Authentication: A Systematic\n  Review",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.05897v1": {
            "Paper Title": "All About Phishing: Exploring User Research through a Systematic\n  Literature Review",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.05409v1": {
            "Paper Title": "When Your Friends Become Sellers: An Empirical Study of Social Commerce\n  Site Beidian",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.04342v2": {
            "Paper Title": "Why Does a Visual Question Have Different Answers?",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": "dataset for comparison. It dif-\nfers from VizWiz in part because the images and questions\nwere created separately. The images originate from the MS-\nCOCO dataset ",
                    "Citation Text": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll \u00b4ar, and C Lawrence\nZitnick. Microsoft COCO: Common objects in context. In\nEuropean Conference on Computer Vision , pages 740\u2013755.\nSpringer, 2014. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1405.0312",
                        "Citation Paper Title": "Title:Microsoft COCO: Common Objects in Context",
                        "Citation Paper Abstract": "Abstract:We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
                        "Citation Paper Authors": "Authors:Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Doll\u00e1r"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": ". Other methods recognize to what extent vi-\nsual content will evoke people\u2019s subjective perceptions, in-\ncluding for visual humor ",
                    "Citation Text": "Arjun Chandrasekaran, Ashwin K. Vijayakumar, Stanislaw\nAntol, Mohit Bansal, Dhruv Batra, C. Lawrence Zitnick, and\nDevi Parikh. We are humor beings: Understanding and pre-\ndicting visual humor. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , pages 4603\u2013\n4612, 2016. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.04407",
                        "Citation Paper Title": "Title:We Are Humor Beings: Understanding and Predicting Visual Humor",
                        "Citation Paper Abstract": "Abstract:Humor is an integral part of human lives. Despite being tremendously impactful, it is perhaps surprising that we do not have a detailed understanding of humor yet. As interactions between humans and AI systems increase, it is imperative that these systems are taught to understand subtleties of human expressions such as humor. In this work, we are interested in the question - what content in a scene causes it to be funny? As a first step towards understanding visual humor, we analyze the humor manifested in abstract scenes and design computational models for them. We collect two datasets of abstract scenes that facilitate the study of humor at both the scene-level and the object-level. We analyze the funny scenes and explore the different types of humor depicted in them via human studies. We model two tasks that we believe demonstrate an understanding of some aspects of visual humor. The tasks involve predicting the funniness of a scene and altering the funniness of a scene. We show that our models perform well quantitatively, and qualitatively through human studies. Our datasets are publicly available.",
                        "Citation Paper Authors": "Authors:Arjun Chandrasekaran, Ashwin K. Vijayakumar, Stanislaw Antol, Mohit Bansal, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1908.01536v2": {
            "Paper Title": "Discriminating Spatial and Temporal Relevance in Deep Taylor\n  Decompositions for Explainable Activity Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.05108v1": {
            "Paper Title": "WiFi-based Real-time Breathing and Heart Rate Monitoring during Sleep",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.04692v1": {
            "Paper Title": "General Hand Guidance Framework using Microsoft HoloLens",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.04629v1": {
            "Paper Title": "Evaluation of a Recommender System for Assisting Novice Game Designers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.07716v2": {
            "Paper Title": "Conditional Parallel Coordinates",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.04087v1": {
            "Paper Title": "Fast Adaptation with Meta-Reinforcement Learning for Trust Modelling in\n  Human-Robot Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.06171v1": {
            "Paper Title": "SleepGuardian: An RF-based Healthcare System Guarding Your Sleep from\n  Afar",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.03380v1": {
            "Paper Title": "MakeSense: An IoT Testbed for Social Research of Indoor Activities",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.04237v1": {
            "Paper Title": "Crowdsourcing real-time viral disease and pest information. A case of\n  nation-wide cassava disease surveillance in a developing country",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.11008v3": {
            "Paper Title": "DeepWait: Pedestrian Wait Time Estimation in Mixed Traffic Conditions\n  Using Deep Survival Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.02641v3": {
            "Paper Title": "An Extensible Interactive Interface for Agent Design",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.02627v1": {
            "Paper Title": "Speculative Execution for Guided Visual Analytics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.02502v1": {
            "Paper Title": "Task-Oriented Optimal Sequencing of Visualization Charts",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.02432v1": {
            "Paper Title": "DronePick: Object Picking and Delivery Teleoperation with the Drone\n  Controlled by a Wearable Tactile Display",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.02412v1": {
            "Paper Title": "From Crowdsourcing to Crowdmining: Using Implicit Human Intelligence for\n  Better Understanding of Crowdsourced Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.02402v1": {
            "Paper Title": "Flexibly-Structured Model for Task-Oriented Dialogues",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.01697v1": {
            "Paper Title": "Why Authors Don't Visualize Uncertainty",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.01523v1": {
            "Paper Title": "3D Reconstruction of Deformable Revolving Object under Heavy Hand\n  Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.05240v1": {
            "Paper Title": "Epistemological approach in immersive virtual environments and the\n  neurophysiology learning process",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.01133v1": {
            "Paper Title": "Machinic Surrogates: Human-Machine Relationships in Computational\n  Creativity",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": "The machine learning back-end leverages an Autoen-coder (AE) which was originally developed by Achlioptas et al. ",
                    "Citation Text": "P. Achlioptas, O. Diamanti, I. Mitliagkas and L. Guibas, \"Learning Representations and Generative Models for 3D Point Clouds,\" in Proceedings of the 35th International Conference on Machine Learning, in PMLR, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.02392",
                        "Citation Paper Title": "Title:Learning Representations and Generative Models for 3D Point Clouds",
                        "Citation Paper Abstract": "Abstract:Three-dimensional geometric data offer an excellent domain for studying representation learning and generative modeling. In this paper, we look at geometric data represented as point clouds. We introduce a deep AutoEncoder (AE) network with state-of-the-art reconstruction quality and generalization ability. The learned representations outperform existing methods on 3D recognition tasks and enable shape editing via simple algebraic manipulations, such as semantic part editing, shape analogies and shape interpolation, as well as shape completion. We perform a thorough study of different generative models including GANs operating on the raw point clouds, significantly improved GANs trained in the fixed latent space of our AEs, and Gaussian Mixture Models (GMMs). To quantitatively evaluate generative models we introduce measures of sample fidelity and diversity based on matchings between sets of point clouds. Interestingly, our evaluation of generalization, fidelity and diversity reveals that GMMs trained in the latent space of our AEs yield the best results overall.",
                        "Citation Paper Authors": "Authors:Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, Leonidas Guibas"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1902.03501v2": {
            "Paper Title": "Assessing the Local Interpretability of Machine Learning Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.00948v1": {
            "Paper Title": "High-Level Control of Drum Track Generation Using Learned Patterns of\n  Rhythmic Interaction",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ", or condition-\ning on user-provided information (such as a style label ",
                    "Citation Text": "H. H. Mao, T. Shin, and G. Cottrell, \u201cDeepj: Style-\nspeci\ufb01c music generation,\u201d in 2018 IEEE 12th Inter-\nnational Conference on Semantic Computing (ICSC) .\nIEEE, 2018, pp. 377\u2013382.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.00887",
                        "Citation Paper Title": "Title:DeepJ: Style-Specific Music Generation",
                        "Citation Paper Abstract": "Abstract:Recent advances in deep neural networks have enabled algorithms to compose music that is comparable to music composed by humans. However, few algorithms allow the user to generate music with tunable parameters. The ability to tune properties of generated music will yield more practical benefits for aiding artists, filmmakers, and composers in their creative tasks. In this paper, we introduce DeepJ - an end-to-end generative model that is capable of composing music conditioned on a specific mixture of composer styles. Our innovations include methods to learn musical style and music dynamics. We use our model to demonstrate a simple technique for controlling the style of generated music as a proof of concept. Evaluation of our model using human raters shows that we have improved over the Biaxial LSTM approach.",
                        "Citation Paper Authors": "Authors:Huanru Henry Mao, Taylor Shin, Garrison W. Cottrell"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1908.00680v1": {
            "Paper Title": "Designing for Mobile and Immersive Visual Analytics in the Field",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.00679v1": {
            "Paper Title": "Investigating Direct Manipulation of Graphical Encodings as a Method for\n  User Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.00661v1": {
            "Paper Title": "Common Fate for Animated Transitions in Visualization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.00629v1": {
            "Paper Title": "Color Crafting: Automating the Construction of Designer Quality Color\n  Ramps",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.00576v1": {
            "Paper Title": "Evaluating Ordering Strategies of Star Glyph Axes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.00494v1": {
            "Paper Title": "POINTS -- Playful objects for inclusive, personalized movement games",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.00475v1": {
            "Paper Title": "Semantic Concept Spaces: Guided Topic Model Refinement using\n  Word-Embedding Projections",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.00456v1": {
            "Paper Title": "Auditing News Curation Systems: A Case Study Examining Algorithmic and\n  Editorial Logic in Apple News",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.00379v1": {
            "Paper Title": "Outstanding: A Multi-Perspective Travel Approach for Virtual Reality\n  Games",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.00286v1": {
            "Paper Title": "Reinforcement Learning for Personalized Dialogue Management",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.00234v1": {
            "Paper Title": "Cultural association based on machine learning for team formation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.00215v1": {
            "Paper Title": "Illusion of Causality in Visualized Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.00192v1": {
            "Paper Title": "Data Changes Everything: Challenges and Opportunities in Data\n  Visualization Design Handoff",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.05681v2": {
            "Paper Title": "Knitting Skeletons: A Computer-Aided Design Tool for Shaping and\n  Patterning of Knitted Garments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.00073v1": {
            "Paper Title": "Biased Average Position Estimates in Line and Bar Graphs:\n  Underestimation, Overestimation, and Perceptual Pull",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.13285v1": {
            "Paper Title": "I-Keyboard: Fully Imaginary Keyboard on Touch Devices Empowered by Deep\n  Neural Decoder",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": ": RNN (uni-rnn), bi-\ndirectional RNN (bi-rnn), encoder-decoder attention (seq2seq) ",
                    "Citation Text": "M.-T. Luong, H. Pham, and C. D. Manning, \u201cEffective ap-\nproaches to attention-based neural machine translation,\u201d arXiv preprint\narXiv:1508.04025 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1508.04025",
                        "Citation Paper Title": "Title:Effective Approaches to Attention-based Neural Machine Translation",
                        "Citation Paper Abstract": "Abstract:An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.",
                        "Citation Paper Authors": "Authors:Minh-Thang Luong, Hieu Pham, Christopher D. Manning"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1907.13214v1": {
            "Paper Title": "Alexa as a CALL platform for children: Where do we start?",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.13187v1": {
            "Paper Title": "CloudDet: Interactive Visual Analysis of Anomalous Performances in Cloud\n  Computing Systems",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": ", were discarded as their\nexecution times are much longer than that of our proposed algorithm.\nWe used the implementation of these models in the scikit-learn ",
                    "Citation Text": "F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel,\nM. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg, et al. Scikit-learn:\nMachine learning in python. Journal of machine learning research ,\n12(Oct):2825\u20132830, 2011.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1201.0490",
                        "Citation Paper Title": "Title:Scikit-learn: Machine Learning in Python",
                        "Citation Paper Abstract": "Abstract:Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from this http URL.",
                        "Citation Paper Authors": "Authors:Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Andreas M\u00fcller, Joel Nothman, Gilles Louppe, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, \u00c9douard Duchesnay"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1908.00387v1": {
            "Paper Title": "Ablate, Variate, and Contemplate: Visual Analytics for Discovering\n  Neural Architectures",
            "Sentences": [
                {
                    "Sentence ID": 46,
                    "Sentence": ": structural and post-hoc. Their respective projections\nare seen in Figure 4b, with the same model highlighted in orange in both\nprojections. 2-D Projections are generated from distance metrics using\nscikit-learn \u2019s implementation of Multidimensional Scaling ",
                    "Citation Text": "F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel,\nM. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg, et al. Scikit-learn:\nMachine learning in python. Journal of machine learning research ,\n12(Oct):2825\u20132830, 2011.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1201.0490",
                        "Citation Paper Title": "Title:Scikit-learn: Machine Learning in Python",
                        "Citation Paper Abstract": "Abstract:Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from this http URL.",
                        "Citation Paper Authors": "Authors:Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Andreas M\u00fcller, Joel Nothman, Gilles Louppe, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, \u00c9douard Duchesnay"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1907.12748v1": {
            "Paper Title": "Influence Flowers of Academic Entities",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.00181v1": {
            "Paper Title": "EcoLens: Visual Analysis of Urban Region Dynamics Using Traffic Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.11498v2": {
            "Paper Title": "Personality is Revealed During Weekends: Towards Data Minimisation for\n  Smartphone Based Personality Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.12079v1": {
            "Paper Title": "TopicSifter: Interactive Search Space Reduction Through Targeted Topic\n  Modeling",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.12001v1": {
            "Paper Title": "Towards Understanding and Modeling Empathy for Use in Motivational\n  Design Thinking",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.11989v1": {
            "Paper Title": "Optimizing Energy Efficiency of Wearable Sensors Using Fog-assisted\n  Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.11656v1": {
            "Paper Title": "Vocal Interactivity in Crowds, Flocks and Swarms: Implications for Voice\n  User Interfaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.11260v1": {
            "Paper Title": "When Human-Computer Interaction Meets Community Citizen Science",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.11184v1": {
            "Paper Title": "HEIDL: Learning Linguistic Expressions with Deep Learning and\n  Human-in-the-Loop",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.11115v1": {
            "Paper Title": "Accurate and Robust Eye Contact Detection During Everyday Mobile Device\n  Interactions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.10917v1": {
            "Paper Title": "An EMG-based Eating Behaviour Monitoring System with Haptic Feedback to\n  Promote Mindful Eating",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.10781v1": {
            "Paper Title": "INS: An Interactive Chinese News Synthesis System",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.10739v1": {
            "Paper Title": "Visual Interaction with Deep Learning Models through Collaborative\n  Semantic Inference",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.05507v2": {
            "Paper Title": "Collaborative Multi-Agent Dialogue Model Training Via Reinforcement\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.10036v1": {
            "Paper Title": "Happiness Entailment: Automating Suggestions for Well-Being",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.09873v1": {
            "Paper Title": "An extended framework for characterizing social robots",
            "Sentences": [
                {
                    "Sentence ID": 128,
                    "Sentence": ". Moreover, novel paradigms for robot hardware are emerging with soft\nrobotics ",
                    "Citation Text": ".28 Kim Baraka, Patr\u00edcia Alves-Oliveira, and Tiago Ribeiro\nExamples of pHRI include physically assistive applications, where a robot has to\nbe in physical contact with the person to execute its tasks, such as getting patients out\nof a chair",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.09873",
                        "Citation Paper Title": "Title:An extended framework for characterizing social robots",
                        "Citation Paper Abstract": "Abstract:Social robots are becoming increasingly diverse in their design, behavior, and usage. In this chapter, we provide a broad-ranging overview of the main characteristics that arise when one considers social robots and their interactions with humans. We specifically contribute a framework for characterizing social robots along 7 dimensions that we found to be most relevant to their design. These dimensions are: appearance, social capabilities, purpose and application area, relational role, autonomy and intelligence, proximity, and temporal profile. Within each dimension, we account for the variety of social robots through a combination of classifications and/or explanations. Our framework builds on and goes beyond existing frameworks, such as classifications and taxonomies found in the literature. More specifically, it contributes to the unification, clarification, and extension of key concepts, drawing from a rich body of relevant literature. This chapter is meant to serve as a resource for researchers, designers, and developers within and outside the field of social robotics. It is intended to provide them with tools to better understand and position existing social robots, as well as to inform their future design.",
                        "Citation Paper Authors": "Authors:Kim Baraka, Patr\u00edcia Alves-Oliveira, Tiago Ribeiro"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1907.09594v1": {
            "Paper Title": "Understanding the Political Ideology of Legislators from Social Media\n  Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.09091v1": {
            "Paper Title": "Text-to-Viz: Automatic Generation of Infographics from\n  Proportion-Related Natural Language Statements",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.08345v2": {
            "Paper Title": "Liger: Combining Interaction Paradigms for Visual Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.08977v1": {
            "Paper Title": "Systematic Enhancement of Functional Connectivity in Brain-Computer\n  Interfacing using Common Spatial Patterns and Tangent Space Mapping",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.00483v2": {
            "Paper Title": "Effects of Foraging in Personalized Content-based Image Recommendation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.08661v1": {
            "Paper Title": "Sound Search by Text Description or Vocal Imitation?",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.08586v1": {
            "Paper Title": "CityScopeAR: Urban Design and Crowdsourced Engagement Platform",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.12899v1": {
            "Paper Title": "A Physical Testbed for Intelligent Transportation Systems",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": "to ours involves\nresearching autonomous driving with \ufb02eets of small-scale toycars ",
                    "Citation Text": "N. Hyldmar, Y . He, and A. Prorok, A Fleet of Miniature Cars for Ex-\nperiments in Cooperative Driving, arXiv:1902.06133v1 [cs.RO], 2019.\n[Online]. Available: http://arxiv.org/abs/1902.06133",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.06133",
                        "Citation Paper Title": "Title:A Fleet of Miniature Cars for Experiments in Cooperative Driving",
                        "Citation Paper Abstract": "Abstract:We introduce a unique experimental testbed that consists of a fleet of 16 miniature Ackermann-steering vehicles. We are motivated by a lack of available low-cost platforms to support research and education in multi-car navigation and trajectory planning. This article elaborates the design of our miniature robotic car, the Cambridge Minicar, as well as the fleet's control architecture. Our experimental testbed allows us to implement state-of-the-art driver models as well as autonomous control strategies, and test their validity in a real, physical multi-lane setup. Through experiments on our miniature highway, we are able to tangibly demonstrate the benefits of cooperative driving on multi-lane road topographies. Our setup paves the way for indoor large-fleet experimental research.",
                        "Citation Paper Authors": "Authors:Nicholas Hyldmar, Yijun He, Amanda Prorok"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1907.08478v1": {
            "Paper Title": "Interactive Learning of Environment Dynamics for Sequential Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.09293v1": {
            "Paper Title": "DREAMT -- Embodied Motivational Conversational Storytelling",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.08325v1": {
            "Paper Title": "Scalable Topological Data Analysis and Visualization for Evaluating\n  Data-Driven Models in Scientific Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.08153v1": {
            "Paper Title": "ReconViguRation: Reconfiguring Physical Keyboards in Virtual Reality",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.11007v2": {
            "Paper Title": "A Case Study of Trust on Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.07861v1": {
            "Paper Title": "Jo: The Smart Journal",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.07717v1": {
            "Paper Title": "Revealing the Role of User Moods in Struggling Search Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.07466v1": {
            "Paper Title": "Beyond Human: Animals as an Escape from Stereotype Avatars in Virtual\n  Reality Games",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": "further analyzed our ability to inhabit nonhumanoid\navatars that have additional body parts.\nClosely related to our research, the works-in-progress paper\nby Krekhov et al. ",
                    "Citation Text": "Andrey Krekhov, Sebastian Cmentowski, and Jens\nKr\u00fcger. 2018a. VR Animals: Surreal Body Ownership\nin Virtual Reality Games. In Extended Abstracts\nPublication of the Annual Symposium on\nComputer-Human Interaction in Play . ACM, to appear.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.07466",
                        "Citation Paper Title": "Title:Beyond Human: Animals as an Escape from Stereotype Avatars in Virtual Reality Games",
                        "Citation Paper Abstract": "Abstract:Virtual reality setups are particularly suited to create a tight bond between players and their avatars up to a degree where we start perceiving the virtual representation as our own body. We hypothesize that such an illusion of virtual body ownership (IVBO) has a particularly high, yet overlooked potential for nonhumanoid avatars. To validate our claim, we use the example of three very different creatures---a scorpion, a rhino, and a bird---to explore possible avatar controls and game mechanics based on specific animal abilities. A quantitative evaluation underpins the high game enjoyment arising from embodying such nonhuman morphologies, including additional body parts and obtaining respective superhuman skills, which allows us to derive a set of novel design implications. Furthermore, the experiment reveals a correlation between IVBO and game enjoyment, which is a further indication that nonhumanoid creatures offer a meaningful design space for VR games worth further investigation.",
                        "Citation Paper Authors": "Authors:Andrey Krekhov, Sebastian Cmentowski, Katharina Emmerich, Jens Kr\u00fcger"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "by reducing the\ncognitive mismatch between physical and visual feedback. If\nplayers stopped waving their arms mid-air, a \u201cfalling\u201d proce-\ndure was applied. That transition was performed rapidly to\nprevent cybersickness ",
                    "Citation Text": "Andrey Krekhov, Sebastian Cmentowski, Katharina\nEmmerich, Maic Masuch, and Jens Kr\u00fcger. 2018b.\nGulliVR: A Walking-Oriented Technique for Navigation\nin Virtual Reality Games Based on Virtual Body\nResizing. In Proceedings of the 2018 Annual Symposium\non Computer-Human Interaction in Play (CHI PLAY\n\u201918). ACM, New York, NY , USA, 243\u2013256. DOI:\nhttp://dx.doi.org/10.1145/3242671.3242704",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.07466",
                        "Citation Paper Title": "Title:Beyond Human: Animals as an Escape from Stereotype Avatars in Virtual Reality Games",
                        "Citation Paper Abstract": "Abstract:Virtual reality setups are particularly suited to create a tight bond between players and their avatars up to a degree where we start perceiving the virtual representation as our own body. We hypothesize that such an illusion of virtual body ownership (IVBO) has a particularly high, yet overlooked potential for nonhumanoid avatars. To validate our claim, we use the example of three very different creatures---a scorpion, a rhino, and a bird---to explore possible avatar controls and game mechanics based on specific animal abilities. A quantitative evaluation underpins the high game enjoyment arising from embodying such nonhuman morphologies, including additional body parts and obtaining respective superhuman skills, which allows us to derive a set of novel design implications. Furthermore, the experiment reveals a correlation between IVBO and game enjoyment, which is a further indication that nonhumanoid creatures offer a meaningful design space for VR games worth further investigation.",
                        "Citation Paper Authors": "Authors:Andrey Krekhov, Sebastian Cmentowski, Katharina Emmerich, Jens Kr\u00fcger"
                    }
                },
                {
                    "Sentence ID": 75,
                    "Sentence": ". However, typical room-\nscale VR equipment tracks only the players\u2019 heads and hands.\nWe see three approaches to overcome that barrier: relying\non only three tracked positions, including markerless track-\ning ",
                    "Citation Text": "W. Xu, A. Chatterjee, M. Zollhofer, H. Rhodin, P. Fua,\nH. Seidel, and C. Theobalt. 2019. Mo2Cap2: Real-time\nMobile 3D Motion Capture with a Cap-mounted Fisheye\nCamera. IEEE Transactions on Visualization and\nComputer Graphics (2019), 1\u20131. DOI:\nhttp://dx.doi.org/10.1109/TVCG.2019.2898650",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.05959",
                        "Citation Paper Title": "Title:Mo2Cap2: Real-time Mobile 3D Motion Capture with a Cap-mounted Fisheye Camera",
                        "Citation Paper Abstract": "Abstract:We propose the first real-time approach for the egocentric estimation of 3D human body pose in a wide range of unconstrained everyday activities. This setting has a unique set of challenges, such as mobility of the hardware setup, and robustness to long capture sessions with fast recovery from tracking failures. We tackle these challenges based on a novel lightweight setup that converts a standard baseball cap to a device for high-quality pose estimation based on a single cap-mounted fisheye camera. From the captured egocentric live stream, our CNN based 3D pose estimation approach runs at 60Hz on a consumer-level GPU. In addition to the novel hardware setup, our other main contributions are: 1) a large ground truth training corpus of top-down fisheye images and 2) a novel disentangled 3D pose estimation approach that takes the unique properties of the egocentric viewpoint into account. As shown by our evaluation, we achieve lower 3D joint error as well as better 2D overlay than the existing baselines.",
                        "Citation Paper Authors": "Authors:Weipeng Xu, Avishek Chatterjee, Michael Zollhoefer, Helge Rhodin, Pascal Fua, Hans-Peter Seidel, Christian Theobalt"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1907.07232v1": {
            "Paper Title": "A Novel Slip-Kalman Filter to Track the Progression of Reading Through\n  Eye-Gaze Measurements",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.07178v1": {
            "Paper Title": "Mediation Challenges and Socio-Technical Gaps for Explainable Deep\n  Learning Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.07564v1": {
            "Paper Title": "Conversational Help for Task Completion and Feature Discovery in\n  Personal Assistants",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.06809v1": {
            "Paper Title": "Ethical Underpinnings in the Design and Management of ICT Projects",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.06360v1": {
            "Paper Title": "The Elusive Model of Technology, Media, Social Development, and\n  Financial Sustainability",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.06637v1": {
            "Paper Title": "The Bach Doodle: Approachable music composition with machine learning at\n  scale",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.06279v1": {
            "Paper Title": "Discourse Behavior of Older Adults Interacting With a Dialogue Agent\n  Competent in Multiple Topics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.05931v1": {
            "Paper Title": "An Exploratory Study of Live-Streamed Programming",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.05800v1": {
            "Paper Title": "Find It: A Novel Way to Learn Through Play",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.05454v1": {
            "Paper Title": "Data by Proxy -- Material Traces as Autographic Visualizations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.04191v2": {
            "Paper Title": "Belief places and spaces: Mapping cognitive environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.05220v1": {
            "Paper Title": "The Illusion of Animal Body Ownership and Its Potential for Virtual\n  Reality Games",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.05131v1": {
            "Paper Title": "PreCall: A Visual Interface for Threshold Optimization in ML Model\n  Selection",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.03518v1": {
            "Paper Title": "Real-Time Elderly Healthcare Monitoring Expert System Using Wireless\n  Sensor Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.11510v1": {
            "Paper Title": "AVEC 2019 Workshop and Challenge: State-of-Mind, Detecting Depression\n  with AI, and Cross-Cultural Affect Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.04702v1": {
            "Paper Title": "Deadeye Visualization Revisited: Investigation of Preattentiveness and\n  Applicability in Virtual Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.04104v1": {
            "Paper Title": "User Guidance for Interactive Camera Calibration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.03877v1": {
            "Paper Title": "Pitako -- Recommending Game Design Elements in Cicero",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.12364v1": {
            "Paper Title": "EyeSec: A Retrofittable Augmented Reality Tool for Troubleshooting\n  Wireless Sensor Networks in the Field",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.03334v1": {
            "Paper Title": "Case-Based Reasoning for Assisting Domain Experts in Processing Fraud\n  Alerts of Black-Box Machine Learning Models",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": ". Additionally, we evalu-\nate our approach on a real-life Fraud Detection data set provided\nby a major Dutch bank. On each of the data set, we train a random\nforest classifier using the implementation in scikit-learn ",
                    "Citation Text": "F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M.\nBlondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour-\nnapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine\nLearning in Python. Journal of Machine Learning Research 12 (2011), 2825\u20132830.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1201.0490",
                        "Citation Paper Title": "Title:Scikit-learn: Machine Learning in Python",
                        "Citation Paper Abstract": "Abstract:Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from this http URL.",
                        "Citation Paper Authors": "Authors:Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Andreas M\u00fcller, Joel Nothman, Gilles Louppe, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, \u00c9douard Duchesnay"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": ". In our experiments, we consider several\ndistance functions, including unweighted, locally weighted, and\nglobally weighted functions.\nSimilar to our goal, Jiang et al . ",
                    "Citation Text": "Heinrich Jiang, Been Kim, Melody Guan, and Maya Gupta. 2018. To Trust Or Not\nTo Trust A Classifier. In Advances in Neural Information Processing Systems 31 ,\nS. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett\n(Eds.). Curran Associates, Inc., 5541\u20135552.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.11783",
                        "Citation Paper Title": "Title:To Trust Or Not To Trust A Classifier",
                        "Citation Paper Abstract": "Abstract:Knowing when a classifier's prediction can be trusted is useful in many applications and critical for safely using AI. While the bulk of the effort in machine learning research has been towards improving classifier performance, understanding when a classifier's predictions should and should not be trusted has received far less attention. The standard approach is to use the classifier's discriminant or confidence score; however, we show there exists an alternative that is more effective in many situations. We propose a new score, called the trust score, which measures the agreement between the classifier and a modified nearest-neighbor classifier on the testing example. We show empirically that high (low) trust scores produce surprisingly high precision at identifying correctly (incorrectly) classified examples, consistently outperforming the classifier's confidence score as well as many other baselines. Further, under some mild distributional assumptions, we show that if the trust score for an example is high (low), the classifier will likely agree (disagree) with the Bayes-optimal classifier. Our guarantees consist of non-asymptotic rates of statistical consistency under various nonparametric settings and build on recent developments in topological data analysis.",
                        "Citation Paper Authors": "Authors:Heinrich Jiang, Been Kim, Melody Y. Guan, Maya Gupta"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1907.03324v1": {
            "Paper Title": "A Human-Grounded Evaluation of SHAP for Alert Processing",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": ". SHAP values were computed using the exact TreeSHAP algo-\nrithm proposed and implemented by Lundberg et al. ",
                    "Citation Text": "Scott M. Lundberg, Gabriel G. Erion, and Su-In Lee. 2018. Consistent Individual-\nized Feature Attribution for Tree Ensembles. arXiv:1802.03888",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.03888",
                        "Citation Paper Title": "Title:Consistent Individualized Feature Attribution for Tree Ensembles",
                        "Citation Paper Abstract": "Abstract:Interpreting predictions from tree ensemble methods such as gradient boosting machines and random forests is important, yet feature attribution for trees is often heuristic and not individualized for each prediction. Here we show that popular feature attribution methods are inconsistent, meaning they can lower a feature's assigned importance when the true impact of that feature actually increases. This is a fundamental problem that casts doubt on any comparison between features. To address it we turn to recent applications of game theory and develop fast exact tree solutions for SHAP (SHapley Additive exPlanation) values, which are the unique consistent and locally accurate attribution values. We then extend SHAP values to interaction effects and define SHAP interaction values. We propose a rich visualization of individualized feature attributions that improves over classic attribution summaries and partial dependence plots, and a unique \"supervised\" clustering (clustering based on feature attributions). We demonstrate better agreement with human intuition through a user study, exponential improvements in run time, improved clustering performance, and better identification of influential features. An implementation of our algorithm has also been merged into XGBoost and LightGBM, see this http URL for details.",
                        "Citation Paper Authors": "Authors:Scott M. Lundberg, Gabriel G. Erion, Su-In Lee"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": ". The related classification\ntask is to predict whether the income of a person exceeds $50,000\nper year based on census data. We select five features and train a\nrandom forest classifier using the implementation of scikit-learn ",
                    "Citation Text": "F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M.\nBlondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour-\nnapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine\nLearning in Python. Journal of Machine Learning Research 12 (2011), 2825\u20132830.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1201.0490",
                        "Citation Paper Title": "Title:Scikit-learn: Machine Learning in Python",
                        "Citation Paper Abstract": "Abstract:Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from this http URL.",
                        "Citation Paper Authors": "Authors:Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Andreas M\u00fcller, Joel Nothman, Gilles Louppe, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, \u00c9douard Duchesnay"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1907.03263v1": {
            "Paper Title": "Ensuring Responsible Outcomes from Technology",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.11039v1": {
            "Paper Title": "Visualization of Emergency Department Clinical Data for Interpretable\n  Patient Phenotyping",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.09802v3": {
            "Paper Title": "Generalized Multiple Correlation Coefficient as a Similarity\n  Measurements between Trajectories",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.13313v5": {
            "Paper Title": "Technical Report of the Video Event Reconstruction and Analysis (VERA)\n  System -- Shooter Localization, Models, Interface, and Beyond",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.02426v1": {
            "Paper Title": "Multimodal Uncertainty Reduction for Intention Recognition in\n  Human-Robot Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.02349v1": {
            "Paper Title": "Experience Management in Multi-player Games",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.10594v1": {
            "Paper Title": "Synchronizing Geospatial Information for Personalized Health Monitoring",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.02102v1": {
            "Paper Title": "EVA: Generating Emotional Behavior of Virtual Agents using Expressive\n  Features of Gait and Gaze",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": ": This motion-captured dataset contains 49gaits obtained\nfrom subjects walking with different styles.\n\u2022EWalk : This dataset contains gaits extracted from 94RGB videos\nusing state-of-the-art 3D pose estimation ",
                    "Citation Text": "Rishabh Dabral, Anurag Mundhada, Uday Kusupati, Safeer Afaque, Abhishek\nSharma, and Arjun Jain. 2018. Learning 3d human pose from structure and\nmotion. In Proceedings of the European Conference on Computer Vision (ECCV) .\n668\u2013683.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.09250",
                        "Citation Paper Title": "Title:Learning 3D Human Pose from Structure and Motion",
                        "Citation Paper Abstract": "Abstract:3D human pose estimation from a single image is a challenging problem, especially for in-the-wild settings due to the lack of 3D annotated data. We propose two anatomically inspired loss functions and use them with a weakly-supervised learning framework to jointly learn from large-scale in-the-wild 2D and indoor/synthetic 3D data. We also present a simple temporal network that exploits temporal and structural cues present in predicted pose sequences to temporally harmonize the pose estimations. We carefully analyze the proposed contributions through loss surface visualizations and sensitivity analysis to facilitate deeper understanding of their working mechanism. Our complete pipeline improves the state-of-the-art by 11.8% and 12% on Human3.6M and MPI-INF-3DHP, respectively, and runs at 30 FPS on a commodity graphics card.",
                        "Citation Paper Authors": "Authors:Rishabh Dabral, Anurag Mundhada, Uday Kusupati, Safeer Afaque, Abhishek Sharma, Arjun Jain"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1907.01423v1": {
            "Paper Title": "Enhancing Email Functionality using Late Bound Content",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.01221v1": {
            "Paper Title": "Visual analytics for team-based invasion sports with significant events\n  and Markov reward process",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "also proposed a rectangle-based subdivision of the playing area for soccer, using rectangles of the same side length. A similar subdivision of the playing area was used by Cervone et al. ",
                    "Citation Text": "D. Cervone, A. D\u2019Amour, L. Bornn, and K. Goldsberry. A multiresolution stochastic process model for predicting basketball possession outcomes. CoRR 1, 1, pp. 1\u201330, 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1408.0777",
                        "Citation Paper Title": "Title:A Multiresolution Stochastic Process Model for Predicting Basketball Possession Outcomes",
                        "Citation Paper Abstract": "Abstract:Basketball games evolve continuously in space and time as players constantly interact with their teammates, the opposing team, and the ball. However, current analyses of basketball outcomes rely on discretized summaries of the game that reduce such interactions to tallies of points, assists, and similar events. In this paper, we propose a framework for using optical player tracking data to estimate, in real time, the expected number of points obtained by the end of a possession. This quantity, called \\textit{expected possession value} (EPV), derives from a stochastic process model for the evolution of a basketball possession; we model this process at multiple levels of resolution, differentiating between continuous, infinitesimal movements of players, and discrete events such as shot attempts and turnovers. Transition kernels are estimated using hierarchical spatiotemporal models that share information across players while remaining computationally tractable on very large data sets. In addition to estimating EPV, these models reveal novel insights on players' decision-making tendencies as a function of their spatial strategy.",
                        "Citation Paper Authors": "Authors:Daniel Cervone, Alex D'Amour, Luke Bornn, Kirk Goldsberry"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": ". Such subdivision of the playing area is very common for team sports and can be found in many other related works ",
                    "Citation Text": "T. Narizuka, K. Yamamoto, and Y. Yamazaki. Statistical properties of position-dependent ball-passing networks in football games. Physica A 412 (Oct. 2014), pp. 157\u2013168, 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1311.0641",
                        "Citation Paper Title": "Title:Statistical properties of position-dependent ball-passing networks in football games",
                        "Citation Paper Abstract": "Abstract:Statistical properties of position-dependent ball-passing networks in real football games are examined. We find that the networks have the small-world property, and their degree distributions are fitted well by a truncated gamma distribution function. In order to reproduce these properties of networks, a model based on a Markov chain is proposed.",
                        "Citation Paper Authors": "Authors:Takuma Narizuka, Ken Yamamoto, Yoshihiro Yamazaki"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1907.01008v1": {
            "Paper Title": "PAGAN: Video Affect Annotation Made Easy",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.00998v1": {
            "Paper Title": "Geographical Security Questions for Fallback Authentication",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.00971v1": {
            "Paper Title": "Universal audio synthesizer control with normalizing flows",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": ". One way to alleviate this problem is to rely\non the Maximum Mean Discrepancy (MMD) instead of the\nKL to regularize the latent space, leading to the Wasser-\nsteinAE (WAE) model ",
                    "Citation Text": "Ilya Tolstikhin, Olivier Bousquet, and Bernhard\nSch\u00f6lkopf, \u201cWasserstein auto-encoders,\u201d Interna-\ntional Conference on Learning Representations , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.01558",
                        "Citation Paper Title": "Title:Wasserstein Auto-Encoders",
                        "Citation Paper Abstract": "Abstract:We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE). This regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality, as measured by the FID score.",
                        "Citation Paper Authors": "Authors:Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, Bernhard Schoelkopf"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "allow to\nmodel highly complex distributions in the latent space. Al-\nthough the use of V AEs for audio applications has only been\nscarcely investigated, Esling et al. ",
                    "Citation Text": "Philippe Esling, Adrien Bitton, and Axel Chemla-\nRomeu-Santos, \u201cGenerative timbre spaces with varia-\ntional audio synthesis,\u201d 21st International DaFX Con-\nference, arXiv:1805.08501 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.08501",
                        "Citation Paper Title": "Title:Generative timbre spaces: regularizing variational auto-encoders with perceptual metrics",
                        "Citation Paper Abstract": "Abstract:Timbre spaces have been used in music perception to study the perceptual relationships between instruments based on dissimilarity ratings. However, these spaces do not generalize to novel examples and do not provide an invertible mapping, preventing audio synthesis. In parallel, generative models have aimed to provide methods for synthesizing novel timbres. However, these systems do not provide an understanding of their inner workings and are usually not related to any perceptually relevant information. Here, we show that Variational Auto-Encoders (VAE) can alleviate all of these limitations by constructing generative timbre spaces. To do so, we adapt VAEs to learn an audio latent space, while using perceptual ratings from timbre studies to regularize the organization of this space. The resulting space allows us to analyze novel instruments, while being able to synthesize audio from any point of this space. We introduce a specific regularization allowing to enforce any given similarity distances onto these spaces. We show that the resulting space provide almost similar distance relationships as timbre spaces. We evaluate several spectral transforms and show that the Non-Stationary Gabor Transform (NSGT) provides the highest correlation to timbre spaces and the best quality of synthesis. Furthermore, we show that these spaces can generalize to novel instruments and can generate any path between instruments to understand their timbre relationships. As these spaces are continuous, we study how audio descriptors behave along the latent dimensions. We show that even though descriptors have an overall non-linear topology, they follow a locally smooth evolution. Based on this, we introduce a method for descriptor-based synthesis and show that we can control the descriptors of an instrument while keeping its timbre structure.",
                        "Citation Paper Authors": "Authors:Philippe Esling, Axel Chemla--Romeu-Santos, Adrien Bitton"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1907.00684v1": {
            "Paper Title": "Enabling Dialogue Management with Dynamically Created Dialogue Actions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.00377v1": {
            "Paper Title": "FVA: Modeling Perceived Friendliness of Virtual Agents Using Movement\n  Characteristics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.00075v1": {
            "Paper Title": "Programming with Timespans in Interactive Visualizations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.00042v1": {
            "Paper Title": "Rhythm Dungeon: A Blockchain-based Music Roguelike Game",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.12088v1": {
            "Paper Title": "Non-user Inclusive Design for Maintaining Harmony of Real-Virtual Human\n  Interaction in Augmented Reality",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.11960v1": {
            "Paper Title": "Studying the Impact of Mood on Identifying Smartphone Users",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.11753v1": {
            "Paper Title": "Dynamic Drawing Guidance via Electromagnetic Haptic Feedback",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.11042v1": {
            "Paper Title": "IDEAL-CITIES: A Trustworthy and Sustainable Framework for Circular Smart\n  Cities",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.13582v2": {
            "Paper Title": "System Administrators Prefer Command Line Interfaces, Don't They? An\n  Exploratory Study of Firewall Interfaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.11123v1": {
            "Paper Title": "What concerns do Chinese parents have about their children's digital\n  adoption and how to better support them?",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": "Jun Zhao, Ulrik Lyngs, and Nigel Shadbolt. 2018. What privacy concerns do parents have\nabout children\u2019s mobile apps, and how can they stay SHARP? CoRR abs/1809.10841 (2018).\nhttp://arxiv.org/abs/1809.10841 ",
                    "Citation Text": "Jun Zhao, Ge Wang, Carys Dally, Petr Slovak, Julian Childs, Max Van Klee, and Nigel Shadbolt.\n2019. \u2018I make up a silly name\u2019: Understanding Children\u2019s Perception of Privacy Risks Online.\nCHI Conference on Human Factors in Computing Systems Proceedings (CHI 2019), May 4\u20139,\n2019, Glasgow, Scotland Uk (2019). DOI:http://dx.doi.org/10.1145/3290605.3300336\n\u2014 KOALA Project Report 3.5 Page 15KOALA Project Report 3.5 April 2019\nFigure 9: More digitally experienced parents expressed more privacy concerns\nbetter labels for this \ufb01gure 7\nFigure 10: children\u2019s online hours and their parents\u2019 awareness of privacy risks\n\u2014 KOALA Project Report 3.5 Page 16KOALA Project Report 3.5 April 2019\nFigure 11: Parents\u2019 level of concern regarding access to devices\u2019 camera, microphone and photo\n(Q24 vs Q25)\nFigure 12: Parents\u2019 level of concern regarding access to devices\u2019 location information (Q24 vs Q25)\n\u2014 KOALA Project Report 3.5 Page 17KOALA Project Report 3.5 April 2019\nFigure 13: Parents\u2019 level of concern regarding access to children\u2019 personal preference information\n(Q24 vs Q25)\nFigure 14: Think about when you are safeguarding your child online, which one below is the current\npractice you have used most frequently? (Q19)\nFigure 15: When you are taking practices to safeguard you child online, what will be their most\ncommon responses? (Q20)\n\u2014 KOALA Project Report 3.5 Page 18KOALA Project Report 3.5 April 2019\nFigure 16: When your child\u2019s app is asking consent to the above information (e.g. camera, micro-\nphone, location information, personal preferences), what do you think your children would do? (Q26)\nFigure 17: Which aspect do you think you need most help with? (Q29)\n\u2014 KOALA Project Report 3.5 Page 19",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.10245",
                        "Citation Paper Title": "Title:`I make up a silly name': Understanding Children's Perception of Privacy Risks Online",
                        "Citation Paper Abstract": "Abstract:Children under 11 are often regarded as too young to comprehend the implications of online privacy. Perhaps as a result, little research has focused on younger kids' risk recognition and coping. Such knowledge is, however, critical for designing efficient safeguarding mechanisms for this age group. Through 12 focus group studies with 29 children aged 6-10 from UK schools, we examined how children described privacy risks related to their use of tablet computers and what information was used by them to identify threats. We found that children could identify and articulate certain privacy risks well, such as information oversharing or revealing real identities online; however, they had less awareness with respect to other risks, such as online tracking or game promotions. Our findings offer promising directions for supporting children's awareness of cyber risks and the ability to protect themselves online.",
                        "Citation Paper Authors": "Authors:Jun Zhao, Ge Wang, Carys Dally, Petr Slovak, Julian Childs, Max Van Klee, Nigel Shadbolt"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "Jun Zhao. 2018. Are Children Well-Supported by Their Parents Concerning Online Privacy\nRisks, and Who Supports the Parents? CoRR abs/1809.10944 (2018). http://arxiv.org/\nabs/1809.10944 ",
                    "Citation Text": "Jun Zhao, Ulrik Lyngs, and Nigel Shadbolt. 2018. What privacy concerns do parents have\nabout children\u2019s mobile apps, and how can they stay SHARP? CoRR abs/1809.10841 (2018).\nhttp://arxiv.org/abs/1809.10841",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.10841",
                        "Citation Paper Title": "Title:What privacy concerns do parents have about children's mobile apps, and how can they stay SHARP?",
                        "Citation Paper Abstract": "Abstract:Tablet computers are widely used by young children. A report in 2016 shows that children aged 5 to 15 years are spending more time online than watching TV. A 2017 update of the same report shows that parents are becoming more concerned about their children's online risks compared to the previous year. Parents are working hard to protect their children's online safety. An increasing number of parents are setting up content filtering at home or having regular discussions with their children regarding online risks. However, although risks related to Social Media platforms or social video sharing sites (like YouTube) are widely known, risks posed by mobile applications or games (i.e. `apps') are less known. Behind the cute characters, apps used by children can not only have the possibility of exposing them to age-inappropriate content or excessive in-app promotions, but may also make a large amount of their personal information accessible to third-party online marketing and advertising industry. Such practices are not unique to children's apps, but young children are probably less capable of resisting the resulting personalised advertisements and game promotions. In this report, we present findings from our online survey of 220 parents with children aged 6-10, mainly from the U.K. and other western countries, regarding their privacy concerns and expectations of their children's use of mobile apps. Parents play a key role in children's use of digital technology, especially for children under 10 years old. Recent reports have highlighted parents' lack of sufficient support for choosing appropriate digital content for their children. Our report sheds some initial light on parents' key struggles and points to immediate steps and possible areas of future development.",
                        "Citation Paper Authors": "Authors:Jun Zhao, Ulrik Lyngs, Nigel Shadbolt"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1906.11004v1": {
            "Paper Title": "Co-Designing in Social VR. Process awareness and suitable\n  representations to empower user participation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.09417v2": {
            "Paper Title": "Keyword Spotting for Hearing Assistive Devices Robust to External\n  Speakers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.10124v1": {
            "Paper Title": "On Multi-Agent Learning in Team Sports Games",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.10557v1": {
            "Paper Title": "Multi-Modal Measurements of Mental Load",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.10378v1": {
            "Paper Title": "Peril v. Promise: IoT and the Ethical Imaginaries",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.10244v1": {
            "Paper Title": "Generating User-friendly Explanations for Loan Denials using GANs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.10229v1": {
            "Paper Title": "Evaluating the Information Security Awareness of Smartphone Users",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.09850v1": {
            "Paper Title": "Multisensory cues facilitate coordination of stepping movements with a\n  virtual reality avatar",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.09816v1": {
            "Paper Title": "Hybrid-Learning approach toward situation recognition and handling",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.10588v1": {
            "Paper Title": "Measuring the Expertise of Workers for Crowdsourcing Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.03638v2": {
            "Paper Title": "Mappa Mundi: An Interactive Artistic Mind Map Generator with Artificial\n  Imagination",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.08891v1": {
            "Paper Title": "Predicting Future Opioid Incidences Today",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.09934v1": {
            "Paper Title": "Scenarios for Educational and Game Activities using Internet of Things\n  Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.08682v1": {
            "Paper Title": "Experiencing Extreme Height for The First Time: The Influence of Height,\n  Self-Judgment of Fear and a Moving Structural Beam on the Heart Rate and\n  Postural Sway During the Quiet Stance",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.08670v1": {
            "Paper Title": "A Mixed VR and Physical Framework to Evaluate Impacts of Virtual Legs\n  and Elevated Narrow Working Space on Construction Workers Gait Pattern",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.08776v1": {
            "Paper Title": "Latent Distribution Assumption for Unbiased and Consistent Consensus\n  Modelling",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.11211v1": {
            "Paper Title": "Predicting Confusion from Eye-Tracking Data with Recurrent Neural\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.07864v1": {
            "Paper Title": "Predicting Personality Traits from Physical Activity Intensity",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.09708v2": {
            "Paper Title": "Explaining Reinforcement Learning to Mere Mortals: An Empirical Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.05564v1": {
            "Paper Title": "Interaction with Ubiquitous Robots and Autonomous IoT",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.07254v1": {
            "Paper Title": "Crowdsourcing in the Absence of Ground Truth -- A Case Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.07183v1": {
            "Paper Title": "Eye Gaze Metrics and Analysis of AOI for Indexing Working Memory towards\n  Predicting ADHD",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01976v1": {
            "Paper Title": "Studying Breakdowns in Interactions with Smart Speakers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01973v1": {
            "Paper Title": "Informing The Future of Data Protection in Smart Homes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.11752v1": {
            "Paper Title": "Challenges of Designing and Developing Tangible Interfaces for Mental\n  Well-being",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.10017v1": {
            "Paper Title": "Confluent-Drawing Parallel Coordinates: Web-Based Interactive Visual\n  Analytics of Large Multi-Dimensional Data",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": ". To visualize\nnon-planar graphs in a planar way and reduce ambiguity, Dickerson\net al. ",
                    "Citation Text": "M. Dickerson, D. Eppstein, M. T. Goodrich, and J. Y . Meng. Con\ufb02uent\ndrawings: visualizing non-planar diagrams in a planar way. In Interna-\ntional Symposium on Graph Drawing , pp. 1\u201312. Springer, 2003.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:cs/0212046",
                        "Citation Paper Title": "Title:Confluent Drawings: Visualizing Non-planar Diagrams in a Planar Way",
                        "Citation Paper Abstract": "Abstract:  In this paper, we introduce a new approach for drawing diagrams that have applications in software visualization. Our approach is to use a technique we call confluent drawing for visualizing non-planar diagrams in a planar way. This approach allows us to draw, in a crossing-free manner, graphs--such as software interaction diagrams--that would normally have many crossings. The main idea of this approach is quite simple: we allow groups of edges to be merged together and drawn as \"tracks\" (similar to train tracks). Producing such confluent diagrams automatically from a graph with many crossings is quite challenging, however, so we offer two heuristic algorithms to test if a non-planar graph can be drawn efficiently in a confluent way. In addition, we identify several large classes of graphs that can be completely categorized as being either confluently drawable or confluently non-drawable.",
                        "Citation Paper Authors": "Authors:Matthew Dickerson, David Eppstein, Michael T. Goodrich, Jeremy Meng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1906.05710v1": {
            "Paper Title": "RodSteward: A Design-to-Assembly System for Fabrication using 3D-Printed\n  Joints and Precision-Cut Rods",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.05498v1": {
            "Paper Title": "Understanding Human Context in 3D Scenes by Learning Spatial Affordances\n  with Virtual Skeleton Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.05415v4": {
            "Paper Title": "Learning from Dialogue after Deployment: Feed Yourself, Chatbot!",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.05047v1": {
            "Paper Title": "Tiger:Wearable Glasses for the 20-20-20 Rule to Alleviate Computer\n  Vision Syndrome",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.05094v1": {
            "Paper Title": "Organic Building Generation in Minecraft",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.04043v1": {
            "Paper Title": "GLTR: Statistical Detection and Visualization of Generated Text",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.04002v1": {
            "Paper Title": "Detecting Clues for Skill Levels and Machine Operation Difficulty from\n  Egocentric Vision",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.03524v1": {
            "Paper Title": "PizzaBox: Studying Internet Connected Physical Object Manipulation based\n  Food Ordering",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.03098v1": {
            "Paper Title": "Multi-modal Active Learning From Human Data: A Deep Reinforcement\n  Learning Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.02569v1": {
            "Paper Title": "Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.02338v1": {
            "Paper Title": "Towards Business Partnership Recommendation Using User Opinion on\n  Facebook",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.01764v1": {
            "Paper Title": "Visual Story Post-Editing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.01418v1": {
            "Paper Title": "An End-User Development approach for Mobile Web Augmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.01417v1": {
            "Paper Title": "A Comparative Study of Milestones for Featuring GUI Prototyping Tools",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.01410v1": {
            "Paper Title": "Distributed Web browsing: supporting frequent uses and opportunistic\n  requirements",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.12581v2": {
            "Paper Title": "Current Trends in the Use of Eye Tracking in Mathematics Education\n  Research: A PME Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.09864v2": {
            "Paper Title": "Why Didn't You Listen to Me? Comparing User Control of Human-in-the-Loop\n  Topic Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.01148v1": {
            "Paper Title": "A Case for Backward Compatibility for Human-AI Teams",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.01122v1": {
            "Paper Title": "Evaluating Voice Skills by Design Guidelines Using an Automatic Voice\n  Crawler",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.02898v3": {
            "Paper Title": "Nutty-based Robot Animation -- Principles and Practices",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.00916v1": {
            "Paper Title": "Software Adaptation and Generalization of Physically-Constrained Games",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.10315v2": {
            "Paper Title": "The Impact of Augmented-Reality Head-Mounted Displays on Users' Movement\n  Behavior: An Exploratory Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.00606v1": {
            "Paper Title": "An Extensive Review of Computational Dance Automation Techniques and\n  Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.00108v1": {
            "Paper Title": "ActiveHARNet: Towards On-Device Deep Bayesian Active Learning for Human\n  Activity Recognition",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": ".\nH[y|x,Dtrain]:=\u2212\u00d5\ncp(y=c|x,Dtrain)logp(y=c|x,Dtrain)\n3.3.2 Bayesian Active Learning by Disagreement (BALD). In BALD,\npool points are chosen that maximize the mutual information be-\ntween predictions and model posterior ",
                    "Citation Text": "Houlsby, N., Husz\u00e1r, F., Ghahramani, Z., and Lengyel, M. Bayesian active\nlearning for classification and preference learning. arXiv preprint arXiv:1112.5745\n(2011).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1112.5745",
                        "Citation Paper Title": "Title:Bayesian Active Learning for Classification and Preference Learning",
                        "Citation Paper Abstract": "Abstract:Information theoretic active learning has been widely studied for probabilistic models. For simple regression an optimal myopic policy is easily tractable. However, for other tasks and with more complex models, such as classification with nonparametric models, the optimal solution is harder to compute. Current approaches make approximations to achieve tractability. We propose an approach that expresses information gain in terms of predictive entropies, and apply this method to the Gaussian Process Classifier (GPC). Our approach makes minimal approximations to the full information theoretic objective. Our experimental performance compares favourably to many popular active learning algorithms, and has equal or lower computational complexity. We compare well to decision theoretic approaches also, which are privy to more information and require much more computational time. Secondly, by developing further a reformulation of binary preference learning to a classification problem, we extend our algorithm to Gaussian Process preference learning.",
                        "Citation Paper Authors": "Authors:Neil Houlsby, Ferenc Husz\u00e1r, Zoubin Ghahramani, M\u00e1t\u00e9 Lengyel"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "mostly han-\ndle low-dimensional data for uncertainty estimations, but do not\ngeneralize to deep neural networks as the data is inherently high-\ndimensional ",
                    "Citation Text": "Gal, Y., Islam, R., and Ghahramani, Z. Deep bayesian active learning with image\ndata. In Proceedings of the 34th International Conference on Machine Learning -\nVolume 70 (2017), ICML\u201917, pp. 1183\u20131192.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.02910",
                        "Citation Paper Title": "Title:Deep Bayesian Active Learning with Image Data",
                        "Citation Paper Abstract": "Abstract:Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).",
                        "Citation Paper Authors": "Authors:Yarin Gal, Riashat Islam, Zoubin Ghahramani"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": ", can also perform approximate inference over a deep\nGaussian process ",
                    "Citation Text": "Gal, Y., and Ghahramani, Z. Dropout as a bayesian approximation: Represent-\ning model uncertainty in deep learning. In Proceedings of the 33rd InternationalConference on International Conference on Machine Learning - Volume 48 (2016),\nICML\u201916, pp. 1050\u20131059.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.02142",
                        "Citation Paper Title": "Title:Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning",
                        "Citation Paper Abstract": "Abstract:Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.",
                        "Citation Paper Authors": "Authors:Yarin Gal, Zoubin Ghahramani"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "discuss about real-time crowd sourcing on-demand to recognize\nactivities using Hidden Markov Models (HMMs) on videos, but\nnot on inertial data. Moreover, deep learning models have vastly\noutperformed HMMs in video classification setting. Bhattacharya\net al. ",
                    "Citation Text": "Bhattacharya, S., Nurmi, P., Hammerla, N., and Pl\u00f6tz, T. Using unlabeled\ndata in a sparse-coding framework for human activity recognition. Pervasive and\nMobile Computing (2014), 242\u2013262.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1312.6995",
                        "Citation Paper Title": "Title:Towards Using Unlabeled Data in a Sparse-coding Framework for Human Activity Recognition",
                        "Citation Paper Abstract": "Abstract:We propose a sparse-coding framework for activity recognition in ubiquitous and mobile computing that alleviates two fundamental problems of current supervised learning approaches. (i) It automatically derives a compact, sparse and meaningful feature representation of sensor data that does not rely on prior expert knowledge and generalizes extremely well across domain boundaries. (ii) It exploits unlabeled sample data for bootstrapping effective activity recognizers, i.e., substantially reduces the amount of ground truth annotation required for model estimation. Such unlabeled data is trivial to obtain, e.g., through contemporary smartphones carried by users as they go about their everyday activities.\nBased on the self-taught learning paradigm we automatically derive an over-complete set of basis vectors from unlabeled data that captures inherent patterns present within activity data. Through projecting raw sensor data onto the feature space defined by such over-complete sets of basis vectors effective feature extraction is pursued. Given these learned feature representations, classification backends are then trained using small amounts of labeled training data.\nWe study the new approach in detail using two datasets which differ in terms of the recognition tasks and sensor modalities. Primarily we focus on transportation mode analysis task, a popular task in mobile-phone based sensing. The sparse-coding framework significantly outperforms the state-of-the-art in supervised learning approaches. Furthermore, we demonstrate the great practical potential of the new approach by successfully evaluating its generalization capabilities across both domain and sensor modalities by considering the popular Opportunity dataset. Our feature learning approach outperforms state-of-the-art approaches to analyzing activities in daily living.",
                        "Citation Paper Authors": "Authors:Sourav Bhattacharya, Petteri Nurmi, Nils Hammerla, Thomas Pl\u00f6tz"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "pro-\nposed Bayesian Active Learning for image classification tasks, and\nis proven to learn from small amounts of unseen data, while Shen\net al. ",
                    "Citation Text": "Shen, Y., Yun, H., Lipton, Z., Kronrod, Y., and Anandkumar, A. Deep active\nlearning for named entity recognition. Proceedings of the 2nd Workshop on\nRepresentation Learning for NLP (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.05928",
                        "Citation Paper Title": "Title:Deep Active Learning for Named Entity Recognition",
                        "Citation Paper Abstract": "Abstract:Deep learning has yielded state-of-the-art performance on many natural language processing tasks including named entity recognition (NER). However, this typically requires large amounts of labeled data. In this work, we demonstrate that the amount of labeled training data can be drastically reduced when deep learning is combined with active learning. While active learning is sample-efficient, it can be computationally expensive since it requires iterative retraining. To speed this up, we introduce a lightweight architecture for NER, viz., the CNN-CNN-LSTM model consisting of convolutional character and word encoders and a long short term memory (LSTM) tag decoder. The model achieves nearly state-of-the-art performance on standard datasets for the task while being computationally much more efficient than best performing models. We carry out incremental active learning, during the training process, and are able to nearly match state-of-the-art performance with just 25\\% of the original training data.",
                        "Citation Paper Authors": "Authors:Yanyao Shen, Hyokun Yun, Zachary C. Lipton, Yakov Kronrod, Animashree Anandkumar"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1905.13172v1": {
            "Paper Title": "Sitara: Spectrum Measurement Goes Mobile Through Crowd-sourcing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.13072v1": {
            "Paper Title": "Somewhere Around That Number: An Interview Study of How Spreadsheet\n  Users Manage Uncertainty",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.12884v1": {
            "Paper Title": "M-GWAP: An Online and Multimodal Game With A Purpose in WordPress for\n  Mental States Annotation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.08937v2": {
            "Paper Title": "Can a Humanoid Robot be part of the Organizational Workforce? A User\n  Study Leveraging Sentiment Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.12487v1": {
            "Paper Title": "Food for thought: Ethical considerations of user trust in computer\n  vision",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.12285v1": {
            "Paper Title": "From User-independent to Personal Human Activity Recognition Models\n  Exploiting the Sensors of a Smartphone",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.11780v1": {
            "Paper Title": "Effect of context in swipe gesture-based continuous authentication on\n  smartphones",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.11775v1": {
            "Paper Title": "Importance of user inputs while using incremental learning to\n  personalize human activity recognition models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.11734v1": {
            "Paper Title": "Fast human motion prediction for human-robot collaboration with wearable\n  interfaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.11695v1": {
            "Paper Title": "The HyperBagGraph DataEdron: An Enriched Browsing Experience of\n  Multimedia Datasets",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.11652v1": {
            "Paper Title": "Crowdsourced Peer Learning Activity for Internet of Things Education: A\n  Case Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.11984v1": {
            "Paper Title": "Minimizing Time-to-Rank: A Learning and Recommendation Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.11791v1": {
            "Paper Title": "Analyzing Turkish F and Turkish E keyboard layouts using learning curves",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.10893v1": {
            "Paper Title": "Adaptive Learning Material Recommendation in Online Language Education",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.10444v1": {
            "Paper Title": "Overt visual attention on rendered 3D objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.10215v1": {
            "Paper Title": "From Search Engines to Search Services: An End-User Driven Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.10212v1": {
            "Paper Title": "A Behavior-Based Ontology for Supporting Automated Assessment of\n  Interactive Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.10141v1": {
            "Paper Title": "Scan-and-Pay on Android is Dangerous",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.09402v1": {
            "Paper Title": "Detecting Events of Daily Living Using Multimodal Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.08878v1": {
            "Paper Title": "Look Who's Talking Now: Implications of AV's Explanations on Driver's\n  Trust, AV Preference, Anxiety and Mental Workload",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.06720v2": {
            "Paper Title": "Visual Analytics of Anomalous User Behaviors: A Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.08418v1": {
            "Paper Title": "Personality and Behavior in Role-based Online Games",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.08367v1": {
            "Paper Title": "SmartNight: Turning Off the Lights on Android",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.07394v1": {
            "Paper Title": "MiSC: Mixed Strategies Crowdsourcing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.07379v1": {
            "Paper Title": "Challenges in Collaborative HRI for Remote Robot Teams",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.07039v1": {
            "Paper Title": "Utilizing Deep Learning Towards Multi-modal Bio-sensing and Vision-based\n  Affective Computing",
            "Sentences": [
                {
                    "Sentence ID": 51,
                    "Sentence": ". It has been shown that a pre-trained VGG-16\nnetwork can be utilized for feature extract and classi\ufb01cation\nfor applications different than it was trained for ",
                    "Citation Text": "A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson,\nCNN Features off-the-shelf: an Astounding Baseline for Recognition,\narXiv:1403.6382v3, 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1403.6382",
                        "Citation Paper Title": "Title:CNN Features off-the-shelf: an Astounding Baseline for Recognition",
                        "Citation Paper Abstract": "Abstract:Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the \\overfeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the \\overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the \\overfeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or $L2$ distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.",
                        "Citation Paper Authors": "Authors:Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, Stefan Carlsson"
                    }
                },
                {
                    "Sentence ID": 50,
                    "Sentence": "to\nextract features from the combined RGB heat-map image.\nThis network consists has been trained with more than a\nmillion images for 1,000 object categories using the Imagenet\nDatabase ",
                    "Citation Text": "Russakovsky, O., Deng, J., Su, H., et al. ImageNet Large Scale Visual\nRecognition Challenge. International Journal of Computer Vision (IJCV).\nV ol 115, Issue 3, 2015, pp. 211252",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.0575",
                        "Citation Paper Title": "Title:ImageNet Large Scale Visual Recognition Challenge",
                        "Citation Paper Abstract": "Abstract:The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions.\nThis paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.",
                        "Citation Paper Authors": "Authors:Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, Li Fei-Fei"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": ". In this manner the mutual information\nI(X;Y)is calculated which is related to conditional entropy\nH(YjX)by\nI(X;Y) =H(Y)\u0000H(YjX) (4)\nThe conditional entropy between all possible pairs of EEG\nchannels was calculated over the complete trial length ",
                    "Citation Text": "Siddharth, Jung, T.P. and Sejnowski, T.J., 2018. Multi-modal Approach\nfor Affective Computing. In Engineering in Medicine and Biology Society\n(EMBC), 2018 IEEE 40th Annual International Conference of the IEEE.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.09452",
                        "Citation Paper Title": "Title:Multi-modal Approach for Affective Computing",
                        "Citation Paper Abstract": "Abstract:Throughout the past decade, many studies have classified human emotions using only a single sensing modality such as face video, electroencephalogram (EEG), electrocardiogram (ECG), galvanic skin response (GSR), etc. The results of these studies are constrained by the limitations of these modalities such as the absence of physiological biomarkers in the face-video analysis, poor spatial resolution in EEG, poor temporal resolution of the GSR etc. Scant research has been conducted to compare the merits of these modalities and understand how to best use them individually and jointly. Using multi-modal AMIGOS dataset, this study compares the performance of human emotion classification using multiple computational approaches applied to face videos and various bio-sensing modalities. Using a novel method for compensating physiological baseline we show an increase in the classification accuracy of various approaches that we use. Finally, we present a multi-modal emotion-classification approach in the domain of affective computing research.",
                        "Citation Paper Authors": "Authors:Siddharth Siddharth, Tzyy-Ping Jung, Terrence J. Sejnowski"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1905.06717v1": {
            "Paper Title": "Multi Web Audio Sequencer: Collaborative Music Making",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.06452v1": {
            "Paper Title": "Revenue, Relevance, Arbitrage and More: Joint Optimization Framework for\n  Search Experiences in Two-Sided Marketplaces",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": "to Atari game learning. They proposed\na scalable algorithm and demonstrated the optimizer\u2019s tolerance\nto stochastic environments. ",
                    "Citation Text": "Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. 2018. Back to Basics:\nBenchmarking Canonical Evolution Strategies for Playing Atari. arXiv preprint\narXiv:1802.08842 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.08842",
                        "Citation Paper Title": "Title:Back to Basics: Benchmarking Canonical Evolution Strategies for Playing Atari",
                        "Citation Paper Abstract": "Abstract:Evolution Strategies (ES) have recently been demonstrated to be a viable alternative to reinforcement learning (RL) algorithms on a set of challenging deep RL problems, including Atari games and MuJoCo humanoid locomotion benchmarks. While the ES algorithms in that work belonged to the specialized class of natural evolution strategies (which resemble approximate gradient RL algorithms, such as REINFORCE), we demonstrate that even a very basic canonical ES algorithm can achieve the same or even better performance. This success of a basic ES algorithm suggests that the state-of-the-art can be advanced further by integrating the many advances made in the field of ES in the last decades.\nWe also demonstrate qualitatively that ES algorithms have very different performance characteristics than traditional RL algorithms: on some games, they learn to exploit the environment and perform much better while on others they can get stuck in suboptimal local minima. Combining their strengths with those of traditional RL algorithms is therefore likely to lead to new advances in the state of the art.",
                        "Citation Paper Authors": "Authors:Patryk Chrabaszcz, Ilya Loshchilov, Frank Hutter"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1905.06289v1": {
            "Paper Title": "A Human-Centered Approach to Interactive Machine Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.06102v1": {
            "Paper Title": "Frontal Screens on Head-Mounted Displays to Increase Awareness of the\n  HMD Users' State in Mixed Presence Collaboration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.05851v1": {
            "Paper Title": "Exploring Interactions with Voice-Controlled TV",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.05810v1": {
            "Paper Title": "Pointing task on smart glasses: Comparison of four interaction\n  techniques",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.05673v1": {
            "Paper Title": "A Qualitative Post-Experience Method for Evaluating Changes in VR\n  Presence Experience Over Time",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.06995v1": {
            "Paper Title": "Making ethical decisions for the immersive web",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.05652v1": {
            "Paper Title": "\"Tom\" pet robot applied to urban autism",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.05390v1": {
            "Paper Title": "WatchOut: A Road Safety Extension for Pedestrians on a Public Windshield\n  Display",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.10423v1": {
            "Paper Title": "Emotion Classification in Response to Tactile Enhanced Multimedia using\n  Frequency Domain Features of Brain Signals",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.04638v1": {
            "Paper Title": "Kyrix: Interactive Visual Data Exploration at Scale",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.04616v1": {
            "Paper Title": "VizNet: Towards A Large-Scale Visualization Learning and Benchmarking\n  Repository",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": ",\nso we added the model predictions with high certainty to\nthe labelled set. The predictions with low certainty were\nreplaced with crowdsourced labels following the uncertainty\nalgorithm ",
                    "Citation Text": "David A Cohn, Zoubin Ghahramani, and Michael I Jordan. 1996. Ac-\ntive learning with statistical models. Journal of artificial intelligence\nresearch 4 (1996), 129\u2013145.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:cs/9603104",
                        "Citation Paper Title": "Title:Active Learning with Statistical Models",
                        "Citation Paper Abstract": "Abstract:  For many types of machine learning algorithms, one can compute the statistically `optimal' way to select training data. In this paper, we review how optimal data selection techniques have been used with feedforward neural networks. We then show how the same principles may be used to select data for two alternative, statistically-based learning architectures: mixtures of Gaussians and locally weighted regression. While the techniques for neural networks are computationally expensive and approximate, the techniques for mixtures of Gaussians and locally weighted regression are both efficient and accurate. Empirically, we observe that the optimality criterion sharply decreases the number of training examples the learner needs in order to achieve good performance.",
                        "Citation Paper Authors": "Authors:D. A. Cohn, Z. Ghahramani, M. I. Jordan"
                    }
                },
                {
                    "Sentence ID": 55,
                    "Sentence": ", a self-governed crowdsourcing marketplace,\nwas instrumental in the creation of the Stanford Question\nAnswering Dataset (SQuAD) ",
                    "Citation Text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.\n2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text.\nInProceedings of the 2016 Conference on Empirical Methods in Natural\nLanguage Processing . Association for Computational Linguistics, 2383\u2013\n2392. https://doi.org/10.18653/v1/D16-1264",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.05250",
                        "Citation Paper Title": "Title:SQuAD: 100,000+ Questions for Machine Comprehension of Text",
                        "Citation Paper Abstract": "Abstract:We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research.\nThe dataset is freely available at this https URL",
                        "Citation Paper Authors": "Authors:Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "combines\nrule-based visualization generation with models trained to\nclassify a visualization as \u201cgood\" or \u201cbad\" and rank lists of\nvisualizations. VizML ",
                    "Citation Text": "Kevin Zeng Hu, Michiel A. Bakker, Stephen Li, Tim Kraska, and C\u00e9sar\nHidalgo. 2018. VizML: A Machine Learning Approach to Visualization\nRecommendation. ArXiv e-prints (Aug. 2018). arXiv:cs.HC/1808.04819",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.04819",
                        "Citation Paper Title": "Title:VizML: A Machine Learning Approach to Visualization Recommendation",
                        "Citation Paper Abstract": "Abstract:Data visualization should be accessible for all analysts with data, not just the few with technical expertise. Visualization recommender systems aim to lower the barrier to exploring basic visualizations by automatically generating results for analysts to search and select, rather than manually specify. Here, we demonstrate a novel machine learning-based approach to visualization recommendation that learns visualization design choices from a large corpus of datasets and associated visualizations. First, we identify five key design choices made by analysts while creating visualizations, such as selecting a visualization type and choosing to encode a column along the X- or Y-axis. We train models to predict these design choices using one million dataset-visualization pairs collected from a popular online visualization platform. Neural networks predict these design choices with high accuracy compared to baseline models. We report and interpret feature importances from one of these baseline models. To evaluate the generalizability and uncertainty of our approach, we benchmark with a crowdsourced test set, and show that the performance of our model is comparable to human performance when predicting consensus visualization type, and exceeds that of other ML-based systems.",
                        "Citation Paper Authors": "Authors:Kevin Z. Hu, Michiel A. Bakker, Stephen Li, Tim Kraska, C\u00e9sar A. Hidalgo"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1905.04577v1": {
            "Paper Title": "Information search in a professional context - exploring a collection of\n  professional search tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.04218v1": {
            "Paper Title": "Quantifying Teaching Behaviour in Robot Learning from Demonstration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.03809v1": {
            "Paper Title": "Wearable Sensor Data Based Human Activity Recognition using Machine\n  Learning: A new approach",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": "). This is the motivation for the\nuse of deep learning in wearable sensor based HAR recently. ",
                    "Citation Text": "Nils Y Hammerla, Shane Halloran, and Thomas Ploetz.\nDeep, convolutional, and recurrent models for human\nactivity recognition using wearables. arXiv preprint\narXiv:1604.08880 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1604.08880",
                        "Citation Paper Title": "Title:Deep, Convolutional, and Recurrent Models for Human Activity Recognition using Wearables",
                        "Citation Paper Abstract": "Abstract:Human activity recognition (HAR) in ubiquitous computing is beginning to adopt deep learning to substitute for well-established analysis techniques that rely on hand-crafted feature extraction and classification techniques. From these isolated applications of custom deep architectures it is, however, difficult to gain an overview of their suitability for problems ranging from the recognition of manipulative gestures to the segmentation and identification of physical activities like running or ascending stairs. In this paper we rigorously explore deep, convolutional, and recurrent approaches across three representative datasets that contain movement data captured with wearable sensors. We describe how to train recurrent approaches in this setting, introduce a novel regularisation approach, and illustrate how they outperform the state-of-the-art on a large benchmark dataset. Across thousands of recognition experiments with randomly sampled model configurations we investigate the suitability of each model for different tasks in HAR, explore the impact of hyperparameters using the fANOVA framework, and provide guidelines for the practitioner who wants to apply deep learning in their problem setting.",
                        "Citation Paper Authors": "Authors:Nils Y. Hammerla, Shane Halloran, Thomas Ploetz"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1905.02823v1": {
            "Paper Title": "Tracking the Progression of Reading Through Eye-gaze Measurements",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.02813v1": {
            "Paper Title": "Fixing Inclusivity Bugs for Information Processing Styles and Learning\n  Styles",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.02812v1": {
            "Paper Title": "From GenderMag to InclusiveMag: An Inclusive Design Meta-Method",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.02691v1": {
            "Paper Title": "Learned human-agent decision-making, communication and joint action in a\n  virtual reality environment",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.01825v1": {
            "Paper Title": "Accessibility Evaluation of Computer Based Tests",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.00653v1": {
            "Paper Title": "A Review on Dyadic Conversation Visualizations - Purposes, Data, Lens of\n  Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.00646v1": {
            "Paper Title": "Impact of Argument Type and Concerns in Argumentation with a Chatbot",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.13364v1": {
            "Paper Title": "Characterizing Pairs Collaboration in a Mobile-equipped Shared-Wall\n  Display Supported Collaborative Setup",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.13037v1": {
            "Paper Title": "Wearable Travel Aid for Environment Perception and Navigation of\n  Visually Impaired People",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.12268v1": {
            "Paper Title": "E-Gotsky: Sequencing Content using the Zone of Proximal Development",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.12152v1": {
            "Paper Title": "PeyeDF: an Eye-Tracking Application for Reading and Self-Indexing\n  Research",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.11961v1": {
            "Paper Title": "CoachAI: A Conversational Agent Assisted Health Coaching Platform",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.11701v1": {
            "Paper Title": "Interactive user interface based on Convolutional Auto-encoders for\n  annotating CT-scans",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.11676v1": {
            "Paper Title": "Presenting Static Friction Sensation at Stick-slip Transition using\n  Pseudo-haptic Effect",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.10354v2": {
            "Paper Title": "HAUAR: Home Automation Using Action Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.11630v1": {
            "Paper Title": "Challenges in Community Resilience Planning and Opportunities with\n  Simulation Modeling",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.11412v1": {
            "Paper Title": "Assistive System in Conversational Agent for Health Coaching: The\n  CoachAI Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.11950v1": {
            "Paper Title": "Attention-based Transfer Learning for Brain-computer Interface",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": "reviewed the current state-of-the-art trans-\nfer learning approaches in BCI. ",
                    "Citation Text": "Chuanqi Tan, Fuchun Sun, Wenchang Zhang, Jianhua\nChen, and Chunfang Liu, \u201cMultimodal classi\ufb01cation\nwith deep convolutional-recurrent neural networks for\nelectroencephalography,\u201d in International Conference\non Neural Information Processing . Springer, 2017, pp.\n767\u2013776.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.10641",
                        "Citation Paper Title": "Title:Multimodal Classification with Deep Convolutional-Recurrent Neural Networks for Electroencephalography",
                        "Citation Paper Abstract": "Abstract:Electroencephalography (EEG) has become the most significant input signal for brain computer interface (BCI) based systems. However, it is very difficult to obtain satisfactory classification accuracy due to traditional methods can not fully exploit multimodal information. Herein, we propose a novel approach to modeling cognitive events from EEG data by reducing it to a video classification problem, which is designed to preserve the multimodal information of EEG. In addition, optical flow is introduced to represent the variant information of EEG. We train a deep neural network (DNN) with convolutional neural network (CNN) and recurrent neural network (RNN) for the EEG classification task by using EEG video and optical flow. The experiments demonstrate that our approach has many advantages, such as more robustness and more accuracy in EEG classification tasks. According to our approach, we designed a mixed BCI-based rehabilitation support system to help stroke patients perform some basic operations.",
                        "Citation Paper Authors": "Authors:Chuanqi Tan, Fuchun Sun, Wenchang Zhang, Jianhua Chen, Chunfang Liu"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "proposed to extract the feature vector by using the\nintermediate layer of VGG, and the feature can be associated\nwith a speci\ufb01c region in the image through the network map.\nIn natural language processing tasks, ",
                    "Citation Text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio, \u201cNeural machine translation by jointly learning to\nalign and translate,\u201d arXiv preprint arXiv:1409.0473 ,\n2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.0473",
                        "Citation Paper Title": "Title:Neural Machine Translation by Jointly Learning to Align and Translate",
                        "Citation Paper Abstract": "Abstract:Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
                        "Citation Paper Authors": "Authors:Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "demonstrated\nthat attention not only works on object detection tasks but\nmany other computer vision tasks like image classi\ufb01cation. ",
                    "Citation Text": "Ryan Kiros, Ruslan Salakhutdinov, and Rich Zemel,\n\u201cMultimodal neural language models,\u201d in International\nConference on Machine Learning , 2014, pp. 595\u2013603.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1411.2539",
                        "Citation Paper Title": "Title:Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models",
                        "Citation Paper Abstract": "Abstract:Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* - \"blue\" + \"red\" is near images of red cars. Sample captions generated for 800 images are made available for comparison.",
                        "Citation Paper Authors": "Authors:Ryan Kiros, Ruslan Salakhutdinov, Richard S. Zemel"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "applied the\nvisual attention mechanism in an RNN network to obtain the\nability to extract information from images or video by adap-\ntively selecting a sequence of regions or locations. In ",
                    "Citation Text": "Jimmy Ba, V olodymyr Mnih, and Koray Kavukcuoglu,\n\u201cMultiple object recognition with visual attention,\u201d\narXiv preprint arXiv:1412.7755 , 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.7755",
                        "Citation Paper Title": "Title:Multiple Object Recognition with Visual Attention",
                        "Citation Paper Abstract": "Abstract:We present an attention-based model for recognizing multiple objects in images. The proposed model is a deep recurrent neural network trained with reinforcement learning to attend to the most relevant regions of the input image. We show that the model learns to both localize and recognize multiple objects despite being given only class labels during training. We evaluate the model on the challenging task of transcribing house number sequences from Google Street View images and show that it is both more accurate than the state-of-the-art convolutional networks and uses fewer parameters and less computation.",
                        "Citation Paper Authors": "Authors:Jimmy Ba, Volodymyr Mnih, Koray Kavukcuoglu"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "enable\nthe use of different domains, tasks, and distributions for train-\ning and testing. ",
                    "Citation Text": "Vinay Jayaram, Morteza Alamgir, Yasemin Altun, Bern-\nhard Scholkopf, and Moritz Grosse-Wentrup, \u201cTransfer\nlearning in brain-computer interfaces,\u201d IEEE Computa-\ntional Intelligence Magazine , vol. 11, no. 1, pp. 20\u201331,\n2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.00296",
                        "Citation Paper Title": "Title:Transfer Learning in Brain-Computer Interfaces",
                        "Citation Paper Abstract": "Abstract:The performance of brain-computer interfaces (BCIs) improves with the amount of available training data, the statistical distribution of this data, however, varies across subjects as well as across sessions within individual subjects, limiting the transferability of training data or trained models between them. In this article, we review current transfer learning techniques in BCIs that exploit shared structure between training data of multiple subjects and/or sessions to increase performance. We then present a framework for transfer learning in the context of BCIs that can be applied to any arbitrary feature space, as well as a novel regression estimation method that is specifically designed for the structure of a system based on the electroencephalogram (EEG). We demonstrate the utility of our framework and method on subject-to-subject transfer in a motor-imagery paradigm as well as on session-to-session transfer in one patient diagnosed with amyotrophic lateral sclerosis (ALS), showing that it is able to outperform other comparable methods on an identical dataset.",
                        "Citation Paper Authors": "Authors:Vinay Jayaram, Morteza Alamgir, Yasemin Altun, Bernhard Sch\u00f6lkopf, Moritz Grosse-Wentrup"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1904.10500v1": {
            "Paper Title": "Natural Language Interactions in Autonomous Vehicles: Intent Detection\n  and Slot Filling from Passenger Utterances",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.10351v1": {
            "Paper Title": "Drishtikon: An advanced navigational aid system for visually impaired\n  people",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.09884v1": {
            "Paper Title": "Health Behaviour Change Techniques in Diabetes Management Applications:\n  A Systematic Review",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.11953v1": {
            "Paper Title": "Temporal Unet: Sample Level Human Action Recognition using WiFi",
            "Sentences": [
                {
                    "Sentence ID": 40,
                    "Sentence": ". Every pixel in\nthe image is labeled as one thing.\n\u0003\u0002\u0004\b\u0006\r\u000b\u000e\n\u0005 \u0001\f\f\t\u0002\u000e\u0005\u0007\n\u0006 \u0003\u0002\u0004\b\u0006\r\u000b\u000e\n\u0005Selected video frames:\nAction annotation:\nFigure 3: One video-based frame-level action recognition example from Kinetics dataset ",
                    "Citation Text": "W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan, F. Viola, T. Green, T. Back,\nP. Natsev et al. , \u201cThe kinetics human action video dataset,\u201d arXiv preprint arXiv:1705.06950 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.06950",
                        "Citation Paper Title": "Title:The Kinetics Human Action Video Dataset",
                        "Citation Paper Abstract": "Abstract:We describe the DeepMind Kinetics human action video dataset. The dataset contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions are human focussed and cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands. We describe the statistics of the dataset, how it was collected, and give some baseline performance figures for neural network architectures trained and tested for human action classification on this dataset. We also carry out a preliminary analysis of whether imbalance in the dataset leads to bias in the classifiers.",
                        "Citation Paper Authors": "Authors:Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, Andrew Zisserman"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1904.09115v1": {
            "Paper Title": "Listen to the Image",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": ". Moreover, it becomes possible\nto generate images that did not exist before. Chen et al. ",
                    "Citation Text": "L. Chen, S. Srivastava, Z. Duan, and C. Xu. Deep cross-\nmodal audio-visual generation. In Proceedings of the on The-\nmatic Workshops of ACM Multimedia 2017 , pages 349\u2013357.\nACM, 2017. 3, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.08292",
                        "Citation Paper Title": "Title:Deep Cross-Modal Audio-Visual Generation",
                        "Citation Paper Abstract": "Abstract:Cross-modal audio-visual perception has been a long-lasting topic in psychology and neurology, and various studies have discovered strong correlations in human perception of auditory and visual stimuli. Despite works in computational multimodal modeling, the problem of cross-modal audio-visual generation has not been systematically studied in the literature. In this paper, we make the first attempt to solve this cross-modal generation problem leveraging the power of deep generative adversarial training. Specifically, we use conditional generative adversarial networks to achieve cross-modal audio-visual generation of musical performances. We explore different encoding methods for audio and visual signals, and work on two scenarios: instrument-oriented generation and pose-oriented generation. Being the first to explore this new problem, we compose two new datasets with pairs of images and sounds of musical performances of different instruments. Our experiments using both classification and human evaluations demonstrate that our model has the ability to generate one modality, i.e., audio/visual, from the other modality, i.e., visual/audio, to a good extent. Our experiments on various design choices along with the datasets will facilitate future research in this new problem space.",
                        "Citation Paper Authors": "Authors:Lele Chen, Sudhanshu Srivastava, Zhiyao Duan, Chenliang Xu"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": ". While more recent work pro-\nposed an immediate way in generating natural sound for\nwild videos ",
                    "Citation Text": "Y . Zhou, Z. Wang, C. Fang, T. Bui, and T. L. Berg. Visual\nto sound: Generating natural sound for videos in the wild.\narXiv preprint arXiv:1712.01393 , 2017. 3, 76. Network Setting and Training\n6.1. Late-blind Model\nIn this section, we provide the architectural details of\nthe proposed late-blind model. First, the audio ConvNet\nfollows the VGGish architecture proposed in",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.01393",
                        "Citation Paper Title": "Title:Visual to Sound: Generating Natural Sound for Videos in the Wild",
                        "Citation Paper Abstract": "Abstract:As two of the five traditional human senses (sight, hearing, taste, smell, and touch), vision and sound are basic sources through which humans understand the world. Often correlated during natural events, these two modalities combine to jointly affect human perception. In this paper, we pose the task of generating sound given visual input. Such capabilities could help enable applications in virtual reality (generating sound for virtual scenes automatically) or provide additional accessibility to images or videos for people with visual impairments. As a first step in this direction, we apply learning-based methods to generate raw waveform samples given input video frames. We evaluate our models on a dataset of videos containing a variety of sounds (such as ambient sounds and sounds from people/animals). Our experiments show that the generated sounds are fairly realistic and have good temporal synchronization with the visual inputs.",
                        "Citation Paper Authors": "Authors:Yipin Zhou, Zhaowen Wang, Chen Fang, Trung Bui, Tamara L. Berg"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "proposed to generate the miss-\ning text for a given image, where the shared semantic pro-\nvided the probability to predict corresponding descriptions.\nFurther, Owens et al. ",
                    "Citation Text": "A. Owens, P. Isola, J. McDermott, A. Torralba, E. H. Adel-\nson, and W. T. Freeman. Visually indicated sounds. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition , pages 2405\u20132413, 2016. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.08512",
                        "Citation Paper Title": "Title:Visually Indicated Sounds",
                        "Citation Paper Abstract": "Abstract:Objects make distinctive sounds when they are hit or scratched. These sounds reveal aspects of an object's material properties, as well as the actions that produced them. In this paper, we propose the task of predicting what sound an object makes when struck as a way of studying physical interactions within a visual scene. We present an algorithm that synthesizes sound from silent videos of people hitting and scratching objects with a drumstick. This algorithm uses a recurrent neural network to predict sound features from videos and then produces a waveform from these features with an example-based synthesis procedure. We show that the sounds predicted by our model are realistic enough to fool participants in a \"real or fake\" psychophysical experiment, and that they convey significant information about material properties and physical interactions.",
                        "Citation Paper Authors": "Authors:Andrew Owens, Phillip Isola, Josh McDermott, Antonio Torralba, Edward H. Adelson, William T. Freeman"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1904.09100v1": {
            "Paper Title": "Detecting driver distraction using stimuli-response EEG analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.08842v1": {
            "Paper Title": "Inspecting and Interacting with Meaningful Music Representations using\n  VAE",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": "recurrent\nneural networks for both encoder and decoder. In this pa-\nper, we use Gated Recurrent Units (GRUs) ",
                    "Citation Text": "J. Chung, C. Gulcehre, K. Cho, and Y. Bengio.\nEmpirical evaluation of gated recurrent neural\nnetworks on sequence modeling. arXiv preprint\narXiv:1412.3555 , 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.3555",
                        "Citation Paper Title": "Title:Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
                        "Citation Paper Abstract": "Abstract:In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.",
                        "Citation Paper Authors": "Authors:Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1904.09818v1": {
            "Paper Title": "One DSL to Rule Them All: IDE-Assisted Code Generation for Agile Data\n  Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.08621v1": {
            "Paper Title": "Improving Interactive Reinforcement Agent Planning with Human\n  Demonstration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.08610v1": {
            "Paper Title": "Client/Server Based Online Environment for Manual Segmentation of\n  Medical Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.08593v1": {
            "Paper Title": "Improving Usability, Efficiency, and Safety of UAV Path Planning through\n  a Virtual Reality Interface",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.08490v1": {
            "Paper Title": "Understanding the Effectiveness of Ultrasonic Microphone Jammer",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.08066v1": {
            "Paper Title": "Collaboration Analysis Using Deep Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.08009v1": {
            "Paper Title": "Accessibility of Virtual Reality Locomotion Modalities to Adults and\n  Minors",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.07986v1": {
            "Paper Title": "Beyond Technical Motives: Perceived User Behavior in Abandoning Wearable\n  Health & Wellness Trackers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.06138v1": {
            "Paper Title": "The development and evaluation of the SmartAbility Android Application\n  to detect users' abilities",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.06134v1": {
            "Paper Title": "Universal Design and Adaptive Interfaces as a Strategy for Induced\n  Disabilities",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.06131v1": {
            "Paper Title": "Situationally Induced Impairment in Navigation Support for Runners",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.06129v1": {
            "Paper Title": "Designing Mobile Interaction Guidelines to Account for Situationally\n  Induced Impairments and Disabilities (SIID) and Severely Constraining\n  Situational Impairments (SCSI)",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.06123v1": {
            "Paper Title": "An ethnographic study of visual impairments for voice user interface\n  design",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.06122v1": {
            "Paper Title": "AirPen: A Touchless Fingertip Based Gestural Interface for Smartphones\n  and Head-Mounted Devices",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.06120v1": {
            "Paper Title": "Method Cards for Designing for Situational Impairment",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.06118v1": {
            "Paper Title": "Ability and Context Based Adaptive System: A Proposal for Machine\n  Learning Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.05248v1": {
            "Paper Title": "Is Two Better than One? Effects of Multiple Agents on User Persuasion",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.04723v1": {
            "Paper Title": "Affordance Analysis of Virtual and Augmented Reality Mediated\n  Communication",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.04399v1": {
            "Paper Title": "Sketchforme: Composing Sketched Scenes from Text Descriptions for\n  Interactive Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.09530v2": {
            "Paper Title": "The Multi-Event-Class Synchronization (MECS) Algorithm",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.04015v1": {
            "Paper Title": "Implementation of a Daemon for OpenBCI",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.06716v1": {
            "Paper Title": "A Method to Discover Digital Collaborative Conversations in Business\n  Collaborations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.03713v1": {
            "Paper Title": "AI Meets Austen: Towards Human-Robot Discussions of Literary Metaphor",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.03656v1": {
            "Paper Title": "Ride N' Rhythm, Bike as an Embodied Musical Instrument to Improve Music\n  Perception for Young Children",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.03616v1": {
            "Paper Title": "A Facial Affect Analysis System for Autism Spectrum Disorder",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": "use head orientation and expression to study autism-\nrelated behavior. Rudovic et al. ",
                    "Citation Text": "Ognjen Rudovic, Jaeryoung Lee, Miles Dai, Bjorn Schuller,\nand Rosalind Picard, \u201cPersonalized machine learning for robot\nperception of affect and engagement in autism therapy,\u201d Sci-\nence. 3. 10.1126/scirobotics.aao6760. , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.01186",
                        "Citation Paper Title": "Title:Personalized Machine Learning for Robot Perception of Affect and Engagement in Autism Therapy",
                        "Citation Paper Abstract": "Abstract:Robots have great potential to facilitate future therapies for children on the autism spectrum. However, existing robots lack the ability to automatically perceive and respond to human affect, which is necessary for establishing and maintaining engaging interactions. Moreover, their inference challenge is made harder by the fact that many individuals with autism have atypical and unusually diverse styles of expressing their affective-cognitive states. To tackle the heterogeneity in behavioral cues of children with autism, we use the latest advances in deep learning to formulate a personalized machine learning (ML) framework for automatic perception of the childrens affective states and engagement during robot-assisted autism therapy. The key to our approach is a novel shift from the traditional ML paradigm - instead of using 'one-size-fits-all' ML models, our personalized ML framework is optimized for each child by leveraging relevant contextual information (demographics and behavioral assessment scores) and individual characteristics of each child. We designed and evaluated this framework using a dataset of multi-modal audio, video and autonomic physiology data of 35 children with autism (age 3-13) and from 2 cultures (Asia and Europe), participating in a 25-minute child-robot interaction (~500k datapoints). Our experiments confirm the feasibility of the robot perception of affect and engagement, showing clear improvements due to the model personalization. The proposed approach has potential to improve existing therapies for autism by offering more efficient monitoring and summarization of the therapy progress.",
                        "Citation Paper Authors": "Authors:Ognjen Rudovic, Jaeryoung Lee, Miles Dai, Bjorn Schuller, Rosalind Picard"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1904.03495v1": {
            "Paper Title": "Usability in the Larger Reality: A Contrarian Argument for the\n  Importance of Social and Political Considerations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.02805v1": {
            "Paper Title": "Assessment of Faster R-CNN in Man-Machine collaborative search",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": "might also be of great interest as explored\nin the recent work of Finlayson et al. ",
                    "Citation Text": "S. G. Finlayson, I. S. Kohane, and A. L. Beam. Adversar-\nial attacks against medical deep learning systems. arXiv\npreprint arXiv:1804.05296 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.05296",
                        "Citation Paper Title": "Title:Adversarial Attacks Against Medical Deep Learning Systems",
                        "Citation Paper Abstract": "Abstract:The discovery of adversarial examples has raised concerns about the practical deployment of deep learning systems. In this paper, we demonstrate that adversarial examples are capable of manipulating deep learning systems across three clinical domains. For each of our representative medical deep learning classifiers, both white and black box attacks were highly successful. Our models are representative of the current state of the art in medical computer vision and, in some cases, directly reflect architectures already seeing deployment in real world clinical settings. In addition to the technical contribution of our paper, we synthesize a large body of knowledge about the healthcare system to argue that medicine may be uniquely susceptible to adversarial attacks, both in terms of monetary incentives and technical vulnerability. To this end, we outline the healthcare economy and the incentives it creates for fraud and provide concrete examples of how and why such attacks could be realistically carried out. We urge practitioners to be aware of current vulnerabilities when deploying deep learning systems in clinical settings, and encourage the machine learning community to further investigate the domain-specific characteristics of medical learning systems.",
                        "Citation Paper Authors": "Authors:Samuel G. Finlayson, Hyung Won Chung, Isaac S. Kohane, Andrew L. Beam"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": ". Finally, the wide-spread\nof VGG-Net and Faster-RCNN make them both ideal\ncandidates for our experiments.\nTraining : We trained the network on tensor\ufb02ow ",
                    "Citation Text": "M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean,\nM. Devin, S. Ghemawat, G. Irving, M. Isard, et al. Tensor-\n\ufb02ow: A system for large-scale machine learning.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.08695",
                        "Citation Paper Title": "Title:TensorFlow: A system for large-scale machine learning",
                        "Citation Paper Abstract": "Abstract:TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous \"parameter server\" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with particularly strong support for training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model in contrast to existing systems, and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.",
                        "Citation Paper Authors": "Authors:Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, Xiaoqiang Zheng"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "given the experiments done by Huang et al.where they\nshow that Faster-RCNN overperforms the other models\nperformance-wise ",
                    "Citation Text": "J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara,\nA. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama,\net al. Speed/accuracy trade-o\ufb00s for modern convolutional\nobject detectors.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.10012",
                        "Citation Paper Title": "Title:Speed/accuracy trade-offs for modern convolutional object detectors",
                        "Citation Paper Abstract": "Abstract:The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems. A number of successful systems have been proposed in recent years, but apples-to-apples comparisons are difficult due to different base feature extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a unified implementation of the Faster R-CNN [Ren et al., 2015], R-FCN [Dai et al., 2016] and SSD [Liu et al., 2015] systems, which we view as \"meta-architectures\" and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and memory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the COCO detection task.",
                        "Citation Paper Authors": "Authors:Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, Kevin Murphy"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "for object detec-\ntion and the candidate region proposals. We picked\nFaster R-CNN over YOLO ",
                    "Citation Text": "J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You\nonly look once: Uni\ufb01ed, real-time object detection. arXiv\npreprint arXiv:1506.02640 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.02640",
                        "Citation Paper Title": "Title:You Only Look Once: Unified, Real-Time Object Detection",
                        "Citation Paper Abstract": "Abstract:We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.\nOur unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.",
                        "Citation Paper Authors": "Authors:Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1905.06715v1": {
            "Paper Title": "Homegrown Governments: Visualizing Regional Governance in the United\n  States",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.02670v1": {
            "Paper Title": "What Twitter Profile and Posted Images Reveal About Depression and\n  Anxiety",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.02348v1": {
            "Paper Title": "Orthogonal Voronoi Diagram and Treemap",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.01778v1": {
            "Paper Title": "Recognition of Advertisement Emotions with Application to Computational\n  Advertising",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.01698v1": {
            "Paper Title": "VRGym: A Virtual Testbed for Physical and Interactive AI",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": "often lacks the human involvement, espe-\ncially in high-level tasks. For instance, although some virtual plat-\nforms ( e.g., OpenAI Gym ",
                    "Citation Text": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie\nTang, and Wojciech Zaremba. 2016. Openai gym.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.01540",
                        "Citation Paper Title": "Title:OpenAI Gym",
                        "Citation Paper Abstract": "Abstract:OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.",
                        "Citation Paper Authors": "Authors:Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1904.01664v1": {
            "Paper Title": "Mirroring to Build Trust in Digital Assistants",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.09094v2": {
            "Paper Title": "Learning Personalized Thermal Preferences via Bayesian Active Learning\n  with Unimodality Constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.12349v1": {
            "Paper Title": "A User-centered Design Study in Scientific Visualization Targeting\n  Domain Experts",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.12133v1": {
            "Paper Title": "A Multimodal Emotion Sensing Platform for Building Emotion-Aware\n  Applications",
            "Sentences": [
                {
                    "Sentence ID": 22,
                    "Sentence": ". This is a 2D\npose estimation algorithm based on RGB video input. The\nalgorithm is trained on the COCO dataset ",
                    "Citation Text": "T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\nP. Doll \u00b4ar, and C. L. Zitnick, \u201cMicrosoft coco: Common objects in\ncontext,\u201d in European conference on computer vision . Springer,\n2014, pp. 740\u2013755.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1405.0312",
                        "Citation Paper Title": "Title:Microsoft COCO: Common Objects in Context",
                        "Citation Paper Abstract": "Abstract:We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
                        "Citation Paper Authors": "Authors:Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Doll\u00e1r"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": ". We used the\npublicly available EmotionAPI2emotion detector, allowing\nother researchers to replicate our method. The emotion\ndetection algorithm is a convolutional neural network (CNN)\nbased on VGG-13, more details can be found in ",
                    "Citation Text": "E. Barsoum, C. Zhang, C. C. Ferrer, and Z. Zhang, \u201cTraining deep\nnetworks for facial expression recognition with crowd-sourced label\ndistribution,\u201d in Proceedings of the 18th ACM International Confer-\nence on Multimodal Interaction . ACM, 2016, pp. 279\u2013283.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1608.01041",
                        "Citation Paper Title": "Title:Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution",
                        "Citation Paper Abstract": "Abstract:Crowd sourcing has become a widely adopted scheme to collect ground truth labels. However, it is a well-known problem that these labels can be very noisy. In this paper, we demonstrate how to learn a deep convolutional neural network (DCNN) from noisy labels, using facial expression recognition as an example. More specifically, we have 10 taggers to label each input image, and compare four different approaches to utilizing the multiple labels: majority voting, multi-label learning, probabilistic label drawing, and cross-entropy loss. We show that the traditional majority voting scheme does not perform as well as the last two approaches that fully leverage the label distribution. An enhanced FER+ data set with multiple labels for each face image will also be shared with the research community.",
                        "Citation Paper Authors": "Authors:Emad Barsoum, Cha Zhang, Cristian Canton Ferrer, Zhengyou Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1903.11812v1": {
            "Paper Title": "Analysis of distracted pedestrians' waiting time: Head-Mounted Immersive\n  Virtual Reality application",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.11672v1": {
            "Paper Title": "MuSE-ing on the Impact of Utterance Ordering On Crowdsourced Emotion\n  Annotations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.09803v1": {
            "Paper Title": "Emotion Recognition based on Third-Order Circular Suprasegmental Hidden\n  Markov Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.09709v1": {
            "Paper Title": "An Interaction Framework for Studying Co-Creative AI",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.09113v1": {
            "Paper Title": "Exploratory studies of human gait changes using depth cameras and\n  considering measurement errors",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.08918v1": {
            "Paper Title": "From Cyber-Security Deception To Manipulation and Gratification Through\n  Gamification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.08592v1": {
            "Paper Title": "EHAAS: Energy Harvesters As A Sensor for Place Recognition on Wearables",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": ", we summarize existing studies of both\nindoor localization techniques and energy harvesting technolo-\ngies.\nA. Indoor localization\nIndoor localization techniques ",
                    "Citation Text": "Faheem Zafari, Athanasios Gkelias, and Kin K. Leung. 2017. A\nSurvey of Indoor Localization Systems and Technologies. CoRR\nabs/1709.01015 (2017). http://arxiv.org/abs/1709.01015",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.01015",
                        "Citation Paper Title": "Title:A Survey of Indoor Localization Systems and Technologies",
                        "Citation Paper Abstract": "Abstract:Indoor localization has recently witnessed an increase in interest, due to the potential wide range of services it can provide by leveraging Internet of Things (IoT), and ubiquitous connectivity. Different techniques, wireless technologies and mechanisms have been proposed in the literature to provide indoor localization services in order to improve the services provided to the users. However, there is a lack of an up-to-date survey paper that incorporates some of the recently proposed accurate and reliable localization systems. In this paper, we aim to provide a detailed survey of different indoor localization techniques such as Angle of Arrival (AoA), Time of Flight (ToF), Return Time of Flight (RTOF), Received Signal Strength (RSS); based on technologies such as WiFi, Radio Frequency Identification Device (RFID), Ultra Wideband (UWB), Bluetooth and systems that have been proposed in the literature. The paper primarily discusses localization and positioning of human users and their devices. We highlight the strengths of the existing systems proposed in the literature. In contrast with the existing surveys, we also evaluate different systems from the perspective of energy efficiency, availability, cost, reception range, latency, scalability and tracking accuracy. Rather than comparing the technologies or techniques, we compare the localization systems and summarize their working principle. We also discuss remaining challenges to accurate indoor localization.",
                        "Citation Paper Authors": "Authors:Faheem Zafari, Athanasios Gkelias, Kin Leung"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1903.08524v1": {
            "Paper Title": "Robot mirroring: A framework for self-tracking feedback through empathy\n  with an artificial agent representing the self",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.12616v1": {
            "Paper Title": "Activity Classification Using Smartphone Gyroscope and Accelerometer\n  Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.12617v1": {
            "Paper Title": "Some Experimental Results of Relieving Discomfort in Virtual Reality by\n  Disturbing Feedback Loop in Human Brain",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.09834v2": {
            "Paper Title": "Human-in-the-loop Active Covariance Learning for Improving Prediction in\n  Small Data Sets",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.07157v1": {
            "Paper Title": "Modeling and Optimization of Human-machine Interaction Processes via the\n  Maximum Entropy Principle",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.07110v1": {
            "Paper Title": "Inducing Mimicry Through Auditory Icons",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.07033v1": {
            "Paper Title": "Impact of Contextual Factors on Snapchat Public Sharing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.06847v1": {
            "Paper Title": "Safe Coordination of Human-Robot Firefighting Teams",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.06643v1": {
            "Paper Title": "GestureKeeper: Gesture Recognition for Controlling Devices in IoT\n  Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.06319v1": {
            "Paper Title": "Stitching Videos from a Fisheye Lens Camera and a Wide-Angle Lens Camera\n  for Telepresence Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.06047v1": {
            "Paper Title": "Inferring Personalized Bayesian Embeddings for Learning from\n  Heterogeneous Demonstration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.05941v1": {
            "Paper Title": "The \"Physics of Diagrams\": Revealing the scientific basis of graphical\n  representation design",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.05757v1": {
            "Paper Title": "VRKitchen: an Interactive 3D Virtual Environment for Task-oriented\n  Learning",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": ", there are have been sev-\neral systems simulating more realistic indoor environments\n[9,31,42,53,54] for visual navigation tasks and basic ob-\nject interactions such as pushing and moving funitures ",
                    "Citation Text": "E. Kolve, R. Mottaghi, D. Gordon, Y . Zhu, A. Gupta, and\nA. Farhadi. AI2-THOR: An Interactive 3D Environment for\nVisual AI. pages 3\u20136, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.05474",
                        "Citation Paper Title": "Title:AI2-THOR: An Interactive 3D Environment for Visual AI",
                        "Citation Paper Abstract": "Abstract:We introduce The House Of inteRactions (THOR), a framework for visual AI research, available at this http URL. AI2-THOR consists of near photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes and interact with objects to perform tasks. AI2-THOR enables research in many different domains including but not limited to deep reinforcement learning, imitation learning, learning by interaction, planning, visual question answering, unsupervised representation learning, object detection and segmentation, and learning models of cognition. The goal of AI2-THOR is to facilitate building visually intelligent models and push the research forward in this domain.",
                        "Citation Paper Authors": "Authors:Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, Aniruddha Kembhavi, Abhinav Gupta, Ali Farhadi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1903.05448v1": {
            "Paper Title": "Animating an Autonomous 3D Talking Avatar",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.05636v1": {
            "Paper Title": "A Comprehensive Analysis of 2D&3D Video Watching of EEG Signals by\n  Increasing PLSR and SVM Classification Results",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.05251v1": {
            "Paper Title": "Situational Awareness, Drivers Trust in Automated Driving Systems and\n  Secondary Task Performance",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.11944v1": {
            "Paper Title": "Taxonomies in DUI Design Patterns: A Systematic Approach for Removing\n  Overlaps Among Design Patterns and Creating a Clear Hierarchy",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.04084v1": {
            "Paper Title": "Exploring OpenStreetMap Availability for Driving Environment\n  Understanding",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.04047v1": {
            "Paper Title": "SacCalib: Reducing Calibration Distortion for Stationary Eye Trackers\n  Using Saccadic Eye Movements",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.03700v1": {
            "Paper Title": "Dataspace: A Reconfigurable Hybrid Reality Environment for Collaborative\n  Information Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.08327v2": {
            "Paper Title": "On How Users Edit Computer-Generated Visual Stories",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": ". Fan\net al. created a hierarchical model that automatically generates stories conditioning on the writing\nprompts ",
                    "Citation Text": "Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical Neural Story Generation. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational\nLinguistics, 889\u2013898. http://aclweb.org/anthology/P18-1082",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.04833",
                        "Citation Paper Title": "Title:Hierarchical Neural Story Generation",
                        "Citation Paper Abstract": "Abstract:We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.",
                        "Citation Paper Authors": "Authors:Angela Fan, Mike Lewis, Yann Dauphin"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": ". Formulating a story as a sequence of events,\nMartin et. al proposed an event representation for neural-network-based story generation ",
                    "Citation Text": "Lara J. Martin, Prithviraj Ammanabrolu, Xinyu Wang, William Hancock, Shruti Singh, Brent Harrison, and Mark O. Riedl.\n2018. Event Representations for Automated Story Generation with Deep Neural Nets. In Proceedings of the Thirty-Second\nAAAI Conference on Artificial Intelligence, (AAAI-18) , Sheila A. McIlraith and Kilian Q. Weinberger (Eds.). AAAI Press,\n868\u2013875. https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17046",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.01331",
                        "Citation Paper Title": "Title:Event Representations for Automated Story Generation with Deep Neural Nets",
                        "Citation Paper Abstract": "Abstract:Automated story generation is the problem of automatically selecting a sequence of events, actions, or words that can be told as a story. We seek to develop a system that can generate stories by learning everything it needs to know from textual story corpora. To date, recurrent neural networks that learn language models at character, word, or sentence levels have had little success generating coherent stories. We explore the question of event representations that provide a mid-level of abstraction between words and sentences in order to retain the semantic information of the original data while minimizing event sparsity. We present a technique for preprocessing textual story data into event sequences. We then present a technique for automated story generation whereby we decompose the problem into the generation of successive events (event2event) and the generation of natural language sentences from events (event2sentence). We give empirical results comparing different event representations and their effects on event successor generation and the translation of events to natural language.",
                        "Citation Paper Authors": "Authors:Lara J. Martin, Prithviraj Ammanabrolu, Xinyu Wang, William Hancock, Shruti Singh, Brent Harrison, Mark O. Riedl"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1903.02978v1": {
            "Paper Title": "Integrating Artificial and Human Intelligence for Efficient Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.02723v1": {
            "Paper Title": "Symmetrical Reality: Toward a Unified Framework for Physical and Virtual\n  Reality",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.11567v1": {
            "Paper Title": "Haptics-Augmented Physics Simulation: Coriolis Effect",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.02618v1": {
            "Paper Title": "Seniors' Media Preference for Receiving Internet Security Information: A\n  Pilot Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.06657v1": {
            "Paper Title": "Effects of Self-Avatar and Gaze on Avoidance Movement Behavior",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.01968v1": {
            "Paper Title": "Augmented Reality Prosthesis Training Setup for Motor Skill Enhancement",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.01780v1": {
            "Paper Title": "Trust and Trustworthiness in Social Recommender Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.01345v1": {
            "Paper Title": "Teaching HCI Design in a Flipped Learning M.Sc. Course Using\n  Eye-Tracking Peer Evaluation Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.01333v1": {
            "Paper Title": "A Serious Game for Introducing Software Engineering Ethics to University\n  Students",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.01210v1": {
            "Paper Title": "Immersive VR as a Tool to Enhance Relaxation for Undergraduate Students\n  with the Aim of Reducing Anxiety - A Pilot Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.00718v1": {
            "Paper Title": "Virtual Representations for Iterative IoT Deployment",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.00283v1": {
            "Paper Title": "Visualizing Multiple Process Attributes in one 3D Process Representation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.00185v1": {
            "Paper Title": "MVC-3D: Adaptive Design Pattern for Virtual and Augmented Reality\n  Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.11247v1": {
            "Paper Title": "Modeling Mobile Interface Tappability Using Crowdsourcing and Deep\n  Learning",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": ". Deep learning models\nhave also been built to identify salient elements in graphic\ndesigns and interfaces ",
                    "Citation Text": "Zoya Bylinskii, Nam Wook Kim, Peter O\u2019Donovan, Sami Alsheikh,\nSpandan Madan, Hanspeter Pfister, Fredo Durand, Bryan Russell, and\nAaron Hertzmann. 2017. Learning Visual Importance for Graphic\nDesigns and Data Visualizations. In Proceedings of the 30th Annual ACM\nSymposium on User Interface Software and Technology (UIST \u201917) . ACM,\nNew York, NY, USA, 57\u201369. https://doi.org/10.1145/3126594.3126653",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.02660",
                        "Citation Paper Title": "Title:Learning Visual Importance for Graphic Designs and Data Visualizations",
                        "Citation Paper Abstract": "Abstract:Knowing where people look and click on visual designs can provide clues about how the designs are perceived, and where the most important or relevant content lies. The most important content of a visual design can be used for effective summarization or to facilitate retrieval from a database. We present automated models that predict the relative importance of different elements in data visualizations and graphic designs. Our models are neural networks trained on human clicks and importance annotations on hundreds of designs. We collected a new dataset of crowdsourced importance, and analyzed the predictions of our models with respect to ground truth importance and human eye movements. We demonstrate how such predictions of importance can be used for automatic design retargeting and thumbnailing. User studies with hundreds of MTurk participants validate that, with limited post-processing, our importance-driven applications are on par with, or outperform, current state-of-the-art methods, including natural image saliency. We also provide a demonstration of how our importance predictions can be built into interactive design tools to offer immediate feedback during the design process.",
                        "Citation Paper Authors": "Authors:Zoya Bylinskii, Nam Wook Kim, Peter O'Donovan, Sami Alsheikh, Spandan Madan, Hanspeter Pfister, Fredo Durand, Bryan Russell, Aaron Hertzmann"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1902.10699v1": {
            "Paper Title": "Validation of smartphone based pavement roughness measures",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.03414v1": {
            "Paper Title": "Artificial Intelligence in Intelligent Tutoring Robots: A Systematic\n  Review and Design Guidelines",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.09289v1": {
            "Paper Title": "A Virtual Teaching Assistant for Personalized Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.09022v1": {
            "Paper Title": "Designing for Health Chatbots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.08068v1": {
            "Paper Title": "Towards Reliable, Automated General Movement Assessment for Perinatal\n  Stroke Screening in Infants Using Wearable Accelerometers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.03529v2": {
            "Paper Title": "Live Emoji: Semantic Emotional Expressiveness of 2D Live Animation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.07807v1": {
            "Paper Title": "Simulating Forces - Learning Through Touch, Virtual Laboratories",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.07683v1": {
            "Paper Title": "Modelling and Analysing Behaviours and Emotions via Complex User\n  Interactions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.07244v1": {
            "Paper Title": "UPCASE - A Method for Self-Assessing the Capability of the Usability\n  Process in Small Organizations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.06442v1": {
            "Paper Title": "In a Silent Way: Communication Between AI and Improvising Musicians\n  Beyond Sound",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": "can perform just as well\nor better in the analysis of sequential data ",
                    "Citation Text": "Shaojie Bai, J Zico Kolter, and Vladlen Koltun. 2018. An empirical eval-\nuation of generic convolutional and recurrent networks for sequence\nmodeling. arXiv preprint arXiv:1803.01271 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.01271",
                        "Citation Paper Title": "Title:An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling",
                        "Citation Paper Abstract": "Abstract:For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at this http URL .",
                        "Citation Paper Authors": "Authors:Shaojie Bai, J. Zico Kolter, Vladlen Koltun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1902.06193v1": {
            "Paper Title": "An Automated Testing Framework for Conversational Agents",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.06019v1": {
            "Paper Title": "Outlining the Design Space of Explainable Intelligent Systems for\n  Medical Diagnosis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.05361v1": {
            "Paper Title": "Crowd Work on a CV? Understanding How AMT Fits into Turkers' Career\n  Goals and Professional Profiles",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.04929v1": {
            "Paper Title": "Integrating Neurophysiological Sensors and Driver Models for Safe and\n  Performant Automated Vehicle Control in Mixed Traffic",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.04861v1": {
            "Paper Title": "Which Neural Network Architecture matches Human Behavior in Artificial\n  Grammar Learning?",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.04262v1": {
            "Paper Title": "Reading Protocol: Understanding what has been Read in Interactive\n  Information Retrieval Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.03722v1": {
            "Paper Title": "A Minimal Template for Interactive Web-based Demonstrations of Musical\n  Machine Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.03541v1": {
            "Paper Title": "Human Computer Interaction Design for Mobile Devices Based on a Smart\n  Healthcare Architecture",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.03527v1": {
            "Paper Title": "Engaging Audiences in Virtual Museums by Interactively Prompting Guiding\n  Questions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.03322v1": {
            "Paper Title": "An Approach to Track Reading Progression Using Eye-Gaze Fixation Points",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.10223v1": {
            "Paper Title": "Virtual Environments for Rehabilitation of Postural Control Dysfunction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.02960v1": {
            "Paper Title": "Human-Centered Tools for Coping with Imperfect Algorithms during Medical\n  Decision-Making",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.02842v1": {
            "Paper Title": "Community Animation: Exploring a design space that leverages geosocial\n  networking to increase community engagement",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.02635v1": {
            "Paper Title": "Are Children Fully Aware of Online Privacy Risks and How Can We Improve\n  Their Coping Ability?",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "Jun Zhao. 2018. Are Children Well-Supported by Their Parents Concerning Online Privacy\nRisks, and Who Supports the Parents? CoRR abs/1809.10944 (2018). http://arxiv.org/\nabs/1809.10944 ",
                    "Citation Text": "Jun Zhao, Ulrik Lyngs, and Nigel Shadbolt. 2018. What privacy concerns do parents have\nabout children\u2019s mobile apps, and how can they stay SHARP? CoRR abs/1809.10841 (2018).\nhttp://arxiv.org/abs/1809.10841\n\u2014 KOALA Project Report 3 Page 7KOALA Project Report 3 January 2019\n\u2014 KOALA Project Report 3 Page 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.10841",
                        "Citation Paper Title": "Title:What privacy concerns do parents have about children's mobile apps, and how can they stay SHARP?",
                        "Citation Paper Abstract": "Abstract:Tablet computers are widely used by young children. A report in 2016 shows that children aged 5 to 15 years are spending more time online than watching TV. A 2017 update of the same report shows that parents are becoming more concerned about their children's online risks compared to the previous year. Parents are working hard to protect their children's online safety. An increasing number of parents are setting up content filtering at home or having regular discussions with their children regarding online risks. However, although risks related to Social Media platforms or social video sharing sites (like YouTube) are widely known, risks posed by mobile applications or games (i.e. `apps') are less known. Behind the cute characters, apps used by children can not only have the possibility of exposing them to age-inappropriate content or excessive in-app promotions, but may also make a large amount of their personal information accessible to third-party online marketing and advertising industry. Such practices are not unique to children's apps, but young children are probably less capable of resisting the resulting personalised advertisements and game promotions. In this report, we present findings from our online survey of 220 parents with children aged 6-10, mainly from the U.K. and other western countries, regarding their privacy concerns and expectations of their children's use of mobile apps. Parents play a key role in children's use of digital technology, especially for children under 10 years old. Recent reports have highlighted parents' lack of sufficient support for choosing appropriate digital content for their children. Our report sheds some initial light on parents' key struggles and points to immediate steps and possible areas of future development.",
                        "Citation Paper Authors": "Authors:Jun Zhao, Ulrik Lyngs, Nigel Shadbolt"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1902.01117v1": {
            "Paper Title": "Exploring Temporal Dependencies in Multimodal Referring Expressions with\n  Mixed Reality",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.00757v1": {
            "Paper Title": "East Asians with Internet Addiction: Prevalence Rates and Support Use\n  Patterns",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.00719v1": {
            "Paper Title": "Learning User Preferences via Reinforcement Learning with Spatial\n  Interface Valuing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.00607v1": {
            "Paper Title": "Detecting Gaze Towards Eyes in Natural Social Interactions and its Use\n  in Child Assessment",
            "Sentences": [
                {
                    "Sentence ID": 33,
                    "Sentence": "achieved impressive performance in object\ndetection challenges such as Microso/f_t COCO ",
                    "Citation Text": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll \u00b4ar, and C Lawrence Zitnick. 2014.\nMicroso/f_t coco: Common objects in context. In European Conference on Computer Vision . Springer, 740\u2013755.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1405.0312",
                        "Citation Paper Title": "Title:Microsoft COCO: Common Objects in Context",
                        "Citation Paper Abstract": "Abstract:We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
                        "Citation Paper Authors": "Authors:Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Doll\u00e1r"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1902.00570v1": {
            "Paper Title": "Exploring attention mechanism for acoustic-based classification of\n  speech utterances into system-directed and non-system-directed",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.11528v1": {
            "Paper Title": "Shaping the Narrative Arc: An Information-Theoretic Approach to\n  Collaborative Dialogue",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.04393v1": {
            "Paper Title": "Evaluating Older Users' Experiences with Commercial Dialogue Systems:\n  Implications for Future Design and Development",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.10449v1": {
            "Paper Title": "Health Behavior Change in HCI: Trends, Patterns, and Opportunities",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.10312v1": {
            "Paper Title": "MultiLock: Mobile Active Authentication based on Multiple Biometric and\n  Behavioral Patterns",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.10268v1": {
            "Paper Title": "Performance comparison of an AI-based Adaptive Learning System in China",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.08335v1": {
            "Paper Title": "Teaching robots to imitate a human with no on-teacher sensors. What are\n  the key challenges?",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": "proposed a two stage method for multi-person\ndetection and 2-D pose estimation in wild from RGB images.\nOpenPose is an alternative algorithm ",
                    "Citation Text": "Z. Cao, T. Simon, S.-E. Wei, and Y . Sheikh, \u201cRealtime multi-person\n2d pose estimation using part af\ufb01nity \ufb01elds,\u201d in CVPR , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.08050",
                        "Citation Paper Title": "Title:Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields",
                        "Citation Paper Abstract": "Abstract:We present an approach to efficiently detect the 2D pose of multiple people in an image. The approach uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. The architecture encodes global context, allowing a greedy bottom-up parsing step that maintains high accuracy while achieving realtime performance, irrespective of the number of people in the image. The architecture is designed to jointly learn part locations and their association via two branches of the same sequential prediction process. Our method placed first in the inaugural COCO 2016 keypoints challenge, and significantly exceeds the previous state-of-the-art result on the MPII Multi-Person benchmark, both in performance and efficiency.",
                        "Citation Paper Authors": "Authors:Zhe Cao, Tomas Simon, Shih-En Wei, Yaser Sheikh"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1901.07151v1": {
            "Paper Title": "Virtual Immersive Reality based Analysis of Behavioral Responses in\n  Connected and Autonomous Vehicle Environment",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.06572v1": {
            "Paper Title": "Moment-to-Moment Detection of Internal Thought from Eye Vergence\n  Behaviour",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.05574v1": {
            "Paper Title": "Visual Reasoning of Feature Attribution with Deep Recurrent Neural\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.04889v1": {
            "Paper Title": "Deep Fusion: An Attention Guided Factorized Bilinear Pooling for\n  Audio-video Emotion Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.02957v2": {
            "Paper Title": "Decision-Making Under Uncertainty in Research Synthesis: Designing for\n  the Garden of Forking Paths",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.04110v1": {
            "Paper Title": "Machine learning for the recognition of emotion in the speech of couples\n  in psychotherapy using the Stanford Suppes Brain Lab Psychotherapy Dataset",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.04024v1": {
            "Paper Title": "Modeling neural dynamics during speech production using a state space\n  variational autoencoder",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": ".\nFraccaro introduced the Kalman Variational Autoencoder\n(KV AE) to disentangle temporal sequences in a latent space\nthat describes nonlinear dynamics ",
                    "Citation Text": "Marco Fraccaro, Simon Kamronn, Ulrich Paquet, and Ole Winther. A\ndisentangled recognition and nonlinear dynamics model for unsuper-\nvised learning. In Advances in Neural Information Processing Systems ,\npages 3601\u20133610, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.05741",
                        "Citation Paper Title": "Title:A Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised Learning",
                        "Citation Paper Abstract": "Abstract:This paper takes a step towards temporal reasoning in a dynamically changing video, not in the pixel space that constitutes its frames, but in a latent space that describes the non-linear dynamics of the objects in its world. We introduce the Kalman variational auto-encoder, a framework for unsupervised learning of sequential data that disentangles two latent representations: an object's representation, coming from a recognition model, and a latent state describing its dynamics. As a result, the evolution of the world can be imagined and missing data imputed, both without the need to generate high dimensional frames at each time step. The model is trained end-to-end on videos of a variety of simulated physical systems, and outperforms competing methods in generative and missing data imputation tasks.",
                        "Citation Paper Authors": "Authors:Marco Fraccaro, Simon Kamronn, Ulrich Paquet, Ole Winther"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": ". Frigola proposed a framework focusing on\nBayesian learning of non-parametric nonlinear SSMs ",
                    "Citation Text": "Roger Frigola, Yutian Chen, and Carl Edward Rasmussen. Variational\ngaussian process state-space models. In Advances in neural informa-\ntion processing systems , pages 3680\u20133688, 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.4905",
                        "Citation Paper Title": "Title:Variational Gaussian Process State-Space Models",
                        "Citation Paper Abstract": "Abstract:State-space models have been successfully used for more than fifty years in different areas of science and engineering. We present a procedure for efficient variational Bayesian learning of nonlinear state-space models based on sparse Gaussian processes. The result of learning is a tractable posterior over nonlinear dynamical systems. In comparison to conventional parametric models, we offer the possibility to straightforwardly trade off model capacity and computational cost whilst avoiding overfitting. Our main algorithm uses a hybrid inference approach combining variational Bayes and sequential Monte Carlo. We also present stochastic variational inference and online learning approaches for fast learning with long time series.",
                        "Citation Paper Authors": "Authors:Roger Frigola, Yutian Chen, Carl E. Rasmussen"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": ".\nIn Krishnan\u2019s work, nonlinear SSMs are parameterized by\nRNNs, allowing the inference and generative models to be\nlearned simultaneously ",
                    "Citation Text": "Rahul G Krishnan, Uri Shalit, and David Sontag. Structured inference\nnetworks for nonlinear state space models. In AAAI , pages 2101\u20132109,\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.09869",
                        "Citation Paper Title": "Title:Structured Inference Networks for Nonlinear State Space Models",
                        "Citation Paper Abstract": "Abstract:Gaussian state space models have been used for decades as generative models of sequential data. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption. We introduce a unified algorithm to efficiently learn a broad class of linear and non-linear state space models, including variants where the emission and transition distributions are modeled by deep neural networks. Our learning algorithm simultaneously learns a compiled inference network and the generative model, leveraging a structured variational approximation parameterized by recurrent neural networks to mimic the posterior distribution. We apply the learning algorithm to both synthetic and real-world datasets, demonstrating its scalability and versatility. We find that using the structured approximation to the posterior results in models with significantly higher held-out likelihood.",
                        "Citation Paper Authors": "Authors:Rahul G. Krishnan, Uri Shalit, David Sontag"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": ".\nRecent developments in unsupervised learning use a varia-\ntional Bayesian approach to implement SSM estimation and\ninference ",
                    "Citation Text": "David Sussillo, Rafal Jozefowicz, LF Abbott, and Chethan Pandar-\ninath. Lfads-latent factor analysis via dynamical systems. arXiv\npreprint arXiv:1608.06315 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1608.06315",
                        "Citation Paper Title": "Title:LFADS - Latent Factor Analysis via Dynamical Systems",
                        "Citation Paper Abstract": "Abstract:Neuroscience is experiencing a data revolution in which many hundreds or thousands of neurons are recorded simultaneously. Currently, there is little consensus on how such data should be analyzed. Here we introduce LFADS (Latent Factor Analysis via Dynamical Systems), a method to infer latent dynamics from simultaneously recorded, single-trial, high-dimensional neural spiking data. LFADS is a sequential model based on a variational auto-encoder. By making a dynamical systems hypothesis regarding the generation of the observed data, LFADS reduces observed spiking to a set of low-dimensional temporal factors, per-trial initial conditions, and inferred inputs. We compare LFADS to existing methods on synthetic data and show that it significantly out-performs them in inferring neural firing rates and latent dynamics.",
                        "Citation Paper Authors": "Authors:David Sussillo, Rafal Jozefowicz, L. F. Abbott, Chethan Pandarinath"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": ".\nBy using sparse Gaussian processes to encode dynamical\nsystems, their variational training procedure enables learning\nof complex systems without risk of over\ufb01tting. Karl et al.\nutilize stochastic gradient variational Bayes modeling as\nthe inference mechanism for highly nonlinear SSMs ",
                    "Citation Text": "Maximilian Karl, Maximilian Soelch, Justin Bayer, and Patrick van der\nSmagt. Deep variational bayes \ufb01lters: Unsupervised learning of state\nspace models from raw data. stat, 1050:23, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.06432",
                        "Citation Paper Title": "Title:Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data",
                        "Citation Paper Abstract": "Abstract:We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.",
                        "Citation Paper Authors": "Authors:Maximilian Karl, Maximilian Soelch, Justin Bayer, Patrick van der Smagt"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1901.03793v1": {
            "Paper Title": "The Importance of Socio-Cultural Differences for Annotating and\n  Detecting the Affective States of Students",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.06237v1": {
            "Paper Title": "BUOCA: Budget-Optimized Crowd Worker Allocation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.03729v1": {
            "Paper Title": "Automated Rationale Generation: A Technique for Explainable AI and its\n  Effects on Human Perceptions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.03536v1": {
            "Paper Title": "Somatic Practices for Understanding Real, Imagined, and Virtual\n  Realities",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.02602v1": {
            "Paper Title": "UAV-GESTURE: A Dataset for UAV Control and Gesture Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.01920v1": {
            "Paper Title": "Data is Personal: Attitudes and Perceptions of Data Visualization in\n  Rural Pennsylvania",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.01001v1": {
            "Paper Title": "Identifying Barriers to Adoption for Rust through Online Discourse",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.00885v1": {
            "Paper Title": "An Interactive Robotic Framework to Facilitate Sensory Experiences for\n  Children with ASD",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.00552v1": {
            "Paper Title": "Measuring Physical Activity of Older Adults via Smartwatch and\n  Stigmergic Receptive Fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.05637v2": {
            "Paper Title": "Simplify Node-RED For End User Development in SeismoCloud",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.04258v3": {
            "Paper Title": "Learning to Switch Among Agents in a Team via 2-Layer Markov Decision\n  Processes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.02476v4": {
            "Paper Title": "Using Machine Teaching to Investigate Human Assumptions when Teaching\n  Reinforcement Learners",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.00822v3": {
            "Paper Title": "Open-Ended Multi-Modal Relational Reasoning for Video Question Answering",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": ". Additionally, 1\u2264h\u2264Hand\nWh\nQ, Wh\nK, Wh\nV\u2208Rdx\u00d7(dx/H).\nBased on the concept of incorporating relative position\ninformation within a self-attention layer ",
                    "Citation Text": "P. Shaw, J. Uszkoreit, and A. Vaswani, \u201cSelf-attention with relative\nposition representations,\u201d in Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 2 (Short Papers) .\nNew Orleans, Louisiana: Association for Computational Linguistics,\nJune 2018, pp. 464\u2013468.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.02155",
                        "Citation Paper Title": "Title:Self-Attention with Relative Position Representations",
                        "Citation Paper Abstract": "Abstract:Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.",
                        "Citation Paper Authors": "Authors:Peter Shaw, Jakob Uszkoreit, Ashish Vaswani"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "to encoding the text. After that, we build a\nmodel based on Relation-Aware Self-Attention ",
                    "Citation Text": "B. Wang, R. Shin, X. Liu, O. Polozov, and M. Richardson, \u201cRat-sql:\nRelation-aware schema encoding and linking for text-to-sql parsers,\u201d\n2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.04942",
                        "Citation Paper Title": "Title:RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers",
                        "Citation Paper Abstract": "Abstract:When translating natural language questions into SQL queries to answer questions from a database, contemporary semantic parsing models struggle to generalize to unseen database schemas. The generalization challenge lies in (a) encoding the database relations in an accessible way for the semantic parser, and (b) modeling alignment between database columns and their mentions in a given query. We present a unified framework, based on the relation-aware self-attention mechanism, to address schema encoding, schema linking, and feature representation within a text-to-SQL encoder. On the challenging Spider dataset this framework boosts the exact match accuracy to 57.2%, surpassing its best counterparts by 8.7% absolute improvement. Further augmented with BERT, it achieves the new state-of-the-art performance of 65.6% on the Spider leaderboard. In addition, we observe qualitative improvements in the model's understanding of schema linking and alignment. Our implementation will be open-sourced at this https URL.",
                        "Citation Paper Authors": "Authors:Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, Matthew Richardson"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "in our model to recognize the speech made by the users and\ntransfer them to the text. We use the Stanford Parser tree and\nBERT ",
                    "Citation Text": "J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training\nof deep bidirectional transformers for language understanding,\u201d arXiv\npreprint arXiv:1810.04805 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.13188v2": {
            "Paper Title": "Control of Computer Pointer Using Hand Gesture Recognition in Motion\n  Pictures",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.12669v2": {
            "Paper Title": "Position and Rotation Invariant Sign Language Recognition from 3D Kinect\n  Data with Recurrent Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.08690v4": {
            "Paper Title": "Developing an Effective and Automated Patient Engagement Estimator for\n  Telehealth: A Machine Learning Approach",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "have been used as single modalities for detecting engagement. Combining di\u000berent modalities\nhas been observed to improve engagement detection accuracy [12, 13, 14]. ",
                    "Citation Text": "M. Frank, G. To\fghi, H. Gu, and R. Fruchter, \\Engagement detection in meetings,\" arXiv\npreprint arXiv:1608.08711 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1608.08711",
                        "Citation Paper Title": "Title:Engagement Detection in Meetings",
                        "Citation Paper Abstract": "Abstract:Group meetings are frequent business events aimed to develop and conduct project work, such as Big Room design and construction project meetings. To be effective in these meetings, participants need to have an engaged mental state. The mental state of participants however, is hidden from other participants, and thereby difficult to evaluate. Mental state is understood as an inner process of thinking and feeling, that is formed of a conglomerate of mental representations and propositional attitudes. There is a need to create transparency of these hidden states to understand, evaluate and influence them. Facilitators need to evaluate the meeting situation and adjust for higher engagement and productivity. This paper presents a framework that defines a spectrum of engagement states and an array of classifiers aimed to detect the engagement state of participants in real time. The Engagement Framework integrates multi-modal information from 2D and 3D imaging and sound. Engagement is detected and evaluated at participants and aggregated at group level. We use empirical data collected at the lab of Konica Minolta, Inc. to test initial applications of this framework. The paper presents examples of the tested engagement classifiers, which are based on research in psychology, communication, and human computer interaction. Their accuracy is illustrated in dyadic interaction for engagement detection. In closing we discuss the potential extension to complex group collaboration settings and future feedback implementations.",
                        "Citation Paper Authors": "Authors:Maria Frank, Ghassem Tofighi, Haisong Gu, Renate Fruchter"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2004.01451v6": {
            "Paper Title": "Comparison of a Head-Mounted Display and a Curved Screen in a\n  Multi-Talker Audiovisual Listening Task",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.08798v3": {
            "Paper Title": "Designing Compact Features for Remote Stroke Rehabilitation Monitoring\n  using Wearable Accelerometers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.11684v4": {
            "Paper Title": "To Explain or Not to Explain: A Study on the Necessity of Explanations\n  for Autonomous Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.12683v6": {
            "Paper Title": "Improving Workflow Integration with xPath: Design and Evaluation of a\n  Human-AI Diagnosis System in Pathology",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": ". For medical tasks, various human-AI collaboration systems have shown their validity in\nimproving doctors\u2019 agreement [ 15,30,52], mental effort ",
                    "Citation Text": "Carrie J. Cai, Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel Smilkov, Martin Wattenberg, Fernanda\nViegas, Greg S. Corrado, Martin C. Stumpe, and Michael Terry. 2019. Human-Centered Tools for Coping with\nImperfect Algorithms During Medical Decision-Making. In Proceedings of the 2019 CHI Conference on Human Factors in\nComputing Systems (Glasgow, Scotland Uk) (CHI \u201919) . Association for Computing Machinery, New York, NY, USA,\n1\u201314. https://doi.org/10.1145/3290605.3300234",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.02960",
                        "Citation Paper Title": "Title:Human-Centered Tools for Coping with Imperfect Algorithms during Medical Decision-Making",
                        "Citation Paper Abstract": "Abstract:Machine learning (ML) is increasingly being used in image retrieval systems for medical decision making. One application of ML is to retrieve visually similar medical images from past patients (e.g. tissue from biopsies) to reference when making a medical decision with a new patient. However, no algorithm can perfectly capture an expert's ideal notion of similarity for every case: an image that is algorithmically determined to be similar may not be medically relevant to a doctor's specific diagnostic needs. In this paper, we identified the needs of pathologists when searching for similar images retrieved using a deep learning algorithm, and developed tools that empower users to cope with the search algorithm on-the-fly, communicating what types of similarity are most important at different moments in time. In two evaluations with pathologists, we found that these refinement tools increased the diagnostic utility of images found and increased user trust in the algorithm. The tools were preferred over a traditional interface, without a loss in diagnostic accuracy. We also observed that users adopted new strategies when using refinement tools, re-purposing them to test and understand the underlying algorithm and to disambiguate ML errors from their own errors. Taken together, these findings inform future human-ML collaborative systems for expert decision-making.",
                        "Citation Paper Authors": "Authors:Carrie J. Cai, Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel Smilkov, Martin Wattenberg, Fernanda Viegas, Greg S. Corrado, Martin C. Stumpe, Michael Terry"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": ". Gu et al. summarized six design lessons for interactive\nAI systems in pathology, suggesting AI systems in pathology should \u201cprovide the actionability of\nthe AI guidance\u201d and \u201cnarrow down to small regions of a large task space\u201d ",
                    "Citation Text": "Hongyan Gu, Jingbin Huang, Lauren Hung, and Xiang \u2019Anthony\u2019 Chen. 2021. Lessons Learned from Designing\nan AI-Enabled Diagnosis Tool for Pathologists. Proc. ACM Hum.-Comput. Interact. 5, CSCW1, Article 10 (apr 2021),\n25 pages. https://doi.org/10.1145/3449084",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.12695",
                        "Citation Paper Title": "Title:Lessons Learned from Designing an AI-Enabled Diagnosis Tool for Pathologists",
                        "Citation Paper Abstract": "Abstract:Despite the promises of data-driven artificial intelligence (AI), little is known about how we can bridge the gulf between traditional physician-driven diagnosis and a plausible future of medicine automated by AI. Specifically, how can we involve AI usefully in physicians' diagnosis workflow given that most AI is still nascent and error-prone (e.g., in digital pathology)? To explore this question, we first propose a series of collaborative techniques to engage human pathologists with AI given AI's capabilities and limitations, based on which we prototype Impetus - a tool where an AI takes various degrees of initiatives to provide various forms of assistance to a pathologist in detecting tumors from histological slides. We summarize observations and lessons learned from a study with eight pathologists and discuss recommendations for future work on human-centered medical AI systems.",
                        "Citation Paper Authors": "Authors:Hongyan Gu, Jingbin Huang, Lauren Hung, Xiang 'Anthony' Chen"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ". Fogliato et al. discovered that demonstrating AI inference at the start of radiologists\u2019\nreading of X-ray images would increase doctors\u2019 agreement ",
                    "Citation Text": "Riccardo Fogliato, Shreya Chappidi, Matthew Lungren, Paul Fisher, Diane Wilson, Michael Fitzke, Mark Parkinson,\nEric Horvitz, Kori Inkpen, and Besmira Nushi. 2022. Who Goes First? Influences of Human-AI Workflow on Decision\nMaking in Clinical Imaging. In 2022 ACM Conference on Fairness, Accountability, and Transparency . 1362\u20131374.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.09696",
                        "Citation Paper Title": "Title:Who Goes First? Influences of Human-AI Workflow on Decision Making in Clinical Imaging",
                        "Citation Paper Abstract": "Abstract:Details of the designs and mechanisms in support of human-AI collaboration must be considered in the real-world fielding of AI technologies. A critical aspect of interaction design for AI-assisted human decision making are policies about the display and sequencing of AI inferences within larger decision-making workflows. We have a poor understanding of the influences of making AI inferences available before versus after human review of a diagnostic task at hand. We explore the effects of providing AI assistance at the start of a diagnostic session in radiology versus after the radiologist has made a provisional decision. We conducted a user study where 19 veterinary radiologists identified radiographic findings present in patients' X-ray images, with the aid of an AI tool. We employed two workflow configurations to analyze (i) anchoring effects, (ii) human-AI team diagnostic performance and agreement, (iii) time spent and confidence in decision making, and (iv) perceived usefulness of the AI. We found that participants who are asked to register provisional responses in advance of reviewing AI inferences are less likely to agree with the AI regardless of whether the advice is accurate and, in instances of disagreement with the AI, are less likely to seek the second opinion of a colleague. These participants also reported the AI advice to be less useful. Surprisingly, requiring provisional decisions on cases in advance of the display of AI inferences did not lengthen the time participants spent on the task. The study provides generalizable and actionable insights for the deployment of clinical AI tools in human-in-the-loop systems and introduces a methodology for studying alternative designs for human-AI collaboration. We make our experimental platform available as open source to facilitate future research on the influence of alternate designs on human-AI workflows.",
                        "Citation Paper Authors": "Authors:Riccardo Fogliato, Shreya Chappidi, Matthew Lungren, Michael Fitzke, Mark Parkinson, Diane Wilson, Paul Fisher, Eric Horvitz, Kori Inkpen, Besmira Nushi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2008.12566v3": {
            "Paper Title": "A Framework for Improving Scholarly Neural Network Diagrams",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.13341v5": {
            "Paper Title": "AudioViewer: Learning to Visualize Sounds",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": ".\nTo test what kind of visualization is best for humans to\nperceive the translated audio, we train and test our model\non three image datasets: The face attributes dataset CelebA-\nHQ ",
                    "Citation Text": "Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.\nProgressive growing of gans for improved quality, stability,\nand variation. arXiv preprint arXiv:1710.10196 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.10196",
                        "Citation Paper Title": "Title:Progressive Growing of GANs for Improved Quality, Stability, and Variation",
                        "Citation Paper Abstract": "Abstract:We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.",
                        "Citation Paper Authors": "Authors:Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": ", but often en-\ncode information as a high-frequency signal that is invisibleto the human eye and susceptible to adversarial attacks ",
                    "Citation Text": "Casey Chu, Andrey Zhmoginov, and Mark Sandler. Cyclegan,\na master of steganography. arXiv preprint arXiv:1712.02950 ,\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.02950",
                        "Citation Paper Title": "Title:CycleGAN, a Master of Steganography",
                        "Citation Paper Abstract": "Abstract:CycleGAN (Zhu et al. 2017) is one recent successful approach to learn a transformation between two image distributions. In a series of experiments, we demonstrate an intriguing property of the model: CycleGAN learns to \"hide\" information about a source image into the images it generates in a nearly imperceptible, high-frequency signal. This trick ensures that the generator can recover the original sample and thus satisfy the cyclic consistency requirement, while the generated image remains realistic. We connect this phenomenon with adversarial attacks by viewing CycleGAN's training procedure as training a generator of adversarial examples and demonstrate that the cyclic consistency loss causes CycleGAN to be especially vulnerable to adversarial attacks.",
                        "Citation Paper Authors": "Authors:Casey Chu, Andrey Zhmoginov, Mark Sandler"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2009.02998v2": {
            "Paper Title": "A Visualization Interface to Improve the Transparency of Collected\n  Personal Data on the Internet",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.07031v2": {
            "Paper Title": "SenseCare: A Research Platform for Medical Image Informatics and\n  Interactive 3D Visualization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.03577v3": {
            "Paper Title": "Event Based, Near Eye Gaze Tracking Beyond 10,000Hz",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.12291v3": {
            "Paper Title": "Learning a Group-Aware Policy for Robot Navigation",
            "Sentences": [
                {
                    "Sentence ID": 53,
                    "Sentence": ".\n\u000fSocialNCE (w/o CL) : Social NCE policy without con-\ntrastive loss. This policy uses behavior cloning using a\ntrained SARL policy as the expert ",
                    "Citation Text": "Y . Liu, Q. Yan, and A. Alahi, \u201cSocial nce: Contrastive learn-\ning of socially-aware motion representations,\u201d arXiv preprint\narXiv:2012.11717 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.11717",
                        "Citation Paper Title": "Title:Social NCE: Contrastive Learning of Socially-aware Motion Representations",
                        "Citation Paper Abstract": "Abstract:Learning socially-aware motion representations is at the core of recent advances in multi-agent problems, such as human motion forecasting and robot navigation in crowds. Despite promising progress, existing representations learned with neural networks still struggle to generalize in closed-loop predictions (e.g., output colliding trajectories). This issue largely arises from the non-i.i.d. nature of sequential prediction in conjunction with ill-distributed training data. Intuitively, if the training data only comes from human behaviors in safe spaces, i.e., from \"positive\" examples, it is difficult for learning algorithms to capture the notion of \"negative\" examples like collisions. In this work, we aim to address this issue by explicitly modeling negative examples through self-supervision: (i) we introduce a social contrastive loss that regularizes the extracted motion representation by discerning the ground-truth positive events from synthetic negative ones; (ii) we construct informative negative samples based on our prior knowledge of rare but dangerous circumstances. Our method substantially reduces the collision rates of recent trajectory forecasting, behavioral cloning and reinforcement learning algorithms, outperforming state-of-the-art methods on several benchmarks. Our code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Yuejiang Liu, Qi Yan, Alexandre Alahi"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "; in particular, attention-based DRL has\nbeen demonstrated to capture human-human and human-\nrobot interactions in crowded environments ",
                    "Citation Text": "C. Chen, Y . Liu, S. Kreiss, and A. Alahi, \u201cCrowd-robot interaction:\nCrowd-aware robot navigation with attention-based deep reinforce-\nment learning,\u201d 2019 International Conference on Robotics and Au-\ntomation (ICRA) , pp. 6015\u20136022, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.08835",
                        "Citation Paper Title": "Title:Crowd-Robot Interaction: Crowd-aware Robot Navigation with Attention-based Deep Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:Mobility in an effective and socially-compliant manner is an essential yet challenging task for robots operating in crowded spaces. Recent works have shown the power of deep reinforcement learning techniques to learn socially cooperative policies. However, their cooperation ability deteriorates as the crowd grows since they typically relax the problem as a one-way Human-Robot interaction problem. In this work, we want to go beyond first-order Human-Robot interaction and more explicitly model Crowd-Robot Interaction (CRI). We propose to (i) rethink pairwise interactions with a self-attention mechanism, and (ii) jointly model Human-Robot as well as Human-Human interactions in the deep reinforcement learning framework. Our model captures the Human-Human interactions occurring in dense crowds that indirectly affects the robot's anticipation capability. Our proposed attentive pooling mechanism learns the collective importance of neighboring humans with respect to their future states. Various experiments demonstrate that our model can anticipate human dynamics and navigate in crowds with time efficiency, outperforming state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Changan Chen, Yuejiang Liu, Sven Kreiss, Alexandre Alahi"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": ". Deep\nReinforcement Learning (DRL) has also been used for robot\nnavigation ",
                    "Citation Text": "L. Tai, G. Paolo, and M. Liu, \u201cVirtual-to-real deep reinforcement\nlearning: Continuous control of mobile robots for mapless navigation,\u201d\nin2017 IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS) . IEEE, 2017, pp. 31\u201336.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.00420",
                        "Citation Paper Title": "Title:Virtual-to-real Deep Reinforcement Learning: Continuous Control of Mobile Robots for Mapless Navigation",
                        "Citation Paper Abstract": "Abstract:We present a learning-based mapless motion planner by taking the sparse 10-dimensional range findings and the target position with respect to the mobile robot coordinate frame as input and the continuous steering commands as output. Traditional motion planners for mobile ground robots with a laser range sensor mostly depend on the obstacle map of the navigation environment where both the highly precise laser sensor and the obstacle map building work of the environment are indispensable. We show that, through an asynchronous deep reinforcement learning method, a mapless motion planner can be trained end-to-end without any manually designed features and prior demonstrations. The trained planner can be directly applied in unseen virtual and real environments. The experiments show that the proposed mapless motion planner can navigate the nonholonomic mobile robot to the desired targets without colliding with any obstacles.",
                        "Citation Paper Authors": "Authors:Lei Tai, Giuseppe Paolo, Ming Liu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2003.11461v4": {
            "Paper Title": "Emotion Recognition From Gait Analyses: Current Research and Future\n  Directions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08155v5": {
            "Paper Title": "Guided Data Discovery in Interactive Visualizations via Active Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.13226v4": {
            "Paper Title": "Task Dynamics of Prior Training Influence Visual Force Estimation\n  Ability During Teleoperation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.08973v2": {
            "Paper Title": "Exposures Exposed: A Measurement and User Study to Assess Mobile Data\n  Privacy in Context",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.13871v2": {
            "Paper Title": "Signs for Ethical AI: A Route Towards Transparency",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.09749v2": {
            "Paper Title": "Experimenting Touchless Gestural Interaction for a University Public\n  Web-based Display",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.07619v3": {
            "Paper Title": "What Makes a Good and Useful Summary? Incorporating Users in Automatic\n  Summarization Research",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.05242v2": {
            "Paper Title": "Leveraging Affect Transfer Learning for Behavior Prediction in an\n  Intelligent Tutoring System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.11048v6": {
            "Paper Title": "GNNLens: A Visual Analytics Approach for Prediction Error Diagnosis of\n  Graph Neural Networks",
            "Sentences": [
                {
                    "Sentence ID": 49,
                    "Sentence": ".\n5.2.2 Node Features based Metrics\n\u000fThe label distribution of the top-k training nodes with the\nmost similar features. The feature similarity between two\nnodes is de\ufb01ned as the cosine distance between the\nfeature vectors of two nodes ",
                    "Citation Text": "Y. Chen, L. Wu, and M. Zaki, \u201cIterative deep graph learning\nfor graph neural networks: Better and robust node embeddings,\u201d\nAdvances in Neural Information Processing Systems , vol. 33, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.13009",
                        "Citation Paper Title": "Title:Iterative Deep Graph Learning for Graph Neural Networks: Better and Robust Node Embeddings",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose an end-to-end graph learning framework, namely Iterative Deep Graph Learning (IDGL), for jointly and iteratively learning graph structure and graph embedding. The key rationale of IDGL is to learn a better graph structure based on better node embeddings, and vice versa (i.e., better node embeddings based on a better graph structure). Our iterative method dynamically stops when the learned graph structure approaches close enough to the graph optimized for the downstream prediction task. In addition, we cast the graph learning problem as a similarity metric learning problem and leverage adaptive graph regularization for controlling the quality of the learned graph. Finally, combining the anchor-based approximation technique, we further propose a scalable version of IDGL, namely IDGL-Anch, which significantly reduces the time and space complexity of IDGL without compromising the performance. Our extensive experiments on nine benchmarks show that our proposed IDGL models can consistently outperform or match the state-of-the-art baselines. Furthermore, IDGL can be more robust to adversarial graphs and cope with both transductive and inductive learning.",
                        "Citation Paper Authors": "Authors:Yu Chen, Lingfei Wu, Mohammed J. Zaki"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "to illus-\ntrate the concept of GNNs. GNNs can be generally ex-\npressed in a neighborhood aggregation or message passing\nscheme ",
                    "Citation Text": "W. L. Hamilton, R. Ying, and J. Leskovec, \u201cRepresentation learning\non graphs: Methods and applications,\u201d IEEE Data Engineering\nBulletin , vol. 40, no. 3, pp. 52\u201374, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.05584",
                        "Citation Paper Title": "Title:Representation Learning on Graphs: Methods and Applications",
                        "Citation Paper Abstract": "Abstract:Machine learning on graphs is an important and ubiquitous task with applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is finding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about a graph (e.g., degree statistics or kernel functions). However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction. Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph neural networks. We review methods to embed individual nodes as well as approaches to embed entire (sub)graphs. In doing so, we develop a unified framework to describe these recent approaches, and we highlight a number of important applications and directions for future work.",
                        "Citation Paper Authors": "Authors:William L. Hamilton, Rex Ying, Jure Leskovec"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": ". To open the black box of RNNs, clustering meth-\nods and correlation visualizations have been proposed to\nuncover the dynamic hidden states and learned patterns in\nRNNs ",
                    "Citation Text": "Y. Ming, S. Cao, R. Zhang, Z. Li, Y. Chen, Y. Song, and H. Qu,\n\u201cUnderstanding hidden memories of recurrent neural networks,\u201d\ninIEEE Conference on Visual Analytics Science and Technology , 2017,\npp. 13\u201324.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.10777",
                        "Citation Paper Title": "Title:Understanding Hidden Memories of Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:Recurrent neural networks (RNNs) have been successfully applied to various natural language processing (NLP) tasks and achieved better results than conventional methods. However, the lack of understanding of the mechanisms behind their effectiveness limits further improvements on their architectures. In this paper, we present a visual analytics method for understanding and comparing RNN models for NLP tasks. We propose a technique to explain the function of individual hidden state units based on their expected response to input texts. We then co-cluster hidden state units and words based on the expected response and visualize co-clustering results as memory chips and word clouds to provide more structured knowledge on RNNs' hidden states. We also propose a glyph-based sequence visualization based on aggregate information to analyze the behavior of an RNN's hidden state at the sentence-level. The usability and effectiveness of our method are demonstrated through case studies and reviews from domain experts.",
                        "Citation Paper Authors": "Authors:Yao Ming, Shaozu Cao, Ruixiang Zhang, Zhen Li, Yuanzhe Chen, Yangqiu Song, Huamin Qu"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "further simpli\ufb01ed the Chebyshev polynomials\nby using the \ufb01rst order of polynomials and renormalization\ntricks, known as GCNs, which have inspired many follow-\nup studies. Spatial approaches directly de\ufb01ne convolution\non spatially close neighbors ",
                    "Citation Text": "W. Hamilton, Z. Ying, and J. Leskovec, \u201cInductive representation\nlearning on large graphs,\u201d in Advances in Neural Information Pro-\ncessing Systems , 2017, pp. 1024\u20131034.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.02216",
                        "Citation Paper Title": "Title:Inductive Representation Learning on Large Graphs",
                        "Citation Paper Abstract": "Abstract:Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.",
                        "Citation Paper Authors": "Authors:William L. Hamilton, Rex Ying, Jure Leskovec"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.07938v2": {
            "Paper Title": "Deciding Fast and Slow: The Role of Cognitive Biases in AI-assisted\n  Decision-making",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.05567v3": {
            "Paper Title": "Debiased-CAM to mitigate image perturbations with faithful visual\n  explanations of machine learning",
            "Sentences": [
                {
                    "Sentence ID": 78,
                    "Sentence": ". Specific to CNNs, coarser saliency maps can be generated\nby aggregating activation maps as a weighted sum across convo-\nlutional kernels [ 13,70,78,89,102]. For this work, we evaluated\nGrad-CAM ",
                    "Citation Text": "Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedan-\ntam, Devi Parikh, and Dhruv Batra. 2017. Grad-cam: Visual explanations from\ndeep networks via gradient-based localization. In Proceedings of the IEEE inter-\nnational conference on computer vision . 618\u2013626.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.02391",
                        "Citation Paper Title": "Title:Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization",
                        "Citation Paper Abstract": "Abstract:We propose a technique for producing \"visual explanations\" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at this https URL, along with a demo at this http URL, and a video at this http URL.",
                        "Citation Paper Authors": "Authors:Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": ". However, these approaches only train on clean data\nand will reinforce biased explanations if trained on obfuscated or\nbiased data. Unlike conventional self-supervised learning with data\naugmentation and contrastive learning to improve feature learn-\ning ",
                    "Citation Text": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020.\nA simple framework for contrastive learning of visual representations. In Inter-\nnational conference on machine learning . PMLR, 1597\u20131607.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.05709",
                        "Citation Paper Title": "Title:A Simple Framework for Contrastive Learning of Visual Representations",
                        "Citation Paper Abstract": "Abstract:This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.",
                        "Citation Paper Authors": "Authors:Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": ". Another approach indirectly\ntrains attention by ablating salient pixels from input images and\nmaximizing the classification loss between the ablated and original\nimages ",
                    "Citation Text": "Kunpeng Li, Ziyan Wu, Kuan-Chuan Peng, Jan Ernst, and Yun Fu. 2018. Tell me\nwhere to look: Guided attention inference network. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition . 9215\u20139223.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.10171",
                        "Citation Paper Title": "Title:Tell Me Where to Look: Guided Attention Inference Network",
                        "Citation Paper Abstract": "Abstract:Weakly supervised learning with only coarse labels can obtain visual explanations of deep neural network such as attention maps by back-propagating gradients. These attention maps are then available as priors for tasks such as object localization and semantic segmentation. In one common framework we address three shortcomings of previous approaches in modeling such attention maps: We (1) first time make attention maps an explicit and natural component of the end-to-end training, (2) provide self-guidance directly on these maps by exploring supervision form the network itself to improve them, and (3) seamlessly bridge the gap between using weak and extra supervision if available. Despite its simplicity, experiments on the semantic segmentation task demonstrate the effectiveness of our methods. We clearly surpass the state-of-the-art on Pascal VOC 2012 val. and test set. Besides, the proposed framework provides a way not only explaining the focus of the learner but also feeding back with direct guidance towards specific tasks. Under mild assumptions our method can also be understood as a plug-in to existing weakly supervised learners to improve their generalization performance.",
                        "Citation Paper Authors": "Authors:Kunpeng Li, Ziyan Wu, Kuan-Chuan Peng, Jan Ernst, Yun Fu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2006.12453v8": {
            "Paper Title": "Fanoos: Multi-Resolution, Multi-Strength, Interactive Explanations for\n  Learned Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.15227v2": {
            "Paper Title": "Federated Visualization: A Privacy-preserving Strategy for Aggregated\n  Visual Query",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": ". Concerning the distribution of training data, federal learning\napproaches can be classi\ufb01ed into three categories: horizontal\nfederated learning (HFL), vertical federated learning (VFL), and\nFederated Transfer Learning (FTL) ",
                    "Citation Text": "Q. Yang, Y . Liu, T. Chen, and Y . Tong, \u201cFederated Machine Learning:\nConcept and Applications,\u201d ACM Transactions on Intelligent Systems and\nTechnology , vol. 10, no. 2, pp. 1\u201319, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.04885",
                        "Citation Paper Title": "Title:Federated Machine Learning: Concept and Applications",
                        "Citation Paper Abstract": "Abstract:Today's AI still faces two major challenges. One is that in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security. We propose a possible solution to these challenges: secure federated learning. Beyond the federated learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated learning framework, which includes horizontal federated learning, vertical federated learning and federated transfer learning. We provide definitions, architectures and applications for the federated learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allow knowledge to be shared without compromising user privacy.",
                        "Citation Paper Authors": "Authors:Qiang Yang, Yang Liu, Tianjian Chen, Yongxin Tong"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": ".\nBy investigating enhanced privacy protection algorithms, different\nlevels of privacy protection can be achieved in FL at a minor\nloss in model performance ",
                    "Citation Text": "H. B. McMahan, D. Ramage, K. Talwar, and L. Zhang, \u201cLearning\nDifferentially Private Recurrent Language Models,\u201d arXiv preprint\narXiv:1710.06963 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.06963",
                        "Citation Paper Title": "Title:Learning Differentially Private Recurrent Language Models",
                        "Citation Paper Abstract": "Abstract:We demonstrate that it is possible to train large recurrent language models with user-level differential privacy guarantees with only a negligible cost in predictive accuracy. Our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent. In particular, we add user-level privacy protection to the federated averaging algorithm, which makes \"large step\" updates from user-level data. Our work demonstrates that given a dataset with a sufficiently large number of users (a requirement easily met by even small internet-scale datasets), achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work. We find that our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a large dataset.",
                        "Citation Paper Authors": "Authors:H. Brendan McMahan, Daniel Ramage, Kunal Talwar, Li Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2007.14666v4": {
            "Paper Title": "Evaluation of Sampling Methods for Scatterplots",
            "Sentences": [
                {
                    "Sentence ID": 41,
                    "Sentence": "in the \ufb01rst scenario and followed the local\noutlier factor algorithm ",
                    "Citation Text": "F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel,\nM. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg, J. Vanderplas, A. Pas-\nsos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-\nlearn: Machine learning in Python. Journal of Machine Learning Research ,\n12(85):2825\u20132830, 2011.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1201.0490",
                        "Citation Paper Title": "Title:Scikit-learn: Machine Learning in Python",
                        "Citation Paper Abstract": "Abstract:Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from this http URL.",
                        "Citation Paper Authors": "Authors:Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Andreas M\u00fcller, Joel Nothman, Gilles Louppe, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, \u00c9douard Duchesnay"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2010.02002v4": {
            "Paper Title": "Enhancing Haptic Distinguishability of Surface Materials with Boosting\n  Technique",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.03196v5": {
            "Paper Title": "Towards Better Driver Safety: Empowering Personal Navigation\n  Technologies with Road Safety Awareness",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": ". As such, we applied a data re-sampling technique, a common\ncountermeasure for handling class-imbalance problems in applied\nmachine learning research (e.g., [ 7,9,66]). Specifically, we applied\nthe commonly-used Synthetic Minority Over-sampling Technique\n(SMOTE) ",
                    "Citation Text": "Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall, and W. Philip Kegelmeyer.\n2002. SMOTE: Synthetic Minority Over-sampling Technique. J. Artif. Int. Res. 16,\n1 (June 2002), 321\u2013357. http://dl.acm.org/citation.cfm?id=1622407.1622416",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1106.1813",
                        "Citation Paper Title": "Title:SMOTE: Synthetic Minority Over-sampling Technique",
                        "Citation Paper Abstract": "Abstract:An approach to the construction of classifiers from    imbalanced datasets is described. A dataset is imbalanced if the    classification categories are not approximately equally    represented. Often real-world data sets are predominately composed of    \"normal\" examples with only a small percentage of \"abnormal\" or    \"interesting\" examples. It is also the case that the cost of    misclassifying an abnormal (interesting) example as a normal example    is often much higher than the cost of the reverse    error. Under-sampling of the majority (normal) class has been proposed    as a good means of increasing the sensitivity of a classifier to the    minority class. This paper shows that a combination of our method of    over-sampling the minority (abnormal) class and under-sampling the    majority (normal) class can achieve better classifier performance (in    ROC space) than only under-sampling the majority class.  This paper    also shows that a combination of our method of over-sampling the    minority class and under-sampling the majority class can achieve    better classifier performance (in ROC space) than varying the loss    ratios in Ripper or class priors in Naive Bayes. Our method of    over-sampling the minority class involves creating synthetic minority    class examples.  Experiments are performed using C4.5, Ripper and a    Naive Bayes classifier. The method is evaluated using the area under    the Receiver Operating Characteristic curve (AUC) and the ROC convex    hull strategy.",
                        "Citation Paper Authors": "Authors:N. V. Chawla, K. W. Bowyer, L. O. Hall, W. P. Kegelmeyer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2005.00777v3": {
            "Paper Title": "Deep Feature Mining via Attention-based BiLSTM-GCN for Human Motor\n  Imagery Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04322v3": {
            "Paper Title": "An Empirical Evaluation of Bluetooth-based Decentralized Contact Tracing\n  in Crowds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.11483v3": {
            "Paper Title": "Quarantined! Examining the Effects of a Community-Wide Moderation\n  Intervention on Reddit",
            "Sentences": [
                {
                    "Sentence ID": 101,
                    "Sentence": ", TD also heavily influenced the news content on other social media\nsites like Twitter ",
                    "Citation Text": "Savvas Zannettou, Tristan Caulfield, Emiliano De Cristofaro, Nicolas Kourtelris, Ilias Leontiadis, Michael Sirivianos,\nGianluca Stringhini, and Jeremy Blackburn. 2017. The web centipede: understanding how web communities influence\neach other through the lens of mainstream and alternative news sources. In Proceedings of the 2017 Internet Measurement\nConference . 405\u2013417.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.06947",
                        "Citation Paper Title": "Title:The Web Centipede: Understanding How Web Communities Influence Each Other Through the Lens of Mainstream and Alternative News Sources",
                        "Citation Paper Abstract": "Abstract:As the number and the diversity of news outlets on the Web grow, so does the opportunity for \"alternative\" sources of information to emerge. Using large social networks like Twitter and Facebook, misleading, false, or agenda-driven information can quickly and seamlessly spread online, deceiving people or influencing their opinions. Also, the increased engagement of tightly knit communities, such as Reddit and 4chan, further compounds the problem, as their users initiate and propagate alternative information, not only within their own communities, but also to different ones as well as various social media. In fact, these platforms have become an important piece of the modern information ecosystem, which, thus far, has not been studied as a whole.\nIn this paper, we begin to fill this gap by studying mainstream and alternative news shared on Twitter, Reddit, and 4chan. By analyzing millions of posts around several axes, we measure how mainstream and alternative news flows between these platforms. Our results indicate that alt-right communities within 4chan and Reddit can have a surprising level of influence on Twitter, providing evidence that \"fringe\" communities often succeed in spreading alternative news to mainstream social networks and the greater Web.",
                        "Citation Paper Authors": "Authors:Savvas Zannettou, Tristan Caulfield, Emiliano De Cristofaro, Nicolas Kourtellis, Ilias Leontiadis, Michael Sirivianos, Gianluca Stringhini, Jeremy Blackburn"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2011.09696v2": {
            "Paper Title": "Towards Emotion-Aware User Simulator for Task-Oriented Dialogue",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "presented a state2seq user simulator for\ntask-completion conversation.\nAlthough the discrepancies between the simulator and the real users might result in a sub-optimal agent ",
                    "Citation Text": "Baolin Peng, Xiujun Li, Jianfeng Gao, Jingjing Liu, and Kam-Fai Wong. Deep dyna-q: Integrating planning\nfor task-completion dialogue policy learning. In Proceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) , pages 2182\u20132192, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.06176",
                        "Citation Paper Title": "Title:Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning",
                        "Citation Paper Abstract": "Abstract:Training a task-completion dialogue agent via reinforcement learning (RL) is costly because it requires many interactions with real users. One common alternative is to use a user simulator. However, a user simulator usually lacks the language complexity of human interlocutors and the biases in its design may tend to degrade the agent. To address these issues, we present Deep Dyna-Q, which to our knowledge is the first deep RL framework that integrates planning for task-completion dialogue policy learning. We incorporate into the dialogue agent a model of the environment, referred to as the world model, to mimic real user response and generate simulated experience. During dialogue policy learning, the world model is constantly updated with real user experience to approach real user behavior, and in turn, the dialogue agent is optimized using both real experience and simulated experience. The effectiveness of our approach is demonstrated on a movie-ticket booking task in both simulated and human-in-the-loop settings.",
                        "Citation Paper Authors": "Authors:Baolin Peng, Xiujun Li, Jianfeng Gao, Jingjing Liu, Kam-Fai Wong, Shang-Yu Su"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2011.06171v2": {
            "Paper Title": "The Usability of Ownership",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.05801v3": {
            "Paper Title": "Migratable AI: Effect of identity and information migration on users\n  perception of conversational AI agents",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.03491v2": {
            "Paper Title": "AI-enabled Prediction of eSports Player Performance Using the Data from\n  Heterogeneous Sensors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00958v2": {
            "Paper Title": "Collection and Validation of Psychophysiological Data from Professional\n  and Amateur Players: a Multimodal eSports Dataset",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": ", the authors\nleverage players\u2019 match history data to estimate players\u2019 cur-\nrent psychological state and use this estimation to predict the\noutcome of matches. Other history-based outcome prediction\napproaches include machine learning on hand-crafted features\nfor the MOBA genre ",
                    "Citation Text": "Y . Yang, T. Qin, and Y .-H. Lei, \u201cReal-time esports match result predic-\ntion,\u201d arXiv preprint arXiv:1701.03162 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1701.03162",
                        "Citation Paper Title": "Title:Real-time eSports Match Result Prediction",
                        "Citation Paper Abstract": "Abstract:In this paper, we try to predict the winning team of a match in the multiplayer eSports game Dota 2. To address the weaknesses of previous work, we consider more aspects of prior (pre-match) features from individual players' match history, as well as real-time (during-match) features at each minute as the match progresses. We use logistic regression, the proposed Attribute Sequence Model, and their combinations as the prediction models. In a dataset of 78362 matches where 20631 matches contain replay data, our experiments show that adding more aspects of prior features improves accuracy from 58.69% to 71.49%, and introducing real-time features achieves up to 93.73% accuracy when predicting at the 40th minute.",
                        "Citation Paper Authors": "Authors:Yifan Yang, Tian Qin, Yu-Heng Lei"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2008.10772v6": {
            "Paper Title": "Adapting Security Warnings to Counter Online Disinformation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.00337v2": {
            "Paper Title": "NHSS: A Speech and Singing Parallel Database",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.07884v2": {
            "Paper Title": "(Un)clear and (In)conspicuous: The right to opt-out of sale under CCPA",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": "; however, in 2019 more than 95% of banners\nfailed to meet GDPR requirements by offering users no or insuffi-\ncient choices [ 34,37], and in 2020 just 11.8% of banners analyzed\nmet minimal GDPR requirements ",
                    "Citation Text": "Midas Nouwens, Ilaria Liccardi, Michael Veale, David Karger, and Lalana Kagal.\n2020. Dark patterns after the GDPR: Scraping consent pop-ups and demonstrating\ntheir influence. In Proceedings of the 2020 CHI Conference on Human Factors in\nComputing Systems . 1\u201313.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2001.02479",
                        "Citation Paper Title": "Title:Dark Patterns after the GDPR: Scraping Consent Pop-ups and Demonstrating their Influence",
                        "Citation Paper Abstract": "Abstract:New consent management platforms (CMPs) have been introduced to the web to conform with the EU's General Data Protection Regulation, particularly its requirements for consent when companies collect and process users' personal data. This work analyses how the most prevalent CMP designs affect people's consent choices. We scraped the designs of the five most popular CMPs on the top 10,000 websites in the UK (n=680). We found that dark patterns and implied consent are ubiquitous; only 11.8% meet the minimal requirements that we set based on European law. Second, we conducted a field experiment with 40 participants to investigate how the eight most common designs affect consent choices. We found that notification style (banner or barrier) has no effect; removing the opt-out button from the first page increases consent by 22--23 percentage points; and providing more granular controls on the first page decreases consent by 8--20 percentage points. This study provides an empirical basis for the necessary regulatory action to enforce the GDPR, in particular the possibility of focusing on the centralised, third-party CMP services as an effective way to increase compliance.",
                        "Citation Paper Authors": "Authors:Midas Nouwens, Ilaria Liccardi, Michael Veale, David Karger, Lalana Kagal"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2007.11117v2": {
            "Paper Title": "Interpretable Anomaly Detection with DIFFI: Depth-based Isolation Forest\n  Feature Importance",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": ", which proposes an improvement of the permutation im-\nportance measure based on a conditional permutation scheme, and ",
                    "Citation Text": "X. Li, Y. Wang, S. Basu, K. Kumbier, B. Yu, A debiased mdi feature im-\nportance measure for random forests, arXiv preprint arXiv:1906.10845\n(2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.10845",
                        "Citation Paper Title": "Title:A Debiased MDI Feature Importance Measure for Random Forests",
                        "Citation Paper Abstract": "Abstract:Tree ensembles such as Random Forests have achieved impressive empirical success across a wide variety of applications. To understand how these models make predictions, people routinely turn to feature importance measures calculated from tree ensembles. It has long been known that Mean Decrease Impurity (MDI), one of the most widely used measures of feature importance, incorrectly assigns high importance to noisy features, leading to systematic bias in feature selection. In this paper, we address the feature selection bias of MDI from both theoretical and methodological perspectives. Based on the original definition of MDI by Breiman et al. for a single tree, we derive a tight non-asymptotic bound on the expected bias of MDI importance of noisy features, showing that deep trees have higher (expected) feature selection bias than shallow ones. However, it is not clear how to reduce the bias of MDI using its existing analytical expression. We derive a new analytical expression for MDI, and based on this new expression, we are able to propose a debiased MDI feature importance measure using out-of-bag samples, called MDI-oob. For both the simulated data and a genomic ChIP dataset, MDI-oob achieves state-of-the-art performance in feature selection from Random Forests for both deep and shallow trees.",
                        "Citation Paper Authors": "Authors:Xiao Li, Yu Wang, Sumanta Basu, Karl Kumbier, Bin Yu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2012.14201v2": {
            "Paper Title": "StudyU: a platform for designing and conducting innovative digital\n  N-of-1 trials",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.09883v4": {
            "Paper Title": "A Review of Critical Features and General Issues of Freely Available\n  mHealth Apps For Dietary Assessment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.13274v5": {
            "Paper Title": "Exploring the contextual factors affecting multimodal emotion\n  recognition in videos",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.05505v6": {
            "Paper Title": "Assessment Modeling: Fundamental Pre-training Tasks for Interactive\n  Educational Systems",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": "to represent word tokens. By applying convolutional neural networks to the sequence of\ntext tokens and an attention mechanism to the series of sentences, the model quantifies the difficulty of the question.\nQuesNet ",
                    "Citation Text": "Yu Yin, Qi Liu, Zhenya Huang, Enhong Chen, Wei Tong, Shijin Wang, and Yu Su. 2019. QuesNet: A Unified Representation for Heterogeneous Test\nQuestions. arXiv preprint arXiv:1905.10949 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.10949",
                        "Citation Paper Title": "Title:QuesNet: A Unified Representation for Heterogeneous Test Questions",
                        "Citation Paper Abstract": "Abstract:Understanding learning materials (e.g. test questions) is a crucial issue in online learning systems, which can promote many applications in education domain. Unfortunately, many supervised approaches suffer from the problem of scarce human labeled data, whereas abundant unlabeled resources are highly underutilized. To alleviate this problem, an effective solution is to use pre-trained representations for question understanding. However, existing pre-training methods in NLP area are infeasible to learn test question representations due to several domain-specific characteristics in education. First, questions usually comprise of heterogeneous data including content text, images and side information. Second, there exists both basic linguistic information as well as domain logic and knowledge. To this end, in this paper, we propose a novel pre-training method, namely QuesNet, for comprehensively learning question representations. Specifically, we first design a unified framework to aggregate question information with its heterogeneous inputs into a comprehensive vector. Then we propose a two-level hierarchical pre-training algorithm to learn better understanding of test questions in an unsupervised way. Here, a novel holed language model objective is developed to extract low-level linguistic features, and a domain-oriented objective is proposed to learn high-level logic and knowledge. Moreover, we show that QuesNet has good capability of being fine-tuned in many question-based tasks. We conduct extensive experiments on large-scale real-world question data, where the experimental results clearly demonstrate the effectiveness of QuesNet for question understanding as well as its superior applicability.",
                        "Citation Paper Authors": "Authors:Yu Yin, Qi Liu, Zhenya Huang, Enhong Chen, Wei Tong, Shijin Wang, Yu Su"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "is a model that\nutilizes the semantic representations of text materials (document, question and options) to predict exam question\ndifficulty (i.e. the percentage of examinees with wrong answer for a particular question). TACNN uses pre-trained\nword2vec embeddings ",
                    "Citation Text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their\ncompositionality. In Advances in neural information processing systems . 3111\u20133119.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1310.4546",
                        "Citation Paper Title": "Title:Distributed Representations of Words and Phrases and their Compositionality",
                        "Citation Paper Abstract": "Abstract:The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
                        "Citation Paper Authors": "Authors:Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": ". Pre-training has been shown to enhance the performance of models in various\nfields including NLP [ 2,5,7,15,17,25], Computer Vision [ 3,22] and Speech Recognition ",
                    "Citation Text": "Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. 2019. wav2vec: Unsupervised Pre-training for Speech Recognition. arXiv\npreprint arXiv:1904.05862 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.05862",
                        "Citation Paper Title": "Title:wav2vec: Unsupervised Pre-training for Speech Recognition",
                        "Citation Paper Abstract": "Abstract:We explore unsupervised pre-training for speech recognition by learning representations of raw audio. wav2vec is trained on large amounts of unlabeled audio data and the resulting representations are then used to improve acoustic model training. We pre-train a simple multi-layer convolutional neural network optimized via a noise contrastive binary classification task. Our experiments on WSJ reduce WER of a strong character-based log-mel filterbank baseline by up to 36% when only a few hours of transcribed data is available. Our approach achieves 2.43% WER on the nov92 test set. This outperforms Deep Speech 2, the best reported character-based system in the literature while using two orders of magnitude less labeled training data.",
                        "Citation Paper Authors": "Authors:Steffen Schneider, Alexei Baevski, Ronan Collobert, Michael Auli"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2001.02807v4": {
            "Paper Title": "smartSDH: An Experimental Study of Mechanism Based Building Control",
            "Sentences": []
        }
    }
}