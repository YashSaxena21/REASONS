{
    "Robotics": {
        "http://arxiv.org/abs/1710.05502v3": {
            "Paper Title": "Non-iterative SLAM for Warehouse Robots Using Ground Textures",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.04132v3": {
            "Paper Title": "A Number Sense as an Emergent Property of the Manipulating Brain",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.04634v2": {
            "Paper Title": "Accurate 3D Object Detection using Energy-Based Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.09435v2": {
            "Paper Title": "Multilevel Motion Planning: A Fiber Bundle Formulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00685v2": {
            "Paper Title": "Fast Biconnectivity Restoration in Multi-Robot Systems for Robust\n  Communication Maintenance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.07917v2": {
            "Paper Title": "Gathering on a Circle with Limited Visibility by Anonymous Oblivious\n  Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.12204v4": {
            "Paper Title": "Learning NP-Hard Multi-Agent Assignment Planning using GNN: Inference on\n  a Random Graph and Provable Auction-Fitted Q-learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07089v3": {
            "Paper Title": "Robust Quadruped Jumping via Deep Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.08722v3": {
            "Paper Title": "RAIST: Learning Risk Aware Traffic Interactions via Spatio-Temporal\n  Graph Convolutional Networks",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": "have demonstrated significant progress\nin tasks such as action recognition ",
                    "Citation Text": "Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial temporal graph\nconvolutional networks for skeleton-based action recognition. arXiv\npreprint arXiv:1801.07455 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.07455",
                        "Citation Paper Title": "Title:Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition",
                        "Citation Paper Abstract": "Abstract:Dynamics of human body skeletons convey significant information for human action recognition. Conventional approaches for modeling skeletons usually rely on hand-crafted parts or traversal rules, thus resulting in limited expressive power and difficulties of generalization. In this work, we propose a novel model of dynamic skeletons called Spatial-Temporal Graph Convolutional Networks (ST-GCN), which moves beyond the limitations of previous methods by automatically learning both the spatial and temporal patterns from data. This formulation not only leads to greater expressive power but also stronger generalization capability. On two large datasets, Kinetics and NTU-RGBD, it achieves substantial improvements over mainstream methods.",
                        "Citation Paper Authors": "Authors:Sijie Yan, Yuanjun Xiong, Dahua Lin"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2011.09246v2": {
            "Paper Title": "Experimental Study on Reinforcement Learning-based Control of an Acrobot",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.07186v4": {
            "Paper Title": "Towards Learning to Imitate from a Single Video Demonstration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08311v3": {
            "Paper Title": "Formal Verification of Robustness and Resilience of Learning-Enabled\n  State Estimation Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.00822v3": {
            "Paper Title": "Open-Ended Multi-Modal Relational Reasoning for Video Question Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.01240v3": {
            "Paper Title": "Learning rewards for robotic ultrasound scanning using probabilistic\n  temporal ranking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.03385v4": {
            "Paper Title": "Learning to Rearrange Deformable Cables, Fabrics, and Bags with\n  Goal-Conditioned Transporter Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.06676v4": {
            "Paper Title": "UnRectDepthNet: Self-Supervised Monocular Depth Estimation using a\n  Generic Framework for Handling Common Camera Distortion Models",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": "mainly to reduce the chance of training reaching\na local minimum. The overall objective function is averagedMethod Resolution Dataset Abs Rel Sq Rel RMSE RMSE log\u03b4 <1.25\u03b4 <1.252\u03b4 <1.253\nlower is better higher is betterOriginal ",
                    "Citation Text": "D. Eigen, C. Puhrsch, and R. Fergus, \u201cDepth map prediction from a\nsingle image using a multi-scale deep network,\u201d in Advances in neural\ninformation processing systems , 2014, pp. 2366\u20132374. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.2283",
                        "Citation Paper Title": "Title:Depth Map Prediction from a Single Image using a Multi-Scale Deep Network",
                        "Citation Paper Abstract": "Abstract:Predicting depth is an essential component in understanding the 3D geometry of a scene. While for stereo images local correspondence suffices for estimation, finding depth relations from a single image is less straightforward, requiring integration of both global and local information from various cues. Moreover, the task is inherently ambiguous, with a large source of uncertainty coming from the overall scale. In this paper, we present a new method that addresses this task by employing two deep network stacks: one that makes a coarse global prediction based on the entire image, and another that refines this prediction locally. We also apply a scale-invariant error to help measure depth relations rather than scale. By leveraging the raw datasets as large sources of training data, our method achieves state-of-the-art results on both NYU Depth and KITTI, and matches detailed depth boundaries without the need for superpixelation.",
                        "Citation Paper Authors": "Authors:David Eigen, Christian Puhrsch, Rob Fergus"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2009.12369v2": {
            "Paper Title": "On Two-Handed Planar Assembly Partitioning with Connectivity Constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.08682v2": {
            "Paper Title": "SeekNet: Improved Human Instance Segmentation and Tracking via\n  Reinforcement Learning Based Optimized Robot Relocation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.09438v4": {
            "Paper Title": "EWareNet: Emotion Aware Human Intent Prediction and Adaptive Spatial\n  Profile Fusion for Social Robot Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.08944v5": {
            "Paper Title": "Near-Optimal Multi-Robot Motion Planning with Finite Sampling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.00839v2": {
            "Paper Title": "Fabric Defect Detection Using Vision-Based Tactile Sensor",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08506v4": {
            "Paper Title": "Direct Policy Optimization using Deterministic Sampling and Collocation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.00440v2": {
            "Paper Title": "From Drinking Philosophers to Asynchronous Path-Following Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.01109v2": {
            "Paper Title": "Collision Probabilities for Continuous-Time Systems Without Sampling\n  [with Appendices]",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.02124v2": {
            "Paper Title": "Generalized Object Detection on Fisheye Cameras for Autonomous Driving:\n  Dataset, Representations and Baseline",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.13077v2": {
            "Paper Title": "Self-Supervised Attention Learning for Depth and Ego-motion Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.09756v4": {
            "Paper Title": "Active Inference and Behavior Trees for Reactive Action Planning and\n  Execution in Robotics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.03053v3": {
            "Paper Title": "Scenario-Transferable Semantic Graph Reasoning for Interaction-Aware\n  Probabilistic Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01055v6": {
            "Paper Title": "QuaRL: Quantization for Fast and Environmentally Sustainable\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.00421v5": {
            "Paper Title": "Air Learning: A Deep Reinforcement Learning Gym for Autonomous Aerial\n  Robot Visual Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.14139v2": {
            "Paper Title": "FutureMapping 2: Gaussian Belief Propagation for Spatial AI",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.03218v5": {
            "Paper Title": "AFR: An Efficient Buffering Algorithm for Cloud Robotic Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.07975v3": {
            "Paper Title": "Learning Visual Robotic Control Efficiently with Contrastive\n  Pre-training and Data Augmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.16162v3": {
            "Paper Title": "Imitative Planning using Conditional Normalizing Flow",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.10033v2": {
            "Paper Title": "Cognition-inspired homeostasis can balance conflicting needs in robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.09045v3": {
            "Paper Title": "Double-Prong ConvLSTM for Spatiotemporal Occupancy Prediction in Dynamic\n  Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.10726v5": {
            "Paper Title": "MINVO Basis: Finding Simplexes with Minimum Volume Enclosing Polynomial\n  Curves",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.10686v5": {
            "Paper Title": "Zero-Shot Imitating Collaborative Manipulation Plans from YouTube\n  Cooking Videos",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.14692v6": {
            "Paper Title": "Bidirectional Sampling Based Search Without Two Point Boundary Value\n  Solution",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.09705v9": {
            "Paper Title": "Explainable Goal-Driven Agents and Robots -- A Comprehensive Review",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.00611v4": {
            "Paper Title": "Neural Lyapunov Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.06461v2": {
            "Paper Title": "Intelligent Physical Attack Against Mobile Robots With\n  Obstacle-Avoidance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.00411v4": {
            "Paper Title": "A Feasibility-Driven Approach to Control-Limited DDP",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02043v2": {
            "Paper Title": "Deep-Learning-Aided Path Planning and Map Construction for Expediting\n  Indoor Mapping",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.02159v3": {
            "Paper Title": "Learning from Sparse Demonstrations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.15014v3": {
            "Paper Title": "Learning from Human Directional Corrections",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03183v4": {
            "Paper Title": "Learning Object-Based State Estimators for Household Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.12291v3": {
            "Paper Title": "Learning a Group-Aware Policy for Robot Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00462v2": {
            "Paper Title": "Alternating Direction Method of Multipliers for Constrained Iterative\n  LQR in Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.14053v2": {
            "Paper Title": "SPINS: Structure Priors aided Inertial Navigation System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.08482v4": {
            "Paper Title": "Collaborative Three-Tier Architecture Non-contact Respiratory Rate\n  Monitoring using Target Tracking and False Peaks Eliminating Algorithms",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.10283v3": {
            "Paper Title": "Adding Neural Network Controllers to Behavior Trees without Destroying\n  Performance Guarantees",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.11892v4": {
            "Paper Title": "Multi-Robot Path Planning Using Medial-Axis-Based Pebble-Graph Embedding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.13226v4": {
            "Paper Title": "Task Dynamics of Prior Training Influence Visual Force Estimation\n  Ability During Teleoperation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02112v4": {
            "Paper Title": "Toward Force Estimation in Robot-Assisted Surgery using Deep Learning\n  with Vision and Robot State",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.09167v2": {
            "Paper Title": "Imitation Learning with Sinkhorn Distances",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.11310v3": {
            "Paper Title": "From Seeing to Moving: A Survey on Learning for Visual Indoor Navigation\n  (VIN)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.08454v2": {
            "Paper Title": "Exploring the Impacts from Datasets to Monocular Depth Estimation (MDE)\n  Models with MineNavi",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.04019v4": {
            "Paper Title": "Data-Augmented Contact Model for Rigid Body Simulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.14549v2": {
            "Paper Title": "Improving Automated Driving through POMDP Planning with Human Internal\n  States",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.03555v5": {
            "Paper Title": "Improving Makespan in Dynamic Task Scheduling for Cloud Robotic Systems\n  with Time Window Constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.13965v5": {
            "Paper Title": "Efficient Jacobian-Based Inverse Kinematics with Sim-to-Real Transfer of\n  Soft Robots by Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.07051v2": {
            "Paper Title": "Probabilistic completeness of RRT for geometric and kinodynamic planning\n  with forward propagation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.01887v4": {
            "Paper Title": "DoorGym: A Scalable Door Opening Environment And Baseline Agent",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.12494v2": {
            "Paper Title": "SEMI: Self-supervised Exploration via Multisensory Incongruity",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.13077v2": {
            "Paper Title": "Learning Selective Sensor Fusion for States Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.14509v4": {
            "Paper Title": "Towards Target-Driven Visual Navigation in Indoor Scenes via Generative\n  Imitation Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.04078v7": {
            "Paper Title": "Reinforcement Learning-based Visual Navigation with\n  Information-Theoretic Regularization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.00277v2": {
            "Paper Title": "Active Learning of Probabilistic Movement Primitives",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.03026v3": {
            "Paper Title": "Learning Task Constraints from Demonstration for Hybrid Force/Position\n  Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04782v2": {
            "Paper Title": "Planning under Uncertainty to Goal Distributions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.01297v5": {
            "Paper Title": "Cooperative Manipulation via Internal Force Regulation: A Rigidity\n  Theory Perspective",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.09792v2": {
            "Paper Title": "Real-Time Detectors for Digital and Physical Adversarial Inputs to\n  Perception Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.11991v3": {
            "Paper Title": "Atlas Fusion -- Modern Framework for Autonomous Agent Sensor Data Fusion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.10862v4": {
            "Paper Title": "An Intuitive Tutorial to Gaussian Processes Regression",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.12438v2": {
            "Paper Title": "State of the Art of Adaptive Cruise Control and Stop and Go Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.07462v3": {
            "Paper Title": "PL-VINS: Real-Time Monocular Visual-Inertial SLAM with Point and Line\n  Features",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.02769v4": {
            "Paper Title": "Online Exploration of an Unknown Region of Interest with a Team of\n  Aerial Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.06252v2": {
            "Paper Title": "SVAM: Saliency-guided Visual Attention Modeling by Autonomous Underwater\n  Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.03736v2": {
            "Paper Title": "Learning Mixed-Integer Convex Optimization Strategies for Robot Planning\n  and Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.15009v4": {
            "Paper Title": "A Unifying Framework for Reinforcement Learning and Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.04291v2": {
            "Paper Title": "Learning Stable Models for Prediction and Control",
            "Sentences": [
                {
                    "Sentence ID": 66,
                    "Sentence": "(referred to\nas SOC) to \ufb01nd locally optimal stable solutions \u02dcKd. The SOC\nalgorithm builds upon the work in ",
                    "Citation Text": "N. Gillis, M. Karow, and P. Sharma, \u201cApproximating the nearest stable\ndiscrete-time system,\u201d Linear Algebra and its Applications , vol. 573, pp.\n37\u201353, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.08033",
                        "Citation Paper Title": "Title:Approximating the nearest stable discrete-time system",
                        "Citation Paper Abstract": "Abstract:In this paper, we consider the problem of stabilizing discrete-time linear systems by computing a nearby stable matrix to an unstable one. To do so, we provide a new characterization for the set of stable matrices. We show that a matrix $A$ is stable if and only if it can be written as $A=S^{-1}UBS$, where $S$ is positive definite, $U$ is orthogonal, and $B$ is a positive semidefinite contraction (that is, the singular values of $B$ are less or equal to 1). This characterization results in an equivalent non-convex optimization problem with a feasible set on which it is easy to project. We propose a very efficient fast projected gradient method to tackle the problem in variables $(S,U,B)$ and generate locally optimal solutions. We show the effectiveness of the proposed method compared to other approaches.",
                        "Citation Paper Authors": "Authors:Nicolas Gillis, Michael Karow, Punit Sharma"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2005.02878v5": {
            "Paper Title": "Multi-Resolution POMDP Planning for Multi-Object Search in 3D",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01208v5": {
            "Paper Title": "Distributed Attack-Robust Submodular Maximization for Multi-Robot\n  Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.10492v2": {
            "Paper Title": "Risk-Aware Submodular Optimization for Multi-Robot Coordination",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.07744v2": {
            "Paper Title": "Guaranteed Globally Optimal Planar Pose Graph and Landmark SLAM via\n  Sparse-Bounded Sums-of-Squares Programming",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.08977v2": {
            "Paper Title": "Visually Grounding Language Instruction for History-Dependent\n  Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.00708v2": {
            "Paper Title": "Learning Search Space Partition for Black-box Optimization using Monte\n  Carlo Tree Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.13880v2": {
            "Paper Title": "REAL-X -- Robot open-Ended Autonomous Learning Architectures: Achieving\n  Truly End-to-End Sensorimotor Autonomous Learning Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.13112v2": {
            "Paper Title": "Motion Planning and Control for Mobile Robot Navigation Using Machine\n  Learning: a Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.05857v2": {
            "Paper Title": "Offline Learning of Counterfactual Predictions for Real-World Robotic\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.08319v2": {
            "Paper Title": "Multi-Step Recurrent Q-Learning for Robotic Velcro Peeling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.07451v3": {
            "Paper Title": "QoS Aware Robot Trajectory Optimization with IRS-Assisted\n  Millimeter-Wave Communications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.11700v4": {
            "Paper Title": "Safe Active Dynamics Learning and Control: A Sequential\n  Exploration-Exploitation Framework",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.12453v8": {
            "Paper Title": "Fanoos: Multi-Resolution, Multi-Strength, Interactive Explanations for\n  Learned Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03748v6": {
            "Paper Title": "B-GAP: Behavior-Rich Simulation and Navigation for Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.08152v4": {
            "Paper Title": "ForMIC: Foraging via Multiagent RL with Implicit Communication",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.09343v3": {
            "Paper Title": "SelfVoxeLO: Self-supervised LiDAR Odometry with Voxel-based Deep Neural\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.15034v2": {
            "Paper Title": "Learning Objective Functions Incrementally by Inverse Optimal Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.07381v2": {
            "Paper Title": "Dissecting liabilities in adversarial surgical robot failures: A\n  national (Danish) and European law perspective",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.12515v3": {
            "Paper Title": "On modularity in reactive control architectures, with an application to\n  formal verification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.07153v4": {
            "Paper Title": "Cover Combinatorial Filters and their Minimization Problem (Extended\n  Version)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.03306v3": {
            "Paper Title": "Grasping and Manipulation with a Multi-Fingered Hand",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.03040v2": {
            "Paper Title": "Understanding Bird's-Eye View of Road Semantics using an Onboard Camera",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.05430v6": {
            "Paper Title": "New Formulation of Mixed-Integer Conic Programming for Globally Optimal\n  Grasp Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.10490v3": {
            "Paper Title": "Perception-Based Temporal Logic Planning in Uncertain Semantic Maps",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.07207v4": {
            "Paper Title": "NeoNav: Improving the Generalization of Visual Navigation via Generating\n  Next Expected Observations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.14406v3": {
            "Paper Title": "Transporter Networks: Rearranging the Visual World for Robotic\n  Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.05058v4": {
            "Paper Title": "Dynamic Object Removal and Spatio-Temporal RGB-D Inpainting via\n  Geometry-Aware Adversarial Learning",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": "\ufb01rst\nsynthesize missing optical \ufb02ow which is then used to propagate\nneighboring pixels to \ufb01ll missing regions. Chang et al. ",
                    "Citation Text": "Y .-L. Chang, Z. Y . Liu, K.-Y . Lee, and W. Hsu, \u201cLearnable gated\ntemporal shift module for deep video inpainting\u201d,\u201d British Machine Vision\nConference (BMVC) , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.01131",
                        "Citation Paper Title": "Title:Learnable Gated Temporal Shift Module for Deep Video Inpainting",
                        "Citation Paper Abstract": "Abstract:How to efficiently utilize temporal information to recover videos in a consistent way is the main issue for video inpainting problems. Conventional 2D CNNs have achieved good performance on image inpainting but often lead to temporally inconsistent results where frames will flicker when applied to videos (see this https URL 3D CNNs can capture temporal information but are computationally intensive and hard to train. In this paper, we present a novel component termed Learnable Gated Temporal Shift Module (LGTSM) for video inpainting models that could effectively tackle arbitrary video masks without additional parameters from 3D convolutions. LGTSM is designed to let 2D convolutions make use of neighboring frames more efficiently, which is crucial for video inpainting. Specifically, in each layer, LGTSM learns to shift some channels to its temporal neighbors so that 2D convolutions could be enhanced to handle temporal information. Meanwhile, a gated convolution is applied to the layer to identify the masked areas that are poisoning for conventional convolutions. On the FaceForensics and Free-form Video Inpainting (FVI) dataset, our model achieves state-of-the-art results with simply 33% of parameters and inference time.",
                        "Citation Paper Authors": "Authors:Ya-Liang Chang, Zhe Yu Liu, Kuan-Ying Lee, Winston Hsu"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2010.11703v2": {
            "Paper Title": "Fast and Incremental Loop Closure Detection with Deep Features and\n  Proximity Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.04333v2": {
            "Paper Title": "Reactive Task and Motion Planning for Robust Whole-Body Dynamic\n  Locomotion in Constrained Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.14325v5": {
            "Paper Title": "Optimal Probabilistic Motion Planning with Potential Infeasible LTL\n  Constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.04650v2": {
            "Paper Title": "State-Only Imitation Learning for Dexterous Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.04954v2": {
            "Paper Title": "ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.07380v2": {
            "Paper Title": "Analysis and Control of Fiber-Reinforced Elastomeric Enclosures (FREEs)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.09808v3": {
            "Paper Title": "Connectivity Maintenance for Multi-Robot Systems Under Motion and\n  Sensing Uncertainties Using Distributed ADMM-based Trajectory Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.06236v4": {
            "Paper Title": "Adaptive Force-based Control for Legged Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.11052v3": {
            "Paper Title": "VDO-SLAM: A Visual Dynamic Object-aware SLAM System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.08054v3": {
            "Paper Title": "New Properties of Triangular Orbits in Elliptic Billiards",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.14641v3": {
            "Paper Title": "Learning to Plan Optimistically: Uncertainty-Guided Deep Exploration via\n  Latent Model Ensembles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.00284v3": {
            "Paper Title": "Visual Identification of Articulated Object Parts",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.06789v5": {
            "Paper Title": "Providentia -- A Large-Scale Sensor System for the Assistance of\n  Autonomous Vehicles and Its Evaluation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.13896v3": {
            "Paper Title": "Robust Multiple-Path Orienteering Problem: Securing Against Adversarial\n  Attacks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.03864v2": {
            "Paper Title": "Contraction $\\mathcal{L}_1$-Adaptive Control using Gaussian Processes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.07202v4": {
            "Paper Title": "Understanding Teacher Gaze Patterns for Robot Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.06854v3": {
            "Paper Title": "Empirical Study of Off-Policy Policy Evaluation for Reinforcement\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.09145v2": {
            "Paper Title": "MROS: Runtime Adaptation For Robot Control Architectures",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.04238v3": {
            "Paper Title": "Control and Trajectory Optimization for Soft Aerial Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.02392v3": {
            "Paper Title": "PointLoc: Deep Pose Regressor for LiDAR Point Cloud Localization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.06089v2": {
            "Paper Title": "Continuous Perception for Classifying Shapes and Weights of Garmentsfor\n  Robotic Vision Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.04180v2": {
            "Paper Title": "Traction Adaptive Motion Planning at the Limits of Handling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.10402v3": {
            "Paper Title": "DevSecOps in Robotics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.11299v3": {
            "Paper Title": "Introducing the Robot Vulnerability Database (RVD)",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.02690v4": {
            "Paper Title": "Robotics CTF (RCTF), a playground for robot hacking",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.10357v4": {
            "Paper Title": "Towards an open standard for assessing the severity of robot security\n  vulnerabilities, the Robot Vulnerability Scoring System (RVSS)",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.04042v4": {
            "Paper Title": "Introducing the Robot Security Framework (RSF), a standardized\n  methodology to perform security assessments in robotics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.06194v2": {
            "Paper Title": "Labeling the Phrases of a Conversational Agent with a Unique\n  Personalized Vocabulary",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.02066v2": {
            "Paper Title": "Follow the Object: Curriculum Learning for Manipulation Tasks with\n  Imagined Goals",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.04514v2": {
            "Paper Title": "Robust walking based on MPC with viability guarantees",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": ", etc.) in contrast with\nexpressive policy representation without any prior (e.g., Neural\nNetworks) ",
                    "Citation Text": "K. Chatzilygeroudis, V . Vassiliades, F. Stulp, S. Calinon, and J.-B.\nMouret, \u201cA survey on policy search algorithms for learning robot\ncontrollers in a handful of trials,\u201d IEEE Transactions on Robotics ,\nvol. 36, no. 2, pp. 328\u2013347, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.02303",
                        "Citation Paper Title": "Title:A survey on policy search algorithms for learning robot controllers in a handful of trials",
                        "Citation Paper Abstract": "Abstract:Most policy search algorithms require thousands of training episodes to find an effective policy, which is often infeasible with a physical robot. This survey article focuses on the extreme other end of the spectrum: how can a robot adapt with only a handful of trials (a dozen) and a few minutes? By analogy with the word \"big-data\", we refer to this challenge as \"micro-data reinforcement learning\". We show that a first strategy is to leverage prior knowledge on the policy structure (e.g., dynamic movement primitives), on the policy parameters (e.g., demonstrations), or on the dynamics (e.g., simulators). A second strategy is to create data-driven surrogate models of the expected reward (e.g., Bayesian optimization) or the dynamical model (e.g., model-based policy search), so that the policy optimizer queries the model instead of the real system. Overall, all successful micro-data algorithms combine these two strategies by varying the kind of model and prior knowledge. The current scientific challenges essentially revolve around scaling up to complex robots (e.g., humanoids), designing generic priors, and optimizing the computing time.",
                        "Citation Paper Authors": "Authors:Konstantinos Chatzilygeroudis, Vassilis Vassiliades, Freek Stulp, Sylvain Calinon, Jean-Baptiste Mouret"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2011.06507v2": {
            "Paper Title": "Reinforcement Learning with Videos: Combining Offline Observations with\n  Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.04971v3": {
            "Paper Title": "Deep Variational Semi-Supervised Novelty Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.04144v2": {
            "Paper Title": "Improved Swarm Engineering: Aligning Intuition and Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.06748v3": {
            "Paper Title": "Safe and Robust Motion Planning for Dynamic Robotics via Control Barrier\n  Functions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.02104v3": {
            "Paper Title": "Stabilization of Complementarity Systems via Contact-Aware Controllers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.09170v5": {
            "Paper Title": "Belief-Grounded Networks for Accelerated Robot Learning under Partial\n  Observability",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.05382v2": {
            "Paper Title": "Social Companion Robots to Reduce Isolation: A Perception Change Due to\n  COVID-19",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04461v2": {
            "Paper Title": "MoboTSP: Solving the Task Sequencing Problem for Mobile Manipulators",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08993v4": {
            "Paper Title": "Planning with Learned Dynamics: Probabilistic Guarantees on Safety and\n  Reachability via Lipschitz Constants",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.08646v2": {
            "Paper Title": "Macro-Action-Based Deep Multi-Agent Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07010v5": {
            "Paper Title": "Monitoring and Diagnosability of Perception Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.08418v2": {
            "Paper Title": "Pedestrian Behavior Prediction for Automated Driving: Requirements,\n  Metrics, and Relevant Features",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.13615v2": {
            "Paper Title": "RoCUS: Robot Controller Understanding via Sampling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.09809v2": {
            "Paper Title": "Parametrized topological complexity of collision-free motion planning in\n  the plane",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.08073v2": {
            "Paper Title": "Unseen Object Instance Segmentation for Robotic Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.13755v2": {
            "Paper Title": "Probabilistic 3D Multi-Modal, Multi-Object Tracking for Autonomous\n  Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00832v3": {
            "Paper Title": "Sparse Multilevel Roadmaps for High-Dimensional Robot Motion Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.10066v5": {
            "Paper Title": "Learning Off-Policy with Online Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.00494v2": {
            "Paper Title": "Design and Development of Underwater Vehicle: ANAHITA",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.00984v2": {
            "Paper Title": "Tight Robot Packing in the Real World: A Complete Manipulation Pipeline\n  with Robust Primitives",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.11295v3": {
            "Paper Title": "Sim-to-real for high-resolution optical tactile sensing: From images to\n  3D contact force distributions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.13032v3": {
            "Paper Title": "Mesh Based Analysis of Low Fractal Dimension Reinforcement Learning\n  Policies",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.08484v3": {
            "Paper Title": "Combining Reinforcement Learning with Model Predictive Control for\n  On-Ramp Merging",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.08854v3": {
            "Paper Title": "Goal-Conditioned End-to-End Visuomotor Control for Versatile Skill\n  Primitives",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": ". Another\nline of work attempts to learn forward dynamics models in\nsuitable latent spaces. After projecting an observation and a\ngoal image into the latent space, a feasible action sequence\ncan then be computed using gradient-based optimisation\nmethods ",
                    "Citation Text": "M. Watter, J. Springenberg, J. Boedecker, and M. Riedmiller, \u201cEmbed\nto control: A locally linear latent dynamics model for control from\nraw images,\u201d in Advances in neural information processing systems ,\n2015, pp. 2746\u20132754.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.07365",
                        "Citation Paper Title": "Title:Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images",
                        "Citation Paper Abstract": "Abstract:We introduce Embed to Control (E2C), a method for model learning and control of non-linear dynamical systems from raw pixel images. E2C consists of a deep generative model, belonging to the family of variational autoencoders, that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear. Our model is derived directly from an optimal control formulation in latent space, supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems.",
                        "Citation Paper Authors": "Authors:Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, Martin Riedmiller"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2011.12690v3": {
            "Paper Title": "DeepKoCo: Efficient latent planning with a task-relevant Koopman\n  representation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.11241v2": {
            "Paper Title": "Data-driven Holistic Framework for Automated Laparoscope Optimal View\n  Control with Learning-based Depth Perception",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.14487v3": {
            "Paper Title": "Teaching Cameras to Feel: Estimating Tactile Physical Properties of\n  Surfaces From Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.12956v2": {
            "Paper Title": "Reinforcement Learning for Robust Missile Autopilot Design",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.12255v3": {
            "Paper Title": "Learning Navigation Skills for Legged Robots with Learned Robot\n  Embeddings",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.03918v3": {
            "Paper Title": "DynaNet: Neural Kalman Dynamical Model for Motion Estimation and\n  Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.09662v3": {
            "Paper Title": "Attention Augmented ConvLSTM for Environment Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.15413v3": {
            "Paper Title": "Measuring and Harnessing Transference in Multi-Task Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.14693v2": {
            "Paper Title": "AM-RRT*: Informed Sampling-based Planning with Assisting Metric",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00594v2": {
            "Paper Title": "Random Fourier Features based SLAM",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.09790v2": {
            "Paper Title": "Neural Radiance Flow for 4D View Synthesis and Video Processing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.13319v2": {
            "Paper Title": "Migratable AI : Investigating users' affect on identity and information\n  migration of a conversational AI agent",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.05801v3": {
            "Paper Title": "Migratable AI: Effect of identity and information migration on users\n  perception of conversational AI agents",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.02877v2": {
            "Paper Title": "Training Agents using Upside-Down Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.01314v2": {
            "Paper Title": "Towards Better Generalization: Joint Depth-Pose Learning without PoseNet",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.03208v3": {
            "Paper Title": "Factorizing Perception and Policy for Interactive Instruction Following",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.09156v2": {
            "Paper Title": "Learning Accurate Long-term Dynamics for Model-based Reinforcement\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.08025v2": {
            "Paper Title": "STT-CBS: A Conflict-Based Search Algorithm for Multi-Agent Path Finding\n  with Stochastic Travel Times",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.05459v3": {
            "Paper Title": "A Self-supervised Learning System for Object Detection in Videos Using\n  Random Walks on Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.06349v2": {
            "Paper Title": "Probabilistic Iterative LQR for Short Time Horizon MPC",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00400v2": {
            "Paper Title": "APPLI: Adaptive Planner Parameter Learning From Interventions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.11446v2": {
            "Paper Title": "3D Object Localization Using 2D Estimates for Computer Vision\n  Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.11717v3": {
            "Paper Title": "Social NCE: Contrastive Learning of Socially-aware Motion\n  Representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.12813v2": {
            "Paper Title": "AlphaPilot: Autonomous Drone Racing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.06332v2": {
            "Paper Title": "Joint Space Control via Deep Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.10700v2": {
            "Paper Title": "Minimal Cases for Computing the Generalized Relative Pose using Affine\n  Correspondences",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.08946v2": {
            "Paper Title": "Reactive Navigation in Partially Familiar Planar Environments Using\n  Semantic Perceptual Feedback",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.12336v2": {
            "Paper Title": "An Approach to Deploy Interactive Robotic Simulators on the Web for HRI\n  Experiments: Results in Social Robot Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.01354v3": {
            "Paper Title": "Unsupervised Monocular Depth Learning with Integrated Intrinsics and\n  Spatio-Temporal Constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.04595v3": {
            "Paper Title": "GRF: Learning a General Radiance Field for 3D Representation and\n  Rendering",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.04114v2": {
            "Paper Title": "RoboWalk: Explicit Augmented Human-Robot Dynamics Modeling for Design\n  Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.05877v3": {
            "Paper Title": "INeRF: Inverting Neural Radiance Fields for Pose Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.03298v2": {
            "Paper Title": "Bifold and Semantic Reasoning for Pedestrian Behavior Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.03110v4": {
            "Paper Title": "Causal Curiosity: RL Agents Discovering Self-supervised Experiments for\n  Causal Representation Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.01959v5": {
            "Paper Title": "Towards Human Haptic Gesture Interpretation for Robotic Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.14091v2": {
            "Paper Title": "Learning Reward Functions from Diverse Sources of Human Feedback:\n  Optimally Integrating Demonstrations and Preferences",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.05150v2": {
            "Paper Title": "Safe Reinforcement Learning with Natural Language Constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08052v3": {
            "Paper Title": "A Learning Approach to Robot-Agnostic Force-Guided High Precision\n  Assembly",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.05093v2": {
            "Paper Title": "Robotic Communications for 5G and Beyond: Challenges and Research\n  Opportunities",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.01859v3": {
            "Paper Title": "Goal-Driven Robotic Pushing Using Tactile and Proprioceptive Feedback",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.09335v3": {
            "Paper Title": "A Tunnel Gaussian Process Model for Learning Interpretable Flight's\n  Landing Parameters",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.02705v3": {
            "Paper Title": "Spatial Language Understanding for Object Search in Partially Observed\n  City-scale Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00509v3": {
            "Paper Title": "PILOT: Efficient Planning by Imitation Learning and Optimisation for\n  Safe Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.09446v2": {
            "Paper Title": "Real-Time Regression with Dividing Local Gaussian Processes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.07623v2": {
            "Paper Title": "Machine Vision for Improved Human-Robot Cooperation in Adverse\n  Underwater Conditions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07171v2": {
            "Paper Title": "Joint Sampling and Trajectory Optimization over Graphs for Online Motion\n  Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.03321v2": {
            "Paper Title": "Global Unifying Intrinsic Calibration for Spinning and Solid-State\n  LiDARs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04929v2": {
            "Paper Title": "Sim2Sim Evaluation of a Novel Data-Efficient Differentiable Physics\n  Engine for Tensegrity Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.00658v2": {
            "Paper Title": "Learning and Using Abstractions for Robot Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.03771v5": {
            "Paper Title": "Composition of Safety Constraints For Fixed-Wing Collision Avoidance\n  Amidst Limited Communications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.01417v3": {
            "Paper Title": "Cycloidal Trajectory Realization on Staircase based on Neural Network\n  Temporal Quantized Lagrange Dynamics (NNTQLD) with Ant Colony Optimization\n  for a 9-Link Bipedal Robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.06070v2": {
            "Paper Title": "Review of Learning-based Longitudinal Motion Planning for Autonomous\n  Vehicles: Research Gaps between Self-driving and Traffic Congestion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.05457v2": {
            "Paper Title": "Neural-Swarm2: Planning and Control of Heterogeneous Multirotor Swarms\n  using Learned Interactions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.11270v3": {
            "Paper Title": "Family Ties: Relating Poncelet 3-Periodics by their Properties",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.11775v4": {
            "Paper Title": "SACBP: Belief Space Planning for Continuous-Time Dynamical Systems via\n  Stochastic Sequential Action Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.03785v2": {
            "Paper Title": "A Topological Approach to Gait Generation for Biped Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.11645v3": {
            "Paper Title": "Accelerating Safe Reinforcement Learning with Constraint-mismatched\n  Policies",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.04047v3": {
            "Paper Title": "DeepRelativeFusion: Dense Monocular SLAM using Single-Image Relative\n  Depth Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.07648v2": {
            "Paper Title": "Language Conditioned Imitation Learning over Unstructured Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.06322v2": {
            "Paper Title": "Whole-Body MPC and Online Gait Sequence Generation for Wheeled-Legged\n  Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.11397v5": {
            "Paper Title": "Imagination-enabled Robot Perception",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03148v2": {
            "Paper Title": "RetinaGAN: An Object-aware Approach to Sim-to-Real Transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.15109v3": {
            "Paper Title": "Outlier-Robust Estimation: Hardness, Minimally Tuned Algorithms, and\n  Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.14745v3": {
            "Paper Title": "Generalized Nonlinear and Finsler Geometry for Robotics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.02504v2": {
            "Paper Title": "Pose-Based Tactile Servoing: Controlled Soft Touch using Deep Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03813v4": {
            "Paper Title": "MAGIC: Learning Macro-Actions for Online POMDP Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.00824v4": {
            "Paper Title": "Goal-Auxiliary Actor-Critic for 6D Robotic Grasping with Point Clouds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00099v2": {
            "Paper Title": "Autonomous Robotic Screening of Tubular Structures based only on\n  Real-Time Ultrasound Imaging Feedback",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.03910v3": {
            "Paper Title": "Proficiency Constrained Multi-Agent Reinforcement Learning for\n  Environment-Adaptive Multi UAV-UGV Teaming",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.10781v3": {
            "Paper Title": "Chitrakar: Robotic System for Drawing Jordan Curve of Facial Portrait",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.14750v4": {
            "Paper Title": "Geometric Fabrics for the Acceleration-based Design of Robotic Motion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.15676v3": {
            "Paper Title": "Optimization Fabrics for Behavioral Design",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03882v2": {
            "Paper Title": "Multi-Modal Learning of Keypoint Predictive Models for Visual Object\n  Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.05418v2": {
            "Paper Title": "Self-supervised Learning of LiDAR Odometry for Robotic Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.06085v2": {
            "Paper Title": "Learning to Generalize Across Long-Horizon Tasks from Human\n  Demonstrations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.08424v2": {
            "Paper Title": "Deep Affordance Foresight: Planning Through What Can Be Done in the\n  Future",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.05719v2": {
            "Paper Title": "Smooth Exploration for Robotic Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01210v3": {
            "Paper Title": "Embodied Language Grounding with 3D Visual Feature Representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.06276v2": {
            "Paper Title": "Dual Control for Exploitation and Exploration (DCEE) in Autonomous\n  Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.01439v2": {
            "Paper Title": "Learning Dexterous Grasping with Object-Centric Visual Affordances",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.11662v2": {
            "Paper Title": "Explicitly Encouraging Low Fractional Dimensional Trajectories Via\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.15412v2": {
            "Paper Title": "WiForce: Wireless Sensing and Localization of Contact Forces on a Space\n  Continuum",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.03548v3": {
            "Paper Title": "Reset-Free Lifelong Learning with Skill-Space Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.15588v2": {
            "Paper Title": "Data-efficient Hindsight Off-policy Option Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.01529v2": {
            "Paper Title": "Closing the Reality Gap with Unsupervised Sim-to-Real Image Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.14215v2": {
            "Paper Title": "Multivariate Uncertainty in Deep Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.10897v2": {
            "Paper Title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.04810v4": {
            "Paper Title": "Antipodal Robotic Grasping using Generative Residual Convolutional\n  Neural Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.10976v7": {
            "Paper Title": "OF-VO: Efficient Navigation among Pedestrians Using Commodity Sensors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00095v2": {
            "Paper Title": "Adversarial Attacks on Optimization based Planners",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03609v3": {
            "Paper Title": "A Few Shot Adaptation of Visual Navigation Skills to New Observations\n  using Meta-Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.02215v3": {
            "Paper Title": "A Two-Stage Optimization-based Motion Planner for Safe Urban Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.11628v2": {
            "Paper Title": "Domain-Adversarial and Conditional State Space Model for Imitation\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.10471v2": {
            "Paper Title": "Online Descriptor Enhancement via Self-Labelling Triplets for Visual\n  Data Association",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.08961v2": {
            "Paper Title": "Reactive Human-to-Robot Handovers of Arbitrary Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.03664v2": {
            "Paper Title": "Learning How to Dynamically Route Autonomous Vehicles on Shared Roads",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.01880v2": {
            "Paper Title": "Intrinsic Robotic Introspection: Learning Internal States From Neuron\n  Activations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04820v3": {
            "Paper Title": "Decentralized Structural-RNN for Robot Crowd Navigation with Deep\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.09505v3": {
            "Paper Title": "Error Tolerant Path Planning for Swarms of Micro Aerial Vehicles with\n  Quality Amplification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.08890v2": {
            "Paper Title": "Self-Supervised Person Detection in 2D Range Data using a Calibrated\n  Camera",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00032v2": {
            "Paper Title": "Multi-Layered Safety for Legged Robots via Control Barrier Functions and\n  Model Predictive Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.03677v2": {
            "Paper Title": "An Intent-based Task-aware Shared Control Framework for Intuitive Hands\n  Free Telemanipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.15107v3": {
            "Paper Title": "OrcVIO: Object residual constrained Visual-Inertial Odometry",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.08794v2": {
            "Paper Title": "Robust Frequency-Based Structure Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.12775v3": {
            "Paper Title": "On the model-based stochastic value gradient for continuous\n  reinforcement learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.06644v2": {
            "Paper Title": "Regularizing Action Policies for Smooth Control with Reinforcement\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.07008v4": {
            "Paper Title": "Monocular Instance Motion Segmentation for Autonomous Driving: KITTI\n  InstanceMotSeg Dataset and Multi-task Baseline",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.11827v3": {
            "Paper Title": "REPAINT: Knowledge Transfer in Deep Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.03522v3": {
            "Paper Title": "Offline Runtime Verification of Safety Requirements using CSP",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.11906v2": {
            "Paper Title": "A principled analysis of Behavior Trees and their generalisations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04813v2": {
            "Paper Title": "Bimanual Regrasping for Suture Needles using Reinforcement Learning for\n  Rapid Motion Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.04060v2": {
            "Paper Title": "Semantic and Geometric Modeling with Neural Message Passing in 3D Scene\n  Graphs for Hierarchical Mechanical Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.11104v3": {
            "Paper Title": "Tensor-variate Mixture of Experts for Proportional Myographic Control of\n  a Robotic Hand",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07112v3": {
            "Paper Title": "Benchmarking Domain Randomisation for Visual Sim-to-Real Transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.09999v3": {
            "Paper Title": "Inverse Constrained Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.12406v2": {
            "Paper Title": "Prediction-Based Reachability for Collision Avoidance in Autonomous\n  Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.12222v3": {
            "Paper Title": "A Modeled Approach for Online Adversarial Test of Operational Vehicle\n  Safety (extended version)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07132v2": {
            "Paper Title": "Region-Based Planning for 3D Within-Hand-Manipulation via Variable\n  Friction Robot Fingers and Extrinsic Contacts",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.03721v6": {
            "Paper Title": "Distributed Certifiably Correct Pose-Graph Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02635v3": {
            "Paper Title": "GPR-based Model Reconstruction System for Underground Utilities Using\n  GPRNet",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.12045v2": {
            "Paper Title": "High Precision Real Time Collision Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.15920v2": {
            "Paper Title": "Recovery RL: Safe Reinforcement Learning with Learned Recovery Zones",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.05104v2": {
            "Paper Title": "Solving Challenging Dexterous Manipulation Tasks With Trajectory\n  Optimisation and Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.00622v2": {
            "Paper Title": "A Multi-spectral Dataset for Evaluating Motion Estimation Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.13229v6": {
            "Paper Title": "3D-OGSE: Online Safe and Smooth Trajectory Generation using Generalized\n  Shape Expansion in Unknown 3-D Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.11693v2": {
            "Paper Title": "Sequential Topological Representations for Predictive Models of\n  Deformable Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.09636v2": {
            "Paper Title": "GM-PHD Filter for Searching and Tracking an Unknown Number of Targets\n  with a Mobile Sensor with Limited FOV",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.00563v2": {
            "Paper Title": "Flightmare: A Flexible Quadrotor Simulator",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.09904v3": {
            "Paper Title": "Robust & Asymptotically Locally Optimal UAV-Trajectory Generation Based\n  on Spline Subdivision",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.08151v3": {
            "Paper Title": "The State of Lifelong Learning in Service Robots: Current Bottlenecks in\n  Object Perception and Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.00739v4": {
            "Paper Title": "OpenUAV Cloud Testbed: a Collaborative Design Studio for Field Robotics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04741v2": {
            "Paper Title": "Learning Task Space Actions for Bipedal Locomotion",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.00191v3": {
            "Paper Title": "Learning a Decision Module by Imitating Driver's Control Behaviors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00664v2": {
            "Paper Title": "Two-Port Analysis of Stability and Transparency in Series Damped Elastic\n  Actuation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04183v3": {
            "Paper Title": "EGO-Swarm: A Fully Autonomous and Decentralized Quadrotor Swarm System\n  in Cluttered Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.06444v2": {
            "Paper Title": "Learning compositional models of robot skills for task and motion\n  planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.15436v2": {
            "Paper Title": "Robotic Grasping of Fully-Occluded Objects using RF Perception",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00800v3": {
            "Paper Title": "Real-to-Sim Registration of Deformable Soft Tissue with Position-Based\n  Dynamics for Surgical Robot Autonomy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.00735v2": {
            "Paper Title": "Trajectory Forecasts in Unknown Environments Conditioned on Grid-Based\n  Plans",
            "Sentences": [
                {
                    "Sentence ID": 43,
                    "Sentence": "2.44 1.57 0.70 0.55 0.11 0.11\nTrajectron ++ (3rd place) ",
                    "Citation Text": "T. Salzmann, B. Ivanovic, P. Chakravarty, and M. Pavone, \u201cTrajectron++:\nDynamically-feasible trajectory forecasting with heterogeneous data,\u201d\narXiv preprint arXiv:2001.03093 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2001.03093",
                        "Citation Paper Title": "Title:Trajectron++: Dynamically-Feasible Trajectory Forecasting With Heterogeneous Data",
                        "Citation Paper Abstract": "Abstract:Reasoning about human motion is an important prerequisite to safe and socially-aware robotic navigation. As a result, multi-agent behavior prediction has become a core component of modern human-robot interactive systems, such as self-driving cars. While there exist many methods for trajectory forecasting, most do not enforce dynamic constraints and do not account for environmental information (e.g., maps). Towards this end, we present Trajectron++, a modular, graph-structured recurrent model that forecasts the trajectories of a general number of diverse agents while incorporating agent dynamics and heterogeneous data (e.g., semantic maps). Trajectron++ is designed to be tightly integrated with robotic planning and control frameworks; for example, it can produce predictions that are optionally conditioned on ego-agent motion plans. We demonstrate its performance on several challenging real-world trajectory forecasting datasets, outperforming a wide array of state-of-the-art deterministic and generative methods.",
                        "Citation Paper Authors": "Authors:Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, Marco Pavone"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2011.07948v4": {
            "Paper Title": "A Follow-the-Leader Strategy using Hierarchical Deep Neural Networks\n  with Grouped Convolutions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.10008v2": {
            "Paper Title": "Online Connectivity-aware Dynamic Deployment for Heterogeneous\n  Multi-Robot Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.13209v2": {
            "Paper Title": "Handling Object Symmetries in CNN-based Pose Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.10934v3": {
            "Paper Title": "LEAF: Latent Exploration Along the Frontier",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.14497v2": {
            "Paper Title": "Conservative Safety Critics for Exploration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.01438v3": {
            "Paper Title": "FlowNet3D++: Geometric Losses For Deep Scene Flow Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.09359v6": {
            "Paper Title": "AWAC: Accelerating Online Reinforcement Learning with Offline Datasets",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03603v2": {
            "Paper Title": "Fast Near-Optimal Heterogeneous Task Allocation via Flow Decomposition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.10981v2": {
            "Paper Title": "Towards Complex and Continuous Manipulation: A Gesture Based\n  Anthropomorphic Robotic Hand Design",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.04037v2": {
            "Paper Title": "Socially-Aware Navigation: A Non-linear Multi-Objective Optimization\n  Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.04523v3": {
            "Paper Title": "Objective Mismatch in Model-based Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.06764v2": {
            "Paper Title": "Scaffolding Reflection in Reinforcement Learning Framework for\n  Confinement Escape Problem",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.04397v3": {
            "Paper Title": "Enhancing LGMD's Looming Selectivity for UAV with Spatial-temporal\n  Distributed Presynaptic Connections",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.04376v2": {
            "Paper Title": "TEAM: Trilateration for Exploration and Mapping with Robotic Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.11061v2": {
            "Paper Title": "MADER: Trajectory Planner in Multi-Agent and Dynamic Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.13916v2": {
            "Paper Title": "Off-Dynamics Reinforcement Learning: Training for Transfer with Domain\n  Classifiers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.03987v2": {
            "Paper Title": "Safety Challenges for Autonomous Vehicles in the Absence of Connectivity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.05909v2": {
            "Paper Title": "Blending MPC & Value Function Approximation for Efficient Reinforcement\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.07284v2": {
            "Paper Title": "Robust Safety-Critical Control for Dynamic Robotics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.12885v5": {
            "Paper Title": "Geometric Back-projection Network for Point Cloud Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07641v4": {
            "Paper Title": "Stein Variational Model Predictive Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00993v2": {
            "Paper Title": "Real-time Semantic Segmentation with Context Aggregation Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.10474v2": {
            "Paper Title": "Probabilistic Radio-Visual Active Sensing for Search and Tracking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04977v2": {
            "Paper Title": "SelfDeco: Self-Supervised Monocular Depth Completion in Challenging\n  Indoor Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.12149v2": {
            "Paper Title": "SpinNet: Learning a General Surface Descriptor for 3D Point Cloud\n  Registration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.10202v2": {
            "Paper Title": "CLIPPER: A Graph-Theoretic Framework for Robust Data Association",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.12790v2": {
            "Paper Title": "Fast Region Proposal Learning for Object Detection for Robotics",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": ", i.e. (i) region candidates\ngeneration, (ii) per region feature extraction and (iii) region\nclassi\ufb01cation and re\ufb01nement. The major trend in the state-\nof-the-art is to integrate the three steps into \u201cmonolithic\u201d\ndeep learning based architectures ",
                    "Citation Text": "Kaiming He, Georgia Gkioxari, Piotr Doll \u00b4ar, and Ross B. Girshick.\nMask r-cnn. 2017 IEEE International Conference on Computer Vision\n(ICCV) , pages 2980\u20132988, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.06870",
                        "Citation Paper Title": "Title:Mask R-CNN",
                        "Citation Paper Abstract": "Abstract:We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: this https URL",
                        "Citation Paper Authors": "Authors:Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, Ross Girshick"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2010.14524v3": {
            "Paper Title": "Section Patterns: Efficiently Solving Narrow Passage Problems in\n  Multilevel Motion Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.04309v3": {
            "Paper Title": "Self-Supervised Policy Adaptation during Deployment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.08051v3": {
            "Paper Title": "Approximate Inverse Reinforcement Learning from Vision-based Imitation\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.01476v2": {
            "Paper Title": "Communication-Aware Multi-robot Coordination with Submodular\n  Maximization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.11583v2": {
            "Paper Title": "Semantic Audio-Visual Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.06511v4": {
            "Paper Title": "Reinforcement Learning with Chromatic Networks for Compact Architecture\n  Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.08345v2": {
            "Paper Title": "Distilling a Hierarchical Policy for Planning and Control via\n  Representation and Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.03137v3": {
            "Paper Title": "Towards Semantic Segmentation of Urban-Scale 3D Point Clouds: A Dataset,\n  Benchmarks and Challenges",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.11681v2": {
            "Paper Title": "Learning Panoptic Segmentation from Instance Contours",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.06118v2": {
            "Paper Title": "I Know What You Meant: Learning Human Objectives by (Under)estimating\n  Their Choice Set",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.09076v3": {
            "Paper Title": "RADIATE: A Radar Dataset for Automotive Perception in Bad Weather",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04692v2": {
            "Paper Title": "DIPN: Deep Interaction Prediction Network with Application to Clutter\n  Removal",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.06201v6": {
            "Paper Title": "Real-Time Trajectory Planning for AGV in the Presence of Moving\n  Obstacles: A First-Search-Then-Optimization Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.07847v2": {
            "Paper Title": "PTP: Parallelized Tracking and Prediction with Graph Neural Networks and\n  Diversity Sampling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.13164v3": {
            "Paper Title": "Joint Object Detection and Multi-Object Tracking with Graph Neural\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.03781v2": {
            "Paper Title": "Learning Visual Affordances with Target-Orientated Deep Q-Network to\n  Grasp Objects by Harnessing Environmental Fixtures",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.10027v4": {
            "Paper Title": "Human Action Recognition in Drone Videos using a Few Aerial Training\n  Examples",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.01891v3": {
            "Paper Title": "Policy Transfer via Kinematic Domain Randomization and Adaptation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.10140v3": {
            "Paper Title": "Voronoi Progressive Widening: Efficient Online Solvers for Continuous\n  State, Action, and Observation POMDPs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.11609v2": {
            "Paper Title": "Reachable Polyhedral Marching (RPM): A Safety Verification Algorithm for\n  Robotic Systems with Deep Neural Network Components",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.10118v2": {
            "Paper Title": "Batteries, camera, action! Learning a semantic control space for\n  expressive robot cinematography",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.09107v2": {
            "Paper Title": "Learning Precise 3D Manipulation from Multiple Uncalibrated Cameras",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.00872v3": {
            "Paper Title": "Scoring Graspability based on Grasp Regression for Better Grasp\n  Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08220v3": {
            "Paper Title": "Predicting the Post-Impact Velocity of a Robotic Arm via Rigid Multibody\n  Models: an Experimental Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04815v2": {
            "Paper Title": "Encoding Defensive Driving as a Dynamic Nash Game",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.09857v2": {
            "Paper Title": "The STDyn-SLAM: A stereo vision and semantic segmentation approach for\n  SLAM in dynamic outdoor environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.05217v3": {
            "Paper Title": "A Variational Infinite Mixture for Probabilistic Inverse Dynamics\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.07277v2": {
            "Paper Title": "Hierarchical Planning for Long-Horizon Manipulation with Geometric and\n  Symbolic Scene Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.08174v2": {
            "Paper Title": "Towards open and expandable cognitive AI architectures for large-scale\n  multi-agent human-robot collaborative learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04840v2": {
            "Paper Title": "Robots of the Lost Arc: Self-Supervised Learning to Dynamically\n  Manipulate Fixed-Endpoint Cables",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.10120v3": {
            "Paper Title": "Semantic Flow for Fast and Accurate Scene Parsing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.03768v2": {
            "Paper Title": "Learning Tactile Models for Factor Graph-based Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06885v4": {
            "Paper Title": "6DLS: Modeling Nonplanar Frictional Surface Contacts for Grasping using\n  6D Limit Surfaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00563v2": {
            "Paper Title": "Learning Robot Trajectories subject to Kinematic Joint Constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.08436v2": {
            "Paper Title": "Shared Cross-Modal Trajectory Prediction for Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.13693v2": {
            "Paper Title": "Spatial Reasoning from Natural Language Instructions for Robot\n  Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.05551v2": {
            "Paper Title": "DI-Fusion: Online Implicit 3D Reconstruction with Deep Priors",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.09678v2": {
            "Paper Title": "IRS: A Large Naturalistic Indoor Robotics Stereo Dataset to Train Deep\n  Models for Disparity and Surface Normal Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.16272v2": {
            "Paper Title": "PATHoBot: A Robot for Glasshouse Crop Phenotyping and Intervention",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": "presented a sweet pepper\nharvesting robot for ad-hoc cropping systems. More recently,\nHalstead et al. ",
                    "Citation Text": "M. Halstead, S. Denman, C. Fookes, and C. McCool, \u201cFruit detection\nin the wild: The impact of varying conditions and cultivar,\u201d in to\nappear in Proceedings of Digital Image Computing: Techniques and\nApplications (DICTA) , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.05560",
                        "Citation Paper Title": "Title:Fruit Quantity and Quality Estimation using a Robotic Vision System",
                        "Citation Paper Abstract": "Abstract:Accurate localisation of crop remains highly challenging in unstructured environments such as farms. Many of the developed systems still rely on the use of hand selected features for crop identification and often neglect the estimation of crop quantity and quality, which is key to assigning labor during farming processes. To alleviate these limitations we present a robotic vision system that can accurately estimate the quantity and quality of sweet pepper (Capsicum annuum L), a key horticultural crop. This system consists of three parts: detection, quality estimation, and tracking. Efficient detection is achieved using the FasterRCNN framework. Quality is then estimated in the same framework by learning a parallel layer which we show experimentally results in superior performance than treating quality as extra classes in the traditional Faster-RCNN framework. Evaluation of these two techniques outlines the improved performance of the parallel layer, where we achieve an F1 score of 77.3 for the parallel technique yet only 72.5 for the best scoring (red) of the multi-class implementation. To track the crop we present a tracking via detection approach, which uses the FasterRCNN with parallel layers, that is also a vision-only solution. This approach is cheap to implement as it only requires a camera and in experiments across 2 days we show that our proposed system can accurately estimate the number of sweet pepper present, within 4.1% of the ground truth.",
                        "Citation Paper Authors": "Authors:M. Halstead, C. McCool, S. Denman, T. Perez, C. Fookes"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2011.02092v2": {
            "Paper Title": "Soft Robot Optimal Control Via Reduced Order Finite Element Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00778v2": {
            "Paper Title": "Learning Sequences of Manipulation Primitives for Robotic Assembly",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07748v3": {
            "Paper Title": "Fast Uncertainty Quantification for Deep Object Pose Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07044v2": {
            "Paper Title": "Tactile SLAM: Real-time inference of shape and pose from planar pushing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.00502v4": {
            "Paper Title": "Task Planning with a Weighted Functional Object-Oriented Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02604v2": {
            "Paper Title": "Leveraging Post Hoc Context for Faster Learning in Bandit Settings with\n  Applications in Robot-Assisted Feeding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.00743v2": {
            "Paper Title": "Bidirectional Attention Network for Monocular Depth Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03142v4": {
            "Paper Title": "Contact Localization for Robot Arms in Motion without Torque Sensing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.02148v3": {
            "Paper Title": "Graph-SIM: A Graph-based Spatiotemporal Interaction Modelling for\n  Pedestrian Action Prediction",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": "use a 3D DenseNet architecture that detects\nand predicts pedestrian actions in a single framework.\nRecurrent approaches, which rely on multiple data modal-\nities, are also widely used ",
                    "Citation Text": "B. Liu, E. Adeli, Z. Cao, K.-H. Lee, A. Shenoi, A. Gaidon, and J. C.\nNiebles, \u201cSpatiotemporal relationship reasoning for pedestrian intent\nprediction,\u201d IEEE RA-L , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.08945",
                        "Citation Paper Title": "Title:Spatiotemporal Relationship Reasoning for Pedestrian Intent Prediction",
                        "Citation Paper Abstract": "Abstract:Reasoning over visual data is a desirable capability for robotics and vision-based applications. Such reasoning enables forecasting of the next events or actions in videos. In recent years, various models have been developed based on convolution operations for prediction or forecasting, but they lack the ability to reason over spatiotemporal data and infer the relationships of different objects in the scene. In this paper, we present a framework based on graph convolution to uncover the spatiotemporal relationships in the scene for reasoning about pedestrian intent. A scene graph is built on top of segmented object instances within and across video frames. Pedestrian intent, defined as the future action of crossing or not-crossing the street, is a very crucial piece of information for autonomous vehicles to navigate safely and more smoothly. We approach the problem of intent prediction from two different perspectives and anticipate the intention-to-cross within both pedestrian-centric and location-centric scenarios. In addition, we introduce a new dataset designed specifically for autonomous-driving scenarios in areas with dense pedestrian populations: the Stanford-TRI Intent Prediction (STIP) dataset. Our experiments on STIP and another benchmark dataset show that our graph modeling framework is able to predict the intention-to-cross of the pedestrians with an accuracy of 79.10% on STIP and 79.28% on \\rev{Joint Attention for Autonomous Driving (JAAD) dataset up to one second earlier than when the actual crossing happens. These results outperform the baseline and previous work. Please refer to this http URL for the dataset and code.",
                        "Citation Paper Authors": "Authors:Bingbin Liu, Ehsan Adeli, Zhangjie Cao, Kuan-Hui Lee, Abhijeet Shenoi, Adrien Gaidon, Juan Carlos Niebles"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2011.01573v2": {
            "Paper Title": "A Laser-based Dual-arm System for Precise Control of Collaborative\n  Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02404v3": {
            "Paper Title": "Dynamics Randomization Revisited:A Case Study for Quadrupedal Locomotion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04872v2": {
            "Paper Title": "An Efficient Closed-Form Method for Optimal Hybrid Force-Velocity\n  Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.06399v2": {
            "Paper Title": "Renormalization for Initialization of Rolling Shutter Visual-Inertial\n  Odometry",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.08705v4": {
            "Paper Title": "Verbal Focus-of-Attention System for Learning-from-Observation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00642v2": {
            "Paper Title": "Technical Report: Reactive Planning for Mobile Manipulation Tasks in\n  Unexplored Semantic Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.04832v2": {
            "Paper Title": "Proactive Interaction Framework for Intelligent Social Receptionist\n  Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03659v2": {
            "Paper Title": "ROBIN: a Graph-Theoretic Approach to Reject Outliers in Robust\n  Estimation using Invariants",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07370v2": {
            "Paper Title": "Locomotion and Control of a Friction-Driven Tripedal Robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.11718v3": {
            "Paper Title": "Safety-Critical Model Predictive Control with Discrete-Time Control\n  Barrier Function",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.07081v2": {
            "Paper Title": "MIDAS: Multi-agent Interaction-aware Decision-making with Adaptive\n  Strategies for Urban Autonomous Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04825v2": {
            "Paper Title": "Multi-Agent Active Search using Realistic Depth-Aware Noise Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.03023v3": {
            "Paper Title": "Volumetric Occupancy Mapping With Probabilistic Depth Completion for\n  Robotic Navigation",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": ", which has state-of-the-art\nperformance. Our boundary estimation network is based on\nthe approach of Zhang et al. ",
                    "Citation Text": "Y . Zhang, S. Song, E. Yumer, M. Savva, J.-Y . Lee, H. Jin, and\nT. Funkhouser, \u201cPhysically-based rendering for indoor scene under-\nstanding using convolutional neural networks,\u201d IEEE Conference on\nComputer Vision and Pattern Recognition , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.07429",
                        "Citation Paper Title": "Title:Physically-Based Rendering for Indoor Scene Understanding Using Convolutional Neural Networks",
                        "Citation Paper Abstract": "Abstract:Indoor scene understanding is central to applications such as robot navigation and human companion assistance. Over the last years, data-driven deep neural networks have outperformed many traditional approaches thanks to their representation learning capabilities. One of the bottlenecks in training for better representations is the amount of available per-pixel ground truth data that is required for core scene understanding tasks such as semantic segmentation, normal prediction, and object edge detection. To address this problem, a number of works proposed using synthetic data. However, a systematic study of how such synthetic data is generated is missing. In this work, we introduce a large-scale synthetic dataset with 400K physically-based rendered images from 45K realistic 3D indoor scenes. We study the effects of rendering methods and scene lighting on training for three computer vision tasks: surface normal prediction, semantic segmentation, and object boundary detection. This study provides insights into the best practices for training with synthetic data (more realistic rendering is worth it) and shows that pretraining with our new synthetic dataset can improve results beyond the current state of the art on all three tasks.",
                        "Citation Paper Authors": "Authors:Yinda Zhang, Shuran Song, Ersin Yumer, Manolis Savva, Joon-Young Lee, Hailin Jin, Thomas Funkhouser"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2012.04512v2": {
            "Paper Title": "SSCNav: Confidence-Aware Semantic Scene Completion for Visual Semantic\n  Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.01758v2": {
            "Paper Title": "Representation Matters: Improving Perception and Exploration for\n  Robotics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.14928v2": {
            "Paper Title": "A Development Cycle for Automated Self-Exploration of Robot Behaviors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00601v2": {
            "Paper Title": "Approximate Solutions to a Class of Reachability Games",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.13695v2": {
            "Paper Title": "Translating Natural Language Instructions to Computer Programs for Robot\n  Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.10586v4": {
            "Paper Title": "Is The Leader Robot an Adequate Sensor for Posture Estimation and\n  Ergonomic Assessment of A Human Teleoperator?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00491v2": {
            "Paper Title": "MRPB 1.0: A Unified Benchmark for the Evaluation of Mobile Robot Local\n  Planning Approaches",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.00085v3": {
            "Paper Title": "Enforcing Almost-Sure Reachability in POMDPs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.07441v2": {
            "Paper Title": "Auto-calibration Method Using Stop Signs for Urban Autonomous Driving\n  Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.00350v3": {
            "Paper Title": "Adaptive Procedural Task Generation for Hard-Exploration Problems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.11867v5": {
            "Paper Title": "Planning in Learned Latent Action Spaces for Generalizable Legged\n  Locomotion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.11344v2": {
            "Paper Title": "Trajectory Prediction using Equivariant Continuous Convolution",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.08215v4": {
            "Paper Title": "Reinforcement Learning based Transmission Range Control in\n  Software-Defined Wireless Sensor Networks with Moving Sensor",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.03779v2": {
            "Paper Title": "Deep Adversarial Reinforcement Learning for Object Disentangling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.06321v2": {
            "Paper Title": "A Deep Learning Framework for Recognizing both Static and Dynamic\n  Gestures",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.01552v2": {
            "Paper Title": "Efficient Sampling of Transition Constraints for Motion Planning under\n  Sliding Contacts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.11456v2": {
            "Paper Title": "Guiding Robot Exploration in Reinforcement Learning via Automated\n  Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.07213v2": {
            "Paper Title": "3D_DEN: Open-ended 3D Object Recognition using Dynamically Expandable\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.09696v2": {
            "Paper Title": "Multi-FinGAN: Generative Coarse-To-Fine Sampling of Multi-Finger Grasps",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.03768v2": {
            "Paper Title": "ALFWorld: Aligning Text and Embodied Environments for Interactive\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.14206v3": {
            "Paper Title": "AdaGrasp: Learning an Adaptive Gripper-Aware Grasping Policy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.07371v2": {
            "Paper Title": "Lifelong Multi-Agent Path Finding in Large-Scale Warehouses",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.13681v2": {
            "Paper Title": "Improving the Generalization of End-to-End Driving through Procedural\n  Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.11234v2": {
            "Paper Title": "Learning Spring Mass Locomotion: Guiding Policies with a Reduced-Order\n  Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.00969v2": {
            "Paper Title": "Predicted Composite Signed-Distance Fields for Real-Time Motion Planning\n  in Dynamic Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.07804v2": {
            "Paper Title": "A Game Theoretic Framework for Model Based Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.13457v2": {
            "Paper Title": "Towards Coordinated Robot Motions: End-to-End Learning of Motion\n  Policies on Transform Trees",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.10631v2": {
            "Paper Title": "OpenBot: Turning Smartphones into Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.10480v2": {
            "Paper Title": "Distributed Map Classification using Local Observations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.07899v2": {
            "Paper Title": "Distributed Sensor Networks Deployed Using Soft Growing Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.06666v2": {
            "Paper Title": "Integration of Fully-Actuated Multirotors into Real-World Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.14787v2": {
            "Paper Title": "Unsupervised Path Regression Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08169v3": {
            "Paper Title": "Uncertainty-aware Contact-safe Model-based Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.13823v2": {
            "Paper Title": "Skeleton-DML: Deep Metric Learning for Skeleton-Based One-Shot Action\n  Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.10323v3": {
            "Paper Title": "Experimental Verification of Stability Theory for a Planar Rigid Body\n  with Two Unilateral Frictional Contacts",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06878v2": {
            "Paper Title": "Model Based Planning with Energy Based Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07215v2": {
            "Paper Title": "SoftGym: Benchmarking Deep Reinforcement Learning for Deformable Object\n  Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08098v2": {
            "Paper Title": "Agile Robot Navigation through Hallucinated Learning and Sober\n  Deployment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.14487v2": {
            "Paper Title": "Continuous Transition: Improving Sample Efficiency for Continuous\n  Control Problems via MixUp",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00524v2": {
            "Paper Title": "Towards Personalized Explanation of Robot Path Planning via User\n  Feedback",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.05241v2": {
            "Paper Title": "TIE: Time-Informed Exploration For Robot Motion Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.11459v2": {
            "Paper Title": "Robust Finite-State Controllers for Uncertain POMDPs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.11888v2": {
            "Paper Title": "W-PoseNet: Dense Correspondence Regularized Pixel Pair Pose Regression",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": "introduces Grassmann Pooling\nbased on SVD decomposition to approximate feature maps.\nA quadratic transformation with the low-rank constraint is\nexploited on pairwise feature interaction either from spatial\nlocations ",
                    "Citation Text": "Y . Li, N. Wang, J. Liu, and X. Hou, \u201cFactorized bilinear models for\nimage recognition,\u201d in ICCV , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.05709",
                        "Citation Paper Title": "Title:Factorized Bilinear Models for Image Recognition",
                        "Citation Paper Abstract": "Abstract:Although Deep Convolutional Neural Networks (CNNs) have liberated their power in various computer vision tasks, the most important components of CNN, convolutional layers and fully connected layers, are still limited to linear transformations. In this paper, we propose a novel Factorized Bilinear (FB) layer to model the pairwise feature interactions by considering the quadratic terms in the transformations. Compared with existing methods that tried to incorporate complex non-linearity structures into CNNs, the factorized parameterization makes our FB layer only require a linear increase of parameters and affordable computational cost. To further reduce the risk of overfitting of the FB layer, a specific remedy called DropFactor is devised during the training process. We also analyze the connection between FB layer and some existing models, and show FB layer is a generalization to them. Finally, we validate the effectiveness of FB layer on several widely adopted datasets including CIFAR-10, CIFAR-100 and ImageNet, and demonstrate superior results compared with various state-of-the-art deep models.",
                        "Citation Paper Authors": "Authors:Yanghao Li, Naiyan Wang, Jiaying Liu, Xiaodi Hou"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2007.15157v3": {
            "Paper Title": "Learning RGB-D Feature Embeddings for Unseen Object Instance\n  Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.15232v3": {
            "Paper Title": "Construction Payment Automation Using Blockchain-Enabled Smart Contracts\n  and Reality Capture Technologies",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.08421v3": {
            "Paper Title": "Reachability-based Trajectory Safeguard (RTS): A Safe and Fast\n  Reinforcement Learning Safety Layer for Continuous Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00072v2": {
            "Paper Title": "Learning Stable Normalizing-Flow Control for Robotic Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.11050v5": {
            "Paper Title": "Geometry-aware Manipulability Learning, Tracking and Transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.10628v2": {
            "Paper Title": "Learning Deep Parameterized Skills from Demonstration for Re-targetable\n  Visuomotor Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.02321v2": {
            "Paper Title": "Can I Pour into It? Robot Imagining Open Containability Affordance of\n  Previously Unseen Objects via Physical Simulations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.07840v3": {
            "Paper Title": "Cooperative Pathfinding based on Multi-agent RRT* Fixed Node",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.00694v2": {
            "Paper Title": "Active Learning for Bayesian 3D Hand Pose Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.06945v3": {
            "Paper Title": "Scene Completeness-Aware Lidar Depth Completion for Driving Scenario",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.00097v3": {
            "Paper Title": "Adaptive Surgical Robotic Training Using Real-Time Stylistic Behavior\n  Feedback Through Haptic Cues",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.05251v3": {
            "Paper Title": "Explainable Agents Through Social Cues: A Review",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.14938v3": {
            "Paper Title": "Adaptive Robust Kernels for Non-Linear Least Squares Problems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.09044v3": {
            "Paper Title": "VisuoSpatial Foresight for Multi-Step, Multi-Task Fabric Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.04855v2": {
            "Paper Title": "Reconstruction of Backbone Curves for Snake Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.15436v2": {
            "Paper Title": "Affordance-Aware Handovers with Human Arm Mobility Constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.09667v3": {
            "Paper Title": "Pattern Formation by Robots with Inaccurate Movements",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.01245v2": {
            "Paper Title": "Vision-based Drone Flocking in Outdoor Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.10762v2": {
            "Paper Title": "Image-based Intraluminal Contact Force Monitoring in Robotic Vascular\n  Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.14070v2": {
            "Paper Title": "Generative Partial Visual-Tactile Fused Object Clustering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.09622v3": {
            "Paper Title": "Learning to Set Waypoints for Audio-Visual Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.09689v4": {
            "Paper Title": "Reinforcement Learning Approaches in Social Robotics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.01791v2": {
            "Paper Title": "Force-guided High-precision Grasping Control of Fragile and Deformable\n  Objects using sEMG-based Force Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.01899v2": {
            "Paper Title": "Learning Barrier Functions with Memory for Robust Safe Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.11538v2": {
            "Paper Title": "Evaluating Agents without Rewards",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.12233v2": {
            "Paper Title": "The Salted Kalman Filter: Kalman Filtering on Hybrid Dynamical Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.01156v2": {
            "Paper Title": "Action sequencing using visual permutations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.02252v2": {
            "Paper Title": "Interferobot: aligning an optical interferometer by a reinforcement\n  learning agent",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.12516v3": {
            "Paper Title": "TAMPC: A Controller for Escaping Traps in Novel Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.05123v2": {
            "Paper Title": "Orientation Attentive Robotic Grasp Synthesis with Augmented Grasp Map\n  Representation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.14551v2": {
            "Paper Title": "Explainable Deep Reinforcement Learning for UAV Autonomous Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.03848v2": {
            "Paper Title": "Guided Curriculum Learning for Walking Over Complex Terrain",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.02307v3": {
            "Paper Title": "EfficientPS: Efficient Panoptic Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.07929v5": {
            "Paper Title": "Multi-Resolution 3D Mapping with Explicit Free Space Representation for\n  Fast and Accurate Mobile Robot Motion Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.05859v3": {
            "Paper Title": "Towards Automatic Manipulation of Intra-cardiac Echocardiography\n  Catheter",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.06017v2": {
            "Paper Title": "Revisiting visual-inertial structure from motion for odometry and SLAM\n  initialization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.02422v3": {
            "Paper Title": "Simulator Predictive Control: Using Learned Task Representations and MPC\n  for Zero-Shot Generalization and Sequencing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.07894v3": {
            "Paper Title": "SwarmCCO: Probabilistic Reactive Collision Avoidance for Quadrotor\n  Swarms under Uncertainty",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04684v2": {
            "Paper Title": "Impedance Optimization for Uncertain Contact Interactions Through Risk\n  Sensitive Optimal Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.00444v2": {
            "Paper Title": "Deep Reinforcement Learning for Autonomous Driving: A Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.09927v3": {
            "Paper Title": "Range Conditioned Dilated Convolutions for Scale Invariant 3D Object\n  Detection",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": ", and then regresses 3D boxes in either downsampled BEV view or 3D point view directly.\nSome representative works are PointRCNN ",
                    "Citation Text": "S. Shi, X. Wang, and H. Li. Pointrcnn: 3d object proposal generation and detection from point\ncloud. In CVPR , pages 770\u2013779, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.03670",
                        "Citation Paper Title": "Title:From Points to Parts: 3D Object Detection from Point Cloud with Part-aware and Part-aggregation Network",
                        "Citation Paper Abstract": "Abstract:3D object detection from LiDAR point cloud is a challenging problem in 3D scene understanding and has many practical applications. In this paper, we extend our preliminary work PointRCNN to a novel and strong point-cloud-based 3D object detection framework, the part-aware and aggregation neural network (Part-$A^2$ net). The whole framework consists of the part-aware stage and the part-aggregation stage. Firstly, the part-aware stage for the first time fully utilizes free-of-charge part supervisions derived from 3D ground-truth boxes to simultaneously predict high quality 3D proposals and accurate intra-object part locations. The predicted intra-object part locations within the same proposal are grouped by our new-designed RoI-aware point cloud pooling module, which results in an effective representation to encode the geometry-specific features of each 3D proposal. Then the part-aggregation stage learns to re-score the box and refine the box location by exploring the spatial relationship of the pooled intra-object part locations. Extensive experiments are conducted to demonstrate the performance improvements from each component of our proposed framework. Our Part-$A^2$ net outperforms all existing 3D detection methods and achieves new state-of-the-art on KITTI 3D object detection dataset by utilizing only the LiDAR point cloud data. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Shaoshuai Shi, Zhe Wang, Jianping Shi, Xiaogang Wang, Hongsheng Li"
                    },
                    "Keywords": [
                        ",",
                        "Driving",
                        "Autonomous",
                        "Image",
                        "3D",
                        "Range",
                        "Detection"
                    ]
                }
            ]
        },
        "http://arxiv.org/abs/2008.03596v2": {
            "Paper Title": "TriFinger: An Open-Source Robot for Learning Dexterity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.05523v4": {
            "Paper Title": "DeepURL: Deep Pose Estimation Framework for Underwater Relative\n  Localization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.12136v2": {
            "Paper Title": "Safe Reinforcement Learning via Curriculum Induction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.16018v2": {
            "Paper Title": "Virtual Surfaces and Attitude Aware Planning and Behaviours for Negative\n  Obstacle Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.07376v2": {
            "Paper Title": "Where are the Keys? -- Learning Object-Centric Navigation Policies on\n  Semantic Maps with Graph Convolutional Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.14479v4": {
            "Paper Title": "Toward Agile Maneuvers in Highly Constrained Spaces: Learning from\n  Hallucination",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.13165v2": {
            "Paper Title": "In-Hand Object-Dynamics Inference using Tactile Fingertips",
            "Sentences": [
                {
                    "Sentence ID": 72,
                    "Sentence": "and use this triangle\u2019s surface normal as sn. To validate\nfriction estimation, we select 8 objects as shown in Fig. 4-(a)\nfrom the YCB dataset ",
                    "Citation Text": "B. Calli, A. Walsman, A. Singh, S. Srinivasa, P. Abbeel, and\nA. M. Dollar, \u201cBenchmarking in manipulation research: Using\nthe yale-cmu-berkeley object and model set,\u201d IEEE Robotics &\nAutomation Magazine , vol. 22, no. 3, pp. 36\u201352, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1502.03143",
                        "Citation Paper Title": "Title:Benchmarking in Manipulation Research: The YCB Object and Model Set and Benchmarking Protocols",
                        "Citation Paper Abstract": "Abstract:In this paper we present the Yale-CMU-Berkeley (YCB) Object and Model set, intended to be used to facilitate benchmarking in robotic manipulation, prosthetic design and rehabilitation research. The objects in the set are designed to cover a wide range of aspects of the manipulation problem; it includes objects of daily life with different shapes, sizes, textures, weight and rigidity, as well as some widely used manipulation tests. The associated database provides high-resolution RGBD scans, physical properties, and geometric models of the objects for easy incorporation into manipulation and planning software platforms. In addition to describing the objects and models in the set along with how they were chosen and derived, we provide a framework and a number of example task protocols, laying out how the set can be used to quantitatively evaluate a range of manipulation approaches including planning, learning, mechanical design, control, and many others. A comprehensive literature survey on existing benchmarks and object datasets is also presented and their scope and limitations are discussed. The set will be freely distributed to research groups worldwide at a series of tutorials at robotics conferences, and will be otherwise available at a reasonable purchase cost. It is our hope that the ready availability of this set along with the ground laid in terms of protocol templates will enable the community of manipulation researchers to more easily compare approaches as well as continually evolve benchmarking tests as the field matures.",
                        "Citation Paper Authors": "Authors:Berk Calli, Aaron Walsman, Arjun Singh, Siddhartha Srinivasa, Pieter Abbeel, Aaron M. Dollar"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2012.05446v3": {
            "Paper Title": "Visual Perception Generalization for Vision-and-Language Navigation via\n  Meta-Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.09164v3": {
            "Paper Title": "Evidential Sparsification of Multimodal Latent Spaces in Conditional\n  Variational Autoencoders",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.07024v3": {
            "Paper Title": "Multi-Object Rearrangement with Monte Carlo Tree Search:A Case Study on\n  Planar Nonprehensile Sorting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.07517v2": {
            "Paper Title": "MATS: An Interpretable Trajectory Forecasting Representation for\n  Planning and Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.11969v2": {
            "Paper Title": "Leveraging Planar Regularities for Point Line Visual-Inertial Odometry",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03512v4": {
            "Paper Title": "Do We Need to Compensate for Motion Distortion and Doppler Effects in\n  Spinning Radar Navigation?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.04767v2": {
            "Paper Title": "Motion Prediction using Trajectory Sets and Self-Driving Domain\n  Knowledge",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.03093v5": {
            "Paper Title": "Trajectron++: Dynamically-Feasible Trajectory Forecasting With\n  Heterogeneous Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.12860v2": {
            "Paper Title": "Sensorimotor representation learning for an \"active self\" in robots: A\n  model survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.12970v5": {
            "Paper Title": "Pontryagin Differentiable Programming: An End-to-End Learning and\n  Control Framework",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.13957v2": {
            "Paper Title": "MELD: Meta-Reinforcement Learning from Images via Latent State Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.11487v2": {
            "Paper Title": "Faithful Euclidean Distance Field from Log-Gaussian Process Implicit\n  Surfaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.13108v2": {
            "Paper Title": "Active and Interactive Mapping with Dynamic Gaussian Process Implicit\n  Surfaces for Mobile Manipulators",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.14796v5": {
            "Paper Title": "AvE: Assistance via Empowerment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.13529v2": {
            "Paper Title": "Lyapunov-Based Reinforcement Learning State Estimator",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07589v3": {
            "Paper Title": "DIRL: Domain-Invariant Representation Learning for Sim-to-Real Transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.09034v2": {
            "Paper Title": "Model-Based Inverse Reinforcement Learning from Visual Demonstrations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.11840v5": {
            "Paper Title": "Efficient Large-Scale Multi-Drone Delivery Using Transit Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08587v2": {
            "Paper Title": "Learning Dexterous Manipulation from Suboptimal Experts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.04787v2": {
            "Paper Title": "An End-to-End Learning Approach for Trajectory Prediction in Pedestrian\n  Zones",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.15948v3": {
            "Paper Title": "Bayes-Adaptive Deep Model-Based Policy Optimisation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.10099v3": {
            "Paper Title": "Crowd-Driven Mapping, Localization and Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.03057v3": {
            "Paper Title": "OrthographicNet: A Deep Transfer Learning Approach for 3D Object\n  Recognition in Open-Ended Domains",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.15485v1": {
            "Paper Title": "Multiple Plans are Better than One: Diverse Stochastic Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.06871v2": {
            "Paper Title": "A Heteroscedastic Likelihood Model for Two-frame Optical Flow",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.15378v1": {
            "Paper Title": "3D Human motion anticipation and classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.15373v1": {
            "Paper Title": "Model-Based Visual Planning with Self-Supervised Functional Distances",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.15358v1": {
            "Paper Title": "A Review into Data Science and Its Approaches in Mechanical Engineering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.15164v1": {
            "Paper Title": "Analysis of Truck Driver Behavior to Design Different Lane Change Styles\n  in Automated Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.09520v7": {
            "Paper Title": "Neural SLAM: Learning to Explore with External Memory",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.09945v2": {
            "Paper Title": "Gesture Recognition for Initiating Human-to-Robot Handovers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.15008v1": {
            "Paper Title": "ALVIO: Adaptive Line and Point Feature-based Visual Inertial Odometry\n  for Robust Localization in Indoor Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.14980v1": {
            "Paper Title": "Perimeter-defense Game between Aerial Defender and Ground Intruder",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.05198v1": {
            "Paper Title": "OpenHPS: An Open Source Hybrid Positioning System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04709v2": {
            "Paper Title": "f-IRL: Inverse Reinforcement Learning via State Marginal Matching",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.14617v1": {
            "Paper Title": "An Efficient Generation Method based on Dynamic Curvature of the\n  Reference Curve for Robust Trajectory Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.14464v1": {
            "Paper Title": "Disentangled Planning and Control in Vision Based Robotics via Reward\n  Machines",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.06136v4": {
            "Paper Title": "Computationally Efficient Obstacle Avoidance Trajectory Planner for UAVs\n  Based on Heuristic Angular Search Method",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.05212v1": {
            "Paper Title": "Ellipse Regression with Predicted Uncertainties for Accurate Multi-View\n  3D Object Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.01053v1": {
            "Paper Title": "Multi-Instance Aware Localization for End-to-End Imitation Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.01055v1": {
            "Paper Title": "Stochastic Action Prediction for Imitation Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.13603v1": {
            "Paper Title": "Modeling Dispositional and Initial learned Trust in Automated Vehicles\n  with Predictability and Explainability",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.11845v3": {
            "Paper Title": "Time Perception: A Review on Psychological, Computational and Robotic\n  Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.13320v1": {
            "Paper Title": "Evolutionary Gait Transfer of Multi-Legged Robots in Complex Terrains",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.13210v1": {
            "Paper Title": "Effective Deployment of CNNs for 3DoF Pose Estimation and Grasping in\n  Industrial Settings",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.13147v1": {
            "Paper Title": "On Radiation-Based Thermal Servoing: New Models, Controls and\n  Experiments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.08595v2": {
            "Paper Title": "Formation and Reconfiguration of Tight Multi-Lane Platoons",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.13057v1": {
            "Paper Title": "A Generalized A* Algorithm for Finding Globally Optimal Paths in\n  Weighted Colored Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.12586v1": {
            "Paper Title": "Distributed Adaptive Control: An ideal Cognitive Architecture candidate\n  for managing a robotic recycling plant",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.13021v2": {
            "Paper Title": "Multimodal Sensor Fusion with Differentiable Filters",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.12437v1": {
            "Paper Title": "Pit30M: A Benchmark for Global Localization in the Age of Self-Driving\n  Cars",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.12411v1": {
            "Paper Title": "Sensing and Reconstruction of 3D Deformation on Pneumatic Soft Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.12377v1": {
            "Paper Title": "DAGMapper: Learning to Map by Discovering Lane Topology",
            "Sentences": [
                {
                    "Sentence ID": 51,
                    "Sentence": "extract building and road network graphs directly in\nthe form of polygons from aerial imagery. [41, 42] en-\nhance Open Street Maps with lane markings, sidewalks and\nparking spots by applying graphical models on top of deep\nfeatures. ",
                    "Citation Text": "Tao Sun, Zonglin Di, Pengyu Che, Chun Liu, and Yin Wang.\nLeveraging crowdsourced gps data for road extraction from\naerial imagery. In CVPR , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.01447",
                        "Citation Paper Title": "Title:Leveraging Crowdsourced GPS Data for Road Extraction from Aerial Imagery",
                        "Citation Paper Abstract": "Abstract:Deep learning is revolutionizing the mapping industry. Under lightweight human curation, computer has generated almost half of the roads in Thailand on OpenStreetMap (OSM) using high-resolution aerial imagery. Bing maps are displaying 125 million computer-generated building polygons in the U.S. While tremendously more efficient than manual mapping, one cannot map out everything from the air. Especially for roads, a small prediction gap by image occlusion renders the entire road useless for routing. Misconnections can be more dangerous. Therefore computer-based mapping often requires local verifications, which is still labor intensive. In this paper, we propose to leverage crowdsourced GPS data to improve and support road extraction from aerial imagery. Through novel data augmentation, GPS rendering, and 1D transpose convolution techniques, we show almost 5% improvements over previous competition winning models, and much better robustness when predicting new areas without any new training data or domain adaptation.",
                        "Citation Paper Authors": "Authors:Tao Sun, Zonglin Di, Pengyu Che, Chun Liu, Yin Wang"
                    },
                    "Keywords": {}
                }
            ]
        },
        "http://arxiv.org/abs/2012.12209v1": {
            "Paper Title": "Emergent Hand Morphology and Control from Optimizing Robust Grasps of\n  Diverse Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.12142v1": {
            "Paper Title": "High-Speed Robot Navigation using Predicted Occupancy Maps",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.11951v1": {
            "Paper Title": "Robotic Process Automation -- A Systematic Literature Review and\n  Assessment Framework",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.11863v1": {
            "Paper Title": "Salient Bundle Adjustment for Visual SLAM",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.06782v4": {
            "Paper Title": "Gradient Surgery for Multi-Task Learning",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": ", only project onto the normal plane of the average gradient of past tasks ",
                    "Citation Text": "Arslan Chaudhry, Marc\u2019Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Ef\ufb01cient\nlifelong learning with a-gem. arXiv:1812.00420 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.00420",
                        "Citation Paper Title": "Title:Efficient Lifelong Learning with A-GEM",
                        "Citation Paper Abstract": "Abstract:In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz & Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC (Kirkpatrick et al., 2016) and other regularization-based methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency.",
                        "Citation Paper Authors": "Authors:Arslan Chaudhry, Marc'Aurelio Ranzato, Marcus Rohrbach, Mohamed Elhoseiny"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1912.04986v3": {
            "Paper Title": "What You See is What You Get: Exploiting Visibility for 3D Object\n  Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.11663v1": {
            "Paper Title": "Combining Deep Reinforcement Learning And Local Control For The Acrobot\n  Swing-up And Balance Task",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.11643v1": {
            "Paper Title": "myGym: Modular Toolkit for Visuomotor Robotic Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.11547v1": {
            "Paper Title": "Offline Reinforcement Learning from Images with Latent Space Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.12361v2": {
            "Paper Title": "Sub-Goal Trees -- a Framework for Goal-Based Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.00191v2": {
            "Paper Title": "Dynamic Legged Manipulation of a Ball Through Multi-Contact Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.10942v1": {
            "Paper Title": "Learning to Localize Through Compressed Binary Maps",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.10902v1": {
            "Paper Title": "Learning to Localize Using a LiDAR Intensity Map",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.10788v1": {
            "Paper Title": "Rapid and High-Fidelity Subsurface Exploration with Multiple Aerial\n  Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.10714v1": {
            "Paper Title": "A Light Field Front-end for Robust SLAM in Dynamic Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.12195v1": {
            "Paper Title": "Labels Are Not Perfect: Inferring Spatial Uncertainty in Object\n  Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.03217v3": {
            "Paper Title": "Conley's fundamental theorem for a class of hybrid systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.09811v1": {
            "Paper Title": "Learning Cross-Domain Correspondence for Control with Dynamics\n  Cycle-Consistency",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.09689v1": {
            "Paper Title": "Trajectory Planning Under Stochastic and Bounded Sensing Uncertainties\n  Using Reachability Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.09378v1": {
            "Paper Title": "Event Camera Calibration of Per-pixel Biased Contrast Threshold",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.09309v1": {
            "Paper Title": "Robotics Enabling the Workforce",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.09264v1": {
            "Paper Title": "MSL-RAPTOR: A 6DoF Relative Pose Tracker for Onboard Robotic Perception",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.09242v1": {
            "Paper Title": "S3CNet: A Sparse Semantic Scene Completion Network for LiDAR Point\n  Clouds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.09052v1": {
            "Paper Title": "Scalable and Safe Multi-Agent Motion Planning with Nonlinear Dynamics\n  and Bounded Disturbances",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.08704v1": {
            "Paper Title": "Sequential Attacks on Kalman Filter-based Forward Collision Warning\n  Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.08703v1": {
            "Paper Title": "Natural grasp intention recognition based on gaze fixation in\n  human-robot interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.08651v1": {
            "Paper Title": "FemtoSats for Exploring Permanently Shadowed Regions on the Moon",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.08637v1": {
            "Paper Title": "Multi-Modal Anomaly Detection for Unstructured and Uncertain\n  Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.08282v1": {
            "Paper Title": "Robots Understanding Contextual Information in Human-Centered\n  Environments using Weakly Supervised Mask Data Distillation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.08061v1": {
            "Paper Title": "Distributed Data Storage and Fusion for Collective Perception in\n  Resource-Limited Mobile Robot Swarms",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.08639v4": {
            "Paper Title": "OffWorld Gym: open-access physical robotics environment for real-world\n  reinforcement learning benchmark and research",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.07773v1": {
            "Paper Title": "PePScenes: A Novel Dataset and Baseline for Pedestrian Action Prediction\n  in 3D",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.07756v1": {
            "Paper Title": "Medical Robots for Infectious Diseases: Lessons and Challenges from the\n  COVID-19 Pandemic",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.09552v2": {
            "Paper Title": "Seeing Through your Skin: Recognizing Objects with a Novel Visuotactile\n  Sensor",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.07556v1": {
            "Paper Title": "Characterizing Universal Reconfigurability of Modular Pivoting Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.10149v5": {
            "Paper Title": "winPIBT: Extended Prioritized Algorithm for Iterative Multi-agent Path\n  Finding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.13187v3": {
            "Paper Title": "Time-Independent Planning for Multiple Moving Agents",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.07196v1": {
            "Paper Title": "Software Quality Assessment for Robot Operating System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.07121v1": {
            "Paper Title": "Deliberative and Conceptual Inference in Service Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.11613v2": {
            "Paper Title": "Reproducible Pruning System on Dynamic Natural Plants for Field\n  Agricultural Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.07029v1": {
            "Paper Title": "Efficient Online Trajectory Planning for Integrator Chain Dynamics using\n  Polynomial Elimination",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.15807v2": {
            "Paper Title": "Using Reinforcement Learning to Herd a Robotic Swarm to a Target\n  Distribution",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.06899v1": {
            "Paper Title": "Semi-supervised reward learning for offline reinforcement learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.06739v1": {
            "Paper Title": "Sampling Training Data for Continual Learning Between Robots and the\n  Cloud",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.06738v1": {
            "Paper Title": "Learning Multi-Arm Manipulation Through Collaborative Teleoperation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.06733v1": {
            "Paper Title": "Human-in-the-Loop Imitation Learning using Remote Teleoperation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.06724v1": {
            "Paper Title": "Synthesis of a Six-Bar Gripper Mechanism for Aerial Grasping",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.06662v1": {
            "Paper Title": "Protective Policy Transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.06615v1": {
            "Paper Title": "Probabilistic Conditional System Invariant Generation with Bayesian\n  Inference",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.04833v4": {
            "Paper Title": "Reward-rational (implicit) choice: A unifying formalism for reward\n  learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.06413v1": {
            "Paper Title": "A Vision-based Sensing Approach for a Spherical Soft Robotic Arm",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.06410v1": {
            "Paper Title": "Learning How to Trade-Off Safety with Agility Using Deep Covariance\n  Estimation for Perception Driven UAV Motion Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.01479v2": {
            "Paper Title": "Estimation of Trocar and Tool Interaction Forces on the da Vinci\n  Research Kit with Two-Step Deep Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.06292v1": {
            "Paper Title": "Spotlight-based 3D Instrument Guidance for Retinal Surgery",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.06291v1": {
            "Paper Title": "Crowd Vetting: Rejecting Adversaries via Collaboration--with Application\n  to Multi-Robot Flocking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.06224v1": {
            "Paper Title": "Structured Policy Representation: Imposing Stability in arbitrarily\n  conditioned dynamic systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.06152v1": {
            "Paper Title": "A Robust Aerial Gripper for Passive Grasping and Impulsive Release using\n  Scotch Yoke Mechanism",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.06117v1": {
            "Paper Title": "How to Train PointGoal Navigation Agents on a (Sample and Compute)\n  Budget",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.05897v1": {
            "Paper Title": "Self-Supervised Learning of Lidar Segmentation for Autonomous Indoor\n  Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.05894v1": {
            "Paper Title": "AutoSelect: Automatic and Dynamic Detection Selection for 3D\n  Multi-Object Tracking",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": "used a single global threshold for\n\ufb01ltering on the entire dataset. To improve performance, a\nfew works ",
                    "Citation Text": "X. Jiang, P. Li, Y . Li, and X. Zhen, \u201cGraph Neural Based End-to-End\nData Association Framework for Online Multiple-Object Tracking,\u201d\narXiv , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.05315",
                        "Citation Paper Title": "Title:Graph Neural Based End-to-end Data Association Framework for Online Multiple-Object Tracking",
                        "Citation Paper Abstract": "Abstract:In this work, we present an end-to-end framework to settle data association in online Multiple-Object Tracking (MOT). Given detection responses, we formulate the frame-by-frame data association as Maximum Weighted Bipartite Matching problem, whose solution is learned using a neural network. The network incorporates an affinity learning module, wherein both appearance and motion cues are investigated to encode object feature representation and compute pairwise affinities. Employing the computed affinities as edge weights, the following matching problem on a bipartite graph is resolved by the optimization module, which leverages a graph neural network to adapt with the varying cardinalities of the association problem and solve the combinatorial hardness with favorable scalability and compatibility. To facilitate effective training of the proposed tracking network, we design a multi-level matrix loss in conjunction with the assembled supervision methodology. Being trained end-to-end, all modules in the tracker can co-adapt and co-operate collaboratively, resulting in improved model adaptiveness and less parameter-tuning efforts. Experiment results on the MOT benchmarks demonstrate the efficacy of the proposed approach.",
                        "Citation Paper Authors": "Authors:Xiaolong Jiang, Peizhao Li, Yanjing Li, Xiantong Zhen"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2011.01968v2": {
            "Paper Title": "Learning 3D Dynamic Scene Representations for Robot Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.05740v1": {
            "Paper Title": "R-AGNO-RPN: A LIDAR-Camera Region Deep Network for Resolution-Agnostic\n  Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.12884v2": {
            "Paper Title": "Improving Redundancy Availability: Dynamic Subtasks Modulation for\n  Robots with Redundancy Insufficiency",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.05336v1": {
            "Paper Title": "Transfer Learning for Efficient Iterative Safety Validation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.05292v1": {
            "Paper Title": "Topological Planning with Transformers for Vision-and-Language\n  Navigation",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": ". With enough\nscale, these pretrained models are adaptive to new tasks, in\nsome cases not even requiring any \ufb01ne-tuning or gradient\nupdates ",
                    "Citation Text": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-\nhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom\nHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\nJeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Rad-ford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners, 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.14165",
                        "Citation Paper Title": "Title:Language Models are Few-Shot Learners",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                        "Citation Paper Authors": "Authors:Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2012.05205v1": {
            "Paper Title": "Tactile Object Pose Estimation from the First Touch with Geometric\n  Contact Rendering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.05032v1": {
            "Paper Title": "ReCoG: A Deep Learning Framework with Heterogeneous Graph for\n  Interaction-Aware Trajectory Prediction",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ", where\none RNN encodes a variable-length sequence of symbols\ninto a \ufb01xed-length vector, and the other RNN decodes the\nvector into another variable-length sequence of symbols. Auto-\nencoders ",
                    "Citation Text": "Y . Bengio, L. Yao, G. Alain, and P. Vincent, \u201cGeneralized denoising\nauto-encoders as generative models,\u201d in Advances in neural information\nprocessing systems , 2013, pp. 899\u2013907.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1305.6663",
                        "Citation Paper Title": "Title:Generalized Denoising Auto-Encoders as Generative Models",
                        "Citation Paper Abstract": "Abstract:Recent work has shown how denoising and contractive autoencoders implicitly capture the structure of the data-generating density, in the case where the corruption noise is Gaussian, the reconstruction error is the squared error, and the data is continuous-valued. This has led to various proposals for sampling from this implicitly learned density function, using Langevin and Metropolis-Hastings MCMC. However, it remained unclear how to connect the training procedure of regularized auto-encoders to the implicit estimation of the underlying data-generating distribution when the data are discrete, or using other forms of corruption process and reconstruction errors. Another issue is the mathematical justification which is only valid in the limit of small corruption noise. We propose here a different attack on the problem, which deals with all these issues: arbitrary (but noisy enough) corruption, arbitrary reconstruction loss (seen as a log-likelihood), handling both discrete and continuous-valued variables, and removing the bias due to non-infinitesimal corruption noise (or non-infinitesimal contractive penalty).",
                        "Citation Paper Authors": "Authors:Yoshua Bengio, Li Yao, Guillaume Alain, Pascal Vincent"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2012.04934v1": {
            "Paper Title": "AMVNet: Assertion-based Multi-View Fusion Network for LiDAR Semantic\n  Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.05061v1": {
            "Paper Title": "Understanding Action Sequences based on Video Captioning for\n  Learning-from-Observation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.12656v2": {
            "Paper Title": "Human-Robot Interaction in a Shared Augmented Reality Workspace",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.04844v1": {
            "Paper Title": "Toward an Affective Touch Robot: Subjective and Physiological Evaluation\n  of Gentle Stroke Motion Using a Human-Imitation Hand",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.04794v1": {
            "Paper Title": "Deep Learning based Multi-Modal Sensing for Tracking and State\n  Extraction of Small Quadcopters",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.04700v1": {
            "Paper Title": "Emergence of Different Modes of Tool Use in a Reaching and Dragging Task",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.00575v2": {
            "Paper Title": "FLIC: Fast Lidar Image Clustering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.04375v1": {
            "Paper Title": "MAP-Elites enables Powerful Stepping Stones and Diversity for Modular\n  Robotics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.05042v1": {
            "Paper Title": "Development of Autonomous Quadcopter",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.04078v1": {
            "Paper Title": "Supporting User Autonomy with Multimodal Fusion to Detect when a User\n  Needs Assistance from a Social Robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.04075v1": {
            "Paper Title": "Efficient Attitude Estimators: A Tutorial and Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.04063v1": {
            "Paper Title": "Cost-effective Machine Learning Inference Offload for Edge Computing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.14349v3": {
            "Paper Title": "Computing Systems for Autonomous Driving: State-of-the-Art and\n  Challenges",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.09957v3": {
            "Paper Title": "Multi-Robot Collision Avoidance under Uncertainty with Probabilistic\n  Safety Barrier Certificates",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.03552v2": {
            "Paper Title": "An Ergodic Measure for Active Learning From Equilibrium",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.03912v1": {
            "Paper Title": "MultiON: Benchmarking Semantic Map Memory using Multi-Object Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.13649v3": {
            "Paper Title": "The EMPATHIC Framework for Task Learning from Implicit Human Feedback",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.13661v2": {
            "Paper Title": "Multi-Task Reinforcement Learning with Soft Modularization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.03449v1": {
            "Paper Title": "Efficient Heuristic Generation for Robot Path Planning with Recurrent\n  Generative Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.02417v2": {
            "Paper Title": "Autonomous Navigation with Mobile Robots using Deep Learning and the\n  Robot Operating System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.03390v1": {
            "Paper Title": "On Infusing Reachability-Based Safety Assurance within Planning\n  Frameworks for Human-Robot Vehicle Interactions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.03234v1": {
            "Paper Title": "Amortized Q-learning with Model-based Action Proposals for Autonomous\n  Driving on Highways",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.03168v1": {
            "Paper Title": "Design of an Optoelectronically Innervated Gripper for Rigid-Soft\n  Interactive Grasping",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.03166v1": {
            "Paper Title": "Conditional Generative Adversarial Networks for Optimal Path Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.11301v3": {
            "Paper Title": "Cross-Lingual Vision-Language Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.03158v1": {
            "Paper Title": "Distributed Multi-agent Meta Learning for Trajectory Design in Wireless\n  Drone Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.03105v1": {
            "Paper Title": "Obstacle avoidance and path finding for mobile robot navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.09134v1": {
            "Paper Title": "Multi-agent navigation based on deep reinforcement learning and\n  traditional pathfinding algorithm",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.02978v1": {
            "Paper Title": "Design and Implementation of Path Trackers for Ackermann Drive based\n  Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.02904v1": {
            "Paper Title": "A Knowledge Driven Approach to Adaptive Assistance Using Preference\n  Reasoning and Explanation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.02836v1": {
            "Paper Title": "Orientation Matters: 6-DoF Autonomous Camera Movement for Minimally\n  Invasive Surgery",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.02788v1": {
            "Paper Title": "Neural Dynamic Policies for End-to-End Sensorimotor Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.11233v3": {
            "Paper Title": "Collaborative Localization of Aerial and Ground Mobile Robots through\n  Orthomosaic Map",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.02399v1": {
            "Paper Title": "P3-LOAM: PPP/LiDAR Loosely Coupled SLAM with Accurate Covariance\n  Estimation and Robust RAIM in Urban Canyon Environment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.02863v2": {
            "Paper Title": "Counterfactual Data Augmentation using Locally Factored Dynamics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.02271v1": {
            "Paper Title": "LAMP: Learning a Motion Policy to Repeatedly Navigate in an Uncertain\n  Environment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.02176v1": {
            "Paper Title": "Material Recognition via Heat Transfer Given Ambiguous Initial\n  Conditions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.02118v1": {
            "Paper Title": "Fast-reactive probabilistic motion planning for high-dimensional robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.02055v1": {
            "Paper Title": "Intervention Design for Effective Sim2Real Transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.01866v1": {
            "Paper Title": "Make One-Shot Video Object Segmentation Efficient Again",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": "propagate and decode segmentation masks based on the \ufb01rst- and query-frame\n2embeddings. STM ",
                    "Citation Text": "Ning Xu Seoung Wug Oh, Joon-Young Lee and Seon Joo Kim. Video object segmentation\nusing space-time memory networks. In Int. Conf. on Computer Vision , 2019. 3, 8, 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.00607",
                        "Citation Paper Title": "Title:Video Object Segmentation using Space-Time Memory Networks",
                        "Citation Paper Abstract": "Abstract:We propose a novel solution for semi-supervised video object segmentation. By the nature of the problem, available cues (e.g. video frame(s) with object masks) become richer with the intermediate predictions. However, the existing methods are unable to fully exploit this rich source of information. We resolve the issue by leveraging memory networks and learn to read relevant information from all available sources. In our framework, the past frames with object masks form an external memory, and the current frame as the query is segmented using the mask information in the memory. Specifically, the query and the memory are densely matched in the feature space, covering all the space-time pixel locations in a feed-forward fashion. Contrast to the previous approaches, the abundant use of the guidance information allows us to better handle the challenges such as appearance changes and occlussions. We validate our method on the latest benchmark sets and achieved the state-of-the-art performance (overall score of 79.4 on Youtube-VOS val set, J of 88.7 and 79.2 on DAVIS 2016/2017 val set respectively) while having a fast runtime (0.16 second/frame on DAVIS 2016 val set).",
                        "Citation Paper Authors": "Authors:Seoung Wug Oh, Joon-Young Lee, Ning Xu, Seon Joo Kim"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2012.01732v1": {
            "Paper Title": "Identification of Prototypical Task Executions Based on Smoothness as\n  Basis of Human-to-Robot Kinematic Skill Transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.01913v2": {
            "Paper Title": "Generalization Guarantees for Imitation Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.01693v1": {
            "Paper Title": "Relational Learning for Skill Preconditions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.01583v1": {
            "Paper Title": "Multimodal Contact Detection using Auditory and Force Features for\n  Reliable Object Placing in Household Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.01526v1": {
            "Paper Title": "From Goals, Waypoints & Paths To Long Term Human Trajectory Forecasting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.01356v1": {
            "Paper Title": "Coinbot: Intelligent Robotic Coin Bag Manipulation Using Deep\n  Reinforcement Learning And Machine Teaching",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.01311v1": {
            "Paper Title": "Top-1 CORSMAL Challenge 2020 Submission: Filling Mass Estimation Using\n  Multi-modal Observations of Human-robot Handovers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.02111v1": {
            "Paper Title": "Deep Inverse Sensor Models as Priors for evidential Occupancy Mapping",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.08684v3": {
            "Paper Title": "Efficient Model-Based Reinforcement Learning through Optimistic Policy\n  Search and Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.00508v1": {
            "Paper Title": "Gaussian Process Based Message Filtering for Robust Multi-Agent\n  Cooperation in the Presence of Adversarial Communication",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.07689v4": {
            "Paper Title": "Soft Multicopter Control using Neural Dynamics Identification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.00298v1": {
            "Paper Title": "End-to-End UAV Simulation for Visual SLAM and Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.00201v1": {
            "Paper Title": "Detect, Reject, Correct: Crossmodal Compensation of Corrupted Sensors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.01925v1": {
            "Paper Title": "IV-Posterior: Inverse Value Estimation for Interpretable Policy\n  Certificates",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.05134v2": {
            "Paper Title": "Deep Imitation Learning for Bimanual Robotic Manipulation",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": ". In the robotics domain, one\ncommon approach is to model robots as graphs comprised of nodes corresponding to joints and edges\ncorresponding to links on the robot body, e.g., ",
                    "Citation Text": "Tingwu Wang, Renjie Liao, Jimmy Ba, and Sanja Fidler. Nervenet: Learning structured policy\nwith graph neural networks. In International Conference on Learning Representations , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.05370",
                        "Citation Paper Title": "Title:Neural Graph Evolution: Towards Efficient Automatic Robot Design",
                        "Citation Paper Abstract": "Abstract:Despite the recent successes in robotic locomotion control, the design of robot relies heavily on human engineering. Automatic robot design has been a long studied subject, but the recent progress has been slowed due to the large combinatorial search space and the difficulty in evaluating the found candidates. To address the two challenges, we formulate automatic robot design as a graph search problem and perform evolution search in graph space. We propose Neural Graph Evolution (NGE), which performs selection on current candidates and evolves new ones iteratively. Different from previous approaches, NGE uses graph neural networks to parameterize the control policies, which reduces evaluation cost on new candidates with the help of skill transfer from previously evaluated designs. In addition, NGE applies Graph Mutation with Uncertainty (GM-UC) by incorporating model uncertainty, which reduces the search space by balancing exploration and exploitation. We show that NGE significantly outperforms previous methods by an order of magnitude. As shown in experiments, NGE is the first algorithm that can automatically discover kinematically preferred robotic graph structures, such as a fish with two symmetrical flat side-fins and a tail, or a cheetah with athletic front and back legs. Instead of using thousands of cores for weeks, NGE efficiently solves searching problem within a day on a single 64 CPU-core Amazon EC2 machine.",
                        "Citation Paper Authors": "Authors:Tingwu Wang, Yuhao Zhou, Sanja Fidler, Jimmy Ba"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2012.00088v1": {
            "Paper Title": "Nothing But Geometric Constraints: A Model-Free Method for Articulated\n  Object Pose Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.00078v1": {
            "Paper Title": "Why Did the Robot Cross the Road? A User Study of Explanation in\n  Human-Robot Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.00053v1": {
            "Paper Title": "Attention-Based Planning with Active Perception",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.15119v1": {
            "Paper Title": "UniCon: Universal Neural Controller For Physics-based Character Motion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.15100v1": {
            "Paper Title": "From the DESK (Dexterous Surgical Skill) to the Battlefield -- A\n  Robotics Exploratory Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.15020v1": {
            "Paper Title": "Dynamic Humanoid Locomotion over Uneven Terrain With Streamlined\n  Perception-Control Pipeline",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.11441v3": {
            "Paper Title": "Task-Agnostic Online Reinforcement Learning with an Infinite Mixture of\n  Gaussian Processes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.14669v1": {
            "Paper Title": "Where to Explore Next? ExHistCNN for History-aware Autonomous 3D\n  Exploration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.14551v1": {
            "Paper Title": "A Customizable Dynamic Scenario Modeling and Data Generation Platform\n  for Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.02232v2": {
            "Paper Title": "Interactive Robot Training for Non-Markov Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.14123v1": {
            "Paper Title": "Robotic grasp detection using a novel two-stage approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.14041v1": {
            "Paper Title": "A RGB-D SLAM Algorithm for Indoor Dynamic Scene",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.13205v2": {
            "Paper Title": "Long-Horizon Visual Planning with Goal-Conditioned Hierarchical\n  Predictors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.06740v2": {
            "Paper Title": "Measuring Visual Generalization in Continuous Control from Pixels",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.13885v1": {
            "Paper Title": "Offline Learning from Demonstrations and Unlabeled Experience",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.13851v1": {
            "Paper Title": "Real-time Active Vision for a Humanoid Soccer Robot Using Deep\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.13706v1": {
            "Paper Title": "ROS Based Visual Programming Tool for Mobile Robot Education and\n  Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.13596v1": {
            "Paper Title": "A Mixed Integer Linear Program For Human And Material Resources\n  Optimization In Emergency Department",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.13467v1": {
            "Paper Title": "Episodic Self-Imitation Learning with Hindsight",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.13199v1": {
            "Paper Title": "Polyhedral Friction Cone Estimator for Object Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.13504v2": {
            "Paper Title": "Deep Probabilistic Feature-metric Tracking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.10864v2": {
            "Paper Title": "Behavioral Repertoires for Soft Tensegrity Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.12912v1": {
            "Paper Title": "DRACO: Weakly Supervised Dense Reconstruction And Canonicalization of\n  Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.12837v1": {
            "Paper Title": "High-Level Description of Robot Architecture",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.12569v1": {
            "Paper Title": "Learning Certified Control using Contraction Metric",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.05701v1": {
            "Paper Title": "An Analysis of Deep Object Detectors For Diver Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.03666v1": {
            "Paper Title": "Impact of Power Supply Noise on Image Sensor Performance in Automotive\n  Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.09906v2": {
            "Paper Title": "Towards Learning Controllable Representations of Physical Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.11972v1": {
            "Paper Title": "Foundations of the Socio-physical Model of Activities (SOMA) for\n  Autonomous Robotic Agents",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.11913v1": {
            "Paper Title": "Semi-supervised Gated Recurrent Neural Networks for Robotic Terrain\n  Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.11785v1": {
            "Paper Title": "An analysis of Reinforcement Learning applied to Coach task in IEEE Very\n  Small Size Soccer",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.11730v1": {
            "Paper Title": "RISE-SLAM: A Resource-aware Inverse Schmidt Estimator for SLAM",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.11722v1": {
            "Paper Title": "From Pixels to Legs: Hierarchical Learning of Quadruped Locomotion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.10019v4": {
            "Paper Title": "Learning a Contact-Adaptive Controller for Robust, Efficient Legged\n  Locomotion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.11552v1": {
            "Paper Title": "MoGaze: A Dataset of Full-Body Motions that Includes Workspace Geometry\n  and Eye-Gaze",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.11723v3": {
            "Paper Title": "Learning from Suboptimal Demonstration via Self-Supervised Reward\n  Regression",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.11440v1": {
            "Paper Title": "The Dynamic of Body and Brain Co-Evolution",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.11357v1": {
            "Paper Title": "CamVox: A Low-cost and Accurate Lidar-assisted Visual SLAM System",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.13052v4": {
            "Paper Title": "GENESIS: Generative Scene Inference and Sampling with Object-Centric\n  Latent Representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.11022v2": {
            "Paper Title": "Learning Stabilizing Controllers for Unstable Linear Quadratic\n  Regulators from a Single Trajectory",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.11270v1": {
            "Paper Title": "COCOI: Contact-aware Online Context Inference for Generalizable\n  Non-planar Pushing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.11191v1": {
            "Paper Title": "Socially Aware Crowd Navigation with Multimodal Pedestrian Trajectory\n  Prediction for Autonomous Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.11190v1": {
            "Paper Title": "Attentional-GCNN: Adaptive Pedestrian Trajectory Prediction towards\n  Generic Autonomous Vehicle Use Cases",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.11104v1": {
            "Paper Title": "Model Predictive Control for Micro Aerial Vehicles: A Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.06977v4": {
            "Paper Title": "Visual Task Progress Estimation with Appearance Invariant Embeddings for\n  Robot Control and Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.10898v1": {
            "Paper Title": "Experimental Assessment of Human-Robot Teaming for Multi-Step Remote\n  Manipulation with Expert Operators",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.10822v1": {
            "Paper Title": "Control and implementation of fluid-driven soft gripper with dynamic\n  uncertainty of object",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.10799v1": {
            "Paper Title": "Deep Smartphone Sensors-WiFi Fusion for Indoor Positioning and Tracking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.10625v1": {
            "Paper Title": "Semantic SLAM with Autonomous Object-Level Data Association",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.10562v1": {
            "Paper Title": "MRAC-RL: A Framework for On-Line Policy Adaptation Under Parametric\n  Model Uncertainty",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.10508v1": {
            "Paper Title": "Planning Folding Motion with Simulation in the Loop Using Laser Forming\n  Origami and Thermal Behaviors as an Example",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.10488v1": {
            "Paper Title": "Utilizing ROS 1 and the Turtlebot3 in a Multi-Robot System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.01360v3": {
            "Paper Title": "DiPE: Deeper into Photometric Errors for Unsupervised Learning of Depth\n  and Ego-motion from Monocular Videos",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.10174v1": {
            "Paper Title": "FLAVA: Find, Localize, Adjust and Verify to Annotate LiDAR-Based Point\n  Clouds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.10150v1": {
            "Paper Title": "Robot Gaining Accurate Pouring Skills through Self-Supervised Learning\n  and Generalization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07656v2": {
            "Paper Title": "Predicting Human Strategies in Simulated Search and Rescue Task",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.08178v2": {
            "Paper Title": "Obstacle avoidance-driven controller for safety-critical aerial robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.10672v3": {
            "Paper Title": "gradSLAM: Automagically differentiable SLAM",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.10024v1": {
            "Paper Title": "Parrot: Data-Driven Behavioral Priors for Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.09407v2": {
            "Paper Title": "Explainable AI for System Failures: Generating Explanations that Improve\n  Human Assistance in Fault Recovery",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.09792v1": {
            "Paper Title": "The Robot Household Marathon Experiment",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.02973v2": {
            "Paper Title": "SAM: Squeeze-and-Mimic Networks for Conditional Visual Driving Policy\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.03964v4": {
            "Paper Title": "Estimating Mass Distribution of Articulated Objects using Non-prehensile\n  Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.09584v1": {
            "Paper Title": "ACRONYM: A Large-Scale Grasp Dataset Based on Simulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.02760v2": {
            "Paper Title": "IV-SLAM: Introspective Vision for Simultaneous Localization and Mapping",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.06749v2": {
            "Paper Title": "Learning hierarchical relationships for object-goal navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.09556v1": {
            "Paper Title": "Visual Diver Face Recognition for Underwater Human-Robot Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.09445v1": {
            "Paper Title": "Cautious Bayesian Optimization for Efficient and Scalable Policy Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.09427v1": {
            "Paper Title": "Fast Motion Understanding with Spatiotemporal Neural Networks and\n  Dynamic Vision Sensors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.09390v1": {
            "Paper Title": "Diverse Plausible Shape Completions from Ambiguous Depth Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.09369v1": {
            "Paper Title": "Attentional Separation-and-Aggregation Network for Self-supervised\n  Depth-Pose Learning in Dynamic Scenes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.09222v1": {
            "Paper Title": "Prognostic and Health Management (PHM) tool for Robot Operating System\n  (ROS)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.09086v1": {
            "Paper Title": "Tracking and Visualizing Signs of Degradation for an Early Failure\n  Prediction of a Rolling Bearing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.09083v1": {
            "Paper Title": "Weighted Entropy Modification for Soft Actor-Critic",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.03426v3": {
            "Paper Title": "Self-Supervised 3D Keypoint Learning for Ego-motion Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.09068v1": {
            "Paper Title": "An analytical diabolo model for robotic learning and control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.02860v2": {
            "Paper Title": "Weakly-Supervised Reinforcement Learning for Controllable Behavior",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.09034v1": {
            "Paper Title": "Domain Concretization from Examples: Addressing Missing Domain Knowledge\n  via Robust Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.09004v1": {
            "Paper Title": "Explaining Conditions for Reinforcement Learning Behaviors from Real and\n  Imagined Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.08499v2": {
            "Paper Title": "Design and Control of Roller Grasper V2 for In-Hand Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.08750v1": {
            "Paper Title": "Iterative Semi-parametric Dynamics Model Learning For Autonomous Racing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.08743v1": {
            "Paper Title": "Curiosity Based Reinforcement Learning on Robot Manufacturing Cell",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.08634v1": {
            "Paper Title": "Exploring Self-Attention for Visual Odometry",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.00524v2": {
            "Paper Title": "Interactive Imitation Learning in State-Space",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.08541v1": {
            "Paper Title": "Efficient Exploration of Reward Functions in Inverse Reinforcement\n  Learning via Bayesian Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.08518v1": {
            "Paper Title": "DeepSeqSLAM: A Trainable CNN+RNN for Joint Global Description and\n  Sequence-based Place Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.08458v1": {
            "Paper Title": "Learning Dense Rewards for Contact-Rich Manipulation Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.08361v1": {
            "Paper Title": "Knowledge-Augmented Dexterous Grasping with Incomplete Sensing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.14480v2": {
            "Paper Title": "One Thousand and One Hours: Self-driving Motion Prediction Dataset",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.08177v1": {
            "Paper Title": "A Long Horizon Planning Framework for Manipulating Rigid Pointcloud\n  Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.14558v2": {
            "Paper Title": "BiTraP: Bi-directional Pedestrian Trajectory Prediction with Multi-modal\n  Goal Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.08106v1": {
            "Paper Title": "Recovering and Simulating Pedestrians in the Wild",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.02753v2": {
            "Paper Title": "robo-gym -- An Open Source Toolkit for Distributed Deep Reinforcement\n  Learning on Real and Simulated Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.11816v3": {
            "Paper Title": "Monitoring and Diagnosability of Perception Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.08027v1": {
            "Paper Title": "ACDER: Augmented Curiosity-Driven Experience Replay",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.03026v2": {
            "Paper Title": "Place Recognition in Forests with Urquhart Tessellations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07972v1": {
            "Paper Title": "Mobile Manipulator for Autonomous Localization, Grasping and Precise\n  Placement of Construction Material in a Semi-structured Environment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07785v1": {
            "Paper Title": "Autonomously Navigating a Surgical Tool Inside the Eye by Learning from\n  Demonstration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07778v1": {
            "Paper Title": "Towards Autonomous Eye Surgery by Combining Deep Imitation Learning with\n  Optimal Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07759v1": {
            "Paper Title": "Time-Efficient Mars Exploration of Simultaneous Coverage and Charging\n  with Multiple Drones",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07713v1": {
            "Paper Title": "DARE: AI-based Diver Action Recognition System using Multi-Channel CNNs\n  for AUV Supervision",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07699v1": {
            "Paper Title": "Efficient falsification approach for autonomous vehicle validation using\n  a parameter optimisation technique based on reinforcement learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07660v1": {
            "Paper Title": "ArraMon: A Joint Navigation-Assembly Instruction Interpretation Task in\n  Dynamic Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.04755v2": {
            "Paper Title": "BayesRace: Learning to race autonomously using prior experience",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.06515v4": {
            "Paper Title": "Evaluation of Cross-View Matching to Improve Ground Vehicle Localization\n  with Aerial Perception",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07626v1": {
            "Paper Title": "Stability Analysis of Complementarity Systems with Neural Network\n  Controllers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07613v1": {
            "Paper Title": "BirdSLAM: Monocular Multibody SLAM in Bird's-Eye View",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.02249v3": {
            "Paper Title": "Let's Get Dirty: GAN Based Data Augmentation for Camera Lens Soiling\n  Detection in Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07387v1": {
            "Paper Title": "Privacy-Preserving Pose Estimation for Human-Robot Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.04017v3": {
            "Paper Title": "SynDistNet: Self-Supervised Monocular Fisheye Camera Distance Estimation\n  Synergized with Semantic Segmentation for Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07384v1": {
            "Paper Title": "Few-shot Object Grounding and Mapping for Natural Language Robot\n  Instruction Following",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07383v1": {
            "Paper Title": "Search-based Planning for Active Sensing in Goal-Directed Coverage Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07357v1": {
            "Paper Title": "Solving Physics Puzzles by Reasoning about Paths",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.09056v2": {
            "Paper Title": "Social-VRNN: One-Shot Multi-modal Trajectory Prediction for Interacting\n  Pedestrians",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07316v1": {
            "Paper Title": "Planning Paths Through Unknown Space by Imagining What Lies Therein",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07213v1": {
            "Paper Title": "PLAS: Latent Action Space for Offline Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.07451v2": {
            "Paper Title": "Learning to Actively Reduce Memory Requirements for Robot Control Tasks",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": "). Lastly, a particularly exciting direction is to explore whether our approach leads to poli-\ncies that are more interpretable (since they only maintain low-dimensional memory representations)\nby visualizing features that impact the memory representation (e.g., using saliency maps ",
                    "Citation Text": "K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside convolutional networks: Visualising\nimage classi\ufb01cation models and saliency maps. arXiv preprint arXiv:1312.6034 , 2013.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1312.6034",
                        "Citation Paper Title": "Title:Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps",
                        "Citation Paper Abstract": "Abstract:This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].",
                        "Citation Paper Authors": "Authors:Karen Simonyan, Andrea Vedaldi, Andrew Zisserman"
                    },
                    "Keywords": [
                        ",",
                        "Navigation",
                        "Reinforcement",
                        "Memory-Ef\ufb01ciency",
                        "Learning"
                    ]
                }
            ]
        },
        "http://arxiv.org/abs/2009.04450v2": {
            "Paper Title": "Map-Adaptive Goal-Based Trajectory Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04627v2": {
            "Paper Title": "Learning to Compose Hierarchical Object-Centric Controllers for Robotic\n  Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07104v1": {
            "Paper Title": "Trajectory Optimization for High-Dimensional Nonlinear Systems under STL\n  Specifications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.06425v2": {
            "Paper Title": "StrObe: Streaming Object Detection from LiDAR Packets",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07026v1": {
            "Paper Title": "Enabling the Sense of Self in a Dual-Arm Robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.07005v1": {
            "Paper Title": "Learning Predictive Models for Ergonomic Control of Prosthetic Devices",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.06985v1": {
            "Paper Title": "Robotic self-representation improves manipulation skills and transfer\n  learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.06431v2": {
            "Paper Title": "Same Object, Different Grasps: Data and Semantic Knowledge for\n  Task-Oriented Grasping",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.06912v1": {
            "Paper Title": "Mechanics of compliant serial manipulator composed of dual-triangle\n  segments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.06853v1": {
            "Paper Title": "DANAE: a denoising autoencoder for underwater attitude estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.06813v1": {
            "Paper Title": "Learning Object Manipulation Skills via Approximate State Estimation\n  from Real Videos",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.06777v1": {
            "Paper Title": "ROLL: Visual Self-Supervised Reinforcement Learning with Object\n  Reasoning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.06752v1": {
            "Paper Title": "Critic PI2: Master Continuous Planning via Policy Improvement with Path\n  Integrals and Deep Actor-Critic Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.06730v1": {
            "Paper Title": "3-D Motion Capture of an Unmodified Drone with Single-chip Millimeter\n  Wave Radar",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.06698v1": {
            "Paper Title": "Robust Policies via Mid-Level Visual Representations: An Experimental\n  Study in Manipulation and Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.00731v3": {
            "Paper Title": "LiRaNet: End-to-End Trajectory Prediction using Spatio-Temporal Radar\n  Fusion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.06417v3": {
            "Paper Title": "Sparse Graphical Memory for Robust Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.04133v2": {
            "Paper Title": "Robot Action Selection Learning via Layered Dimension Informed Program\n  Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.06619v1": {
            "Paper Title": "Learning Latent Representations to Influence Multi-Agent Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01077v2": {
            "Paper Title": "Task-Relevant Adversarial Imitation Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.06544v1": {
            "Paper Title": "Self-supervised reinforcement learning for speaker localisation with the\n  iCub humanoid robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.14543v2": {
            "Paper Title": "Unsupervised Domain Adaptation for Visual Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.06498v1": {
            "Paper Title": "Fit2Form: 3D Generative Model for Robot Gripper Form Design",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.06464v1": {
            "Paper Title": "3D-OES: Viewpoint-Invariant Object-Factorized Environment Simulators",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.11919v3": {
            "Paper Title": "Counterfactual Policy Evaluation for Decision-Making in Autonomous\n  Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.06399v1": {
            "Paper Title": "Fast robust peg-in-hole insertion with continuous visual servoing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.06217v1": {
            "Paper Title": "Accessible Torque Bandwidth of a Series Elastic Actuator Considering the\n  Thermodynamic Limitations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.06165v1": {
            "Paper Title": "Universal Embeddings for Spatio-Temporal Tagging of Self-Driving Logs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.06163v1": {
            "Paper Title": "Intermittent Visual Servoing: Efficiently Learning Policies Robust to\n  Instrument Changes for High-precision Surgical Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.05632v2": {
            "Paper Title": "Exploratory Grasping: Asymptotically Optimal Algorithms for Grasping\n  Challenging Polyhedral Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.12698v2": {
            "Paper Title": "Learning Dense Visual Correspondences in Simulation to Smooth and Fold\n  Real Fabrics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.05970v1": {
            "Paper Title": "Transformers for One-Shot Visual Imitation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.06082v4": {
            "Paper Title": "An Adversarial Objective for Scalable Exploration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.05786v1": {
            "Paper Title": "SPRITE: Stewart Platform Robot for Interactive Tabletop Engagement",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.05767v1": {
            "Paper Title": "Simulating Autonomous Driving in Massive Mixed Urban Traffic",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.05741v1": {
            "Paper Title": "Behaviorally Diverse Traffic Simulation via Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08017v2": {
            "Paper Title": "Autonomous Person-Specific Following Robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.05715v1": {
            "Paper Title": "Reinforcement Learning with Time-dependent Goals for Robotic Musicians",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.05661v1": {
            "Paper Title": "Accelerating Grasp Exploration by Leveraging Learned Priors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.10695v4": {
            "Paper Title": "GDN: A Coarse-To-Fine (C2F) Representation for End-To-End 6-DoF Grasp\n  Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.05565v1": {
            "Paper Title": "Docking two multirotors in midair using relative vision measurements",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.05559v1": {
            "Paper Title": "Learning Bayes Filter Models for Tactile Localization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.09565v3": {
            "Paper Title": "A Model Predictive Approach for Online Mobile Manipulation of\n  Nonholonomic Objects using Learned Dynamics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04816v2": {
            "Paper Title": "StylePredict: Machine Theory of Mind for Human Driver Behavior From\n  Trajectories",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.08973v2": {
            "Paper Title": "GRAC: Self-Guided and Self-Regularized Actor-Critic",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.05513v1": {
            "Paper Title": "Zero-Shot Terrain Generalization for Visual Locomotion Policies",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.06194v1": {
            "Paper Title": "A Factor-Graph Approach for Optimization Problems with Dynamics\n  Constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.03683v1": {
            "Paper Title": "A New Framework for Registration of Semantic Point Clouds from Stereo\n  and RGB-D Cameras",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.10737v4": {
            "Paper Title": "Multi-agent Interactive Prediction under Challenging Driving Scenarios",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.05298v1": {
            "Paper Title": "Computational Design and Fabrication of Corrugated Mechanisms from\n  Behavioral Specifications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.05289v1": {
            "Paper Title": "Learning to Communicate and Correct Pose Errors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.05226v1": {
            "Paper Title": "On-line force capability evaluation based on efficient polytope vertex\n  search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.05180v1": {
            "Paper Title": "Generation of Human-aware Navigation Maps using Graph Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04424v2": {
            "Paper Title": "Playing optical tweezers with deep reinforcement learning: in virtual,\n  physical and augmented environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.05079v1": {
            "Paper Title": "Model Predictive Control for Human-Centred Lower Limb Robotic Assistance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.16342v2": {
            "Paper Title": "Robust Quadrupedal Locomotion on Sloped Terrains: A Linear Policy\n  Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.05039v1": {
            "Paper Title": "OnionBot: A System for Collaborative Computational Cooking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04999v1": {
            "Paper Title": "Untangling Dense Knots by Learning Task-Relevant Keypoints",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.12852v2": {
            "Paper Title": "Probably Approximately Correct Vision-Based Planning using Motion\n  Primitives",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04862v1": {
            "Paper Title": "On Efficient and Robust Metrics for RANSAC Hypotheses and 3D Rigid\n  Registration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04796v1": {
            "Paper Title": "Modeling Trust in Human-Robot Interaction: A Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04752v1": {
            "Paper Title": "Trajectory Planning for Autonomous Vehicles Using Hierarchical\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04702v1": {
            "Paper Title": "Safe Trajectory Planning Using Reinforcement Learning for Self Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.10383v2": {
            "Paper Title": "Classified Regression for Bayesian Optimization: Robot Learning with\n  Unknown Penalties",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.04460v2": {
            "Paper Title": "Hardware as Policy: Mechanical and Computational Co-Optimization using\n  Deep Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04515v1": {
            "Paper Title": "SENSAR: A Visual Tool for Intelligent Robots for Collaborative\n  Human-Robot Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.09417v2": {
            "Paper Title": "Action-Based Representation Learning for Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.01096v3": {
            "Paper Title": "Invariant Policy Optimization: Towards Stronger Generalization in\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04282v1": {
            "Paper Title": "Reward Conditioned Neural Movement Primitives for Population Based\n  Variational Policy Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.11334v3": {
            "Paper Title": "ACNMP: Skill Transfer and Task Extrapolation through Learning from\n  Demonstration and Reinforcement Learning via Representation Sharing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04250v1": {
            "Paper Title": "A Learning-Based Tune-Free Control Framework for Large Scale Autonomous\n  Driving System Deployment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04240v1": {
            "Paper Title": "GPU Accelerated Convex Approximations for Fast Multi-Agent Trajectory\n  Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04222v1": {
            "Paper Title": "Multiagent Rollout and Policy Iteration for POMDP with Application to\n  Multi-Robot Repair Problems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.06906v2": {
            "Paper Title": "Model-based Reinforcement Learning for Decentralized Multiagent\n  Rendezvous",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04910v1": {
            "Paper Title": "Spring-Rod System Identification via Differentiable Physics Engine",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04173v1": {
            "Paper Title": "Geometric Structure Aided Visual Inertial Localization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04141v1": {
            "Paper Title": "Uncertainty-Aware Constraint Learning for Adaptive Safe Motion Planning\n  from Demonstrations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.05846v2": {
            "Paper Title": "SSP: Single Shot Future Trajectory Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04118v1": {
            "Paper Title": "Joint Estimation of Expertise and Reward Preferences From Human\n  Demonstrations",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": ".\nThe second group considers less structured representations\nsuch as deep neural networks ",
                    "Citation Text": "P. Christiano, J. Leike, T. B. Brown, M. Martic, S. Legg, and D. Amodei,\n\u201cDeep reinforcement learning from human preferences,\u201d 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03741",
                        "Citation Paper Title": "Title:Deep reinforcement learning from human preferences",
                        "Citation Paper Abstract": "Abstract:For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.",
                        "Citation Paper Authors": "Authors:Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, Dario Amodei"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2011.03922v1": {
            "Paper Title": "Learning World Transition Model for Socially Aware Robot Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03894v1": {
            "Paper Title": "Multimodal Trajectory Prediction via Topological Invariance for\n  Navigation at Uncontrolled Intersections",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.10180v2": {
            "Paper Title": "Sampling-based Reachability Analysis: A Random Set Theory Approach with\n  Adversarial Sampling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.04904v1": {
            "Paper Title": "Feasible Region-based Identification Using Duality (Extended Version)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.13202v3": {
            "Paper Title": "CAMPs: Learning Context-Specific Abstractions for Efficient Planning in\n  Factored MDPs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.16404v2": {
            "Paper Title": "Unsupervised Monocular Depth Learning in Dynamic Scenes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03807v1": {
            "Paper Title": "Sim-to-Real Transfer for Vision-and-Language Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.11174v4": {
            "Paper Title": "Learning Obstacle Representations for Neural Motion Planning",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": "encoded\nwith a multi-layer perpeceptron (MLP). We show that obstacles representation is critical to solve\ncomplex problems with rich workspace variations and propose to rely on a point cloud representation\nof obstacles coupled with PointNet ",
                    "Citation Text": "C. R. Qi, H. Su, K. Mo, and L. J. Guibas. PointNet: Deep learning on point sets for 3D classi\ufb01cation\nand segmentation. In Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition,\nCVPR 2017 , 2017. ISBN 9781538604571.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.00593",
                        "Citation Paper Title": "Title:PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation",
                        "Citation Paper Abstract": "Abstract:Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds and well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.",
                        "Citation Paper Authors": "Authors:Charles R. Qi, Hao Su, Kaichun Mo, Leonidas J. Guibas"
                    },
                    "Keywords": [
                        "obstacle",
                        "avoidance",
                        ",",
                        "learning",
                        "planning",
                        "representation",
                        "neural",
                        "motion"
                    ]
                }
            ]
        },
        "http://arxiv.org/abs/1909.12517v2": {
            "Paper Title": "TORM: Fast and Accurate Trajectory Optimization of Redundant Manipulator\n  given an End-Effector Path",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.08376v3": {
            "Paper Title": "Inverting the Pose Forecasting Pipeline with SPF2: Sequential Pointcloud\n  Forecasting for Sequential Pose Forecasting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03651v1": {
            "Paper Title": "Strawberry Detection Using a Heterogeneous Multi-Processor Platform",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03648v1": {
            "Paper Title": "Sliding on Manifolds: Geometric Attitude Control with Quaternions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03635v1": {
            "Paper Title": "Motion Prediction on Self-driving Cars: A Review",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.03146v3": {
            "Paper Title": "A Review of Robot Learning for Manipulation: Challenges,\n  Representations, and Algorithms",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.13134v2": {
            "Paper Title": "Data-efficient visuomotor policy training using reinforcement learning\n  and generative models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03464v1": {
            "Paper Title": "HAVEN: A Unity-based Virtual Robot Environment to Showcase HRI-based\n  Augmented Reality",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.01122v2": {
            "Paper Title": "SLAM in the Field: An Evaluation of Monocular Mapping and Localization\n  on Challenging Dynamic Agricultural Environment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.01649v3": {
            "Paper Title": "Robotic Grasping through Combined Image-Based Grasp Proposal and 3D\n  Reconstruction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.02967v2": {
            "Paper Title": "Stochastic-YOLO: Efficient Probabilistic Object Detection under Dataset\n  Shifts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.00715v2": {
            "Paper Title": "Learning to Drive (L2D) as a Low-Cost Benchmark for Real-World\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03290v1": {
            "Paper Title": "Event-VPR: End-to-End Weakly Supervised Network Architecture for\n  Event-based Visual Place Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.04487v3": {
            "Paper Title": "Self-Supervised Object-in-Gripper Segmentation from Robotic Motions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.00024v3": {
            "Paper Title": "DROGON: A Trajectory Prediction Model based on Intention-Conditioned\n  Behavior Reasoning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03252v1": {
            "Paper Title": "Learning Behavior Trees with Genetic Programming in Unpredictable\n  Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03216v1": {
            "Paper Title": "Task-relevant Representation Learning for Networked Robotic Perception",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.14286v1": {
            "Paper Title": "Six Degree of Freedom Robotic Arm with Mimicking Mechanism",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03136v1": {
            "Paper Title": "STReSSD: Sim-To-Real from Sound for Stochastic Dynamics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.01952v2": {
            "Paper Title": "Learning Active Task-Oriented Exploration Policies for Bridging the\n  Sim-to-Real Gap",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.12160v4": {
            "Paper Title": "In-Hand Object Pose Tracking via Contact Feedback and GPU-Accelerated\n  Robotic Simulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00155v2": {
            "Paper Title": "Deep Reactive Planning in Dynamic Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.10201v2": {
            "Paper Title": "Action-Conditional Recurrent Kalman Networks For Forward and Inverse\n  Dynamics Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03077v1": {
            "Paper Title": "MorphEyes: Variable Baseline Stereo For Quadrotor Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.02092v2": {
            "Paper Title": "Customized Handling of Unintended Interface Operation in Assistive\n  Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.08032v4": {
            "Paper Title": "Inferring the Material Properties of Granular Media for Robotic Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.16361v2": {
            "Paper Title": "Towards Preference Learning for Autonomous Ground Robot Navigation Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02888v1": {
            "Paper Title": "Improving Robotic Grasping on Monocular Images Via Multi-Task Learning\n  and Positional Loss",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02853v1": {
            "Paper Title": "UAV-AdNet: Unsupervised Anomaly Detection using Deep Neural Networks for\n  Aerial Surveillance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02781v1": {
            "Paper Title": "ROS-Mobile: An Android application for the Robot Operating System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02731v1": {
            "Paper Title": "Why robots should be technical: Correcting mental models through\n  technical architecture concepts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02658v1": {
            "Paper Title": "Compositional Scalable Object SLAM",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.02798v2": {
            "Paper Title": "Policy learning in SE(3) action spaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02608v1": {
            "Paper Title": "Learning a Decentralized Multi-arm Motion Planner",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02574v1": {
            "Paper Title": "Learning Trajectories for Visual-Inertial System Calibration via\n  Model-based Heuristic Deep Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02573v1": {
            "Paper Title": "EEGS: A Transparent Model of Emotions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.04561v2": {
            "Paper Title": "Auxiliary Tasks Speed Up Learning PointGoal Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02506v1": {
            "Paper Title": "The dynamic effect of mechanical losses of actuators on the equations of\n  motion of legged robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02462v1": {
            "Paper Title": "Towards Robotic Assembly by Predicting Robust, Precise and Task-oriented\n  Grasps",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.02616v2": {
            "Paper Title": "The Emergence of Adversarial Communication in Multi-Agent Reinforcement\n  Learning",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": ". Other work focuses on the development of\nlearning algorithms for non-cooperative multi-player games [22, 23]. Yet none of these approaches\ninclude dedicated communication channels between agents.\nMore closely related to our work, the work in ",
                    "Citation Text": "R. Lowe, Y . Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch. Multi-agent actor-critic for mixed\ncooperative-competitive environments. Neural Information Processing Systems (NIPS) , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.02275",
                        "Citation Paper Title": "Title:Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments",
                        "Citation Paper Abstract": "Abstract:We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.",
                        "Citation Paper Authors": "Authors:Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, Igor Mordatch"
                    },
                    "Keywords": [
                        ",",
                        "Ad-",
                        "Graph",
                        "Reinforcement",
                        "Neural",
                        "Networks",
                        "Learning",
                        "Multi-Agent"
                    ]
                }
            ]
        },
        "http://arxiv.org/abs/2011.02441v1": {
            "Paper Title": "Robust Entry Vehicle Guidance with Sampling-Based Invariant Funnels",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02403v1": {
            "Paper Title": "IDE-Net: Interactive Driving Event and Pattern Extraction from Human\n  Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.02263v3": {
            "Paper Title": "Projection Mapping Implementation: Enabling Direct Externalization of\n  Perception Results and Action Intent to Improve Robot Explainability",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02373v1": {
            "Paper Title": "Moving Forward in Formation: A Decentralized Hierarchical Learning\n  Approach to Multi-Agent Moving Together",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02920v1": {
            "Paper Title": "Asynchronous Deep Model Reference Adaptive Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02156v1": {
            "Paper Title": "CoT-AMFlow: Adaptive Modulation Network with Co-Teaching Strategy for\n  Unsupervised Optical Flow Estimation",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": "was the \ufb01rst end-to-end deep neural network for optical \ufb02ow estimation. It employs a\ncorrelation layer to compute feature correspondence. Later on, PWC-Net ",
                    "Citation Text": "D. Sun, X. Yang, M.-Y . Liu, and J. Kautz. PWC-Net: CNNs for optical \ufb02ow using pyramid,\nwarping, and cost volume. In Proc. IEEE Conf. Comput. Vision Pattern Recognit. (CVPR) ,\npages 8934\u20138943, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.02371",
                        "Citation Paper Title": "Title:PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume",
                        "Citation Paper Abstract": "Abstract:We present a compact but effective CNN model for optical flow, called PWC-Net. PWC-Net has been designed according to simple and well-established principles: pyramidal processing, warping, and the use of a cost volume. Cast in a learnable feature pyramid, PWC-Net uses the cur- rent optical flow estimate to warp the CNN features of the second image. It then uses the warped features and features of the first image to construct a cost volume, which is processed by a CNN to estimate the optical flow. PWC-Net is 17 times smaller in size and easier to train than the recent FlowNet2 model. Moreover, it outperforms all published optical flow methods on the MPI Sintel final pass and KITTI 2015 benchmarks, running at about 35 fps on Sintel resolution (1024x436) images. Our models are available on this https URL.",
                        "Citation Paper Authors": "Authors:Deqing Sun, Xiaodong Yang, Ming-Yu Liu, Jan Kautz"
                    },
                    "Keywords": [
                        "unsupervised",
                        ",",
                        "strategy",
                        "learning",
                        "co-teaching",
                        "optical",
                        "\ufb02ow"
                    ]
                }
            ]
        },
        "http://arxiv.org/abs/2011.02076v1": {
            "Paper Title": "An On-Line POMDP Solver for Continuous Observation Spaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02047v1": {
            "Paper Title": "When Shall I Be Empathetic? The Utility of Empathetic Parameter\n  Estimation in Multi-Agent Interactions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.01999v1": {
            "Paper Title": "Autonomous Wall Building with a UGV-UAV Team at MBZIRC 2020",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.01975v1": {
            "Paper Title": "Rearrangement: A Challenge for Embodied AI",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.01969v1": {
            "Paper Title": "Face-work for Human-Agent Joint Decision-Making",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.01928v1": {
            "Paper Title": "Generalization to New Actions in Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.01917v1": {
            "Paper Title": "A Dynamics Simulator for Soft Growing Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.01872v1": {
            "Paper Title": "Predicting Terrain Mechanical Properties in Sight for Planetary Rovers\n  with Semantic Clues",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.01810v1": {
            "Paper Title": "Safe, Passive Control for Mechanical Systems with Application to\n  Physical Human-Robot Interactions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.09959v2": {
            "Paper Title": "When We First Met: Visual-Inertial Person Localization for Co-Robot\n  Rendezvous",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.08550v3": {
            "Paper Title": "Learning to Walk in the Real World with Minimal Human Effort",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.13315v3": {
            "Paper Title": "Benchmarking Metric Ground Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.01298v1": {
            "Paper Title": "Shaping Rewards for Reinforcement Learning with Imperfect Demonstrations\n  using Generative Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.03778v2": {
            "Paper Title": "3D Shape Reconstruction from Vision and Touch",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.01163v1": {
            "Paper Title": "Pushing the Envelope of Rotation Averaging for Visual SLAM",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.01056v1": {
            "Paper Title": "Toward Mutual Trust Modeling in Human-Robot Collaboration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.01046v1": {
            "Paper Title": "NEARL: Non-Explicit Action Reinforcement Learning for Robotic Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.09474v4": {
            "Paper Title": "Deep Phase Correlation for End-to-End Heterogeneous Sensor Measurements\n  Matching",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00646v1": {
            "Paper Title": "DRF: A Framework for High-Accuracy Autonomous Driving Vehicle Modeling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00608v1": {
            "Paper Title": "Unsupervised Metric Relocalization Using Transform Consistency Loss",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00605v1": {
            "Paper Title": "Technical Report: A New Hopping Controller for Highly Dynamical Bipeds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00488v1": {
            "Paper Title": "Fast Adaptation of Manipulator Trajectories to Task Perturbation By\n  Differentiating through the Optimal Solution",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00450v1": {
            "Paper Title": "HM4: Hidden Markov Model with Memory Management for Visual Place\n  Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.11193v2": {
            "Paper Title": "ContactNets: Learning Discontinuous Contact Dynamics with Smooth,\n  Implicit Representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.11696v2": {
            "Paper Title": "CAZSL: Zero-Shot Regression for Pushing Models by Generalizing Through\n  Context",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00397v1": {
            "Paper Title": "APPLR: Adaptive Planner Parameter Learning from Reinforcement",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00372v1": {
            "Paper Title": "Pose Estimation of Specular and Symmetrical Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00359v1": {
            "Paper Title": "TartanVO: A Generalizable Learning-based VO",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.13483v3": {
            "Paper Title": "High Acceleration Reinforcement Learning for Real-World Juggling with\n  Binary Rewards",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00320v1": {
            "Paper Title": "Scene Flow from Point Clouds with or without Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00168v1": {
            "Paper Title": "Multimodal and self-supervised representation learning for automatic\n  gesture recognition in surgical robotics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.00038v1": {
            "Paper Title": "Waymo Public Road Safety Performance Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.15492v2": {
            "Paper Title": "\"What, not how\": Solving an under-actuated insertion task from scratch",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.16298v1": {
            "Paper Title": "Learning Vision-based Reactive Policies for Obstacle Avoidance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03348v1": {
            "Paper Title": "Drone Positioning for Visible Light Communication with Drone-Mounted LED\n  and Camera",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.15347v1": {
            "Paper Title": "Distance Invariant Sparse Autoencoder for Wireless Signal Strength\n  Mapping",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.09565v5": {
            "Paper Title": "A Continuous Teleoperation Subspace with Empirical and Algorithmic\n  Mapping Algorithms for Non-Anthropomorphic Hands",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.15033v1": {
            "Paper Title": "The Amazing Race TM: Robot Edition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.14913v1": {
            "Paper Title": "Visually Guided Balloon Popping with an Autonomous MAV at MBZIRC 2020",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.14876v1": {
            "Paper Title": "Fighting Copycat Agents in Behavioral Cloning from Observation Histories",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.14834v1": {
            "Paper Title": "DeepQ Stepper: A framework for reactive dynamic walking on uneven\n  terrain",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.14691v1": {
            "Paper Title": "Implicit Integration for Articulated Bodies with Contact via the\n  Nonconvex Maximal Dissipation Principle",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.14603v1": {
            "Paper Title": "Learning to be Safe: Deep RL with a Safety Critic",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.14597v1": {
            "Paper Title": "Learning to Generate Cost-to-Go Functions for Efficient Motion Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.04374v3": {
            "Paper Title": "Chance-Constrained Trajectory Optimization for Safe Exploration and\n  Learning of Nonlinear Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.14537v1": {
            "Paper Title": "The State of Industrial Robotics: Emerging Technologies, Challenges, and\n  Key Research Directions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.14500v1": {
            "Paper Title": "COG: Connecting New Skills to Past Experience with Offline Reinforcement\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.14444v1": {
            "Paper Title": "Can Reinforcement Learning for Continuous Control Generalize Across\n  Physics Engines?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.14296v1": {
            "Paper Title": "Fit to Measure: Reasoning about Sizes for Robust Object Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.00956v2": {
            "Paper Title": "Guided Meta-Policy Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.10034v2": {
            "Paper Title": "Semantic Visual Navigation by Watching YouTube Videos",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.12483v2": {
            "Paper Title": "Mapping with Reflection -- Detection and Utilization of Reflection in 3D\n  Lidar Scans",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.13983v1": {
            "Paper Title": "A Neuro-Symbolic Humanlike Arm Controller for Sophia the Robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.13942v1": {
            "Paper Title": "Exploiting the Nonlinear Stiffness of TMP Origami Folding to Enhance\n  Robotic Jumping Performance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.07911v2": {
            "Paper Title": "Generating Automatic Curricula via Self-Supervised Active Domain\n  Randomization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.04359v2": {
            "Paper Title": "Low-viewpoint forest depth dataset for sparse rover swarms",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.13565v1": {
            "Paper Title": "POMDP Manipulation Planning under Object Composition Uncertainty",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.13439v1": {
            "Paper Title": "On Embodied Visual Navigation in Real Environments Through Habitat",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.13407v1": {
            "Paper Title": "Behavioral decision-making for urban autonomous driving in the presence\n  of pedestrians using Deep Recurrent Q-Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.02320v2": {
            "Paper Title": "RoboFly: An insect-sized robot with simplified fabrication that is\n  capable of flight, ground, and water surface locomotion",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.13025v2": {
            "Paper Title": "Learning an Action-Conditional Model for Haptic Texture Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.12401v2": {
            "Paper Title": "Predictive Information Accelerates Learning in RL",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.13129v1": {
            "Paper Title": "ImitationFlow: Learning Deep Stable Stochastic Dynamic Systems by\n  Normalizing Flows",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.13072v1": {
            "Paper Title": "LIRO: Tightly Coupled Lidar-Inertia-Ranging Odometry",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.13056v1": {
            "Paper Title": "Proactive Action Visual Residual Reinforcement Learning for Contact-Rich\n  Tasks Using a Torque-Controlled Robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.00219v3": {
            "Paper Title": "Deep Kinematic Models for Kinematically Feasible Vehicle Trajectory\n  Predictions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.12974v1": {
            "Paper Title": "Improving the Exploration of Deep Reinforcement Learning in Continuous\n  Domains using Planning for Policy Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.08129v1": {
            "Paper Title": "Tissue characterization based on the analysis on i3DUS data for\n  diagnosis support in neurosurgery",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.09306v2": {
            "Paper Title": "Learning About Objects by Learning to Interact with Them",
            "Sentences": [
                {
                    "Sentence ID": 54,
                    "Sentence": "infer the mass by observing objects sliding down a ramp. ",
                    "Citation Text": "Trevor Scott Standley, Ozan Sener, Dawn Chen, and Silvio Savarese. image2mass: Estimating the mass of\nan object from its image. In CoRL , 2017. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.07553",
                        "Citation Paper Title": "Title:Which Tasks Should Be Learned Together in Multi-task Learning?",
                        "Citation Paper Abstract": "Abstract:Many computer vision applications require solving multiple tasks in real-time. A neural network can be trained to solve multiple tasks simultaneously using multi-task learning. This can save computation at inference time as only a single network needs to be evaluated. Unfortunately, this often leads to inferior overall performance as task objectives can compete, which consequently poses the question: which tasks should and should not be learned together in one network when employing multi-task learning? We study task cooperation and competition in several different learning settings and propose a framework for assigning tasks to a few neural networks such that cooperating tasks are computed by the same neural network, while competing tasks are computed by different networks. Our framework offers a time-accuracy trade-off and can produce better accuracy using less inference time than not only a single large multi-task neural network but also many single-task networks.",
                        "Citation Paper Authors": "Authors:Trevor Standley, Amir R. Zamir, Dawn Chen, Leonidas Guibas, Jitendra Malik, Silvio Savarese"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2006.04843v2": {
            "Paper Title": "Modeling Long-horizon Tasks as Sequential Interaction Landscapes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.12639v1": {
            "Paper Title": "The RobotSlang Benchmark: Dialog-guided Robot Localization and\n  Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.12598v1": {
            "Paper Title": "A Software Architecture for Autonomous Vehicles: Team LRM-B Entry in the\n  First CARLA Autonomous Driving Challenge",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.12488v1": {
            "Paper Title": "CLOUD: Contrastive Learning of Unsupervised Dynamics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.02996v2": {
            "Paper Title": "Variable Autonomy of Whole-body Control for Inspection and Intervention\n  in Industrial Environments using Legged Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03331v1": {
            "Paper Title": "Scalable Unsupervised Multi-Criteria Trajectory Segmentation and Driving\n  Preference Mining",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.12274v1": {
            "Paper Title": "VIRAL-Fusion: A Visual-Inertial-Ranging-Lidar Sensor Fusion Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.12273v1": {
            "Paper Title": "Kinodynamic Planning for an Energy-Efficient Autonomous Ornithopter",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.12091v1": {
            "Paper Title": "Migratable AI: Personalizing Dialog Conversations with migration context",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.03330v1": {
            "Paper Title": "Safe trajectory of a piece moved by a robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.12083v1": {
            "Paper Title": "Language-Conditioned Imitation Learning for Robot Manipulation Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.11944v1": {
            "Paper Title": "Accelerating Reinforcement Learning with Learned Skill Priors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.11940v1": {
            "Paper Title": "Motion Planner Augmented Reinforcement Learning for Robot Manipulation\n  in Obstructed Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.07364v2": {
            "Paper Title": "Residual Force Control for Agile Human Behavior Imitation and Extended\n  Motion Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.11911v1": {
            "Paper Title": "Source localization using particle filtering on FPGA for robotic\n  navigation with imprecise binary measurement",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.13924v4": {
            "Paper Title": "EvolveGraph: Multi-Agent Trajectory Prediction with Dynamic Relational\n  Reasoning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.02015v2": {
            "Paper Title": "Reach Out and Help: Assisted Remote Collaboration through a Handheld\n  Robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.08744v3": {
            "Paper Title": "PLOP: Probabilistic poLynomial Objects trajectory Planning for\n  autonomous driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.09517v3": {
            "Paper Title": "IN2LAAMA: INertial Lidar Localisation Autocalibration And MApping",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.11476v1": {
            "Paper Title": "Modeling and Validation of Soft Robotic Snake Locomotion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.11473v1": {
            "Paper Title": "A Novel Variable Stiffness Soft Robotic Gripper",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.11376v1": {
            "Paper Title": "Heterogeneous Vehicle Routing and Teaming with Gaussian Distributed\n  Energy Uncertainty",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.11323v1": {
            "Paper Title": "Learning to Plan Optimally with Flow-based Motion Planner",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.12440v1": {
            "Paper Title": "Importance-Aware Semantic Segmentation in Self-Driving with Discrete\n  Wasserstein Training",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.11296v1": {
            "Paper Title": "System Design and Control of an Apple Harvesting Robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.11063v1": {
            "Paper Title": "Safe planning and control under uncertainty for self-driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.04425v3": {
            "Paper Title": "A sub-modular receding horizon solution for mobile multi-agent\n  persistent monitoring",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.10740v1": {
            "Paper Title": "Safety Verification of Model Based Reinforcement Learning Controllers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.10706v1": {
            "Paper Title": "Can We Enable the Drone to be a Filmmaker?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.10645v1": {
            "Paper Title": "Axiom Learning and Belief Tracing for Transparent Decision Making in\n  Robotics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.02851v4": {
            "Paper Title": "Planning under non-rational perception of uncertain spatial costs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.13254v2": {
            "Paper Title": "Environmental Adaptation of Robot Morphology and Control through\n  Real-world Evolution",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.10190v1": {
            "Paper Title": "Model Predictive Contouring Control for Collision Avoidance in\n  Unstructured Dynamic Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.10064v1": {
            "Paper Title": "Decision Making in Joint Push-Grasp Action Space for Large-Scale Object\n  Sorting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.14179v4": {
            "Paper Title": "A Graph Attention Spatio-temporal Convolutional Network for 3D Human\n  Pose Estimation in Video",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.09909v1": {
            "Paper Title": "The Role of Robotics in Infectious Disease Crises",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.09903v1": {
            "Paper Title": "Teleoperated aerial manipulator and its avatar. Part 1: Communication,\n  system's interconnection, control, and virtual world",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.02266v2": {
            "Paper Title": "milliEgo: Single-chip mmWave Radar Aided Egomotion Estimation via Deep\n  Sensor Fusion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.12075v2": {
            "Paper Title": "Exploiting Variable Impedance for Energy Efficient Sequential Movements",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.09819v1": {
            "Paper Title": "Comparative Analysis of Control Barrier Functions and Artificial\n  Potential Fields for Obstacle Avoidance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.09635v1": {
            "Paper Title": "Deep Reinforcement Learning with Population-Coded Spiking Neural Network\n  for Continuous Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.09618v1": {
            "Paper Title": "Aerial Mobile Manipulator System to Enable Dexterous Manipulations with\n  Increased Precision",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.09582v1": {
            "Paper Title": "Learning to Reconstruct and Segment 3D Objects",
            "Sentences": [
                {
                    "Sentence ID": 273,
                    "Sentence": "Xu Wang, Jingming He, and Lin Ma. Exploiting Local and Global Structure\nfor Point Cloud Semantic Segmentation with Contextual Point Representations.\nAdvances in Neural Information Processing Systems , pages 4573{4583, 2019.\n126 ",
                    "Citation Text": "Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein,\nand Justin M. Solomon. Dynamic Graph CNN for Learning on Point Clouds.\nACM Transactions on Graphics , 38(5), 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.07829",
                        "Citation Paper Title": "Title:Dynamic Graph CNN for Learning on Point Clouds",
                        "Citation Paper Abstract": "Abstract:Point clouds provide a flexible geometric representation suitable for countless applications in computer graphics; they also comprise the raw output of most 3D data acquisition devices. While hand-designed features on point clouds have long been proposed in graphics and vision, however, the recent overwhelming success of convolutional neural networks (CNNs) for image analysis suggests the value of adapting insight from CNN to the point cloud world. Point clouds inherently lack topological information so designing a model to recover topology can enrich the representation power of point clouds. To this end, we propose a new neural network module dubbed EdgeConv suitable for CNN-based high-level tasks on point clouds including classification and segmentation. EdgeConv acts on graphs dynamically computed in each layer of the network. It is differentiable and can be plugged into existing architectures. Compared to existing modules operating in extrinsic space or treating each point independently, EdgeConv has several appealing properties: It incorporates local neighborhood information; it can be stacked applied to learn global shape properties; and in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding. We show the performance of our model on standard benchmarks including ModelNet40, ShapeNetPart, and S3DIS.",
                        "Citation Paper Authors": "Authors:Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, Justin M. Solomon"
                    },
                    "Keywords": []
                },
                {
                    "Sentence ID": 201,
                    "Sentence": "Mark Pauly, Niloy J. Mitra, Johannes Wallner, Helmut Pottmann, and\nLeonidas J. Guibas. Discovering Structural Regularity in 3D Geometry. ACM\nTransactions on Graphics , 27(3):1, 2008. ",
                    "Citation Text": "Yanjun Peng, Ming Chang, Qiong Wang, Yinling Qian, Yingkui Zhang,\nMingqiang Wei, and Xiangyun Liao. Sparse-to-Dense Multi-Encoder Shape\nCompletion of Unstructured Point Cloud. IEEE Access , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.03452",
                        "Citation Paper Title": "Title:Shape As Points: A Differentiable Poisson Solver",
                        "Citation Paper Abstract": "Abstract:In recent years, neural implicit representations gained popularity in 3D reconstruction due to their expressiveness and flexibility. However, the implicit nature of neural implicit representations results in slow inference time and requires careful initialization. In this paper, we revisit the classic yet ubiquitous point cloud representation and introduce a differentiable point-to-mesh layer using a differentiable formulation of Poisson Surface Reconstruction (PSR) that allows for a GPU-accelerated fast solution of the indicator function given an oriented point cloud. The differentiable PSR layer allows us to efficiently and differentiably bridge the explicit 3D point representation with the 3D mesh via the implicit indicator field, enabling end-to-end optimization of surface reconstruction metrics such as Chamfer distance. This duality between points and meshes hence allows us to represent shapes as oriented point clouds, which are explicit, lightweight and expressive. Compared to neural implicit representations, our Shape-As-Points (SAP) model is more interpretable, lightweight, and accelerates inference time by one order of magnitude. Compared to other explicit representations such as points, patches, and meshes, SAP produces topology-agnostic, watertight manifold surfaces. We demonstrate the effectiveness of SAP on the task of surface reconstruction from unoriented point clouds and learning-based reconstruction.",
                        "Citation Paper Authors": "Authors:Songyou Peng, Chiyu \"Max\" Jiang, Yiyi Liao, Michael Niemeyer, Marc Pollefeys, Andreas Geiger"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2006.06769v2": {
            "Paper Title": "One Ring to Rule Them All: Certifiably Robust Geometric Perception with\n  Outliers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08948v1": {
            "Paper Title": "Multiple Future Prediction Leveraging Synthetic Trajectories",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.07715v2": {
            "Paper Title": "TEASER: Fast and Certifiable Point Cloud Registration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08696v1": {
            "Paper Title": "A Systematic Approach to Computing the Manipulator Jacobian and Hessian\n  using the Elementary Transform Sequence",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.04899v2": {
            "Paper Title": "Human-Supervised Semi-Autonomous Mobile Manipulators for Safely and\n  Efficiently Executing Machine Tending Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08595v1": {
            "Paper Title": "Flow-FL: Data-Driven Federated Learning for Spatio-Temporal Predictions\n  in Multi-Robot Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.11722v1": {
            "Paper Title": "Prediction-Based GNSS Spoofing Attack Detection for Autonomous Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08473v1": {
            "Paper Title": "SMAC: Symbiotic Multi-Agent Construction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08353v1": {
            "Paper Title": "On the Guaranteed Almost Equivalence between Imitation Learning from\n  Observation and Demonstration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.10823v2": {
            "Paper Title": "Ants, robots, humans: a self-organizing, complex systems modeling\n  approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08167v1": {
            "Paper Title": "Piecewise-Linear Motion Planning amidst Static, Moving, or Morphing\n  Obstacles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.04814v2": {
            "Paper Title": "All-Weather sub-50-cm Radar-Inertial Positioning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08022v1": {
            "Paper Title": "Fundamental Linear Algebra Problem of Gaussian Inference",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.05982v4": {
            "Paper Title": "LaserFlow: Efficient and Probabilistic Object Detection and Motion\n  Forecasting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.04394v5": {
            "Paper Title": "Planning and Execution using Inaccurate Models with Provable Guarantees",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.09942v3": {
            "Paper Title": "CMAX++ : Leveraging Experience in Planning and Execution using\n  Inaccurate Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.07820v1": {
            "Paper Title": "DynaSLAM II: Tightly-Coupled Multi-Object Tracking and SLAM",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.07133v2": {
            "Paper Title": "A Geometric Approach to On-road Motion Planning for Long and Multi-Body\n  Heavy-Duty Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.07476v1": {
            "Paper Title": "Locomotion Design for an Internally Actuated Cubic Robot for Exploration\n  of Low Gravity Bodies in the Solar System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.07467v1": {
            "Paper Title": "Human-guided Robot Behavior Learning: A GAN-assisted Preference-based\n  Reinforcement Learning Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.08082v2": {
            "Paper Title": "Distributed Reinforcement Learning of Targeted Grasping with Active\n  Vision for Mobile Manipulators",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.06491v1": {
            "Paper Title": "Broadly-Exploring, Local-Policy Trees for Long-Horizon Task Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.07378v2": {
            "Paper Title": "BOP Challenge 2020 on 6D Object Localization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.14711v2": {
            "Paper Title": "S3K: Self-Supervised Semantic Keypoints for Robotic Manipulation via\n  Multi-View Consistency",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.06117v1": {
            "Paper Title": "Map-Based Temporally Consistent Geolocalization through Learning Motion\n  Trajectories",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.06116v1": {
            "Paper Title": "Mass Estimation in Manipulation Tasks of Domestic Service Robots using\n  Fault Reconstruction Techniques",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.04837v2": {
            "Paper Title": "CurbScan: Curb Detection and Tracking Using Multi-Sensor Fusion",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.09762v3": {
            "Paper Title": "TTR-Based Reward for Reinforcement Learning with Implicit Model Priors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.05957v1": {
            "Paper Title": "Robots State Estimation and Observability Analysis Based on Statistical\n  Motion Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.10139v3": {
            "Paper Title": "Learning Dynamic-Objective Policies from a Class of Optimal Trajectories",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.05484v1": {
            "Paper Title": "Multiparty Motion Coordination: From Choreographies to Robotics Programs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.03423v3": {
            "Paper Title": "Pose Estimation for Ground Robots: On Manifold Representation,\n  Integration, Re-Parameterization, and Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.05247v1": {
            "Paper Title": "Telerobotic Operation of Intensive Care Unit Ventilators",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.05233v1": {
            "Paper Title": "An Energy-Efficient High Definition Map Data Distribution Mechanism for\n  Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.05201v1": {
            "Paper Title": "Autonomous Parking by Successive Convexification and Compound State\n  Triggers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.05190v1": {
            "Paper Title": "Learning Adaptive Language Interfaces through Decomposition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.05115v1": {
            "Paper Title": "Autonomous Vehicle Visual Signals for Pedestrians: Experiments and\n  Design Recommendations",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": ", (2) Simulation approach in which participants interact\nwith vehicle in a virtual immersive environment ",
                    "Citation Text": "S. K. Jayaraman, C. Creech, L. P. Robert Jr., D. M. Tilbury, X. J.\nYang, A. K. Pradhan, and K. M. Tsui, \u201cTrust in A V: An Uncertainty\nReduction Model of A V-Pedestrian Interactions,\u201d HRI, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.09996",
                        "Citation Paper Title": "Title:Analysis and Prediction of Pedestrian Crosswalk Behavior during Automated Vehicle Interactions",
                        "Citation Paper Abstract": "Abstract:For safe navigation around pedestrians, automated vehicles (AVs) need to plan their motion by accurately predicting pedestrians trajectories over long time horizons. Current approaches to AV motion planning around crosswalks predict only for short time horizons (1-2 s) and are based on data from pedestrian interactions with human-driven vehicles (HDVs). In this paper, we develop a hybrid systems model that uses pedestrians gap acceptance behavior and constant velocity dynamics for long-term pedestrian trajectory prediction when interacting with AVs. Results demonstrate the applicability of the model for long-term (> 5 s) pedestrian trajectory prediction at crosswalks. Further we compared measures of pedestrian crossing behaviors in the immersive virtual environment (when interacting with AVs) to that in the real world (results of published studies of pedestrians interacting with HDVs), and found similarities between the two. These similarities demonstrate the applicability of the hybrid model of AV interactions developed from an immersive virtual environment (IVE) for real-world scenarios for both AVs and HDVs.",
                        "Citation Paper Authors": "Authors:Suresh Kumaar Jayaraman, Dawn M. Tilbury, X. Jessie Yang, Anuj K. Pradhan, Lionel P. Robert Jr"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2004.09039v2": {
            "Paper Title": "X-Ray: Mechanical Search for an Occluded Object by Minimizing Support of\n  Learned Occupancy Distributions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.04979v1": {
            "Paper Title": "A Termination Criterion for Probabilistic PointClouds Registration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.04914v1": {
            "Paper Title": "Helpfulness as a Key Metric of Human-Robot Collaboration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.04849v1": {
            "Paper Title": "Modeling Human Temporal Uncertainty in Human-Agent Teams",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.02516v2": {
            "Paper Title": "Mapping of Sparse 3D Data using Alternating Projection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.04652v1": {
            "Paper Title": "Towards Social HRI for Improving Children's Healthcare Experiences",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.04602v1": {
            "Paper Title": "Integrating Intrinsic and Extrinsic Explainability: The Relevance of\n  Understanding Neural Networks for Human-Robot Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.04573v1": {
            "Paper Title": "Task-Space Control Interface for SoftBank Humanoid Robots and its\n  Human-Robot Interaction Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.04570v1": {
            "Paper Title": "Explainable Representations of the Social State: A Model for Social\n  Human-Robot Interactions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.04339v1": {
            "Paper Title": "MMGSD: Multi-Modal Gaussian Shape Descriptors for Correspondence\n  Matching in 1D and 2D Deformable Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.04203v1": {
            "Paper Title": "Efficient Real-Time Radial Distortion Correction for UAVs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.08861v4": {
            "Paper Title": "Encoding Physical Constraints in Differentiable Newton-Euler Algorithm",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.00202v2": {
            "Paper Title": "Heteroscedastic Bayesian Optimisation for Stochastic Model Predictive\n  Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.02291v2": {
            "Paper Title": "Rigid Body Dynamic Simulation with Line and Surface Contact",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.05994v2": {
            "Paper Title": "Adversarially Guided Self-Play for Adopting Social Conventions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.09207v2": {
            "Paper Title": "L2B: Learning to Balance the Safety-Efficiency Trade-off in Interactive\n  Crowd-aware Robot Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.01391v3": {
            "Paper Title": "An Infinite, Converging, Sequence of Brocard Porisms",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.03505v1": {
            "Paper Title": "Learning from demonstration using products of experts: applications to\n  manipulation and task prioritization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.03345v1": {
            "Paper Title": "Trajectory Planning for Automated Driving in Intersection Scenarios\n  using Driver Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.03209v1": {
            "Paper Title": "Learning Arbitrary-Goal Fabric Folding with One Hour of Real Robot\n  Experience",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.13561v4": {
            "Paper Title": "Imagine That! Leveraging Emergent Affordances for 3D Tool Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.08718v2": {
            "Paper Title": "Analytic Manifold Learning: Unifying and Evaluating Representations for\n  Continuous Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.04076v4": {
            "Paper Title": "FisheyeDistanceNet: Self-Supervised Scale-Aware Distance Estimation\n  using Monocular Fisheye Camera for Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.07022v1": {
            "Paper Title": "Towards a Policy-as-a-Service Framework to Enable Compliant, Trustworthy\n  AI and HRI Systems in the Wild",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.00728v2": {
            "Paper Title": "RMM: A Recursive Mental Model for Dialog Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.03174v1": {
            "Paper Title": "Dynamic Simulation-Guided Design of Tumbling Magnetic Microrobots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.02293v1": {
            "Paper Title": "Using Soft Actor-Critic for Low-Level UAV Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.03170v1": {
            "Paper Title": "Modeling and Prediction of Rigid Body Motion with Planar Non-Convex\n  Contact",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.02137v1": {
            "Paper Title": "Blockchain for Multi-Robot Collaboration to Combat COVID-19 and Future\n  Pandemics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.01977v1": {
            "Paper Title": "DLT federation for Edge robotics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.00969v3": {
            "Paper Title": "Learning to Scaffold the Development of Robotic Manipulation Skills",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": "combine reinforcement learning with a motion planner\nto shape the state cost in a high-precision setting. Luo et al. ",
                    "Citation Text": "J. Luo, E. Solowjow, C. Wen, J. A. Ojea, A. M. Agogino,\nA. Tamar, and P. Abbeel, \u201cReinforcement learning on variable\nimpedance controller for high-precision robotic assembly,\u201d\narXiv preprint arXiv:1903.01066 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.01066",
                        "Citation Paper Title": "Title:Reinforcement Learning on Variable Impedance Controller for High-Precision Robotic Assembly",
                        "Citation Paper Abstract": "Abstract:Precise robotic manipulation skills are desirable in many industrial settings, reinforcement learning (RL) methods hold the promise of acquiring these skills autonomously. In this paper, we explicitly consider incorporating operational space force/torque information into reinforcement learning; this is motivated by humans heuristically mapping perceived forces to control actions, which results in completing high-precision tasks in a fairly easy manner. Our approach combines RL with force/torque information by incorporating a proper operational space force controller; where we also exploit different ablations on processing this information. Moreover, we propose a neural network architecture that generalizes to reasonable variations of the environment. We evaluate our method on the open-source Siemens Robot Learning Challenge, which requires precise and delicate force-controlled behavior to assemble a tight-fit gear wheel set.",
                        "Citation Paper Authors": "Authors:Jianlan Luo, Eugen Solowjow, Chengtao Wen, Juan Aparicio Ojea, Alice M. Agogino, Aviv Tamar, Pieter Abbeel"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2010.01298v1": {
            "Paper Title": "Beyond Tabula-Rasa: a Modular Reinforcement Learning Approach for\n  Physically Embedded 3D Sokoban",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": "on solving Rubik\u2019s cubes\nwith a \ufb01xed abstract planner and a learned RL controller; and Chaplot et al. ",
                    "Citation Text": "D. S. Chaplot, D. Gandhi, S. Gupta, A. Gupta, and R. Salakhutdinov. Learning to explore using active\nneural slam. In International Conference on Learning Representations , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.05155",
                        "Citation Paper Title": "Title:Learning to Explore using Active Neural SLAM",
                        "Citation Paper Abstract": "Abstract:This work presents a modular and hierarchical approach to learn policies for exploring 3D environments, called `Active Neural SLAM'. Our approach leverages the strengths of both classical and learning-based methods, by using analytical path planners with learned SLAM module, and global and local policies. The use of learning provides flexibility with respect to input modalities (in the SLAM module), leverages structural regularities of the world (in global policies), and provides robustness to errors in state estimation (in local policies). Such use of learning within each module retains its benefits, while at the same time, hierarchical decomposition and modular training allow us to sidestep the high sample complexities associated with training end-to-end policies. Our experiments in visually and physically realistic simulated 3D environments demonstrate the effectiveness of our approach over past learning and geometry-based approaches. The proposed model can also be easily transferred to the PointGoal task and was the winning entry of the CVPR 2019 Habitat PointGoal Navigation Challenge.",
                        "Citation Paper Authors": "Authors:Devendra Singh Chaplot, Dhiraj Gandhi, Saurabh Gupta, Abhinav Gupta, Ruslan Salakhutdinov"
                    },
                    "Keywords": [
                        ",",
                        "learning",
                        "planning",
                        "partial",
                        "observability",
                        "Hierarchical",
                        "reinforcement"
                    ]
                }
            ]
        },
        "http://arxiv.org/abs/2010.01164v1": {
            "Paper Title": "Manipulation of Articulated Objects using Dual-arm Robots via Answer Set\n  Programming",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.01083v1": {
            "Paper Title": "Integrated Task and Motion Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.00327v2": {
            "Paper Title": "Using monodromy to statistically estimate the number of solutions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.00741v1": {
            "Paper Title": "Smart-Inspect: Micro Scale Localization and Classification of Smartphone\n  Glass Defects for Industrial Automation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.09587v3": {
            "Paper Title": "A Unified Access Control Model for Calibration Traceability in\n  Safety-Critical IoT",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.14501v2": {
            "Paper Title": "Multi-Pen Robust Robotic 3D Drawing Using Closed-Loop Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.13151v3": {
            "Paper Title": "Modeling and Predicting Trust Dynamics in Human-Robot Teaming: A\n  Bayesian Inference Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.00156v1": {
            "Paper Title": "GeoD: Consensus-based Geodesic Distributed Pose Graph Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.07950v2": {
            "Paper Title": "Learning visual policies for building 3D shape categories",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.06404v3": {
            "Paper Title": "Learning to Generate 6-DoF Grasp Poses with Reachability Awareness",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.14628v1": {
            "Paper Title": "Meta Partial Benders Decomposition for the Logistics Service Network\n  Design Problem",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.14363v1": {
            "Paper Title": "Co-design of Control and Planning for Multi-rotor UAVs with Signal\n  Temporal Logic Specifications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.14089v1": {
            "Paper Title": "Four-Arm Collaboration: Two Dual-Arm Robots Work Together to Maneuver\n  Tethered Tools",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.13817v1": {
            "Paper Title": "Parameter Identification for Multirobot Systems Using Optimization Based\n  Controllers (Extended Version)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.13732v1": {
            "Paper Title": "Learning Skills to Patch Plans Based on Inaccurate Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.14428v1": {
            "Paper Title": "A General Framework for Charger Scheduling Optimization Problems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.04067v2": {
            "Paper Title": "Active Preference Learning using Maximum Regret",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.13475v1": {
            "Paper Title": "Enhancing Continuous Control of Mobile Robots for End-to-End Visual\n  Active Tracking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.14987v4": {
            "Paper Title": "Presentation and Analysis of a Multimodal Dataset for Grounded Language\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.12887v5": {
            "Paper Title": "End-to-end Recurrent Multi-Object Tracking and Trajectory Prediction\n  with Relational Reasoning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.13146v1": {
            "Paper Title": "Amodal 3D Reconstruction for Robotic Manipulation via Stability and\n  Connectivity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.13066v1": {
            "Paper Title": "Vision based Target Interception using Aerial Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.12916v1": {
            "Paper Title": "Interaction-Based Trajectory Prediction Over a Hybrid Traffic Graph",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.12864v1": {
            "Paper Title": "Predicting Sim-to-Real Transfer with Probabilistic Dynamics Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.10886v2": {
            "Paper Title": "Stability-Guaranteed Reinforcement Learning for Contact-rich\n  Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.12796v1": {
            "Paper Title": "Agile Reactive Navigation for A Non-Holonomic Mobile Robot Using A Pixel\n  Processor Array",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.03779v2": {
            "Paper Title": "Investigating the Importance of Shape Features, Color Constancy, Color\n  Spaces and Similarity Measures in Open-Ended 3D Object Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.03584v2": {
            "Paper Title": "Multi-Agent Collaboration for Building Construction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.11937v1": {
            "Paper Title": "daVinciNet: Joint Prediction of Motion and Surgical State in\n  Robot-Assisted Surgery",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.11905v1": {
            "Paper Title": "A New Approach for Tactical Decision Making in Lane Changing: Sample\n  Efficient Deep Q Learning with a Safety Feedback Reward",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.11888v1": {
            "Paper Title": "Virtual Forward Dynamics Models for Cartesian Robot Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.11852v1": {
            "Paper Title": "Learning Equality Constraints for Motion Planning on Manifolds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.11219v1": {
            "Paper Title": "Dual-SLAM: A framework for robust single camera navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.11135v1": {
            "Paper Title": "DL-IAPS and PJSO: A Path/Speed Decoupled Trajectory Optimization and its\n  Application in Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.02108v2": {
            "Paper Title": "Self-Supervised Localisation between Range Sensors and Overhead Imagery",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.09370v2": {
            "Paper Title": "Comparing Feedback Linearization and Adaptive Backstepping Control for\n  Airborne Orientation of Agile Ground Robots using Wheel Reaction Torque",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.01825v2": {
            "Paper Title": "Robust Reinforcement Learning using Adversarial Populations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.10825v1": {
            "Paper Title": "Angular Luminance for Material Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.05273v2": {
            "Paper Title": "Safe Multi-Agent Interaction through Robust Control Barrier Functions\n  with Learned Uncertainties",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.04127v2": {
            "Paper Title": "Single Image Super-Resolution for Domain-Specific Ultra-Low Bandwidth\n  Image Transmission",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.10191v1": {
            "Paper Title": "Adaptive Meta-Learning for Identification of Rover-Terrain Dynamics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.13143v2": {
            "Paper Title": "Euclideanizing Flows: Diffeomorphic Reduction for Learning Stable\n  Dynamical Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.01797v2": {
            "Paper Title": "Object-Independent Human-to-Robot Handovers using Real Time Robotic\n  Vision",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.08577v2": {
            "Paper Title": "Making Sense of the Robotized Pandemic Response: A Comparison of Global\n  and Canadian Robot Deployments and Success Factors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.09595v1": {
            "Paper Title": "RL STaR Platform: Reinforcement Learning for Simulation based Training\n  of Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.09467v1": {
            "Paper Title": "Addressing reward bias in Adversarial Imitation Learning with neutral\n  reward functions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.09308v1": {
            "Paper Title": "What is the Best Grid-Map for Self-Driving Cars Localization? An\n  Evaluation under Diverse Types of Illumination, Traffic, and Environment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.09234v1": {
            "Paper Title": "Shimon the Rapper: A Real-Time System for Human-Robot Interactive Rap\n  Battles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.09151v1": {
            "Paper Title": "Design and Development of a Gecko-Adhesive Gripper for the Astrobee\n  Free-Flying Robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.09149v1": {
            "Paper Title": "Co-Evolution of Multi-Robot Controllers and Task Cues for Off-World Open\n  Pit Mining",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.05735v2": {
            "Paper Title": "Human-Robot Collaboration in Microgravity: the Object Handover Problem",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.04220v3": {
            "Paper Title": "Robust, Perception Based Control with Quadrotors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.08746v1": {
            "Paper Title": "Moving object detection for visual odometry in a dynamic environment\n  based on occlusion accumulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.08724v1": {
            "Paper Title": "Pose Correction Algorithm for Relative Frames between Keyframes in SLAM",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.08618v1": {
            "Paper Title": "6-DoF Grasp Planning using Fast 3D Reconstruction and Grasp Quality CNN",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.09857v1": {
            "Paper Title": "Heterogeneous Fixed-wing Aerial Vehicles for Resilient Coverage of an\n  Area",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.08422v1": {
            "Paper Title": "Elastica: A compliant mechanics environment for soft robotic control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.08292v1": {
            "Paper Title": "Learning to Identify Physical Parameters from Video Using Differentiable\n  Physics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.08211v1": {
            "Paper Title": "Can ROS be used securely in industry? Red teaming ROS-Industrial",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.08140v1": {
            "Paper Title": "POMP: Pomcp-based Online Motion Planning for active visual search in\n  indoor environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.07879v1": {
            "Paper Title": "Using Sensory Time-cue to enable Unsupervised Multimodal Meta-learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.07647v1": {
            "Paper Title": "Related by Similarity II: Poncelet 3-Periodics in the Homothetic Pair\n  and the Brocard Porism",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.07565v1": {
            "Paper Title": "Domain Adaptation for Outdoor Robot Traversability Estimation from RGB\n  data with Safety-Preserving Loss",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.09172v2": {
            "Paper Title": "Robot Sound Interpretation: Combining Sight and Sound in Learning-Based\n  Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.07404v1": {
            "Paper Title": "Cell A* for Navigation of Unmanned Aerial Vehicles in Partially-known\n  Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.07083v1": {
            "Paper Title": "Event-Driven Visual-Tactile Sensing and Learning for Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.02927v3": {
            "Paper Title": "Smart Cloud: Scalable Cloud Robotic Architecture for Web-powered\n  Multi-Robot Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.06886v1": {
            "Paper Title": "Attention-SLAM: A Visual Monocular SLAM Learning from Human Gaze",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.06801v1": {
            "Paper Title": "Approximate Piecewise Constant Curvature Equivalent Model and Their\n  Application to Continuum Robot Configuration Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.09398v2": {
            "Paper Title": "Deep Constrained Q-learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.06436v1": {
            "Paper Title": "Automatic Trajectory Synthesis for Real-Time Temporal Logic",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.06423v1": {
            "Paper Title": "A Task Allocation Approach for Human-Robot Collaboration in Product\n  Defects Inspection Scenarios",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.06292v1": {
            "Paper Title": "A Multisensory Learning Architecture for Rotation-invariant Object\n  Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.06021v1": {
            "Paper Title": "Rumor-robust Decentralized Gaussian Process Learning, Fusion, and\n  Planning for Modeling Multiple Moving Targets",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.11109v3": {
            "Paper Title": "Attention-Guided Lightweight Network for Real-Time Segmentation of\n  Robotic Surgical Instruments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.08807v1": {
            "Paper Title": "Monte Carlo Tree Search Based Tactical Maneuvering",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.05770v2": {
            "Paper Title": "Recognizing Object Affordances to Support Scene Reasoning for\n  Manipulation Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.05702v1": {
            "Paper Title": "Risk-Sensitive Sequential Action Control with Multi-Modal Human\n  Trajectory Forecasting for Safe Crowd-Robot Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.05420v2": {
            "Paper Title": "Mobile Robot Path Planning in Dynamic Environments through Globally\n  Guided Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.02518v2": {
            "Paper Title": "Autonomous Vehicle Benchmarking using Unbiased Metrics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.05246v1": {
            "Paper Title": "The Robotic Vision Scene Understanding Challenge",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.05176v1": {
            "Paper Title": "A First Step Towards Distribution Invariant Regression Metrics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.05085v1": {
            "Paper Title": "Keypoints into the Future: Self-Supervised Correspondence in Model-Based\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.04625v1": {
            "Paper Title": "Path planning model of mobile robots in the context of crowds",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.04105v3": {
            "Paper Title": "Rapid Path Planning for Dubins Vehicles under Environmental Currents",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.04298v1": {
            "Paper Title": "Vision-Based Autonomous Drone Control using Supervised Learning in\n  Simulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.05560v2": {
            "Paper Title": "Toward Hierarchical Self-Supervised Monocular Absolute Depth Estimation\n  for Autonomous Driving Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.03994v1": {
            "Paper Title": "Long-Horizon Prediction and Uncertainty Propagation with Residual Point\n  Contact Learners",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.04920v2": {
            "Paper Title": "GPU Parallelization of Policy Iteration RRT#",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.03829v1": {
            "Paper Title": "A Robotic Positive Psychology Coach to Improve College Students'\n  Wellbeing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.11906v3": {
            "Paper Title": "Jerk Control of Floating Base Systems with Contact-Stable Parametrised\n  Force Feedback",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.03705v1": {
            "Paper Title": "Comparison of camera-based and 3D LiDAR-based loop closures across\n  weather conditions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.03011v1": {
            "Paper Title": "Critical Business Decision Making for Technology Startups -- A PerceptIn\n  Case Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.02846v1": {
            "Paper Title": "Animated Cassie: A Dynamic Relatable Robotic Character",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.12174v1": {
            "Paper Title": "Goal-Directed Occupancy Prediction for Lane-Following Actors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.07965v2": {
            "Paper Title": "Analysis of Social Robotic Navigation approaches: CNN Encoder and\n  Incremental Learning as an alternative to Deep Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.02041v1": {
            "Paper Title": "AIR-Act2Act: Human-human interaction dataset for teaching non-verbal\n  social behaviors to robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.04339v2": {
            "Paper Title": "Collaborative Behavior Models for Optimized Human-Robot Teamwork",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.01921v1": {
            "Paper Title": "A Visual Analytics Approach to Debugging Cooperative, Autonomous\n  Multi-Robot Systems' Worldviews",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.01875v1": {
            "Paper Title": "Depth Completion via Inductive Fusion of Planar LIDAR and Monocular\n  Camera",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.13374v2": {
            "Paper Title": "Multi-mode Trajectory Optimization for Impact-aware Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.01565v1": {
            "Paper Title": "Detection-Aware Trajectory Generation for a Drone Cinematographer",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": "for the output video footage taken from\nthe drone.\nA. Experimental setup\nWe performed high-\ufb01delity simulations in Unreal engine\nwith Airsim plugin ",
                    "Citation Text": "S. Shah, D. Dey, C. Lovett, and A. Kapoor, \u201cAirsim: High-\ufb01delity\nvisual and physical simulation for autonomous vehicles,\u201d in Field and\nservice robotics , pp. 621\u2013635, Springer, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.05065",
                        "Citation Paper Title": "Title:AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles",
                        "Citation Paper Abstract": "Abstract:Developing and testing algorithms for autonomous vehicles in real world is an expensive and time consuming process. Also, in order to utilize recent advances in machine intelligence and deep learning we need to collect a large amount of annotated training data in a variety of conditions and environments. We present a new simulator built on Unreal Engine that offers physically and visually realistic simulations for both of these goals. Our simulator includes a physics engine that can operate at a high frequency for real-time hardware-in-the-loop (HITL) simulations with support for popular protocols (e.g. MavLink). The simulator is designed from the ground up to be extensible to accommodate new types of vehicles, hardware platforms and software protocols. In addition, the modular design enables various components to be easily usable independently in other projects. We demonstrate the simulator by first implementing a quadrotor as an autonomous vehicle and then experimentally comparing the software components with real-world flights.",
                        "Citation Paper Authors": "Authors:Shital Shah, Debadeepta Dey, Chris Lovett, Ashish Kapoor"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2003.10010v2": {
            "Paper Title": "One-Shot Informed Robotic Visual Search in the Wild",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.11675v2": {
            "Paper Title": "Risk-Aware Planning and Assignment for Ground Vehicles using Uncertain\n  Perception from Aerial Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.02545v3": {
            "Paper Title": "Trajectory Prediction for Autonomous Driving based on Multi-Head\n  Attention with Joint Agent-Map Representation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.00182v3": {
            "Paper Title": "Collaborative target-tracking control using multiple autonomous\n  fixed-wing UAVs with constant speeds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.01284v1": {
            "Paper Title": "Proposed Efficient Design for Unmanned Surface Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.14911v2": {
            "Paper Title": "Can Autonomous Vehicles Identify, Recover From, and Adapt to\n  Distribution Shifts?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.00862v1": {
            "Paper Title": "Efficient Multi-Robot Exploration with Energy Constraint based on\n  Optimal Transport Theory",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.05147v1": {
            "Paper Title": "Practical Cross-modal Manifold Alignment for Grounded Language",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.10663v2": {
            "Paper Title": "Reconfigurable Behavior Trees: Towards an Executive Framework Meeting\n  High-level Decision Making and Control Layer Features",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.01709v2": {
            "Paper Title": "Hierarchically Decoupled Imitation for Morphological Transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.02160v2": {
            "Paper Title": "Physics Enhanced Data-Driven Models with Variational Gaussian Processes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.13221v1": {
            "Paper Title": "Human-in-the-Loop Methods for Data-Driven and Reinforcement Learning\n  Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.00764v2": {
            "Paper Title": "Single-Shot Panoptic Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.00528v2": {
            "Paper Title": "Towards Robust Visual Tracking for Unmanned Aerial Vehicle with\n  Tri-Attentional Correlation Filters",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.13171v2": {
            "Paper Title": "ObjectNav Revisited: On Evaluation of Embodied Agents Navigating to\n  Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.04323v2": {
            "Paper Title": "ALLSTEPS: Curriculum-driven Learning of Stepping Stone Skills",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.10227v2": {
            "Paper Title": "Nengo and low-power AI hardware for robust, embedded neurorobotics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.08362v2": {
            "Paper Title": "A Receding Horizon Multi-Objective Planner for Autonomous Surface\n  Vehicles in Urban Waterways",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.01179v3": {
            "Paper Title": "PillarFlow: End-to-end Birds-eye-view Flow Estimation for Autonomous\n  Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.12818v1": {
            "Paper Title": "ChildBot: Multi-Robot Perception and Interaction with Children",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.12760v1": {
            "Paper Title": "AllenAct: A Framework for Embodied AI Research",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.12725v1": {
            "Paper Title": "iviz: A ROS Visualization App for Mobile Devices",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.12639v1": {
            "Paper Title": "Path Planning for Shepherding a Swarm in a Cluttered Environment using\n  Differential Evolution",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.12610v1": {
            "Paper Title": "Collaborative Multi-Robot Systems for Search and Rescue: Coordination\n  and Perception",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.06422v3": {
            "Paper Title": "Vid2Param: Modelling of Dynamics Parameters from Video",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.12284v2": {
            "Paper Title": "learn2learn: A Library for Meta-Learning Research",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.12451v1": {
            "Paper Title": "Meta Reinforcement Learning-Based Lane Change Strategy for Autonomous\n  Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.12449v1": {
            "Paper Title": "Long-term map maintenance pipeline for autonomous vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.11911v1": {
            "Paper Title": "Domain Adaptation Through Task Distillation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.11543v4": {
            "Paper Title": "Continuous Deep Hierarchical Reinforcement Learning for Ground-Air Swarm\n  Shepherding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.11833v1": {
            "Paper Title": "Deep learning-based computer vision to recognize and classify suturing\n  gestures in robot-assisted surgery",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.11609v1": {
            "Paper Title": "Identification of Challenging Highway-Scenarios for the Safety\n  Validation of Automated Vehicles Based on Real Driving Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.11580v1": {
            "Paper Title": "Automatic Generation of Road Geometries to Create Challenging Scenarios\n  for Automated Vehicles Based on the Sensor Setup",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.11466v1": {
            "Paper Title": "Self-Supervised Goal-Conditioned Pick and Place",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.04225v5": {
            "Paper Title": "PAC-Bayes Control: Learning Policies that Provably Generalize to Novel\n  Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.11598v1": {
            "Paper Title": "End-to-End 3D Multi-Object Tracking and Trajectory Forecasting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.10816v1": {
            "Paper Title": "A Robotic Line Scan System with Adaptive ROI for Inspection of Defects\n  over Convex Free-form Specular Surfaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.10759v1": {
            "Paper Title": "Visualization of Intended Assistance for Acceptance of Shared Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.10638v1": {
            "Paper Title": "Tool Macgyvering: A Novel Framework for Combining Tool Substitution and\n  Construction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.10123v1": {
            "Paper Title": "Good Graph to Optimize: Cost-Effective, Budget-Aware Bundle Adjustment\n  in Visual SLAM",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.10084v1": {
            "Paper Title": "DeComplex: Task planning from complex natural instructions by a\n  collocating robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.10078v1": {
            "Paper Title": "Let me join you! Real-time F-formation recognition by a socially aware\n  robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.12098v3": {
            "Paper Title": "Quaternion Equivariant Capsule Networks for 3D Point Clouds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.10098v2": {
            "Paper Title": "An RLS-Based Instantaneous Velocity Estimator for Extended Radar\n  Tracking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.03900v2": {
            "Paper Title": "FormulaZero: Distributionally Robust Online Adaptation via Offline\n  Population Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.02399v2": {
            "Paper Title": "Optimization Fabrics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.09707v1": {
            "Paper Title": "SOTER on ROS: A Run-Time Assurance Framework on the Robot Operating\n  System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.09672v1": {
            "Paper Title": "Towards Autonomous Driving: a Multi-Modal 360$^{\\circ}$ Perception\n  Proposal",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.09644v1": {
            "Paper Title": "Blending of Learning-based Tracking and Object Detection for Monocular\n  Camera-based Target Following",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.06585v2": {
            "Paper Title": "COVID-Robot: Monitoring Social Distancing Constraints in Crowded\n  Scenarios",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.13781v4": {
            "Paper Title": "A Maneuver-based Urban Driving Dataset and Model for Cooperative Vehicle\n  Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.01087v2": {
            "Paper Title": "Line Walking and Balancing for Legged Robots with Point Feet",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.09427v1": {
            "Paper Title": "Combining Control Barrier Functions and Behavior Trees for Multi-Agent\n  Underwater Coverage Missions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.09876v3": {
            "Paper Title": "Underwater Caging and Capture for Autonomous Underwater Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.09377v1": {
            "Paper Title": "Curriculum Learning with Hindsight Experience Replay for Sequential\n  Object Manipulation Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.08294v2": {
            "Paper Title": "TNT: Target-driveN Trajectory Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.08812v2": {
            "Paper Title": "Expressing Diverse Human Driving Behavior with Probabilistic Rewards and\n  Online Inference",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.09506v1": {
            "Paper Title": "Graph Neural Networks for 3D Multi-Object Tracking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.09050v1": {
            "Paper Title": "Markov Chain-Based Stochastic Strategies for Robotic Surveillance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.08893v1": {
            "Paper Title": "Switching Model Predictive Control for Online Structural Reformations of\n  a Foldable Quadrotor",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.08889v1": {
            "Paper Title": "Autonomous Social Distancing in Urban Environments using a Quadruped\n  Robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.08817v1": {
            "Paper Title": "Grasping Detection Network with Uncertainty Estimation for\n  Confidence-Driven Semi-Supervised Domain Adaptation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.12624v1": {
            "Paper Title": "A Framework for Studying Reinforcement Learning and Sim-to-Real in Robot\n  Soccer",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06491v2": {
            "Paper Title": "Selfie Drone Stick: A Natural Interface for Quadcopter Photography",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.08173v1": {
            "Paper Title": "Uncertainty-aware Self-supervised 3D Data Association",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.07331v2": {
            "Paper Title": "Interactive Visualization for Debugging RL",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.08063v1": {
            "Paper Title": "AB3DMOT: A Baseline for 3D Multi-Object Tracking and New Evaluation\n  Metrics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.02075v3": {
            "Paper Title": "Toward Sim-to-Real Directional Semantic Grasping",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.07863v1": {
            "Paper Title": "Ubiquitous Distributed Deep Reinforcement Learning at the Edge:\n  Analyzing Byzantine Agents in Discrete Action Spaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.12015v2": {
            "Paper Title": "Towards Better Performance and More Explainable Uncertainty for 3D\n  Object Detection of Autonomous Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.07615v1": {
            "Paper Title": "PufferBot: Actuated Expandable Structures for Aerial Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.07436v1": {
            "Paper Title": "Multi-Agent Coverage in Urban Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.07203v1": {
            "Paper Title": "Category-Level 3D Non-Rigid Registration from Single-View RGB Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.07196v1": {
            "Paper Title": "LIC-Fusion 2.0: LiDAR-Inertial-Camera Odometry with Sliding-Window\n  Plane-Feature Tracking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.03420v3": {
            "Paper Title": "Curious Hierarchical Actor-Critic Reinforcement Learning",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "Kingma, D.P., Ba, J.L.: Adam: A Method for Stochastic Optimization. In:\nInternational Conference on Learning Representations (ICLR). p. online\n(2015) ",
                    "Citation Text": "Kulkarni, T.D., Narasimhan, K., Saeedi, A., Tenenbaum, J.B.: Hierarchical\nDeep Reinforcement Learning: Integrating Temporal Abstraction and In-\ntrinsic Motivation. Conference on Neural Information Processing Systems\n(NeurIPS) pp. 3675{3683 (2016),",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1604.06057",
                        "Citation Paper Title": "Title:Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation",
                        "Citation Paper Abstract": "Abstract:Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning. A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete stochastic decision process, and (2) the classic ATARI game `Montezuma's Revenge'.",
                        "Citation Paper Authors": "Authors:Tejas D. Kulkarni, Karthik R. Narasimhan, Ardavan Saeedi, Joshua B. Tenenbaum"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2008.05699v3": {
            "Paper Title": "A Vision-Based Control Method for Autonomous Landing of Vertical Flight\n  Aircraft On a Moving Platform Without Using GPS",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.06972v1": {
            "Paper Title": "Real-Time Spatio-Temporal LiDAR Point Cloud Compression",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.06917v2": {
            "Paper Title": "End-to-End Velocity Estimation For Autonomous Racing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.06904v1": {
            "Paper Title": "A Biomimetic Tactile Fingerprint Induces Incipient Slip",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.06899v1": {
            "Paper Title": "Modification of Gesture-Determined-Dynamic Function with Consideration\n  of Margins for Motion Planning of Humanoid Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.07872v2": {
            "Paper Title": "Gentlemen on the Road: Understanding How Pedestrians Interpret Yielding\n  Behavior of Autonomous Vehicles using Machine Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.06696v1": {
            "Paper Title": "Autonomous Braking and Throttle System: A Deep Reinforcement Learning\n  Approach for Naturalistic Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.06686v1": {
            "Paper Title": "Crossing The Gap: A Deep Dive into Zero-Shot Sim-to-Real Transfer for\n  Dynamics",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": ". Our work focusses on the zero-shot\nregime, and Yu et al. ",
                    "Citation Text": "W. Yu, J. Tan, C. K. Liu, and G. Turk. Preparing for the unknown:\nLearning a universal policy with online system identi\ufb01cation. In\nN. M. Amato, S. S. Srinivasa, N. Ayanian, and S. Kuindersma,\neditors, Robotics: Science and Systems XIII, Massachusetts Institute of\nTechnology, Cambridge, Massachusetts, USA, July 12-16, 2017 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.02453",
                        "Citation Paper Title": "Title:Preparing for the Unknown: Learning a Universal Policy with Online System Identification",
                        "Citation Paper Abstract": "Abstract:We present a new method of learning control policies that successfully operate under unknown dynamic models. We create such policies by leveraging a large number of training examples that are generated using a physical simulator. Our system is made of two components: a Universal Policy (UP) and a function for Online System Identification (OSI). We describe our control policy as universal because it is trained over a wide array of dynamic models. These variations in the dynamic model may include differences in mass and inertia of the robots' components, variable friction coefficients, or unknown mass of an object to be manipulated. By training the Universal Policy with this variation, the control policy is prepared for a wider array of possible conditions when executed in an unknown environment. The second part of our system uses the recent state and action history of the system to predict the dynamics model parameters mu. The value of mu from the Online System Identification is then provided as input to the control policy (along with the system state). Together, UP-OSI is a robust control policy that can be used across a wide range of dynamic models, and that is also responsive to sudden changes in the environment. We have evaluated the performance of this system on a variety of tasks, including the problem of cart-pole swing-up, the double inverted pendulum, locomotion of a hopper, and block-throwing of a manipulator. UP-OSI is effective at these tasks across a wide range of dynamic models. Moreover, when tested with dynamic models outside of the training range, UP-OSI outperforms the Universal Policy alone, even when UP is given the actual value of the model dynamics. In addition to the benefits of creating more robust controllers, UP-OSI also holds out promise of narrowing the Reality Gap between simulated and real physical systems.",
                        "Citation Paper Authors": "Authors:Wenhao Yu, Jie Tan, C. Karen Liu, Greg Turk"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2008.06630v1": {
            "Paper Title": "Neural Ray Surfaces for Self-Supervised Learning of Depth and Ego-motion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.06626v1": {
            "Paper Title": "Safe Reinforcement Learning in Constrained Markov Decision Processes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.12830v4": {
            "Paper Title": "The Differentiable Cross-Entropy Method",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.05096v2": {
            "Paper Title": "Multi-Agent Routing Value Iteration Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.06389v1": {
            "Paper Title": "Sample-efficient Cross-Entropy Method for Real-time Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.07535v2": {
            "Paper Title": "Tactile Model O: Fabrication and testing of a 3d-printed, three-fingered\n  tactile robot hand",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.06189v1": {
            "Paper Title": "An Improved Deep Convolutional Neural Network-Based Autonomous Road\n  Inspection Scheme Using Unmanned Aerial Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.06020v1": {
            "Paper Title": "Testing the Safety of Self-driving Vehicles by Simulating Perception and\n  Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.05930v1": {
            "Paper Title": "Perceive, Predict, and Plan: Safe Motion Planning Through Interpretable\n  Semantic Representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.10318v3": {
            "Paper Title": "Drift with Devil: Security of Multi-Sensor Fusion based Localization in\n  High-Level Autonomous Driving under GPS Spoofing (Extended Version)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.05416v1": {
            "Paper Title": "DXSLAM: A Robust and Efficient Visual SLAM System with Deep Features",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.04809v2": {
            "Paper Title": "Extension of Full and Reduced Order Observers for Image-based Depth\n  Estimation using Concurrent Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.05309v1": {
            "Paper Title": "Factor Graph based 3D Multi-Object Tracking in Point Clouds",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.05808v2": {
            "Paper Title": "Learning to Live Life on the Edge: Online Learning for Data-Efficient\n  Tactile Contour Following",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.05240v1": {
            "Paper Title": "Walking on TacTip toes: A tactile sensing foot for walking robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.05112v1": {
            "Paper Title": "Dynamically Constrained Motion Planning Networks for Non-Holonomic\n  Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.05032v1": {
            "Paper Title": "Kinematic Modeling and Compliance Modulation of Redundant Manipulators\n  Under Bracing Constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.04899v1": {
            "Paper Title": "Visual Imitation Made Easy",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.00682v2": {
            "Paper Title": "Accelerated Robot Learning via Human Brain Signals",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.08102v2": {
            "Paper Title": "Hybrid Systems Differential Dynamic Programming for Whole-Body Motion\n  Planning of Legged Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.04627v1": {
            "Paper Title": "A Comparison of Humanoid Robot Simulators: A Quantitative Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.04619v1": {
            "Paper Title": "Deep UAV Localization with Reference View Rendering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.04589v1": {
            "Paper Title": "Model-Based Quality-Diversity Search for Efficient Robot Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.04465v1": {
            "Paper Title": "Safe and Effective Picking Paths in Clutter given Discrete Distributions\n  of Object Poses",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.07170v2": {
            "Paper Title": "Goal-Aware Prediction: Learning to Model What Matters",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.04442v1": {
            "Paper Title": "Spatio-temporal Attention Model for Tactile Texture Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.01267v2": {
            "Paper Title": "Formal composition of hybrid systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.04197v1": {
            "Paper Title": "Deep Learning-based Human Detection for UAVs with Optical and Infrared\n  Cameras: System and Experiments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.04047v1": {
            "Paper Title": "Driving among Flatmobiles: Bird-Eye-View occupancy grids from a\n  monocular camera for holistic trajectory planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.04007v1": {
            "Paper Title": "Imitation Learning for Autonomous Trajectory Learning of Robot Arms in\n  Space",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.03915v1": {
            "Paper Title": "Automatic Failure Recovery and Re-Initialization for Online UAV Tracking\n  with Joint Scale and Aspect Ratio Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.03912v1": {
            "Paper Title": "DR^2Track: Towards Real-Time Visual Tracking for UAV via Distractor\n  Repressed Dynamic Regression",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.03826v1": {
            "Paper Title": "Contact-Rich Trajectory Generation in Confined Environments Using\n  Iterative Convex Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.03801v1": {
            "Paper Title": "Can I lift it? Humanoid robot reasoning about the feasibility of lifting\n  a heavy box with unknown physical properties",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.03694v1": {
            "Paper Title": "LiDAR Data Enrichment Using Deep Learning Based on High-Resolution\n  Image: An Approach to Achieve High-Performance LiDAR SLAM Using Low-cost\n  LiDAR",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.03573v1": {
            "Paper Title": "Explanation Generation for Multi-Modal Multi-Agent Path Finding with\n  Optimal Resource Utilization using Answer Set Programming",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.06495v2": {
            "Paper Title": "Human Preference-Based Learning for High-dimensional Optimization of\n  Exoskeleton Walking Gaits",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.03525v1": {
            "Paper Title": "Non-Adversarial Imitation Learning and its Connections to Adversarial\n  Methods",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.03496v1": {
            "Paper Title": "Human Robot Collaborative Assembly Planning: An Answer Set Programming\n  Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.03324v1": {
            "Paper Title": "Fisher Information Field: an Efficient and Differentiable Map for\n  Perception-aware Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.03285v1": {
            "Paper Title": "Physics-Based Dexterous Manipulations with Estimated Hand Poses and\n  Residual Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.07876v3": {
            "Paper Title": "Learning to Manipulate Object Collections Using Grounded State\n  Representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.02881v1": {
            "Paper Title": "Parts-Based Articulated Object Localization in Clutter Using Belief\n  Propagation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.13886v3": {
            "Paper Title": "TITAN: Future Forecast using Action Priors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.02840v1": {
            "Paper Title": "Assisted Perception: Optimizing Observations to Communicate State",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.03330v3": {
            "Paper Title": "EU Long-term Dataset with Multiple Sensors for Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.01110v2": {
            "Paper Title": "Encoding formulas as deep networks: Reinforcement learning for zero-shot\n  execution of LTL formulas",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.06680v2": {
            "Paper Title": "Momentum-Based Policy Gradient Methods",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.02646v1": {
            "Paper Title": "Deep Reinforcement Learning for Tactile Robotics: Learning to Type on a\n  Braille Keyboard",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.02596v1": {
            "Paper Title": "Image Generation for Efficient Neural Network Training in Autonomous\n  Drone Racing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.02540v1": {
            "Paper Title": "Active Improvement of Control Policies with Bayesian Gaussian Mixture\n  Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.02532v1": {
            "Paper Title": "Online Weight-adaptive Nonlinear Model Predictive Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.12228v1": {
            "Paper Title": "Towards General and Autonomous Learning of Core Skills: A Case Study in\n  Locomotion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.02068v2": {
            "Paper Title": "PLG-IN: Pluggable Geometric Consistency Loss with Wasserstein Distance\n  in Monocular Depth Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.04424v2": {
            "Paper Title": "CMetric: A Driving Behavior Measure Using Centrality Functions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.01118v2": {
            "Paper Title": "Forecasting Trajectory and Behavior of Road-Agents Using Spectral\n  Clustering in Graph-LSTMs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.09014v3": {
            "Paper Title": "Learning Hybrid Object Kinematics for Efficient Hierarchical Planning\n  Under Uncertainty",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.02236v1": {
            "Paper Title": "Real-time and Autonomous Detection of Helipad for Landing Quad-Rotors by\n  Visual Servoing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.02164v1": {
            "Paper Title": "Supporting Robotic Software Migration Using Static Analysis and\n  Model-Driven Engineering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.02116v1": {
            "Paper Title": "Quality and Diversity in Evolutionary Modular Robotics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.02020v1": {
            "Paper Title": "A variable rest length impedance grasping strategy in the\n  port-Hamiltonian framework",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.01347v2": {
            "Paper Title": "BRM Localization: UAV Localization in GNSS-Denied Environments Based on\n  Matching of Numerical Map and UAV Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.01926v1": {
            "Paper Title": "What to Do When You Can't Do It All: Temporal Logic Planning with Soft\n  Temporal Logic Constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.01921v1": {
            "Paper Title": "A Probabilistic Model for Planar Sliding of Objects with Unknown\n  Material Properties: Identification and Robust Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.10209v3": {
            "Paper Title": "Energy-Efficient Motion Planning for Multi-Modal Hybrid Locomotion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.08876v3": {
            "Paper Title": "Learning to Fly via Deep Model-Based Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.00851v2": {
            "Paper Title": "Planning to Score a Goal in Robotic Football with Heuristic Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.01339v1": {
            "Paper Title": "Collecting the Public Perception of AI and Robot Rights",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.03698v4": {
            "Paper Title": "Skew-Fit: State-Covering Self-Supervised Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.05489v2": {
            "Paper Title": "Material Mapping in Unknown Environments using Tapping Sound",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.01281v1": {
            "Paper Title": "Stochastic Grounded Action Transformation for Robot Learning in\n  Simulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.01279v1": {
            "Paper Title": "Reinforced Grounded Action Transformation for Sim-to-Real Transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.01205v1": {
            "Paper Title": "Concurrent Training Improves the Performance of Behavioral Cloning from\n  Observation",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": ". In analogy with GANs, GAIL focuses on learn-\ning a discriminator that is capable of distinguishing expert demonstrations from robot trajectories\nwhile simultaneously training the robot to avoid detection by the discriminator ",
                    "Citation Text": "I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,\nand Y . Bengio. Generative adversarial nets. In Advances in neural information processing\nsystems , pages 2672\u20132680, 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.2661",
                        "Citation Paper Title": "Title:Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio"
                    },
                    "Keywords": [
                        "Theory",
                        "Learning",
                        "Imitation",
                        ","
                    ]
                }
            ]
        },
        "http://arxiv.org/abs/2008.01148v1": {
            "Paper Title": "HAMLET: A Hierarchical Multimodal Attention-based Human Activity\n  Recognition Algorithm",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.13165v3": {
            "Paper Title": "Relational Graph Learning for Crowd Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.06887v5": {
            "Paper Title": "Multimodal Interaction-aware Motion Prediction for Autonomous Street\n  Crossing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.00609v2": {
            "Paper Title": "Optimizing Dynamic Trajectories for Robustness to Disturbances Using\n  Polytopic Projections",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.06255v2": {
            "Paper Title": "CPC: Complementary Progress Constraints for Time-Optimal Quadrotor\n  Trajectories",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.00699v1": {
            "Paper Title": "Getting to Know One Another: Calibrating Intent, Capabilities and Trust\n  for Human-Robot Collaboration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.07096v3": {
            "Paper Title": "Virtual Reality for Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.11190v3": {
            "Paper Title": "2018 Robotic Scene Segmentation Challenge",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.00603v1": {
            "Paper Title": "Learning Agile Locomotion via Adversarial Training",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.02624v3": {
            "Paper Title": "Learning Efficient Representation for Intrinsic Motivation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.00516v1": {
            "Paper Title": "Deep-Reinforcement-Learning-Based Semantic Navigation of Mobile Robots\n  in Dynamic Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.00504v1": {
            "Paper Title": "Variational Filtering with Copula Models for SLAM",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.00456v1": {
            "Paper Title": "Hindsight for Foresight: Unsupervised Structured Dynamics Models from\n  Physical Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.01655v1": {
            "Paper Title": "Deep Visual Odometry with Adaptive Memory",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.12493v3": {
            "Paper Title": "Invisible Marker: Automatic Annotation of Segmentation Masks for Object\n  Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.12494v3": {
            "Paper Title": "Deep Gated Multi-modal Learning: In-hand Object Pose Changes Estimation\n  using Tactile and Image Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.00372v1": {
            "Paper Title": "Better Together: Online Probabilistic Clique Change Detection in 3D\n  Landmark-Based Maps",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.14033v2": {
            "Paper Title": "Plan Arithmetic: Compositional Plan Vectors for Multi-Task Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.00326v1": {
            "Paper Title": "PERCH 2.0 : Fast and Accurate GPU-based Perception via Search for Object\n  Pose Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.03249v2": {
            "Paper Title": "A Characterization of Semi-Synchrony for Asynchronous Robots with\n  Limited Visibility, and its Application to Luminous Synchronizer Design",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.08046v1": {
            "Paper Title": "TactileSGNet: A Spiking Graph Neural Network for Event-based Tactile\n  Object Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.07088v4": {
            "Paper Title": "Adaptive Robot-Assisted Feeding: An Online Learning Framework for\n  Acquiring Previously Unseen Food Items",
            "Sentences": [
                {
                    "Sentence ID": 45,
                    "Sentence": ", both of\nwhich are simple to implement and perform well in practice,\nalthough they do not achieve optimal regret guarantees. More\nrecent advances include LinUCB ",
                    "Citation Text": "L. Li, W. Chu, J. Langford, and R. E. Schapire, \u201cA Contextual-\nBandit approach to personalized news article recommendation,\u201d arXiv\npreprint arXiv:1003.0146 , Feb. 2010.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1003.0146",
                        "Citation Paper Title": "Title:A Contextual-Bandit Approach to Personalized News Article Recommendation",
                        "Citation Paper Abstract": "Abstract:Personalized web services strive to adapt their services (advertisements, news articles, etc) to individual users by making use of both content and user information. Despite a few recent advances, this problem remains challenging for at least two reasons. First, web service is featured with dynamically changing pools of content, rendering traditional collaborative filtering methods inapplicable. Second, the scale of most web services of practical interest calls for solutions that are both fast in learning and computation.\nIn this work, we model personalized recommendation of news articles as a contextual bandit problem, a principled approach in which a learning algorithm sequentially selects articles to serve users based on contextual information about the users and articles, while simultaneously adapting its article-selection strategy based on user-click feedback to maximize total user clicks.\nThe contributions of this work are three-fold. First, we propose a new, general contextual bandit algorithm that is computationally efficient and well motivated from learning theory. Second, we argue that any bandit algorithm can be reliably evaluated offline using previously recorded random traffic. Finally, using this offline evaluation method, we successfully applied our new algorithm to a Yahoo! Front Page Today Module dataset containing over 33 million events. Results showed a 12.5% click lift compared to a standard context-free bandit algorithm, and the advantage becomes even greater when data gets more scarce.",
                        "Citation Paper Authors": "Authors:Lihong Li, Wei Chu, John Langford, Robert E. Schapire"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2008.00109v1": {
            "Paper Title": "Characterization of Assistive Robot Arm Teleoperation: A Preliminary\n  Study to Inform Shared Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.00067v1": {
            "Paper Title": "Infusing Reachability-Based Safety into Planning and Control for\n  Multi-agent Interactions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.01156v2": {
            "Paper Title": "Real-World Human-Robot Collaborative Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.14079v2": {
            "Paper Title": "DR-SPAAM: A Spatial-Attention and Auto-regressive Model for Person\n  Detection in 2D Range Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.10190v2": {
            "Paper Title": "Never Stop Learning: The Effectiveness of Fine-Tuning in Robotic\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.15945v1": {
            "Paper Title": "Autonomous Navigation in Complex Environments with Deep Multimodal\n  Fusion Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.13503v4": {
            "Paper Title": "Side-Tuning: A Baseline for Network Adaptation via Additive Side\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.11154v2": {
            "Paper Title": "HARMONIC: A Multimodal Dataset of Assistive Human-Robot Collaboration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.03614v2": {
            "Paper Title": "Anticipatory Human-Robot Collaboration via Multi-Objective Trajectory\n  Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.15724v1": {
            "Paper Title": "MAPPER: Multi-Agent Path Planning with Evolutionary Reinforcement\n  Learning in Mixed Dynamic Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.01160v2": {
            "Paper Title": "Multimodal Material Classification for Robots using Spectroscopy and\n  High Resolution Texture Imaging",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.15308v1": {
            "Paper Title": "Natural Gradient Shared Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.14545v2": {
            "Paper Title": "Learning Object-conditioned Exploration using Distributed Soft Actor\n  Critic",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.01239v3": {
            "Paper Title": "Rapidly Adaptable Legged Robots via Evolutionary Meta-Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.01962v3": {
            "Paper Title": "Characterizing an Analogical Concept Memory for Architectures\n  Implementing the Common Model of Cognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.14880v1": {
            "Paper Title": "Towards Cooperative Transport of a Suspended Payload via Two Aerial\n  Robots with Inertial Sensing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.14850v1": {
            "Paper Title": "Mechatronics-Driven Musical Expressivity for Robotic Percussionists",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.14838v1": {
            "Paper Title": "A Survey of Robotics and Emotion: Classifications and Models of\n  Emotional Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.14759v1": {
            "Paper Title": "Targetless Calibration of LiDAR-IMU System Based on Continuous-time\n  Batch Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.04784v2": {
            "Paper Title": "Reconstruction of 3D flight trajectories from ad-hoc camera networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.06488v3": {
            "Paper Title": "RTLola Cleared for Take-Off: Monitoring Autonomous Aircraft",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.14646v1": {
            "Paper Title": "Modular Transfer Learning with Transition Mismatch Compensation for\n  Excessive Disturbance Rejection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.14606v1": {
            "Paper Title": "3D Fusion of Infrared Images with Dense RGB Reconstruction from Multiple\n  Views -- with Application to Fire-fighting Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.14603v1": {
            "Paper Title": "Predictive Probability Path Planning Model For Dynamic Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.09441v5": {
            "Paper Title": "DeepMNavigate: Deep Reinforced Multi-Robot Navigation Unifying Local &\n  Global Collision Avoidance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.13188v2": {
            "Paper Title": "Substituting Restorative Benefits of Being Outdoors through Interactive\n  Augmented Spatial Soundscapes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.09633v2": {
            "Paper Title": "Motion Planning for Heterogeneous Unmanned Systems under Partial\n  Observation from UAV",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.02415v2": {
            "Paper Title": "Anatomical Mesh-Based Virtual Fixtures for Surgical Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.01062v2": {
            "Paper Title": "ProxEmo: Gait-based Emotion Learning and Multi-view Proxemic Fusion for\n  Socially-Aware Robot Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.14290v1": {
            "Paper Title": "Learning Stable Manoeuvres in Quadruped Robots from Expert\n  Demonstrations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.00801v1": {
            "Paper Title": "Real-Time Point Cloud Fusion of Multi-LiDAR Infrastructure Sensor Setups\n  with Unknown Spatial Location and Orientation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.13993v1": {
            "Paper Title": "Robust Ego and Object 6-DoF Motion Estimation and Tracking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.03647v2": {
            "Paper Title": "Artistic Style in Robotic Painting; a Machine Learning Approach to\n  Learning Brushstroke from Human Artists",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.13971v1": {
            "Paper Title": "Accurate, Low-Latency Visual Perception for Autonomous\n  Racing:Challenges, Mechanisms, and Practical Solutions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.13729v1": {
            "Paper Title": "Noisy Agents: Self-supervised Exploration by Predicting Auditory Events",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.07705v3": {
            "Paper Title": "Towards Bounding-Box Free Panoptic Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.13704v1": {
            "Paper Title": "WGANVO: Monocular Visual Odometry based on Generative Adversarial\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.13486v1": {
            "Paper Title": "Complex Robotic Manipulation via Graph-Based Hindsight Goal Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.05117v3": {
            "Paper Title": "Multiplicative Controller Fusion: Leveraging Algorithmic Priors for\n  Sample-efficient Reinforcement Learning and Safe Sim-To-Real Transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.10323v2": {
            "Paper Title": "Pillar-based Object Detection for Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.09107v2": {
            "Paper Title": "Synthetic and Real Inputs for Tool Segmentation in Robotic Surgery",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.13109v3": {
            "Paper Title": "Dynamic Multi-Robot Task Allocation under Uncertainty and Temporal\n  Constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.12990v1": {
            "Paper Title": "Demo: Edge-centric Telepresence Avatar Robot for Geographically\n  Distributed Environment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.14256v1": {
            "Paper Title": "RMPflow: A Geometric Framework for Generation of Multi-Task Motion\n  Policies",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.12803v1": {
            "Paper Title": "Joint Mind Modeling for Explanation Generation in Complex Human-Robot\n  Collaborative Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.12640v1": {
            "Paper Title": "Autonomous Exploration Under Uncertainty via Deep Reinforcement Learning\n  on Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.04842v2": {
            "Paper Title": "An Interior Point Method Solving Motion Planning Problems with Narrow\n  Passages",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.12553v1": {
            "Paper Title": "Style Transfer for Co-Speech Gesture Animation: A Multi-Speaker\n  Conditional-Mixture Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.07487v3": {
            "Paper Title": "Improving Human Performance Using Mixed Granularity of Control in\n  Multi-Human Multi-Robot Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.12397v1": {
            "Paper Title": "Learning the Solution Manifold in Optimization and Its Application in\n  Motion Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2008.12230v1": {
            "Paper Title": "Quantum Cooperative Robotics and Autonomy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.10577v2": {
            "Paper Title": "Distributed Motion Control for Multiple Connected Surface Vessels",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.08514v2": {
            "Paper Title": "Spatio-Temporal Graph Transformer Networks for Pedestrian Trajectory\n  Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.12148v1": {
            "Paper Title": "Enhanced Transfer Learning for Autonomous Driving with Systematic\n  Accident Simulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.12065v1": {
            "Paper Title": "Polylidar3D -- Fast Polygon Extraction from 3D Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.12036v1": {
            "Paper Title": "Implicit Latent Variable Model for Scene-Consistent Motion Forecasting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.01197v3": {
            "Paper Title": "Learning to Collide: An Adaptive Safety-Critical Scenarios Generating\n  Method",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.11668v1": {
            "Paper Title": "Analogical Reasoning for Visually Grounded Language Acquisition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.11646v1": {
            "Paper Title": "Understanding Multi-Modal Perception Using Behavioral Cloning for\n  Peg-In-a-Hole Insertion Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.11627v1": {
            "Paper Title": "Learning User-Preferred Mappings for Intuitive Robot Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.08150v3": {
            "Paper Title": "NEMO: Future Object Localization Using Noisy Ego Priors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.00095v2": {
            "Paper Title": "Deep Learning for Vision-based Prediction: A Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.14471v1": {
            "Paper Title": "Learning to predict metal deformations in hot-rolling processes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.07372v4": {
            "Paper Title": "RIDM: Reinforced Inverse Dynamics Modeling for Learning from a Single\n  Observed Demonstration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.10743v1": {
            "Paper Title": "Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection\n  and Tracking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.05526v3": {
            "Paper Title": "Making Robots Draw A Vivid Portrait In Two Minutes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.11043v2": {
            "Paper Title": "Safe Optimal Control under Parametric Uncertainties",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.00443v5": {
            "Paper Title": "Environment-agnostic Multitask Learning for Natural Language Grounded\n  Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.12907v2": {
            "Paper Title": "Enabling Robots to Understand Incomplete Natural Language Instructions\n  Using Commonsense Reasoning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.02907v2": {
            "Paper Title": "In-flight range optimization of multicopters using multivariable\n  extremum seeking with adaptive step size",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.08797v2": {
            "Paper Title": "You Are Here: Geolocation by Embedding Maps and Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.10441v1": {
            "Paper Title": "Zero-Error Tracking for Autonomous Vehicles through Epsilon-Trajectory\n  Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.10407v1": {
            "Paper Title": "Fusing Concurrent Orthogonal Wide-aperture Sonar Images for Dense\n  Underwater 3D Reconstruction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.10038v1": {
            "Paper Title": "Anticipating Human Intention for Full-Body Motion Prediction in Object\n  Grasping and Placing Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.03217v2": {
            "Paper Title": "RGB-D SLAM in Dynamic Environments Using Point Correlations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.08601v2": {
            "Paper Title": "CoNES: Convex Natural Evolutionary Strategies",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.09841v1": {
            "Paper Title": "Seeing the Un-Scene: Learning Amodal Semantic Maps for Room Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.09746v1": {
            "Paper Title": "Beyond Single Stage Encoder-Decoder Networks: Deep Decoders for Semantic\n  Image Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.09626v1": {
            "Paper Title": "Optimal tool path planning for 3D printing with spatio-temporal and\n  thermal constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.09595v1": {
            "Paper Title": "Risk-aware Path and Motion Planning for a Tethered Aerial Visual\n  Assistant in Unstructured or Confined Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.09581v1": {
            "Paper Title": "Fast Adaptable Mobile Robot Navigation in Dynamic Environment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.06392v2": {
            "Paper Title": "DeepHAZMAT: Hazardous Materials Sign Detection and Segmentation with\n  Restricted Computational Resources",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.10505v3": {
            "Paper Title": "Experiments with Tractable Feedback in Robotic Planning under\n  Uncertainty: Insights over a wide range of noise regimes (Extended Report)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.03450v3": {
            "Paper Title": "Learning to Accelerate Decomposition for Multi-Directional 3D Printing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.05599v2": {
            "Paper Title": "Online monitoring for safe pedestrian-vehicle interactions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.09243v1": {
            "Paper Title": "Multi-robot Cooperative Object Transportation using Decentralized Deep\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.09232v1": {
            "Paper Title": "Information Requirements of Collision-Based Micromanipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.06409v2": {
            "Paper Title": "Probabilistic Future Prediction for Video Scene Understanding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.08854v1": {
            "Paper Title": "DVI: Depth Guided Video Inpainting for Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.08672v1": {
            "Paper Title": "Toward Forgetting-Sensitive Referring Expression Generationfor\n  Integrated Robot Architectures",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.08616v1": {
            "Paper Title": "Collision Avoidance Robotics Via Meta-Learning (CARML)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.08553v1": {
            "Paper Title": "Smooth Deformation Field-based Mismatch Removal in Real-time",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.10091v2": {
            "Paper Title": "Label Efficient Visual Abstractions for Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.08451v1": {
            "Paper Title": "Specification mining and automated task planning for autonomous robots\n  based on a graph-based spatial temporal logic",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.00425v4": {
            "Paper Title": "Generating Grasp Poses for a High-DOF Gripper Using Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.08251v1": {
            "Paper Title": "Efficient State Abstraction using Object-centered Predicates for\n  Manipulation Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.04807v2": {
            "Paper Title": "6D Camera Relocalization in Ambiguous Scenes via Continuous Multimodal\n  Inference",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.08097v1": {
            "Paper Title": "TrashCan: A Semantically-Segmented Dataset towards Visual Detection of\n  Marine Debris",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.13236v2": {
            "Paper Title": "The Best of Both Modes: Separately Leveraging RGB and Depth for Unseen\n  Object Instance Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.00116v4": {
            "Paper Title": "APPLD: Adaptive Planner Parameter Learning from Demonstration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.01875v2": {
            "Paper Title": "Localising Faster: Efficient and precise lidar-based robot localisation\n  in large-scale environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.10999v4": {
            "Paper Title": "Non-Gaussian Chance-Constrained Trajectory Planning for Autonomous\n  Vehicles under Agent Uncertainty",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.07793v1": {
            "Paper Title": "Developmental Reinforcement Learning of Control Policy of a Quadcopter\n  UAV with Thrust Vectoring Rotors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.14288v3": {
            "Paper Title": "Actor-Critic Reinforcement Learning for Control with Stability Guarantee",
            "Sentences": [
                {
                    "Sentence ID": 41,
                    "Sentence": ",\none of the state-of-the-art actor-critic algorithms that outper-\nform a series of RL methods such as DDPG ",
                    "Citation Text": "T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y . Tassa,\nD. Silver, and D. Wierstra, \u201cContinuous control with deep reinforcement\nlearning,\u201d arXiv preprint arXiv:1509.02971 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1509.02971",
                        "Citation Paper Title": "Title:Continuous control with deep reinforcement learning",
                        "Citation Paper Abstract": "Abstract:We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.",
                        "Citation Paper Authors": "Authors:Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra"
                    },
                    "Keywords": []
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ".\nFurthermore, the exploitation of multi-layer neural networks\nas function approximators ",
                    "Citation Text": "T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y . Tassa,\nD. Silver, and D. Wierstra, \u201cContinuous control with deep reinforcement\nlearning,\u201d arXiv preprint arXiv:1509.02971 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1509.02971",
                        "Citation Paper Title": "Title:Continuous control with deep reinforcement learning",
                        "Citation Paper Abstract": "Abstract:We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.",
                        "Citation Paper Authors": "Authors:Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2007.07702v1": {
            "Paper Title": "Lunar Terrain Relative Navigation Using a Convolutional Neural Network\n  for Visual Crater Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.07686v1": {
            "Paper Title": "Relative Pose Estimation of Calibrated Cameras with Known\n  $\\mathrm{SE}(3)$ Invariants",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.01530v2": {
            "Paper Title": "Deep Differentiable Grasp Planner for High-DOF Grippers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.07630v1": {
            "Paper Title": "Learning Multiplicative Interactions with Bayesian Neural Networks for\n  Visual-Inertial Odometry",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.07703v4": {
            "Paper Title": "Unsupervised Intra-domain Adaptation for Semantic Segmentation through\n  Self-Supervision",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.09822v3": {
            "Paper Title": "Confidence Regularized Self-Training",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.05917v2": {
            "Paper Title": "Neural Collision Clearance Estimator for Batched Motion Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.04232v2": {
            "Paper Title": "Hierarchical Kinematic Human Mesh Recovery",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.06095v2": {
            "Paper Title": "Graph Neural Networks for Decentralized Multi-Robot Path Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.14121v2": {
            "Paper Title": "HATSUKI : An anime character like robot figure platform with anime-style\n  expressions and imitation learning based action generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.06965v1": {
            "Paper Title": "Automated Synthetic-to-Real Generalization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.06904v1": {
            "Paper Title": "Fast Lane-Level Intersection Estimation using Markov Chain Monte Carlo\n  Sampling and B-Spline Refinement",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.02530v3": {
            "Paper Title": "Approximation Algorithms for Multi-Robot Patrol-Scheduling with Min-Max\n  Latency",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.06669v1": {
            "Paper Title": "Reinforcement Learning of Musculoskeletal Control from Functional\n  Simulations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.06531v1": {
            "Paper Title": "A Robotic Framework for Making Eye Contact with Humans",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.03026v2": {
            "Paper Title": "DA4AD: End-to-End Deep Attention-based Visual Localization for\n  Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.09053v1": {
            "Paper Title": "Situated Multimodal Control of a Mobile Robot: Navigation through a\n  Virtual Environment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.06473v1": {
            "Paper Title": "Designing Personalized Interaction of a Socially Assistive Robot for\n  Stroke Rehabilitation Therapy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.02250v2": {
            "Paper Title": "Stereo Visual Inertial Pose Estimation Based on Feedforward-Feedback\n  Loops",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.09146v2": {
            "Paper Title": "Deadlock Analysis and Resolution in Multi-Robot Systems (Extended\n  Version)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.06080v1": {
            "Paper Title": "Robotic Non-Destructive Testing of Manmade Structures: A Review of the\n  Literature",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.06045v1": {
            "Paper Title": "Augmenting Differentiable Simulators with Neural Networks to Close the\n  Sim2Real Gap",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.01992v2": {
            "Paper Title": "A Survey on Sensor Technologies for Unmanned Ground Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.05778v1": {
            "Paper Title": "Evaluating the Potential of Drone Swarms in Nonverbal HRI Communication",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.05655v1": {
            "Paper Title": "Evolving Graphical Planner: Contextual Global Planning for\n  Vision-and-Language Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.04159v3": {
            "Paper Title": "Tightly-coupled Fusion of Global Positional Measurements in\n  Optimization-based Visual-Inertial Odometry",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.05097v2": {
            "Paper Title": "Residual Policy Learning for Shared Autonomy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.05394v1": {
            "Paper Title": "How An Automated Gesture Imitation Game Can Improve Social Interactions\n  With Teenagers With ASD",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.05156v1": {
            "Paper Title": "A Survey on Autonomous Vehicle Control in the Era of Mixed-Autonomy:\n  From Physics-Based to AI-Guided Driving Policy Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.04916v1": {
            "Paper Title": "Explainability of Intelligent Transportation Systems using Knowledge\n  Compilation: a Traffic Light Controller Case",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01741v3": {
            "Paper Title": "Improving Sample Efficiency in Model-Free Reinforcement Learning from\n  Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.05490v1": {
            "Paper Title": "Camera-Lidar Integration: Probabilistic sensor fusion for semantic\n  mapping",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.04563v1": {
            "Paper Title": "Design and Development of a Robotic Vehicle for Shallow-Water Marine\n  Inspections",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.04557v1": {
            "Paper Title": "Alleviating the Burden of Labeling: Sentence Generation by Attention\n  Branch Encoder-Decoder Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.01921v2": {
            "Paper Title": "Human-Robot Team Coordination with Dynamic and Latent Human Task\n  Proficiencies: Scheduling with Learning Curves",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.04407v1": {
            "Paper Title": "Multi-Swarm Herding: Protecting against Adversarial Swarms",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.07972v3": {
            "Paper Title": "Adaptive Curriculum Generation from Demonstrations for Sim-to-Real\n  Visuomotor Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.01813v2": {
            "Paper Title": "AVP-SLAM: Semantic Visual Mapping and Localization for Autonomous\n  Vehicles in the Parking Lot",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.10876v2": {
            "Paper Title": "Flexible and Efficient Long-Range Planning Through Curious Exploration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01215v4": {
            "Paper Title": "ES-MAML: Simple Hessian-Free Meta Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.03514v1": {
            "Paper Title": "Imitation Learning Approach for AI Driving Olympics Trained on\n  Real-world and Simulation Data Simultaneously",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.03501v1": {
            "Paper Title": "Coverage of an Environment Using Energy-Constrained Unmanned Aerial\n  Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.03409v1": {
            "Paper Title": "Optical Navigation in Unstructured Dynamic Railroad Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.08726v3": {
            "Paper Title": "Interpretable End-to-end Urban Autonomous Driving with Latent Deep\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.02832v1": {
            "Paper Title": "Maximum Entropy Gain Exploration for Long Horizon Multi-goal\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.11564v2": {
            "Paper Title": "Mid-flight Propeller Failure Detection and Control of\n  Propeller-deficient Quadcopter using Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.01711v3": {
            "Paper Title": "Blockchain-Powered Collaboration in Heterogeneous Swarms of Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.04304v1": {
            "Paper Title": "Unsupervised Online Grounding of Natural Language during Human-Robot\n  Interactions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.12780v2": {
            "Paper Title": "Describing Physics For Physical Reasoning: Force-based Sequential\n  Manipulation Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.02132v1": {
            "Paper Title": "Self-Assessment of Grasp Affordance Transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.02067v1": {
            "Paper Title": "Sensor-Based Control for Collaborative Robots: Fundamentals, Challenges\n  and Opportunities",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.02065v1": {
            "Paper Title": "Efficient and accurate object detection with simultaneous classification\n  and tracking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.01959v1": {
            "Paper Title": "Experimental Evaluation of 3D-LIDAR Camera Extrinsic Calibration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.01851v1": {
            "Paper Title": "Swoosh! Rattle! Thump! -- Actions that Sound",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.01787v1": {
            "Paper Title": "Evaluating Uncertainty Estimation Methods on 3D Semantic Segmentation of\n  Point Clouds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.01691v1": {
            "Paper Title": "The Review Unmanned Surface Vehicle Path Planning: Based on\n  Multi-modality Constraint",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.01595v1": {
            "Paper Title": "LOL: Lidar-Only Odometry and Localization in 3D Point Cloud Maps",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.03825v3": {
            "Paper Title": "LiDAR Iris for Loop-Closure Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.01009v1": {
            "Paper Title": "Human-centered collaborative robots with deep reinforcement learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.00987v1": {
            "Paper Title": "ADD: Analytically Differentiable Dynamics for Multi-Body Systems with\n  Frictional Contact",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.00982v1": {
            "Paper Title": "Towards Generalization and Data Efficient Learning of Deep Robotic\n  Grasping",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.00920v1": {
            "Paper Title": "A Learning-Driven Framework with Spatial Optimization For Surgical\n  Suture Thread Reconstruction and Autonomous Grasping Under Multiple\n  Topologies and Environmental Noises",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.00869v1": {
            "Paper Title": "\u03b5-BMC: A Bayesian Ensemble Approach to Epsilon-Greedy\n  Exploration in Model-Free Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.00643v2": {
            "Paper Title": "Object Goal Navigation using Goal-Oriented Semantic Exploration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.00798v1": {
            "Paper Title": "Deliberate Exploration Supports Navigation in Unfamiliar Worlds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.00775v1": {
            "Paper Title": "Achieving Multi-Tasking Robots in Multi-Robot Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.00291v1": {
            "Paper Title": "FlowControl: Optical Flow Based Visual Servoing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.01120v1": {
            "Paper Title": "Motion Prediction in Visual Object Tracking",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.03661v3": {
            "Paper Title": "Adaptive Trajectory Estimation with Power Limited Steering Model under\n  Perturbation Compensation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.00142v2": {
            "Paper Title": "A real-time multi-constraints obstacle avoidance method using LiDAR",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.00948v2": {
            "Paper Title": "State-Continuity Approximation of Markov Decision Processes via Finite\n  Element Methods for Autonomous System Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.05960v2": {
            "Paper Title": "Planning to Explore via Self-Supervised World Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2007.00100v1": {
            "Paper Title": "Robust Multi-Agent Task Assignment in Failure-Prone and Adversarial\n  Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.16868v1": {
            "Paper Title": "Predicting Sample Collision with Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.01753v4": {
            "Paper Title": "Cognitive and motor compliance in intentional human-robot interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.00600v3": {
            "Paper Title": "Static Free Space Detection with Laser Scanner using Occupancy Grid Maps",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.16503v1": {
            "Paper Title": "Vehicle Re-ID for Surround-view Camera System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.12567v2": {
            "Paper Title": "A Survey on Deep Learning for Localization and Mapping: Towards the Age\n  of Spatial Machine Intelligence",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.16235v1": {
            "Paper Title": "Vision-Based Goal-Conditioned Policies for Underwater Navigation in the\n  Presence of Obstacles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.16232v1": {
            "Paper Title": "Learning Robot Skills with Temporal Variational Inference",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.15718v1": {
            "Paper Title": "Steer with Me: A Predictive, Potential Field-Based Control Approach for\n  Semi-Autonomous, Teleoperated Road Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.02059v2": {
            "Paper Title": "Learning to Dynamically Coordinate Multi-Robot Teams in Graph Attention\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.15699v1": {
            "Paper Title": "MIMC-VINS: A Versatile and Resilient Multi-IMU Multi-Camera\n  Visual-Inertial Navigation System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.15664v1": {
            "Paper Title": "Minimizing The Maximum Distance Traveled To Form Patterns With Systems\n  of Mobile Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.15647v1": {
            "Paper Title": "I can attend a meeting too! Towards a human-like telepresence avatar\n  robot to attend meeting on your behalf",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.05111v2": {
            "Paper Title": "Multi-Sound-Source Localization Using Machine Learning for Small\n  Autonomous Unmanned Vehicles with a Self-Rotating Bi-Microphone Array",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.15503v1": {
            "Paper Title": "Task-driven Perception and Manipulation for Constrained Placement of\n  Unknown Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.15477v1": {
            "Paper Title": "A convex data-driven approach for nonlinear control synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.14711v2": {
            "Paper Title": "PnPNet: End-to-End Perception and Prediction with Tracking in the Loop",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": "uses\na variational auto-encoder to generate trajectory proposals\nand re\ufb01nes them based on semantic scene context and in-\nteractions between agents. To better model the interac-\ntions, game theory is used to formulate the problem ",
                    "Citation Text": "Wei-Chiu Ma, De-An Huang, Namhoon Lee, and Kris M\nKitani. Forecasting interactive dynamics of pedestrians with\n\ufb01ctitious play. In CVPR , 2017. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1604.01431",
                        "Citation Paper Title": "Title:Forecasting Interactive Dynamics of Pedestrians with Fictitious Play",
                        "Citation Paper Abstract": "Abstract:We develop predictive models of pedestrian dynamics by encoding the coupled nature of multi-pedestrian interaction using game theory, and deep learning-based visual analysis to estimate person-specific behavior parameters. Building predictive models for multi-pedestrian interactions however, is very challenging due to two reasons: (1) the dynamics of interaction are complex interdependent processes, where the predicted behavior of one pedestrian can affect the actions taken by others and (2) dynamics are variable depending on an individuals physical characteristics (e.g., an older person may walk slowly while the younger person may walk faster). To address these challenges, we (1) utilize concepts from game theory to model the interdependent decision making process of multiple pedestrians and (2) use visual classifiers to learn a mapping from pedestrian appearance to behavior parameters. We evaluate our proposed model on several public multiple pedestrian interaction video datasets. Results show that our strategic planning model explains human interactions 25% better when compared to state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Wei-Chiu Ma, De-An Huang, Namhoon Lee, Kris M. Kitani"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2001.02223v2": {
            "Paper Title": "Dynamic Task Weighting Methods for Multi-task Networks in Autonomous\n  Driving Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.06013v3": {
            "Paper Title": "Motion Planning Networks: Bridging the Gap Between Learning-based and\n  Classical Motion Planners",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": "has also emerged as a\nprominent tool to solve continuous control and planning prob-\nlems ",
                    "Citation Text": "T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y . Tassa,\nD. Silver, and D. Wierstra, \u201cContinuous control with deep reinforcement\nlearning,\u201d arXiv preprint arXiv:1509.02971 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1509.02971",
                        "Citation Paper Title": "Title:Continuous control with deep reinforcement learning",
                        "Citation Paper Abstract": "Abstract:We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.",
                        "Citation Paper Authors": "Authors:Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2002.10570v2": {
            "Paper Title": "Real-time Fusion Network for RGB-D Semantic Segmentation Incorporating\n  Unexpected Obstacle Detection for Road-driving Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.01188v2": {
            "Paper Title": "Adaptive Online Planning for Continual Lifelong Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.01629v2": {
            "Paper Title": "Can Increasing Input Dimensionality Improve Deep Reinforcement Learning?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.00336v2": {
            "Paper Title": "3D Object Detection on Point Clouds using Local Ground-aware and\n  Adaptive Representation of scenes' surface",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.15110v1": {
            "Paper Title": "Learning predictive representations in autonomous driving to improve\n  deep reinforcement learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.06801v2": {
            "Paper Title": "Scalable Autonomous Vehicle Safety Validation through Dynamic\n  Programming and Scene Decomposition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.06805v2": {
            "Paper Title": "Interpretable Safety Validation for Autonomous Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.14718v1": {
            "Paper Title": "Asynchronous Multi Agent Active Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.02744v3": {
            "Paper Title": "LiStereo: Generate Dense Depth Maps from LIDAR and Stereo Imagery",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.14479v1": {
            "Paper Title": "Fair navigation planning: a humanitarian robot use case",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.14467v1": {
            "Paper Title": "Robust Relative Hand Placement For Bi-Manual Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.14281v1": {
            "Paper Title": "Optimal Trajectory Planning for Flexible Robots with Large Deformation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.15015v1": {
            "Paper Title": "SASO: Joint 3D Semantic-Instance Segmentation via Multi-scale Semantic\n  Association and Salient Point Clustering Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.06764v2": {
            "Paper Title": "Learning to Engage with Interactive Systems: A Field Study on Deep\n  Reinforcement Learning in a Public Museum",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.06129v2": {
            "Paper Title": "LIBRE: The Multiple 3D LiDAR Dataset",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.13413v1": {
            "Paper Title": "A Hierarchical Framework for Long-term and Robust Deployment of Field\n  Ground Robots in Large-Scale Farming",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.10611v3": {
            "Paper Title": "A Successive-Elimination Approach to Adaptive Robotic Sensing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.13360v1": {
            "Paper Title": "Evaluation of Sampling Methods for Robotic Sediment Sampling Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.13267v1": {
            "Paper Title": "Learning-to-Fly: Learning-based Collision Avoidance for Scalable Urban\n  Air Mobility",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.13253v1": {
            "Paper Title": "Robot Object Retrieval with Contextual Natural Language Queries",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": "use a similar approach\nalbeit using deep neural networks to avoid parsing and feature\nconstruction by hand. Chen et al. ",
                    "Citation Text": "Kevin Chen, Christopher B. Choy, Manolis Savva, An-\ngel X. Chang, Thomas Funkhouser, and Silvio Savarese.\nText2Shape: Generating Shapes from Natural Language\nby Learning Joint Embeddings. In Asian Conference on\nComputer Vision , pages 100\u2013116. Springer, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.08495",
                        "Citation Paper Title": "Title:Text2Shape: Generating Shapes from Natural Language by Learning Joint Embeddings",
                        "Citation Paper Abstract": "Abstract:We present a method for generating colored 3D shapes from natural language. To this end, we first learn joint embeddings of freeform text descriptions and colored 3D shapes. Our model combines and extends learning by association and metric learning approaches to learn implicit cross-modal connections, and produces a joint representation that captures the many-to-many relations between language and physical properties of 3D shapes such as color and shape. To evaluate our approach, we collect a large dataset of natural language descriptions for physical 3D objects in the ShapeNet dataset. With this learned joint embedding we demonstrate text-to-shape retrieval that outperforms baseline approaches. Using our embeddings with a novel conditional Wasserstein GAN framework, we generate colored 3D shapes from text. Our method is the first to connect natural language text with realistic 3D objects exhibiting rich variations in color, texture, and shape detail. See video at this https URL",
                        "Citation Paper Authors": "Authors:Kevin Chen, Christopher B. Choy, Manolis Savva, Angel X. Chang, Thomas Funkhouser, Silvio Savarese"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2006.13153v1": {
            "Paper Title": "Learning dynamics for improving control of overactuated flying systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.13084v1": {
            "Paper Title": "Single-Shot 3D Detection of Vehicles from Monocular RGB Images via\n  Geometry Constrained Keypoints in Real-Time",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.03535v3": {
            "Paper Title": "Phase Portraits as Movement Primitives for Fast Humanoid Robot Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01287v2": {
            "Paper Title": "Exoskeleton-covered soft finger with vision-based proprioception and\n  tactile sensing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.12033v2": {
            "Paper Title": "Deep Learning for 3D Point Clouds: A Survey",
            "Sentences": [
                {
                    "Sentence ID": 54,
                    "Sentence": ", the local structural\ninformation between points cannot be captured. Therefore,\nQi et al. ",
                    "Citation Text": "C. R. Qi, L. Yi, H. Su, and L. J. Guibas, \u201cPointNet++: Deep\nhierarchical feature learning on point sets in a metric space,\u201d in\nNeurIPS, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.02413",
                        "Citation Paper Title": "Title:PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space",
                        "Citation Paper Abstract": "Abstract:Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.",
                        "Citation Paper Authors": "Authors:Charles R. Qi, Li Yi, Hao Su, Leonidas J. Guibas"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2006.12729v1": {
            "Paper Title": "Grasp State Assessment of Deformable Objects Using Visual-Tactile Fusion\n  Perception",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.12712v1": {
            "Paper Title": "PoseGAN: A Pose-to-Image Translation Framework for Camera Localization",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": ". Owing to their\nability to learn highly structured probability distribution, cGANs have been widely\nused in applications like high \ufb01delity image generation [34, 35], image editing ",
                    "Citation Text": "T. Park, M. Liu, T. Wang, J. Zhu, Semantic image synthesis with spatially-\nadaptive normalization, In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (2019) 2337\u20132346.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.07291",
                        "Citation Paper Title": "Title:Semantic Image Synthesis with Spatially-Adaptive Normalization",
                        "Citation Paper Abstract": "Abstract:We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the deep network, which is then processed through stacks of convolution, normalization, and nonlinearity layers. We show that this is suboptimal as the normalization layers tend to ``wash away'' semantic information. To address the issue, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned transformation. Experiments on several challenging datasets demonstrate the advantage of the proposed method over existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows user control over both semantic and style. Code is available at this https URL .",
                        "Citation Paper Authors": "Authors:Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan Zhu"
                    },
                    "Keywords": [
                        "localization",
                        ",",
                        "(",
                        "GANs",
                        ")",
                        "Generative",
                        "Networks",
                        "camera",
                        "Adversarial"
                    ]
                }
            ]
        },
        "http://arxiv.org/abs/2006.12576v1": {
            "Paper Title": "Graph Neural Networks and Reinforcement Learning for Behavior Generation\n  in Semantic Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.12551v1": {
            "Paper Title": "PICO: Primitive Imitation for COntrol",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.12466v1": {
            "Paper Title": "Information Theoretic Regret Bounds for Online Nonlinear Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.12023v1": {
            "Paper Title": "The space of sections of a smooth function",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.03778v3": {
            "Paper Title": "LGSVL Simulator: A High Fidelity Simulator for Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.04443v3": {
            "Paper Title": "AVID: Learning Multi-Stage Tasks via Pixel-Level Translation of Human\n  Videos",
            "Sentences": [
                {
                    "Sentence ID": 56,
                    "Sentence": ". Thus, this comparison serves to help\ndetermine what bene\ufb01ts A VID derives from stage-wise RL.\nBehavioral Cloning from [Robot] Observation\n(BCO) ",
                    "Citation Text": "F. Torabi, G. Warnell, and P. Stone, \u201cBehavioral cloning from observa-\ntion,\u201d in IJCAI , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.01954",
                        "Citation Paper Title": "Title:Behavioral Cloning from Observation",
                        "Citation Paper Abstract": "Abstract:Humans often learn how to perform tasks via imitation: they observe others perform a task, and then very quickly infer the appropriate actions to take based on their observations. While extending this paradigm to autonomous agents is a well-studied problem in general, there are two particular aspects that have largely been overlooked: (1) that the learning is done from observation only (i.e., without explicit action information), and (2) that the learning is typically done very quickly. In this work, we propose a two-phase, autonomous imitation learning technique called behavioral cloning from observation (BCO), that aims to provide improved performance with respect to both of these aspects. First, we allow the agent to acquire experience in a self-supervised fashion. This experience is used to develop a model which is then utilized to learn a particular task by observing an expert perform that task without the knowledge of the specific actions taken. We experimentally compare BCO to imitation learning methods, including the state-of-the-art, generative adversarial imitation learning (GAIL) technique, and we show comparable task performance in several different simulation domains while exhibiting increased learning speed after expert trajectories become available.",
                        "Citation Paper Authors": "Authors:Faraz Torabi, Garrett Warnell, Peter Stone"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2006.11729v1": {
            "Paper Title": "Kiwifruit detection in challenging conditions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.11656v1": {
            "Paper Title": "For the Thrill of it All: A bridge among Linux, Robot Operating System,\n  Android and Unmanned Aerial Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.11631v1": {
            "Paper Title": "Estimating Model Uncertainty of Neural Networks in Sparse Information\n  Form",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.11509v2": {
            "Paper Title": "Visual-Inertial Telepresence for Aerial Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.11615v1": {
            "Paper Title": "Scalable Identification of Partially Observed Systems with\n  Certainty-Equivalent EM",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.11492v1": {
            "Paper Title": "A Distributed Multi-Robot Coordination Algorithm for Navigation in Tight\n  Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.04218v2": {
            "Paper Title": "Deep Reinforcement Learning for Human-Like Driving Policies in Collision\n  Avoidance Tasks of Self-Driving Cars",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": ".\nReinforcement learning methods are based on a reward\nfunction and are often not designed to reproduce human-like\nbehavior. For example, Lillicrap et al, ",
                    "Citation Text": "T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y . Tassa,\nD. Silver, and D. Wierstra, \u201cContinuous control with deep reinforcement\nlearning,\u201d arXiv preprint arXiv:1509.02971 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1509.02971",
                        "Citation Paper Title": "Title:Continuous control with deep reinforcement learning",
                        "Citation Paper Abstract": "Abstract:We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.",
                        "Citation Paper Authors": "Authors:Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2006.11034v1": {
            "Paper Title": "Low-cost Retina-like Robotic Lidars Based on Incommensurable Scanning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.08517v2": {
            "Paper Title": "Provably Constant-time Planning and Replanning for Real-time Grasping\n  Objects off a Conveyor Belt",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.10807v1": {
            "Paper Title": "Semantic Linking Maps for Active Visual Object Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.10701v1": {
            "Paper Title": "Deep Reinforcement Learning amidst Lifelong Non-Stationarity",
            "Sentences": [
                {
                    "Sentence ID": 46,
                    "Sentence": ",\nwhich learns to model partially observed environments with a latent variable model but does not\naddress inter-episode non-stationarity. This comparison allows us to evaluate the importance of\nmodeling non-stationarity between episodes. Finally, we include proximal policy optimization\n(PPO) ",
                    "Citation Text": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.06347",
                        "Citation Paper Title": "Title:Proximal Policy Optimization Algorithms",
                        "Citation Paper Abstract": "Abstract:We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.",
                        "Citation Paper Authors": "Authors:John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2005.03611v2": {
            "Paper Title": "Real-Time Context-aware Detection of Unsafe Events in Robot-Assisted\n  Surgery",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.08568v2": {
            "Paper Title": "ActiveMoCap: Optimized Viewpoint Selection for Active Human Motion\n  Capture",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.12384v1": {
            "Paper Title": "AI-Augmented Multi Function Radar Engineering with Digital Twin: Towards\n  Proactivity",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.00714v2": {
            "Paper Title": "Action Anticipation for Collaborative Environments: The Impact of\n  Contextual Information and Uncertainty-Based Prediction",
            "Sentences": [
                {
                    "Sentence ID": 45,
                    "Sentence": "used Zas the last layer of an encoder, not in\nall network activations or weights. Hence, in 2015, ",
                    "Citation Text": "C. Blundell, J. Cornebise, K. Kavukcuoglu,\nD. Wierstra, Weight uncertainty in neural net-\nwork, in: International Conference on Machine\nLearning, 2015, pp. 1613{1622.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1505.05424",
                        "Citation Paper Title": "Title:Weight Uncertainty in Neural Networks",
                        "Citation Paper Abstract": "Abstract:We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.",
                        "Citation Paper Authors": "Authors:Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra"
                    },
                    "Keywords": [
                        "Deep",
                        ",",
                        "Action",
                        "Information",
                        "Context",
                        "Early",
                        "Prediction",
                        "Bayesian",
                        "Learning",
                        "Anticipation"
                    ]
                }
            ]
        },
        "http://arxiv.org/abs/1912.04344v2": {
            "Paper Title": "Grasping in the Wild:Learning 6DoF Closed-Loop Grasping from Low-Cost\n  Demonstrations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.09761v1": {
            "Paper Title": "Evaluation of 3D CNN Semantic Mapping for Rover Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.00057v2": {
            "Paper Title": "Simulation Framework for Mobile Robots in Planetary-Like Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.08396v3": {
            "Paper Title": "Keep Doing What Worked: Behavioral Modelling Priors for Offline\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.09641v1": {
            "Paper Title": "Automatic Curriculum Learning through Value Disagreement",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.09564v1": {
            "Paper Title": "ShieldNN: A Provably Safe NN Filter for Unsafe NN Controllers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.06289v2": {
            "Paper Title": "3D Dynamic Scene Graphs: Actionable Spatial Perception with Places,\n  Objects, and Humans",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.09540v1": {
            "Paper Title": "COLREG-Compliant Collision Avoidance for Unmanned Surface Vehicle using\n  Deep Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.09117v1": {
            "Paper Title": "End-to-End Real-time Catheter Segmentation with Optical Flow-Guided\n  Warping during Endovascular Intervention",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.09034v1": {
            "Paper Title": "Deep Learning based Segmentation of Fish in Noisy Forward Looking MBES\n  Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.06636v2": {
            "Paper Title": "Catch & Carry: Reusable Neural Controllers for Vision-Guided Whole-Body\n  Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.09001v1": {
            "Paper Title": "RL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.08903v1": {
            "Paper Title": "Depth by Poking: Learning to Estimate Depth from Self-Supervised\n  Grasping",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": ", where a model with parameters \u0012\nmaps input images IandDto an output image ^Z. FCNs ",
                    "Citation Text": "J. Long, E. Shelhamer, and T. Darrell, \u201cFully convolutional networks\nfor semantic segmentation,\u201d in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , 2015, pp. 3431\u20133440.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1411.4038",
                        "Citation Paper Title": "Title:Fully Convolutional Networks for Semantic Segmentation",
                        "Citation Paper Abstract": "Abstract:Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build \"fully convolutional\" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.",
                        "Citation Paper Authors": "Authors:Jonathan Long, Evan Shelhamer, Trevor Darrell"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2006.08383v2": {
            "Paper Title": "Pixel Invisibility: Detecting Objects Invisible in Color Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.09093v3": {
            "Paper Title": "The Surprising Effectiveness of Linear Models for Visual Foresight in\n  Object Pile Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.09539v2": {
            "Paper Title": "Deep urban unaided precise Global Navigation Satellite System vehicle\n  positioning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.08547v1": {
            "Paper Title": "Visibility Guided NMS: Efficient Boosting of Amodal Object Detection in\n  Crowded Traffic Scenes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.08373v1": {
            "Paper Title": "Self-supervised Learning for Precise Pick-and-place without Object Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.06660v4": {
            "Paper Title": "What happens to a ToF LiDAR in fog?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.08155v1": {
            "Paper Title": "Selection of an Integrated Security Area for locating a State Military\n  Organization (SMO) based on group decision system: a multicriteria approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.08153v1": {
            "Paper Title": "Intelligent Decision Support System for Updating Control Plans",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.08045v1": {
            "Paper Title": "Design of a sensing module for a kiwifruit flower pollinator robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.07969v1": {
            "Paper Title": "Coordinated Control of UAVs for Human-Centered Active Sensing of\n  Wildfires",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.07864v1": {
            "Paper Title": "Cityscapes 3D: Dataset and Benchmark for 9 DoF Vehicle Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.07518v1": {
            "Paper Title": "Decentralized decision making and navigation strategy for tracking\n  intruders in a cluttered area by a group of mobile robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.07508v1": {
            "Paper Title": "Realistic Physics Based Character Controller",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.07412v1": {
            "Paper Title": "BI-MAML: Balanced Incremental Approach for Meta Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.07544v2": {
            "Paper Title": "Planning with Abstract Learned Models While Learning Transferable\n  Subtasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.07244v1": {
            "Paper Title": "Algorithmic Design for Embodied Intelligence in Synthetic Cells",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.07164v1": {
            "Paper Title": "ESAD: Endoscopic Surgeon Action Detection Dataset",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.09436v1": {
            "Paper Title": "SAMBA: Safe Model-Based & Active Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.07022v1": {
            "Paper Title": "A Study of the Minimum Safe Distance between Human Driven and Driverless\n  Cars Using Safe Distance Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.04698v2": {
            "Paper Title": "Improving Place Recognition Using Dynamic Object Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.05067v3": {
            "Paper Title": "Fast Generation of High Fidelity RGB-D Images by Deep-Learning with\n  Adaptive Convolution",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.06247v2": {
            "Paper Title": "Improving Movement Predictions of Traffic Actors in Bird's-Eye View\n  Models using GANs and Differentiable Trajectory Rasterization",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": ", which results in a much\nmore dif\ufb01cult task for the discriminator as we con\ufb01rm in\nSection IV. We employ the fully-convolutional DCGAN\narchitecture ",
                    "Citation Text": "A. Radford, L. Metz, and S. Chintala, \u201cUnsupervised representation\nlearning with deep convolutional generative adversarial networks,\u201d\narXiv preprint arXiv:1511.06434 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06434",
                        "Citation Paper Title": "Title:Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.",
                        "Citation Paper Authors": "Authors:Alec Radford, Luke Metz, Soumith Chintala"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1912.00416v3": {
            "Paper Title": "LatentFusion: End-to-End Differentiable Reconstruction and Rendering for\n  Unseen Object Pose Estimation",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": ",\nthen recovering the pose of the object using the predictions.\nThe third category estimates the pose of objects by aligning\nthe rendering of the 3D model to the image. DeepIM ",
                    "Citation Text": "Yi Li, Gu Wang, Xiangyang Ji, Yu Xiang, and Dieter Fox.\nDeepIM: Deep iterative matching for 6D pose estimation.\nInEuropean Conference on Computer Vision (ECCV) , pages\n683\u2013698, 2018. 2, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.00175",
                        "Citation Paper Title": "Title:DeepIM: Deep Iterative Matching for 6D Pose Estimation",
                        "Citation Paper Abstract": "Abstract:Estimating the 6D pose of objects from images is an important problem in various applications such as robot manipulation and virtual reality. While direct regression of images to object poses has limited accuracy, matching rendered images of an object against the observed image can produce accurate results. In this work, we propose a novel deep neural network for 6D pose matching named DeepIM. Given an initial pose estimation, our network is able to iteratively refine the pose by matching the rendered image against the observed image. The network is trained to predict a relative pose transformation using an untangled representation of 3D location and 3D orientation and an iterative training process. Experiments on two commonly used benchmarks for 6D pose estimation demonstrate that DeepIM achieves large improvements over state-of-the-art methods. We furthermore show that DeepIM is able to match previously unseen objects.",
                        "Citation Paper Authors": "Authors:Yi Li, Gu Wang, Xiangyang Ji, Yu Xiang, Dieter Fox"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2006.06874v1": {
            "Paper Title": "Learning to Play by Imitating Humans",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.06861v1": {
            "Paper Title": "Robustness to Adversarial Attacks in Learning-Enabled Controllers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.06753v1": {
            "Paper Title": "PRGFlow: Benchmarking SWAP-Aware Unified Deep Visual Inertial Odometry",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.06715v1": {
            "Paper Title": "Data Driven Prediction Architecture for Autonomous Driving and its\n  Application on Apollo Platform",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.06620v1": {
            "Paper Title": "From proprioception to long-horizon planning in novel environments: A\n  hierarchical RL model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.06431v1": {
            "Paper Title": "Complementary Visual Neuronal Systems Model for Collision Sensing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.06314v1": {
            "Paper Title": "Geometric and Stiffness Modeling and Design of Calibration Experiments\n  for the 7 dof Serial Manipulator KUKA iiwa 14 R820",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.05768v2": {
            "Paper Title": "Deep Drone Acrobatics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.05289v2": {
            "Paper Title": "Creating a Robot Coach for Mindfulness and Wellbeing: A Longitudinal\n  Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.00314v2": {
            "Paper Title": "\"Closed Proportional-Integral-Derivative-Loop Model\" Following Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.05935v1": {
            "Paper Title": "Learning to Play Table Tennis From Scratch using Muscular Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.05729v1": {
            "Paper Title": "A Bayesian Framework for Nash Equilibrium Inference in Human-Robot\n  Parallel Play",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.05725v1": {
            "Paper Title": "Bayesian Experience Reuse for Learning from Multiple Demonstrators",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.00203v2": {
            "Paper Title": "Contextual Policy Transfer in Reinforcement Learning Domains via Deep\n  Mixtures-of-Experts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.08477v3": {
            "Paper Title": "Optimally Guarding Perimeters and Regions with Mobile Range Sensors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.05239v1": {
            "Paper Title": "A Smooth Robustness Measure of Signal Temporal Logic for Symbolic\n  Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.05137v1": {
            "Paper Title": "Precise Robot Localization in Architectural 3D Plans",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.04082v2": {
            "Paper Title": "End-to-end Learning for Inter-Vehicle Distance and Relative Velocity\n  Estimation in ADAS with a Monocular Camera",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.04989v1": {
            "Paper Title": "Synchronous Robotic Framework",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.04973v1": {
            "Paper Title": "Pixel-Wise Motion Deblurring of Thermal Videos",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.01544v2": {
            "Paper Title": "\"Can you do this?\" Self-Assessment Dialogues with Autonomous Robots\n  Before, During, and After a Mission",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.04765v1": {
            "Paper Title": "An Astrocyte-Modulated Neuromorphic Central Pattern Generator for\n  Hexapod Robot Locomotion on Intel's Loihi",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.11142v3": {
            "Paper Title": "CAGE: Context-Aware Grasping Engine",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.04356v1": {
            "Paper Title": "Associate-3Ddet: Perceptual-to-Conceptual Association for 3D Point Cloud\n  Object Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.11300v1": {
            "Paper Title": "From Demonstrations to Task-Space Specifications: Using Causal Analysis\n  to Extract Rule Parameterization from Demonstrations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.00697v3": {
            "Paper Title": "Translating Natural Language Instructions for Behavioral Robot\n  Navigation with a Multi-Head Attention Mechanism",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.04271v1": {
            "Paper Title": "Multi-Task Reinforcement Learning based Mobile Manipulation Control for\n  Dynamic Object Tracking and Grasping",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01636v2": {
            "Paper Title": "Self-supervised learning for autonomous vehicles perception: A\n  conciliation between analytical and learning methods",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.10541v2": {
            "Paper Title": "Search and Rescue under the Forest Canopy using Multiple UAVs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.04227v1": {
            "Paper Title": "Subterranean MAV Navigation based on Nonlinear MPC with Collision\n  Avoidance Constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.09094v2": {
            "Paper Title": "Self-supervised classification of dynamic obstacles using the temporal\n  information provided by videos",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.04225v1": {
            "Paper Title": "Unsupervised Learning for Subterranean Junction Recognition Based on 2D\n  Point Cloud",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.04223v1": {
            "Paper Title": "MAV Navigation in Unknown Dark Underground Mines Using Deep Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.04103v1": {
            "Paper Title": "An Autonomous Path Planning Method for Unmanned Aerial Vehicle based on\n  A Tangent Intersection and Target Guidance Strategy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.04072v1": {
            "Paper Title": "Implications of Human Irrationality for Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.04053v1": {
            "Paper Title": "A Novel Grip Force Measurement Concept for Tactile Stimulation\n  Mechanisms -- Design, Validation, and User Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.03826v1": {
            "Paper Title": "A Control Method to Compensate Ground Level Changes for Running Bipedal\n  Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.03784v1": {
            "Paper Title": "A ROS-based Framework for Monitoring Human and Robot Conditions in a\n  Human-Multi-robot Team",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.03618v2": {
            "Paper Title": "Efficient Black-box Assessment of Autonomous Vehicle Safety",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.03636v1": {
            "Paper Title": "Hybrid Control for Learning Motor Skills",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.03537v1": {
            "Paper Title": "A Soft Humanoid Hand with In-Finger Visual Perception",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.03492v1": {
            "Paper Title": "VALUE: Large Scale Voting-based Automatic Labelling for Urban\n  Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.03486v1": {
            "Paper Title": "Segmentation of Surgical Instruments for Minimally-Invasive\n  Robot-Assisted Procedures Using Generative Deep Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.03201v1": {
            "Paper Title": "Egocentric Object Manipulation Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.03169v1": {
            "Paper Title": "Fast CRDNN: Towards on Site Training of Mobile Construction Machines",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.03015v3": {
            "Paper Title": "Learning to Correspond Dynamical Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.01587v2": {
            "Paper Title": "Deep Learning Tubes for Tube MPC",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.04515v1": {
            "Paper Title": "Deep Learning for Posture Control Nonlinear Model System and Noise\n  Identification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.07584v2": {
            "Paper Title": "Reinforcement Learning for Safety-Critical Control under Model\n  Uncertainty, using Control Lyapunov Functions and Control Barrier Functions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.02996v1": {
            "Paper Title": "Manipulation with Shared Grasping",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.12200v3": {
            "Paper Title": "Scaling data-driven robotics with reward sketching and batch\n  reinforcement learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.02636v1": {
            "Paper Title": "The Importance of Prior Knowledge in Precise Multimodal Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.02575v2": {
            "Paper Title": "Active Preference-Based Gaussian Process Regression for Reward Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.11641v5": {
            "Paper Title": "ARCSnake: An Archimedes' Screw-Propelled, Reconfigurable Robot Snake for\n  Complex Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.02116v1": {
            "Paper Title": "Aerial Manipulation Using Hybrid Force and Position NMPC Applied to\n  Aerial Writing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.02098v1": {
            "Paper Title": "GFPNet: A Deep Network for Learning Shape Completion in Generic Fitted\n  Primitives",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.09998v3": {
            "Paper Title": "Learning Resilient Behaviors for Navigation Under Uncertainty",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.02008v1": {
            "Paper Title": "Kernel Taylor-Based Value Function Approximation for Continuous-State\n  Markov Decision Processes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.00176v2": {
            "Paper Title": "When2com: Multi-Agent Perception via Communication Graph Grouping",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.11058v3": {
            "Paper Title": "Robust Distributed Planar Formation Control for Higher-Order Holonomic\n  and Nonholonomic Agents",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.09171v3": {
            "Paper Title": "Planning Beyond the Sensing Horizon Using a Learned Context",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.07274v2": {
            "Paper Title": "Bi3D: Stereo Depth Estimation via Binary Classifications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.15975v1": {
            "Paper Title": "Robots and COVID-19: Challenges in integrating robots for collaborative\n  automation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.00857v1": {
            "Paper Title": "3D Lidar Mapping Relative Accuracy Automatic Evaluation Algorithm",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.00811v1": {
            "Paper Title": "Time Variable Minimum Torque Trajectory Optimization for Autonomous\n  Excavator",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.00589v1": {
            "Paper Title": "Deep R-Learning for Continual Area Sweeping",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.00545v1": {
            "Paper Title": "Motion2Vec: Semi-Supervised Representation Learning from Surgical Videos",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.12888v2": {
            "Paper Title": "Imitation Learning as $f$-Divergence Minimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.00424v1": {
            "Paper Title": "HMPO: Human Motion Prediction in Occluded Environments for Safe Motion\n  Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.00420v1": {
            "Paper Title": "VIR-SLAM: Visual, Inertial, and Ranging SLAM for single and multi-robot\n  systems",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": ", which is not applicable for navigation\nin unexplored, unstructured environments. Wang et al. ",
                    "Citation Text": "C. Wang, H. Zhang, T.-M. Nguyen, and L. Xie, \u201cUltra-wideband aided\nfast localization and mapping system,\u201d in 2017 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS) . IEEE, 2017,\npp. 1602\u20131609.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.00156",
                        "Citation Paper Title": "Title:Ultra-Wideband Aided Fast Localization and Mapping System",
                        "Citation Paper Abstract": "Abstract:This paper proposes an ultra-wideband (UWB) aided localization and mapping system that leverages on inertial sensor and depth camera. Inspired by the fact that visual odometry (VO) system, regardless of its accuracy in the short term, still faces challenges with accumulated errors in the long run or under unfavourable environments, the UWB ranging measurements are fused to remove the visual drift and improve the robustness. A general framework is developed which consists of three parallel threads, two of which carry out the visual-inertial odometry (VIO) and UWB localization respectively. The other mapping thread integrates visual tracking constraints into a pose graph with the proposed smooth and virtual range constraints, such that an optimization is performed to provide robust trajectory estimation. Experiments show that the proposed system is able to create dense drift-free maps in real-time even running on an ultra-low power processor in featureless environments.",
                        "Citation Paper Authors": "Authors:Chen Wang, Handuo Zhang, Thien-Minh Nguyen, Lihua Xie"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2006.00369v1": {
            "Paper Title": "Thinging-Oriented Modeling of Unmanned Aerial Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.01330v5": {
            "Paper Title": "Robotic Pick-and-Place of Novel Objects in Clutter with Multi-Affordance\n  Grasping and Cross-Domain Image Matching",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.11239v3": {
            "Paper Title": "TossingBot: Learning to Throw Arbitrary Objects with Residual Physics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.00168v1": {
            "Paper Title": "Optical Flow based Visual Potential Field for Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.13006v3": {
            "Paper Title": "Anytime Integrated Task and Motion Policies for Stochastic Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.00037v1": {
            "Paper Title": "Human-Centric Active Perception for Autonomous Observation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.00025v1": {
            "Paper Title": "Environmental regulation using Plasticoding for the evolution of robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.14703v1": {
            "Paper Title": "Data-Driven Convergence Prediction of Astrobots Swarms",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.07973v2": {
            "Paper Title": "On Simple Reactive Neural Networks for Behaviour-Based Reinforcement\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.14502v1": {
            "Paper Title": "Unconstrained Matching of 2D and 3D Descriptors for 6-DOF Pose\n  Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2006.01233v1": {
            "Paper Title": "Hibikino-Musashi@Home 2019 Team Description Paper",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.14451v1": {
            "Paper Title": "Hibikino-Musashi@Home 2020 Team Description Paper",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.11290v2": {
            "Paper Title": "An Energy-based Approach to Ensure the Stability of Learned Dynamical\n  Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01808v3": {
            "Paper Title": "Estimating Lower Limb Kinematics using a Lie Group Constrained EKF and a\n  Reduced Wearable IMU Count",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.14406v1": {
            "Paper Title": "Non-Linearity Measure for POMDP-based Motion Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.14401v1": {
            "Paper Title": "Sim2Real for Peg-Hole Insertion with Eye-in-Hand Camera",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.14391v1": {
            "Paper Title": "Stochastic Modeling of Distance to Collision for Robot Manipulators",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.14347v1": {
            "Paper Title": "An Observer Design for Visual Simultaneous Localisation and Mapping with\n  Output Equivariance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.12256v2": {
            "Paper Title": "Neural Topological SLAM for Visual Navigation",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": ".\nMetric Spatial Map + RL. An end-to-end RL model which\nuses geometric projections of the depth image to create a\nlocal map and passes it to the RL policy, adapted from Chen\net al. ",
                    "Citation Text": "Tao Chen, Saurabh Gupta, and Abhinav Gupta. Learning\nexploration policies for navigation. In International Confer-\nence on Learning Representations , 2019. 2, 6, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.01959",
                        "Citation Paper Title": "Title:Learning Exploration Policies for Navigation",
                        "Citation Paper Abstract": "Abstract:Numerous past works have tackled the problem of task-driven navigation. But, how to effectively explore a new environment to enable a variety of down-stream tasks has received much less attention. In this work, we study how agents can autonomously explore realistic and complex 3D environments without the context of task-rewards. We propose a learning-based approach and investigate different policy architectures, reward functions, and training paradigms. We find that the use of policies with spatial memory that are bootstrapped with imitation learning and finally finetuned with coverage rewards derived purely from on-board sensors can be effective at exploring novel environments. We show that our learned exploration policies can explore better than classical approaches based on geometry alone and generic learning-based exploration techniques. Finally, we also show how such task-agnostic exploration can be used for down-stream tasks. Code and Videos are available at: this https URL.",
                        "Citation Paper Authors": "Authors:Tao Chen, Saurabh Gupta, Abhinav Gupta"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2005.14250v1": {
            "Paper Title": "Low-Cost Fiducial-based 6-Axis Force-Torque Sensor",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.14156v1": {
            "Paper Title": "Unlucky Explorer: A Complete non-Overlapping Map Exploration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.13759v1": {
            "Paper Title": "Stereo Vision Based Single-Shot 6D Object Pose Estimation for\n  Bin-Picking by a Robot Manipulator",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.13704v1": {
            "Paper Title": "Graph-based Proprioceptive Localization Using a Discrete Heading-Length\n  Feature Sequence Matching Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.14398v2": {
            "Paper Title": "Robotic Table Tennis with Model-Free Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.08189v2": {
            "Paper Title": "MOPT: Multi-Object Panoptic Tracking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.13139v1": {
            "Paper Title": "Predictive Modeling of Periodic Behavior for Human-Robot Symbiotic\n  Walking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.13133v1": {
            "Paper Title": "Robust Trajectory Forecasting for Multiple Intelligent Agents in Dynamic\n  Scene",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.13131v1": {
            "Paper Title": "Efficient Pig Counting in Crowds with Keypoints Tracking and\n  Spatial-aware Temporal Response Filtering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.10872v2": {
            "Paper Title": "Guided Uncertainty-Aware Policy Optimization: Combining Learning and\n  Model-Based Strategies for Sample-Efficient Policy Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.09417v2": {
            "Paper Title": "Pass-Fail Criteria for Scenario-Based Testing of Automated Driving\n  Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.12651v1": {
            "Paper Title": "What the HoloLens Maps Is Your Workspace: Fast Mapping and Set-up of\n  Robot Cells via Head Mounted Displays and Augmented Reality",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.12599v1": {
            "Paper Title": "Adaptive Robot Navigation with Collision Avoidance subject to 2nd-order\n  Uncertain Dynamics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.12551v1": {
            "Paper Title": "Keep it Simple: Image Statistics Matching for Domain Adaptation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.11794v2": {
            "Paper Title": "Vision-based control of a knuckle boom crane with online cable length\n  estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.12508v1": {
            "Paper Title": "Learning Whole-Body Human-Robot Haptic Interaction in Social Contexts",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.12316v3": {
            "Paper Title": "Preference-Based Learning for Exoskeleton Gait Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.02553v4": {
            "Paper Title": "A Closer Look at Deep Policy Gradients",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.05449v2": {
            "Paper Title": "CIAO$^\\star$: MPC-based Safe Motion Planning in Predictable Dynamic\n  Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.11895v1": {
            "Paper Title": "Reinforcement Learning with Iterative Reasoning for Merging in Dense\n  Traffic",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.11810v1": {
            "Paper Title": "Learning visual servo policies via planner cloning",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": ". Viereck et al.\ndo something similar using an L1 loss term ",
                    "Citation Text": "Ulrich Viereck, Andreas ten Pas, Kate Saenko, and Rob\nPlatt. Learning a visuomotor controller for real world\nrobotic grasping using easily simulated depth images.\nCoRR , abs/1706.04652, 2017. URL http://arxiv.org/abs/\n1706.04652.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.04652",
                        "Citation Paper Title": "Title:Learning a visuomotor controller for real world robotic grasping using simulated depth images",
                        "Citation Paper Abstract": "Abstract:We want to build robots that are useful in unstructured real world applications, such as doing work in the household. Grasping in particular is an important skill in this domain, yet it remains a challenge. One of the key hurdles is handling unexpected changes or motion in the objects being grasped and kinematic noise or other errors in the robot. This paper proposes an approach to learning a closed-loop controller for robotic grasping that dynamically guides the gripper to the object. We use a wrist-mounted sensor to acquire depth images in front of the gripper and train a convolutional neural network to learn a distance function to true grasps for grasp configurations over an image. The training sensor data is generated in simulation, a major advantage over previous work that uses real robot experience, which is costly to obtain. Despite being trained in simulation, our approach works well on real noisy sensor images. We compare our controller in simulated and real robot experiments to a strong baseline for grasp pose detection, and find that our approach significantly outperforms the baseline in the presence of kinematic noise, perceptual errors and disturbances of the object during grasping.",
                        "Citation Paper Authors": "Authors:Ulrich Viereck, Andreas ten Pas, Kate Saenko, Robert Platt"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2004.12051v2": {
            "Paper Title": "GPO: Global Plane Optimization for Fast and Accurate Monocular SLAM\n  Initialization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.11470v1": {
            "Paper Title": "Learning from Naturalistic Driving Data for Human-like Autonomous\n  Highway Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.11445v1": {
            "Paper Title": "Evaluation of Non-Collocated Force Feedback Driven by Signal-Independent\n  Noise",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.14404v2": {
            "Paper Title": "Meta-Reinforcement Learning for Robotic Industrial Insertion Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.13976v1": {
            "Paper Title": "Towards Automated Safety Coverage and Testing for Autonomous Vehicles\n  with Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.11267v1": {
            "Paper Title": "Givenness Hierarchy Theoretic Cognitive Status Filtering",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.05850v3": {
            "Paper Title": "Rapid Probabilistic Interest Learning from Domain-Specific Pairwise\n  Image Comparisons",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.10994v1": {
            "Paper Title": "Abstractions for computing all robotic sensors that suffice to solve a\n  planning problem",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.07093v4": {
            "Paper Title": "BARNet: Bilinear Attention Network with Adaptive Receptive Fields for\n  Surgical Instrument Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.01291v2": {
            "Paper Title": "On the Utility of Model Learning in HRI",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.10622v2": {
            "Paper Title": "Triple-GAIL: A Multi-Modal Imitation Learning Framework with Generative\n  Adversarial Nets",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.13534v1": {
            "Paper Title": "Robot-assisted Backscatter Localization for IoT Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.09641v2": {
            "Paper Title": "A Second-Order Lower Bound for Globally Optimal 2D Registration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.11669v2": {
            "Paper Title": "Learning Task-Oriented Grasping from Human Activity Datasets",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.03628v2": {
            "Paper Title": "6-DOF Grasping for Target-driven Object Manipulation in Clutter",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.10381v1": {
            "Paper Title": "MDPs with Unawareness in Robotics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.09260v3": {
            "Paper Title": "4D Generic Video Object Proposals",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.09999v1": {
            "Paper Title": "Model Predictive Instantaneous Safety Metric for Evaluation of Automated\n  Driving Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.09968v1": {
            "Paper Title": "Development of a Shape-memorable Adaptive Pin Array Fixture",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.06193v2": {
            "Paper Title": "From Videos to URLs: A Multi-Browser Guide To Extract User's Behavior\n  with Optical Character Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.07792v2": {
            "Paper Title": "GRIP++: Enhanced Graph-based Interaction-aware Trajectory Prediction for\n  Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.11228v3": {
            "Paper Title": "Compositional Transfer in Hierarchical Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.08376v7": {
            "Paper Title": "LQG Control and Sensing Co-Design",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.09530v1": {
            "Paper Title": "Differentiable Mapping Networks: Learning Structured Map Representations\n  for Sparse Visual Localization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.09484v1": {
            "Paper Title": "Self-supervised Transfer Learning for Instance Segmentation through\n  Physical Interaction",
            "Sentences": [
                {
                    "Sentence ID": 33,
                    "Sentence": "learn to seg-\nment objects by tracking their movement, but only consider\nsingle object videos that are passively observed. Milan et\nal. ",
                    "Citation Text": "A. Milan, T. Pham, K. Vijay, D. Morrison, A. Tow, L. Liu, J. Erskine,\nR. Grinover, A. Gurman, T. Hunn, N. Kelly-Boxall, D. Lee, M. Mc-\nTaggart, G. Rallos, A. Razjigaev, T. Rowntree, T. Shen, R. Smith,\nS. Wade-McCue, Z. Zhuang, C. Lehnert, G. Lin, I. Reid, P. Corke,\nand J. Leitner, \u201cSemantic Segmentation from Limited Training Data,\u201d\nin2018 IEEE International Conference on Robotics and Automation\n(ICRA) , May 2018, pp. 1908\u20131915, iSSN: 2577-087X.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.07665",
                        "Citation Paper Title": "Title:Semantic Segmentation from Limited Training Data",
                        "Citation Paper Abstract": "Abstract:We present our approach for robotic perception in cluttered scenes that led to winning the recent Amazon Robotics Challenge (ARC) 2017. Next to small objects with shiny and transparent surfaces, the biggest challenge of the 2017 competition was the introduction of unseen categories. In contrast to traditional approaches which require large collections of annotated data and many hours of training, the task here was to obtain a robust perception pipeline with only few minutes of data acquisition and training time. To that end, we present two strategies that we explored. One is a deep metric learning approach that works in three separate steps: semantic-agnostic boundary detection, patch classification and pixel-wise voting. The other is a fully-supervised semantic segmentation approach with efficient dataset collection. We conduct an extensive analysis of the two methods on our ARC 2017 dataset. Interestingly, only few examples of each class are sufficient to fine-tune even very deep convolutional neural networks for this specific task.",
                        "Citation Paper Authors": "Authors:A. Milan, T. Pham, K. Vijay, D. Morrison, A.W. Tow, L. Liu, J. Erskine, R. Grinover, A. Gurman, T. Hunn, N. Kelly-Boxall, D. Lee, M. McTaggart, G. Rallos, A. Razjigaev, T. Rowntree, T. Shen, R. Smith, S. Wade-McCue, Z. Zhuang, C. Lehnert, G. Lin, I. Reid, P. Corke, J. Leitner"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2005.09476v1": {
            "Paper Title": "Learning to Herd Agents Amongst Obstacles: Training Robust Shepherding\n  Behaviors using Deep Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.10706v3": {
            "Paper Title": "Interactive Differentiable Simulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.02805v2": {
            "Paper Title": "KeyPose: Multi-View 3D Labeling and Keypoint Estimation for Transparent\n  Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.05730v3": {
            "Paper Title": "VeREFINE: Integrating Object Pose Verification with Physics-guided\n  Iterative Refinement",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.10269v2": {
            "Paper Title": "Design and Autonomous Stabilization of a Ballistically Launched\n  Multirotor",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": ". For our visual inertial odometry\npipeline, we utilize the open-source Robust Visual Inertial\nOdometry (ROVIO), an extended Kalman Filter that tracks\nboth 3D landmarks and image patch features ",
                    "Citation Text": "M. Bloesch, M. Burri, S. Omari, M. Hutter, and R. Siegwart, \u201cIterated\nextended Kalman \ufb01lter based visual-inertial odometry using direct pho-\ntometric feedback,\u201d The International Journal of Robotics Research ,\nvol. 36, no. 10, pp. 1053\u20131072, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.05285",
                        "Citation Paper Title": "Title:A Primer on the Differential Calculus of 3D Orientations",
                        "Citation Paper Abstract": "Abstract:The proper handling of 3D orientations is a central element in many optimization problems in engineering. Unfortunately many researchers and engineers struggle with the formulation of such problems and often fall back to suboptimal solutions. The existence of many different conventions further complicates this issue, especially when interfacing multiple differing implementations. This document discusses an alternative approach which makes use of a more abstract notion of 3D orientations. The relative orientation between two coordinate systems is primarily identified by the coordinate mapping it induces. This is combined with the standard exponential map in order to introduce representation-independent and minimal differentials, which are very convenient in optimization based methods.",
                        "Citation Paper Authors": "Authors:Michael Bloesch, Hannes Sommer, Tristan Laidlow, Michael Burri, Gabriel Nuetzi, P\u00e9ter Fankhauser, Dario Bellicoso, Christian Gehring, Stefan Leutenegger, Marco Hutter, Roland Siegwart"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2005.08351v1": {
            "Paper Title": "On Efficient Connectivity-Preserving Transformations in a Grid",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.08153v1": {
            "Paper Title": "Coordinated Coverage and Fault Tolerance using Fixed-Wing Unmanned\n  Aerial Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.13675v2": {
            "Paper Title": "Form2Fit: Learning Shape Priors for Generalizable Assembly from\n  Disassembly",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.11792v5": {
            "Paper Title": "Maximum Causal Entropy Specification Inference from Demonstrations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.05953v2": {
            "Paper Title": "Optimized Synthesis of Snapping Fixtures",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.07832v1": {
            "Paper Title": "Model-based Randomness Monitor for Stealthy Sensor Attacks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.13402v8": {
            "Paper Title": "Safety Augmented Value Estimation from Demonstrations (SAVED): Safe Deep\n  Model-Based RL for Sparse Cost Robotic Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.01410v2": {
            "Paper Title": "ABC-LMPC: Safe Sample-Based Learning MPC for Stochastic Nonlinear\n  Dynamical Systems with Adjustable Boundary Conditions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.03423v7": {
            "Paper Title": "On-Policy Robot Imitation Learning from a Converging Supervisor",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.07821v1": {
            "Paper Title": "Memoryless Cumulative Sign Detector for Stealthy CPS Sensor Attacks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.10120v2": {
            "Paper Title": "Generating Robust Supervision for Learning-Based Visual Navigation Using\n  Hamilton-Jacobi Reachability",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.07541v1": {
            "Paper Title": "Simple Sensor Intentions for Exploration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.07513v1": {
            "Paper Title": "A Distributional View on Multi-Objective Policy Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.07443v1": {
            "Paper Title": "Excursion Search for Constrained Bayesian Optimization under a Limited\n  Budget of Failures",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.07695v1": {
            "Paper Title": "How to Close Sim-Real Gap? Transfer with Segmentation!",
            "Sentences": []
        },
        "http://arxiv.org/abs/1709.08826v2": {
            "Paper Title": "Sensing-Constrained LQG Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.06760v1": {
            "Paper Title": "Physical Human-Robot Interaction with a Tethered Aerial Vehicle:\n  Application to a Force-based Human Guiding Problem",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.06694v1": {
            "Paper Title": "Safe Robot Navigation in Cluttered Environments using Invariant\n  Ellipsoids and a Reference Governor",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.03558v3": {
            "Paper Title": "FASTER: Fast and Safe Trajectory Planner for Flights in Unknown\n  Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.06594v1": {
            "Paper Title": "Action Image Representation: Learning Scalable Deep Grasping Policies\n  with Zero Real World Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.06582v1": {
            "Paper Title": "Pedestrian Action Anticipation using Contextual Feature Fusion in\n  Stacked RNNs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.05842v2": {
            "Paper Title": "A Survey of Behavior Trees in Robotics and AI",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.07495v1": {
            "Paper Title": "Local Gathering of Mobile Robots in Three Dimensions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.06223v1": {
            "Paper Title": "DREAM Architecture: a Developmental Approach to Open-Ended Learning in\n  Robotics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.12650v2": {
            "Paper Title": "Multiple quadrotors carrying a flexible hose: dynamics, differential\n  flatness and control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.05940v1": {
            "Paper Title": "Trust Considerations for Explainable Robots: A Human Factors Perspective",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.05445v1": {
            "Paper Title": "Polyrhythmic Bimanual Coordination Training using Haptic Force Feedback",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.05421v1": {
            "Paper Title": "Inferring Obstacles and Path Validity from Visibility-Constrained\n  Demonstrations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.05410v1": {
            "Paper Title": "Identifying Mechanical Models through Differentiable Simulations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.10475v2": {
            "Paper Title": "String Diagrams for Assembly Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.05269v1": {
            "Paper Title": "Deep-Learning-based Automated Palm Tree Counting and Geolocation in\n  Large Farms from Aerial Geotagged Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.03210v2": {
            "Paper Title": "Shared Autonomy with Learned Latent Actions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.03076v2": {
            "Paper Title": "Guided Policy Search Model-based Reinforcement Learning for Urban\n  Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.02648v2": {
            "Paper Title": "Collaborative Localization for Micro Aerial Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.04570v1": {
            "Paper Title": "Belief Rule Based Expert System to Identify the Crime Zones",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.04439v1": {
            "Paper Title": "Automated Failure-Mode Clustering and Labeling for Informed\n  Car-To-Driver Handover in Autonomous Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.04319v1": {
            "Paper Title": "ST-MNIST -- The Spiking Tactile MNIST Neuromorphic Dataset",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.04298v1": {
            "Paper Title": "Attentional Bottleneck: Towards an Interpretable Deep Driving Network",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": "introduced an end-to-mid neural motion generator, which takes Lidar data and\nan HD map as inputs and outputs a future trajectory. This model also detects 3D\nbounding boxes as an intermediate representation. Bansal et al. ",
                    "Citation Text": "Bansal, M., Krizhevsky, A., Ogale, A.: Chau\u000beurnet: Learning to drive by\nimitating the best and synthesizing the worst. RSS (2019)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.03079",
                        "Citation Paper Title": "Title:ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing the Worst",
                        "Citation Paper Abstract": "Abstract:Our goal is to train a policy for autonomous driving via imitation learning that is robust enough to drive a real vehicle. We find that standard behavior cloning is insufficient for handling complex driving scenarios, even when we leverage a perception system for preprocessing the input and a controller for executing the output on the car: 30 million examples are still not enough. We propose exposing the learner to synthesized data in the form of perturbations to the expert's driving, which creates interesting situations such as collisions and/or going off the road. Rather than purely imitating all data, we augment the imitation loss with additional losses that penalize undesirable events and encourage progress -- the perturbations then provide an important signal for these losses and lead to robustness of the learned model. We show that the ChauffeurNet model can handle complex situations in simulation, and present ablation experiments that emphasize the importance of each of our proposed changes and show that the model is responding to the appropriate causal factors. Finally, we demonstrate the model driving a car in the real world.",
                        "Citation Paper Authors": "Authors:Mayank Bansal, Alex Krizhevsky, Abhijit Ogale"
                    },
                    "Keywords": [
                        ",",
                        "generation",
                        "Self-driving",
                        "Motion",
                        "vehicles",
                        "AI",
                        "eXplainable"
                    ]
                }
            ]
        },
        "http://arxiv.org/abs/1904.03182v2": {
            "Paper Title": "Probabilistic Regression of Rotations using Quaternion Averaging and a\n  Deep Multi-Headed Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.05084v1": {
            "Paper Title": "Robot art, in the eye of the beholder?: Personalization through\n  self-disclosure facilitates visual communication of emotions in\n  representational art",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.03809v1": {
            "Paper Title": "A Monte Carlo Approach to Closing the Reality Gap",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.04213v1": {
            "Paper Title": "Cascade Attribute Network: Decomposing Reinforcement Learning Control\n  Policies using Hierarchical Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.03691v1": {
            "Paper Title": "A Hand Motion-guided Articulation and Segmentation Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.11759v3": {
            "Paper Title": "A memory of motion for visual predictive control tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.03342v1": {
            "Paper Title": "Arranging Test Tubes in Racks Using Combined Task and Motion Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.13003v4": {
            "Paper Title": "DualSMC: Tunneling Differentiable Filtering and Planning under\n  Continuous POMDPs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.05178v1": {
            "Paper Title": "DeepRacing: Parameterized Trajectories for Autonomous Racing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.00336v2": {
            "Paper Title": "On-board Deep-learning-based Unmanned Aerial Vehicle Fault Cause\n  Detection and Identification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.10116v3": {
            "Paper Title": "Probabilistic Safety Constraints for Learned High Relative Degree System\n  Dynamics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.04191v1": {
            "Paper Title": "Adaptive Motion Planning with Artificial Potential Fields Using a Prior\n  Path",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.02767v1": {
            "Paper Title": "Maximum Likelihood Methods for Inverse Learning of Optimal Controllers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.04354v2": {
            "Paper Title": "Inference-Based Strategy Alignment for General-Sum Differential Games",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.02632v1": {
            "Paper Title": "Robotic Arm Control and Task Training through Deep Reinforcement\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.02597v1": {
            "Paper Title": "Online Parameter Estimation for Human Driver Behavior Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.02588v1": {
            "Paper Title": "DeepClaw: A Robotic Hardware Benchmarking Platform for Learning Object\n  Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.02233v3": {
            "Paper Title": "LINS: A Lidar-Inertial State Estimator for Robust and Efficient\n  Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.02198v1": {
            "Paper Title": "RadarSLAM: Radar based Large-Scale SLAM in All Weathers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.02031v1": {
            "Paper Title": "Sense-Assess-eXplain (SAX): Building Trust in Autonomous Vehicles in\n  Challenging Real-World Driving Scenarios",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.11027v5": {
            "Paper Title": "nuScenes: A multimodal dataset for autonomous driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.01951v1": {
            "Paper Title": "A Versatile Data-Driven Framework for Model-Independent Control of\n  Continuum Manipulators Interacting With Obstructed Environments With Unknown\n  Geometry and Stiffness",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.12349v3": {
            "Paper Title": "Technical Report: Reactive Semantic Planning in Unexplored Semantic\n  Environments Using Deep Perceptual Feedback",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.01138v1": {
            "Paper Title": "Off-Policy Adversarial Inverse Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.01020v1": {
            "Paper Title": "Investigating the Effects of Robot Engagement Communication on Learning\n  from Demonstration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.00828v1": {
            "Paper Title": "DroTrack: High-speed Drone-based Object Tracking Under Uncertainty",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.00825v1": {
            "Paper Title": "SIGVerse: A cloud-based VR platform for research on social and embodied\n  human-robot interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.07276v2": {
            "Paper Title": "Improving Input-Output Linearizing Controllers for Bipedal Robots via\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.00704v1": {
            "Paper Title": "Automotive-Radar-Based 50-cm Urban Positioning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.02857v2": {
            "Paper Title": "Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous\n  Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.00167v1": {
            "Paper Title": "Information-Collection in Robotic Process Monitoring: An Active\n  Perception Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.00385v1": {
            "Paper Title": "Lie Algebraic Unscented Kalman Filter for Pose Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.14684v1": {
            "Paper Title": "Sim-to-Real Transfer with Incremental Environment Complexity for\n  Reinforcement Learning of Depth-Based Robot Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.07796v4": {
            "Paper Title": "A Comprehensive Review of Shepherding as a Bio-inspired Swarm-Robotics\n  Guidance Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.05477v2": {
            "Paper Title": "Maximum Likelihood Constraint Inference for Inverse Reinforcement\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.14629v1": {
            "Paper Title": "Stealth UAV through Coanda Effect",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.04385v2": {
            "Paper Title": "Visualizing Local Minima in Multi-Robot Motion Planning using Multilevel\n  Morse Theory",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.13997v1": {
            "Paper Title": "End-to-End Design for Self-Reconfigurable Heterogeneous Robotic Swarms",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": ". UWB\nenables accurate localization in multi-robot systems and, in\nparticular, in drones ",
                    "Citation Text": "J. Pe \u02dcna Queralta, C. M. Almansa, F. Schiano, D. Floreano, and T. West-\nerlund, \u201cUwb-based system for uav localization in gnss-denied environ-\nments: Characterization and dataset,\u201d arXiv preprint arXiv:2003.04380 ,\n2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.04380",
                        "Citation Paper Title": "Title:UWB-based system for UAV Localization in GNSS-Denied Environments: Characterization and Dataset",
                        "Citation Paper Abstract": "Abstract:Small unmanned aerial vehicles (UAV) have penetrated multiple domains over the past years. In GNSS-denied or indoor environments, aerial robots require a robust and stable localization system, often with external feedback, in order to fly safely. Motion capture systems are typically utilized indoors when accurate localization is needed. However, these systems are expensive and most require a fixed setup. Recently, visual-inertial odometry and similar methods have advanced to a point where autonomous UAVs can rely on them for localization. The main limitation in this case comes from the environment, as well as in long-term autonomy due to accumulating error if loop closure cannot be performed efficiently. For instance, the impact of low visibility due to dust or smoke in post-disaster scenarios might render the odometry methods inapplicable. In this paper, we study and characterize an ultra-wideband (UWB) system for navigation and localization of aerial robots indoors based on Decawave's DWM1001 UWB node. The system is portable, inexpensive and can be battery powered in its totality. We show the viability of this system for autonomous flight of UAVs, and provide open-source methods and data that enable its widespread application even with movable anchor systems. We characterize the accuracy based on the position of the UAV with respect to the anchors, its altitude and speed, and the distribution of the anchors in space. Finally, we analyze the accuracy of the self-calibration of the anchors' positions.",
                        "Citation Paper Authors": "Authors:Jorge Pe\u00f1a Queralta, Carmen Mart\u00ednez Almansa, Fabrizio Schiano, Dario Floreano, Tomi Westerlund"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1903.02552v3": {
            "Paper Title": "Design of A Two-point Steering Path Planner Using Geometric Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.03089v2": {
            "Paper Title": "Realtime Collision Avoidance for Mobile Robots in Dense Crowds using\n  Implicit Multi-sensor Fusion and Deep Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.13859v1": {
            "Paper Title": "A First Principles Approach for Data-Efficient System Identification of\n  Spring-Rod Systems via Differentiable Physics Engines",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.03691v2": {
            "Paper Title": "Soft-Bubble grippers for robust and perceptive manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.13509v1": {
            "Paper Title": "Related by Similiarity: Poristic Triangles and 3-Periodics in the\n  Elliptic Billiard",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.13497v1": {
            "Paper Title": "A framework for adaptive width control of dense contour-parallel\n  toolpaths in fused deposition modeling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.13358v1": {
            "Paper Title": "Transferable Active Grasping and Real Embodied Dataset",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.08361v2": {
            "Paper Title": "A Data-driven, Falsification-based Model of Human Driver Behavior",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.13194v1": {
            "Paper Title": "Learning for Microrobot Exploration: Model-based Locomotion,\n  Sparse-robust Navigation, and Low-power Deep Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.12974v1": {
            "Paper Title": "Emergent Real-World Robotic Skills via Unsupervised Off-Policy\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.12962v1": {
            "Paper Title": "Social and Emotional Skills Training with Embodied Moxie",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.12873v1": {
            "Paper Title": "Maximum Entropy Multi-Task Inverse RL",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.12729v1": {
            "Paper Title": "Single Shot 6D Object Pose Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.12611v1": {
            "Paper Title": "Continuous hand-eye calibration using 3D points",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.08111v3": {
            "Paper Title": "HCNAF: Hyper-Conditioned Neural Autoregressive Flow and its Application\n  for Probabilistic Occupancy Map Forecasting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.12570v1": {
            "Paper Title": "The Ingredients of Real-World Robotic Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.12526v1": {
            "Paper Title": "Subgoal Planning Algorithm for Autonomous Vehicle Guidance",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.00486v4": {
            "Paper Title": "Impact of Traffic Lights on Trajectory Forecasting of Human-driven\n  Vehicles Near Signalized Intersections",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.12481v1": {
            "Paper Title": "GymFG: A Framework with a Gym Interface for FlightGear",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.09946v3": {
            "Paper Title": "Actively Learning Gaussian Process Dynamics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.03870v3": {
            "Paper Title": "Synthesis of Feedback Controller for Nonlinear Control Systems with\n  Optimal Region of Attraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.14774v1": {
            "Paper Title": "IROS 2019 Lifelong Robotic Vision Challenge -- Lifelong Object\n  Recognition Report",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.12248v1": {
            "Paper Title": "Joint Inference of States, Robot Knowledge, and Human (False-)Beliefs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.06089v4": {
            "Paper Title": "Thinking While Moving: Deep Reinforcement Learning with Concurrent\n  Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.12232v1": {
            "Paper Title": "Reconstruct, Rasterize and Backprop: Dense shape and pose estimation\n  from a single image",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.12188v1": {
            "Paper Title": "On the Generalization Capability of Evolved Counter-propagation\n  Neuro-controllers for Robot Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.11938v1": {
            "Paper Title": "Towards Differentiable Resampling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.10572v2": {
            "Paper Title": "It is time for Factor Graph Optimization for GNSS/INS Integration:\n  Comparison between FGO and EKF",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.09231v4": {
            "Paper Title": "Camera-to-Robot Pose Estimation from a Single Image",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": ", building upon work\nin keypoint detection for human pose estimation ",
                    "Citation Text": "S.-E. Wei, V . Ramakrishna, T. Kanade, and Y . Sheikh, \u201cConvolutional\npose machines,\u201d in CVPR , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.00134",
                        "Citation Paper Title": "Title:Convolutional Pose Machines",
                        "Citation Paper Abstract": "Abstract:Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation. The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference. Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.",
                        "Citation Paper Authors": "Authors:Shih-En Wei, Varun Ramakrishna, Takeo Kanade, Yaser Sheikh"
                    },
                    "Keywords": [
                        ",",
                        "in",
                        "A",
                        "system",
                        "\u201d",
                        "visual",
                        "robust",
                        "and",
                        "\ufb02exible",
                        "\ufb01ducial"
                    ]
                }
            ]
        },
        "http://arxiv.org/abs/2004.11258v1": {
            "Paper Title": "Hybrid Control from Scratch: A Design Methodology for Assured Robotic\n  Missions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.11238v1": {
            "Paper Title": "Learning Constrained Dynamics with Gauss Principle adhering Gaussian\n  Processes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.11171v1": {
            "Paper Title": "Multi-task closed-loop inverse kinematics stability through semidefinite\n  programming",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.10927v1": {
            "Paper Title": "Cooperative Perception with Deep Reinforcement Learning for Connected\n  Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.06746v2": {
            "Paper Title": "Bridging the Gap Between Safety and Real-Time Performance in\n  Receding-Horizon Trajectory Design for Mobile Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.10847v1": {
            "Paper Title": "Enabling Human-Robot Collaboration via Holistic Human Perception and\n  Partner-Aware Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.03003v2": {
            "Paper Title": "Stochastic Optimal Control as Approximate Input Inference",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.10439v1": {
            "Paper Title": "Tactical Decision-Making in Autonomous Driving by Reinforcement Learning\n  with Uncertainty Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.10437v1": {
            "Paper Title": "A fully distributed motion coordination strategy for multi-robot systems\n  with local information",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.10251v1": {
            "Paper Title": "Industrial Robot Grasping with Deep Learning using a Programmable Logic\n  Controller (PLC)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.10016v1": {
            "Paper Title": "Unsupervised Domain Adaptation through Inter-modal Rotation for RGB-D\n  Object Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.09233v1": {
            "Paper Title": "Push and Drag: An Active Obstacle Separation Method for Fruit Harvesting\n  Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.03200v2": {
            "Paper Title": "Practical Reinforcement Learning For MPC: Learning from sparse\n  objectives in under an hour on a real robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.12283v2": {
            "Paper Title": "RCS: a fast path planning algorithm for Unmanned Aerial Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.08763v1": {
            "Paper Title": "Model-Predictive Control via Cross-Entropy and Gradient-Based\n  Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.08752v1": {
            "Paper Title": "Zeus: A System Description of the Two-Time Winner of the Collegiate SAE\n  AutoDrive Competition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.12012v2": {
            "Paper Title": "Deep Stereo using Adaptive Thin Volume Representation with Uncertainty\n  Awareness",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.08648v1": {
            "Paper Title": "Modeling Survival in model-based Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.08368v1": {
            "Paper Title": "Robotic Room Traversal using Optical Range Finding",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.03112v2": {
            "Paper Title": "Accurate Vision-based Manipulation through Contact Reasoning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.08057v1": {
            "Paper Title": "Diversity-based Design Assist for Large Legged Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.08024v1": {
            "Paper Title": "A study of influential factors in designing self-reconfigurable robots\n  for green manufacturing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.07980v1": {
            "Paper Title": "Co-simulation Platform for Developing InfoRich Energy-Efficient\n  Connected and Automated Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.07806v1": {
            "Paper Title": "MobiAxis: An Embodied Learning Task for Teaching Multiplication with a\n  Social Robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.07699v1": {
            "Paper Title": "Predictive Whole-Body Control of Humanoid Robot Locomotion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.04143v2": {
            "Paper Title": "Self-Supervised Monocular Scene Flow Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.07368v1": {
            "Paper Title": "A Study on the Challenges of Using Robotics Simulators for Testing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.05700v2": {
            "Paper Title": "BADGR: An Autonomous Self-Supervised Learning-Based Navigation System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.07197v1": {
            "Paper Title": "Resilience in multi-robot multi-target tracking with unknown number of\n  targets through reconfiguration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01917v2": {
            "Paper Title": "Resilient Coverage: Exploring the Local-to-Global Trade-off",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.07400v1": {
            "Paper Title": "Affordances in Robotic Tasks -- A Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.06858v1": {
            "Paper Title": "Quadrupedal Locomotion via Event-Based Predictive Control and QP-Based\n  Virtual Constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.06856v1": {
            "Paper Title": "Combining Geometric and Information-Theoretic Approaches for Multi-Robot\n  Exploration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.06799v1": {
            "Paper Title": "RoboTHOR: An Open Simulation-to-Real Embodied AI Platform",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.06763v1": {
            "Paper Title": "Parametric Design of Underwater Optical Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.06762v1": {
            "Paper Title": "Autocalibration of a Mobile UWB Localization System for Ad-Hoc\n  Multi-Robot Deployments in GNSS-Denied Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.06684v1": {
            "Paper Title": "Multi-Resolution A*",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.01138v2": {
            "Paper Title": "Disentangling Human Dynamics for Pedestrian Locomotion Forecasting with\n  Noisy Supervision",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.00497v2": {
            "Paper Title": "Just Go with the Flow: Self-Supervised Scene Flow Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.05965v1": {
            "Paper Title": "Distributed Multi-Target Tracking for Autonomous Vehicle Fleets",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.05534v1": {
            "Paper Title": "Online Initialization and Extrinsic Spatial-Temporal Calibration for\n  Monocular Visual-Inertial Odometry",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.02356v2": {
            "Paper Title": "Scalable Synthesis of Minimum-Information Linear-Gaussian Control by\n  Distributed Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.05242v1": {
            "Paper Title": "Simulation-based Lidar Super-resolution for Ground Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.05233v1": {
            "Paper Title": "Shape Estimation for Elongated Deformable Object using B-spline Chained\n  Multiple Random Matrices Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.05155v1": {
            "Paper Title": "Learning to Explore using Active Neural SLAM",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.04976v1": {
            "Paper Title": "A Flexible Connector for Soft Modular Robots Based on Micropatterned\n  Intersurface Jamming",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.04954v1": {
            "Paper Title": "Learning to Visually Navigate in Photorealistic Environments Without any\n  Supervision",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.04851v1": {
            "Paper Title": "Spatial Priming for Detecting Human-Object Interactions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.08333v2": {
            "Paper Title": "Exactly Sparse Gaussian Variational Inference with Application to\n  Derivative-Free Batch Nonlinear State Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.04697v1": {
            "Paper Title": "Learning to Drive Off Road on Smooth Terrain in Unstructured\n  Environments Using an On-Board Camera and Sparse Aerial Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.04428v1": {
            "Paper Title": "Care Robots with Sexual Assistance Functions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.04374v1": {
            "Paper Title": "Co-Robots as Care Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.04336v1": {
            "Paper Title": "MoreFusion: Multi-object Reasoning for 6D Pose Estimation from\n  Volumetric Fusion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.04322v1": {
            "Paper Title": "Quasi-Newton Solver for Robust Non-Rigid Registration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.04227v1": {
            "Paper Title": "Formal Test Synthesis for Safety-Critical Autonomous Systems based on\n  Control Barrier Functions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.11913v2": {
            "Paper Title": "Category-Level Articulated Object Pose Estimation",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": "used a random forest to vote for\npose parameters on canonical body parts for each point\nin a depth image, followed by a variant of the Kabsch\nalgorithm to estimate joint parameters using RANSAC-\nbased energy minimization. Desingh et al. ",
                    "Citation Text": "Karthik Desingh, Shiyang Lu, Anthony Opipari, and\nOdest Chadwicke Jenkins. Factored pose estimation\nof articulated objects using ef\ufb01cient nonparametric\nbelief propagation. arXiv preprint arXiv:1812.03647 ,\n2018. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.03647",
                        "Citation Paper Title": "Title:Factored Pose Estimation of Articulated Objects using Efficient Nonparametric Belief Propagation",
                        "Citation Paper Abstract": "Abstract:Robots working in human environments often encounter a wide range of articulated objects, such as tools, cabinets, and other jointed objects. Such articulated objects can take an infinite number of possible poses, as a point in a potentially high-dimensional continuous space. A robot must perceive this continuous pose to manipulate the object to a desired pose. This problem of perception and manipulation of articulated objects remains a challenge due to its high dimensionality and multi-modal uncertainty. In this paper, we propose a factored approach to estimate the poses of articulated objects using an efficient nonparametric belief propagation algorithm. We consider inputs as geometrical models with articulation constraints, and observed RGBD sensor data. The proposed framework produces object-part pose beliefs iteratively. The problem is formulated as a pairwise Markov Random Field (MRF) where each hidden node (continuous pose variable) is an observed object-part's pose and the edges denote the articulation constraints between the parts. We propose articulated pose estimation by Pull Message Passing algorithm for Nonparametric Belief Propagation (PMPNBP) and evaluate its convergence properties over scenes with articulated objects.",
                        "Citation Paper Authors": "Authors:Karthik Desingh, Shiyang Lu, Anthony Opipari, Odest Chadwicke Jenkins"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1912.09351v2": {
            "Paper Title": "Instance-wise Depth and Motion Learning from Monocular Videos",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.03572v1": {
            "Paper Title": "Disp R-CNN: Stereo 3D Object Detection via Shape Prior Guided Instance\n  Disparity Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.02673v1": {
            "Paper Title": "SHOP-VRB: A Visual Reasoning Benchmark for Object Perception",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.02415v1": {
            "Paper Title": "Postural Stability in Human Running with Step-down Perturbations: An\n  Experimental and Numerical Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.10388v2": {
            "Paper Title": "Higher-Order Function Networks for Learning Composable 3D Object\n  Representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.13369v2": {
            "Paper Title": "A Hamilton-Jacobi Reachability-Based Framework for Predicting and\n  Analyzing Human Motion for Safe Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.10638v2": {
            "Paper Title": "Towards Learning a Generic Agent for Vision-and-Language Navigation via\n  Pre-training",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.12559v2": {
            "Paper Title": "Implementation of Survivor Detection Strategies Using Drones",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.09513v2": {
            "Paper Title": "UrbanLoco: A Full Sensor Suite Dataset for Mapping and Localization in\n  Urban Scenes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.01288v1": {
            "Paper Title": "Extraction and Assessment of Naturalistic Human Driving Trajectories\n  from Infrastructure Camera and Radar Sensors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.01286v1": {
            "Paper Title": "RACE: Reinforced Cooperative Autonomous Vehicle Collision AvoidancE",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.00543v2": {
            "Paper Title": "Physically Realizable Adversarial Examples for LiDAR Object Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.00899v1": {
            "Paper Title": "Go Fetch: Mobile Manipulation in Unstructured Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.00862v1": {
            "Paper Title": "Enabling End-Users to Deploy Flexible Human-Robot Teams to Factories of\n  the Future",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.00801v1": {
            "Paper Title": "Exploration of Reinforcement Learning for Event Camera using Car-like\n  Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.10298v2": {
            "Paper Title": "CoverNet: Multimodal Behavior Prediction using Trajectory Sets",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.00663v1": {
            "Paper Title": "Synchronizing Probability Measures on Rotations via Optimal Transport",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.00605v1": {
            "Paper Title": "EPOS: Estimating 6D Pose of Objects with Symmetries",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.06855v4": {
            "Paper Title": "On the Hardware Feasibility of Nonlinear Trajectory Optimization for\n  Legged Locomotion based on a Simplified Dynamics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.10348v2": {
            "Paper Title": "Monte-Carlo Tree Search for Efficient Visually Guided Rearrangement\n  Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.00530v1": {
            "Paper Title": "Learning Sparse Rewarded Tasks from Sub-Optimal Demonstrations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.00239v1": {
            "Paper Title": "Exponentially Stable First Order Control on Matrix Lie Groups",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.00167v1": {
            "Paper Title": "A Workload Adaptive Haptic Shared Control Scheme for Semi-Autonomous\n  Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.10455v2": {
            "Paper Title": "Looking at the right stuff: Guided semantic-gaze for autonomous driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.14414v1": {
            "Paper Title": "Optical Non-Line-of-Sight Physics-based 3D Human Pose Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.14368v1": {
            "Paper Title": "Enabling Topological Planning with Monocular Vision",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.10011v2": {
            "Paper Title": "Optimization of Operation Strategy for Primary Torque based hydrostatic\n  Drivetrain using Artificial Intelligence",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.13930v1": {
            "Paper Title": "Cross Scene Prediction via Modeling Dynamic Correlation using Latent\n  Space Shared Auto-Encoders",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.01734v2": {
            "Paper Title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday\n  Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.13617v1": {
            "Paper Title": "A Systematic Mapping Study on Blockchain Technology for Digital\n  Protection of Communication with Industrial Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.07999v3": {
            "Paper Title": "Feasible Region: an Actuation-Aware Extension of the Support Region",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.07732v3": {
            "Paper Title": "Biped Stabilization by Linear Feedback of the Variable-Height Inverted\n  Pendulum Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.13150v1": {
            "Paper Title": "Experience Selection Using Dynamics Similarity for Efficient\n  Multi-Source Transfer Learning Between Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.13109v1": {
            "Paper Title": "Scene-Aware Error Modeling of LiDAR/Visual Odometry for Fusion-based\n  Vehicle Localization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.12980v1": {
            "Paper Title": "ClusterVO: Clustering Moving Instances and Estimating Visual Odometry\n  for Self and Surroundings",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.12924v1": {
            "Paper Title": "Optimized Directed Roadmap Graph for Multi-Agent Path Finding Using\n  Stochastic Gradient Descent",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.04903v2": {
            "Paper Title": "Control of Mobile Robots Using Barrier Functions Under Temporal Logic\n  Specifications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.10379v2": {
            "Paper Title": "Moment State Dynamical Systems for Nonlinear Chance-Constrained Motion\n  Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.02693v4": {
            "Paper Title": "3D Packing for Self-Supervised Monocular Depth Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.04553v3": {
            "Paper Title": "Towards Low-Latency High-Bandwidth Control of Quadrotors using Event\n  Cameras",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.12772v1": {
            "Paper Title": "Towards an immersive user interface for waypoint navigation of a mobile\n  robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.12471v1": {
            "Paper Title": "Towards Autonomous Industrial-Scale Bathymetric Surveying",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.12045v1": {
            "Paper Title": "Use the Force, Luke! Learning to Predict Physical Forces by Simulating\n  Effects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.11766v1": {
            "Paper Title": "DeepCrashTest: Turning Dashcam Videos into Virtual Crash Tests for\n  Automated Driving Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.11699v1": {
            "Paper Title": "Functionally Divided Manipulation Synergy for Controlling Multi-fingered\n  Hands",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.08810v2": {
            "Paper Title": "CAPRICORN: Communication Aware Place Recognition using Interpretable\n  Constellations of Objects in Robot Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.11598v1": {
            "Paper Title": "Design, Implementation and Control of an Underactuated Hand Exoskeleton",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.08096v2": {
            "Paper Title": "Autonomous Last-mile Delivery Vehicles in Complex Traffic Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.00197v4": {
            "Paper Title": "Temporally Identity-Aware SSD with Attentional LSTM",
            "Sentences": []
        },
        "http://arxiv.org/abs/1712.00736v4": {
            "Paper Title": "Towards Real-Time Advancement of Underwater Visual Quality with GAN",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.11192v1": {
            "Paper Title": "Aerial Imagery based LIDAR Localization for Autonomous Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.11076v1": {
            "Paper Title": "Removing Dynamic Objects for Static Scene Reconstruction using Light\n  Fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.09754v2": {
            "Paper Title": "Learning 3D Part Assembly from a Single Image",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.10863v1": {
            "Paper Title": "Where to relocate?: Object rearrangement inside cluttered and confined\n  environments for robotic manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.04231v2": {
            "Paper Title": "PVN3D: A Deep Point-wise 3D Keypoints Voting Network for 6DoF Pose\n  Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.10553v1": {
            "Paper Title": "RoboMem: Giving Long Term Memory to Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.00721v4": {
            "Paper Title": "LIT: Light-field Inference of Transparency for Refractive Object\n  Localization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.08705v5": {
            "Paper Title": "PDDLStream: Integrating Symbolic Planners and Blackbox Samplers via\n  Optimistic Adaptive Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.10308v1": {
            "Paper Title": "A Developmental Neuro-Robotics Approach for Boosting the Recognition of\n  Handwritten Digits",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.04577v2": {
            "Paper Title": "Online Replanning in Belief Space for Partially Observable Task and\n  Motion Problems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.10247v1": {
            "Paper Title": "Linear Time-Varying MPC for Nonprehensile Object Manipulation with a\n  Nonholonomic Mobile Robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.10167v1": {
            "Paper Title": "Performance Evaluation of Low-Cost Machine Vision Cameras for\n  Image-Based Grasp Verification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.06962v3": {
            "Paper Title": "DeepTemporalSeg: Temporally Consistent Semantic Segmentation of 3D LiDAR\n  Scans",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.00330v2": {
            "Paper Title": "Close-Proximity Underwater Terrain Mapping Using Learning-based Coarse\n  Range Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.10066v1": {
            "Paper Title": "Caption Generation of Robot Behaviors based on Unsupervised Learning of\n  Action Segments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.10026v1": {
            "Paper Title": "Learning to Walk: Spike Based Reinforcement Learning for Hexapod Robot\n  Central Pattern Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.09998v1": {
            "Paper Title": "Efficient Behavior-aware Control of Automated Vehicles at Crosswalks\n  using Minimal Information Pedestrian Prediction Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.09996v1": {
            "Paper Title": "Analysis and Prediction of Pedestrian Crosswalk Behavior during\n  Automated Vehicle Interactions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.07340v2": {
            "Paper Title": "A Sketch-Based System for Human-Guided Constrained Object Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.11973v1": {
            "Paper Title": "GISNet: Graph-Based Information Sharing Network For Vehicle Trajectory\n  Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.12736v2": {
            "Paper Title": "DiversityGAN: Diversity-Aware Vehicle Motion Prediction via Latent\n  Semantic Sampling",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.11924v2": {
            "Paper Title": "In Perfect Shape: Certifiably Optimal 3D Shape Reconstruction from 2D\n  Landmarks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.09644v1": {
            "Paper Title": "PointNet++ Grasping: Learning An End-to-end Spatial Grasp Generation\n  Algorithm from Sparse Point Clouds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.01779v3": {
            "Paper Title": "Human Posture Recognition and Gesture Imitation with a Humanoid Robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.09575v1": {
            "Paper Title": "Who2com: Collaborative Perception via Learnable Handshake Communication",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.09485v1": {
            "Paper Title": "A generic ontology and recovery protocols for Human-Robot Collaboration\n  (HRC) systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.09401v1": {
            "Paper Title": "Robust Plan Execution with Unexpected Observations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.09371v1": {
            "Paper Title": "Learning-based Bias Correction for Ultra-wideband Localization of\n  Resource-constrained Mobile Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.09242v1": {
            "Paper Title": "Hybrid aerial ground locomotion with a single passive wheel",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.09041v1": {
            "Paper Title": "Design and Experiments with LoCO AUV: A Low Cost Open-Source Autonomous\n  Underwater Vehicle",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.09025v1": {
            "Paper Title": "LoCoQuad: A Low-Cost Arachnoid Quadruped Robot for Research and\n  Education",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.08974v1": {
            "Paper Title": "Latent Space Roadmap for Visual Action Planning of Deformable and Rigid\n  Object Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.00681v2": {
            "Paper Title": "An Iterative Quadratic Method for General-Sum Differential Games with\n  Feedback Linearizable Dynamics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.08515v1": {
            "Paper Title": "SAPIEN: A SimulAted Part-based Interactive ENvironment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.08256v1": {
            "Paper Title": "Aerial Manipulation using Model Predictive Control for Opening a Hinged\n  Door",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01843v2": {
            "Paper Title": "Prediction of Human Full-Body Movements with Motion Optimization and\n  Recurrent Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.08056v1": {
            "Paper Title": "OmniSLAM: Omnidirectional Localization and Dense Mapping for\n  Wide-baseline Multi-camera Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.08034v1": {
            "Paper Title": "Generating Socially Acceptable Perturbations for Efficient Evaluation of\n  Autonomous Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.01603v3": {
            "Paper Title": "Dream to Control: Learning Behaviors by Latent Imagination",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.07745v1": {
            "Paper Title": "Learning to Optimize Autonomy in Competence-Aware Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.07726v1": {
            "Paper Title": "An Experimental Evaluation of Robustness and Precision for Long-term\n  LiDAR-based Localization in Highly Changing Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.13395v2": {
            "Paper Title": "Dynamics Learning with Cascaded Variational Inference for Multi-Step\n  Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.08293v1": {
            "Paper Title": "The Shapeshifter: a Morphing, Multi-Agent,Multi-Modal Robotic Platform\n  for the Exploration of Titan (preprint version)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.07425v1": {
            "Paper Title": "Towards Transparent Robotic Planning via Contrastive Explanations",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": ". Furthermore, explanation\ncreation and policy transparency can be based in \ufb01nding\ncritical states, or the most important states, when reduc-\ntion of the explanation is necessary ",
                    "Citation Text": "S. H. Huang, K. Bhatia, P. Abbeel, and A. D. Dragan, \u201cEstablish-\ning appropriate trust via critical states,\u201d in 2018 IEEE/RSJ Inter-\nnational Conference on Intelligent Robots and Systems (IROS) .\nIEEE, 2018, pp. 3929\u20133936.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.08174",
                        "Citation Paper Title": "Title:Establishing Appropriate Trust via Critical States",
                        "Citation Paper Abstract": "Abstract:In order to effectively interact with or supervise a robot, humans need to have an accurate mental model of its capabilities and how it acts. Learned neural network policies make that particularly challenging. We propose an approach for helping end-users build a mental model of such policies. Our key observation is that for most tasks, the essence of the policy is captured in a few critical states: states in which it is very important to take a certain action. Our user studies show that if the robot shows a human what its understanding of the task's critical states is, then the human can make a more informed decision about whether to deploy the policy, and if she does deploy it, when she needs to take control from it at execution time.",
                        "Citation Paper Authors": "Authors:Sandy H. Huang, Kush Bhatia, Pieter Abbeel, Anca D. Dragan"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2003.00186v2": {
            "Paper Title": "HVNet: Hybrid Voxel Network for LiDAR Based 3D Object Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.10868v2": {
            "Paper Title": "End-to-End Model-Free Reinforcement Learning for Urban Driving using\n  Implicit Affordances",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.07137v1": {
            "Paper Title": "Active Depth Estimation: Stability Analysis and its Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.07007v1": {
            "Paper Title": "Modeling and Experimental Validation of a Fractal Tetrahedron UAS\n  Assembly",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.06965v1": {
            "Paper Title": "OmniTact: A Multi-Directional High Resolution Touch Sensor",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.06743v1": {
            "Paper Title": "Planning with Selective Physics-based Simulation for Manipulation Among\n  Movable Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.06734v1": {
            "Paper Title": "Active Perception and Representation for Robotic Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.05603v2": {
            "Paper Title": "Are We Ready for Service Robots? The OpenLORIS-Scene Datasets for\n  Lifelong SLAM",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.08638v6": {
            "Paper Title": "Joint Anchor-Feature Refinement for Real-Time Accurate Object Detection\n  in Images and Videos",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.02655v2": {
            "Paper Title": "PPMC RL Training Algorithm: Rough Terrain Intelligent Robots through\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.00530v3": {
            "Paper Title": "Toward Autonomous Robotic Micro-Suturing using Optical Coherence\n  Tomography Calibration and Path Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.06000v1": {
            "Paper Title": "Human Grasp Classification for Reactive Human-to-Robot Handovers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.05970v1": {
            "Paper Title": "LiDAR guided Small obstacle Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.07195v1": {
            "Paper Title": "An Experiment in Morphological Development for Learning ANN Based\n  Controllers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.08434v2": {
            "Paper Title": "Fast, Compact and Highly Scalable Visual Place Recognition through\n  Sequence-based Matching of Overloaded Representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.05654v1": {
            "Paper Title": "AirSim Drone Racing Lab",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.05624v1": {
            "Paper Title": "A Multi-task Learning Framework for Grasping-Position Detection and\n  Few-Shot Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.05564v1": {
            "Paper Title": "Securing Autonomous Service Robots through Fuzzing, Detection, and\n  Mitigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.05550v1": {
            "Paper Title": "The Application of Market-based Multi-Robot Task Allocation to Ambulance\n  Dispatch",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.05436v1": {
            "Paper Title": "Learning Predictive Representations for Deformable Objects Using\n  Contrastive Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.05395v1": {
            "Paper Title": "Frozone: Freezing-Free, Pedestrian-Friendly Navigation in Human Crowds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.05224v1": {
            "Paper Title": "3-Survivor: A Rough Terrain Negotiable Teleoperated Mobile Rescue Robot\n  with Passive Control Mechanism",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.10972v2": {
            "Paper Title": "Residual Reactive Navigation: Combining Classical and Learned Navigation\n  Strategies For Deployment in Unknown Environments",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": "utilise a similar uncertainty estimation method to enable\nsafe pedestrian avoidance. Clements et al. ",
                    "Citation Text": "W. R. Clements, B.-M. Robaglia, B. Van Delft, R. B. Slaoui, and\nS. Toth, \u201cEstimating risk and uncertainty in deep reinforcement\nlearning,\u201d arXiv preprint arXiv:1905.09638 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.09638",
                        "Citation Paper Title": "Title:Estimating Risk and Uncertainty in Deep Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:Reinforcement learning agents are faced with two types of uncertainty. Epistemic uncertainty stems from limited data and is useful for exploration, whereas aleatoric uncertainty arises from stochastic environments and must be accounted for in risk-sensitive applications. We highlight the challenges involved in simultaneously estimating both of them, and propose a framework for disentangling and estimating these uncertainties on learned Q-values. We derive unbiased estimators of these uncertainties and introduce an uncertainty-aware DQN algorithm, which we show exhibits safe learning behavior and outperforms other DQN variants on the MinAtar testbed.",
                        "Citation Paper Authors": "Authors:William R. Clements, Bastien Van Delft, Beno\u00eet-Marie Robaglia, Reda Bahi Slaoui, S\u00e9bastien Toth"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2003.05115v1": {
            "Paper Title": "Development of a Robotic System for Automated Decaking of 3D-Printed\n  Parts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.01920v2": {
            "Paper Title": "ETRI-Activity3D: A Large-Scale RGB-D Dataset for Robots to Recognize\n  Daily Activities of the Elderly",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.05102v1": {
            "Paper Title": "FlowFusion: Dynamic Dense RGB-D SLAM Based on Optical Flow",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.04950v1": {
            "Paper Title": "Synthesis of Control Barrier Functions Using a Supervised Machine\n  Learning Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.02432v3": {
            "Paper Title": "Dynamic-Weighted Simplex Strategy for Learning Enabled Cyber Physical\n  Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.04772v1": {
            "Paper Title": "Multi-Task Recurrent Neural Network for Surgical Gesture Recognition and\n  Progress Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.04699v1": {
            "Paper Title": "Look Around You: Sequence-based Radar Place Recognition with Learned\n  Rotational Invariance",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.04871v2": {
            "Paper Title": "Global visual localization in LiDAR-maps through shared 2D-3D embedding\n  space",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.04645v1": {
            "Paper Title": "HeatNet: Bridging the Day-Night Domain Gap in Semantic Segmentation with\n  Thermal Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.04569v1": {
            "Paper Title": "DymSLAM:4D Dynamic Scene Reconstruction Based on Geometrical Motion\n  Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.04474v1": {
            "Paper Title": "Learning a generative model for robot control using visual feedback",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.11293v2": {
            "Paper Title": "Efficient Saliency Maps for Explainable AI",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.04409v1": {
            "Paper Title": "Signal-based self-organization of a chain of UAVs for subterranean\n  exploration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.04260v1": {
            "Paper Title": "SOIC: Semantic Online Initialization and Calibration for LiDAR and\n  Camera",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.03913v1": {
            "Paper Title": "FarSee-Net: Real-Time Semantic Segmentation by Efficient Multi-scale\n  Context Aggregation and Feature Space Super-resolution",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.02910v2": {
            "Paper Title": "Scaled Autonomy: Enabling Human Operators to Control Robot Fleets",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06993v2": {
            "Paper Title": "Learning Visuomotor Policies for Aerial Navigation Using Cross-Modal\n  Representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.03726v1": {
            "Paper Title": "Transferable Task Execution from Pixels through Deep Planning Domain\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.03678v1": {
            "Paper Title": "Online Dynamic Motion Planning and Control for Wheeled Biped Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.03258v4": {
            "Paper Title": "Learn and Link: Learning Critical Regions for Efficient Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.03626v1": {
            "Paper Title": "Discrimination Among Multiple Cutaneous and Proprioceptive Hand Percepts\n  Evoked by Nerve Stimulation with Utah Slanted Electrode Arrays in Human\n  Amputees",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.03543v1": {
            "Paper Title": "Experimental Comparison of Global Motion Planning Algorithms for Wheeled\n  Mobile Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.03528v1": {
            "Paper Title": "Exploratory Study: Children's with Autism Awareness of being Imitated by\n  Nao Robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.01895v2": {
            "Paper Title": "Learning a Spatial Field in Minimum Time with a Team of Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1712.06489v2": {
            "Paper Title": "Multi-Fidelity Reinforcement Learning with Gaussian Processes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.10159v2": {
            "Paper Title": "Self-supervised 6D Object Pose Estimation for Robot Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.01256v2": {
            "Paper Title": "Augmented Reality on the Large Scene Based on a Markerless Registration\n  Framework",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.12908v3": {
            "Paper Title": "Certified Adversarial Robustness for Deep Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.03047v1": {
            "Paper Title": "Robotic Assembly across Multiple Contact Stiffnesses with Robust Force\n  Controllers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.06487v2": {
            "Paper Title": "OpenLORIS-Object: A Robotic Vision Dataset and Benchmark for Lifelong\n  Deep Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.02992v1": {
            "Paper Title": "Neural-Swarm: Decentralized Close-Proximity Multirotor Control Using\n  Learned Interactions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.02946v1": {
            "Paper Title": "DeepMEL: Compiling Visual Multi-Experience Localization into a Deep\n  Neural Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.01744v2": {
            "Paper Title": "LAMP: Large-Scale Autonomous Mapping and Positioning for Exploration of\n  Perceptually-Degraded Subterranean Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.02768v1": {
            "Paper Title": "A Geometric Perspective on Visual Imitation Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.02757v1": {
            "Paper Title": "Safe Planning for Self-Driving Via Adaptive Constrained ILQR",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.02746v1": {
            "Paper Title": "Efficient Uncertainty-aware Decision-making for Automated Driving Using\n  Guided Branching",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.14475v2": {
            "Paper Title": "Dynamic Cloth Manipulation with Deep Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.02640v1": {
            "Paper Title": "Learning the sense of touch in simulation: a sim-to-real strategy for\n  vision-based tactile sensing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.02401v1": {
            "Paper Title": "GOMP: Grasp-Optimized Motion Planning for Bin Picking",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.05518v2": {
            "Paper Title": "Graph-Structured Visual Imitation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.02256v3": {
            "Paper Title": "CLEAR: A Consistent Lifting, Embedding, and Alignment Rectification\n  Algorithm for Multi-View Data Association",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.02327v1": {
            "Paper Title": "Learning View and Target Invariant Visual Servoing for Navigation",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": "use siamese\nnetwork to extract feature maps from the input images and\nestimate the discrete motion. Pathak et al. ",
                    "Citation Text": "D. Pathak, P. Mahmoudieh, G. Luo, P. Agrawal, D. Chen, Y . Shentu,\nE. Shelhamer, J. Malik, A. A. Efros, and T. Darrell, \u201cZero-shot visual\nimitation,\u201d in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition Workshops , 2018, pp. 2050\u20132053.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.08606",
                        "Citation Paper Title": "Title:Zero-Shot Visual Imitation",
                        "Citation Paper Abstract": "Abstract:The current dominant paradigm for imitation learning relies on strong supervision of expert actions to learn both 'what' and 'how' to imitate. We pursue an alternative paradigm wherein an agent first explores the world without any expert supervision and then distills its experience into a goal-conditioned skill policy with a novel forward consistency loss. In our framework, the role of the expert is only to communicate the goals (i.e., what to imitate) during inference. The learned policy is then employed to mimic the expert (i.e., how to imitate) after seeing just a sequence of images demonstrating the desired task. Our method is 'zero-shot' in the sense that the agent never has access to expert actions during training or for the task demonstration at inference. We evaluate our zero-shot imitator in two real-world settings: complex rope manipulation with a Baxter robot and navigation in previously unseen office environments with a TurtleBot. Through further experiments in VizDoom simulation, we provide evidence that better mechanisms for exploration lead to learning a more capable policy which in turn improves end task performance. Videos, models, and more details are available at this https URL",
                        "Citation Paper Authors": "Authors:Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu, Evan Shelhamer, Jitendra Malik, Alexei A. Efros, Trevor Darrell"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/2003.02305v1": {
            "Paper Title": "Touch the Wind: Simultaneous Airflow, Drag and Interaction Sensing on a\n  Multirotor",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.10138v2": {
            "Paper Title": "Reflective-AR Display: An Interaction Methodology for Virtual-Real\n  Alignment in Medical Robotics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.02176v1": {
            "Paper Title": "Annotated-skeleton Biased Motion Planning for Faster Relevant Region\n  Discovery",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.12905v3": {
            "Paper Title": "Simulation-based reinforcement learning for real-world autonomous\n  driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.12971v2": {
            "Paper Title": "Zero-shot Imitation Learning from Demonstrations for Legged Robot Visual\n  Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.05819v3": {
            "Paper Title": "Uncertainty-aware Short-term Motion Prediction of Traffic Actors for\n  Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.10406v2": {
            "Paper Title": "Rethinking Temporal Object Detection from Robotic Perspectives",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": ".\nProblem & motivation. Different from images, video frames\nare interrelated on time series. Thus, almost all VID methods\nleverage across-time information to improve detection perfor-\nmance ",
                    "Citation Text": "W. Han, P. Khorrami, T. L. Paine, P. Ramachandran, M. Babaeizadeh,\nH. Shi, J. Li, S. Yan, and T. S. Huang, \u201cSeq-NMS for video object\ndetection,\u201d arXiv:1602.08465 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.08465",
                        "Citation Paper Title": "Title:Seq-NMS for Video Object Detection",
                        "Citation Paper Abstract": "Abstract:Video object detection is challenging because objects that are easily detected in one frame may be difficult to detect in another frame within the same clip. Recently, there have been major advances for doing object detection in a single image. These methods typically contain three phases: (i) object proposal generation (ii) object classification and (iii) post-processing. We propose a modification of the post-processing phase that uses high-scoring object detections from nearby frames to boost scores of weaker detections within the same clip. We show that our method obtains superior results to state-of-the-art single image object detection techniques. Our method placed 3rd in the video object detection (VID) task of the ImageNet Large Scale Visual Recognition Challenge 2015 (ILSVRC2015).",
                        "Citation Paper Authors": "Authors:Wei Han, Pooya Khorrami, Tom Le Paine, Prajit Ramachandran, Mohammad Babaeizadeh, Honghui Shi, Jianan Li, Shuicheng Yan, Thomas S. Huang"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1912.01652v2": {
            "Paper Title": "Physics-based Simulation of Continuous-Wave LIDAR for Localization,\n  Calibration and Tracking",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.00952v3": {
            "Paper Title": "Robot-Supervised Learning for Object Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.02490v3": {
            "Paper Title": "Kimera: an Open-Source Library for Real-Time Metric-Semantic\n  Localization and Mapping",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.01886v1": {
            "Paper Title": "Efficient statistical validation with edge cases to evaluate Highly\n  Automated Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.05611v2": {
            "Paper Title": "UNO: Uncertainty-aware Noisy-Or Multimodal Fusion for Unanticipated\n  Input Degradation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.01835v1": {
            "Paper Title": "Learning Rope Manipulation Policies Using Dense Object Descriptors\n  Trained on Synthetic Depth Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.08776v2": {
            "Paper Title": "Learning Multi-Robot Decentralized Macro-Action-Based Policies via a\n  Centralized Q-Net",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.09351v3": {
            "Paper Title": "Multi-objective Model-based Policy Search for Data-efficient Learning\n  with Sparse Rewards",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.05751v2": {
            "Paper Title": "Trajectory Optimization for Unknown Constrained Systems using\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.00127v3": {
            "Paper Title": "A Mobile Manipulation System for One-Shot Teaching of Complex Tasks in\n  Homes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.12118v2": {
            "Paper Title": "From Motions to Emotions: Can the Fundamental Emotions be Expressed in a\n  Robot Swarm?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.01758v1": {
            "Paper Title": "Safe, Optimal, Real-time Trajectory Planning with a Parallel Constrained\n  Bernstein Algorithm",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": ").\n2) Reachability-based Trajectory Design: Reachability-\nbased Trajectory Design (RTD) is an optimization-based re-\nceding horizon planner that requires solving a POP at each\nplanning iteration ",
                    "Citation Text": "S. Kousik, S. Vaskov, F. Bu, M. Johnson-Roberson, and R.\nVasudevan, \u201cBridging the gap between safety and real-time\nperformance in receding-horizon trajectory design for mobile\nrobots,\u201d ArXiv preprint arXiv:1809.06746 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.06746",
                        "Citation Paper Title": "Title:Bridging the Gap Between Safety and Real-Time Performance in Receding-Horizon Trajectory Design for Mobile Robots",
                        "Citation Paper Abstract": "Abstract:To operate with limited sensor horizons in unpredictable environments, autonomous robots use a receding-horizon strategy to plan trajectories, wherein they execute a short plan while creating the next plan. However, creating safe, dynamically-feasible trajectories in real time is challenging; and, planners must ensure persistent feasibility, meaning a new trajectory is always available before the previous one has finished executing. Existing approaches make a tradeoff between model complexity and planning speed, which can require sacrificing guarantees of safety and dynamic feasibility. This work presents the Reachability-based Trajectory Design (RTD) method for trajectory planning. RTD begins with an offline Forward Reachable Set (FRS) computation of a robot's motion when tracking parameterized trajectories; the FRS provably bounds tracking error. At runtime, the FRS is used to map obstacles to parameterized trajectories, allowing RTD to select a safe trajectory at every planning iteration. RTD prescribes an obstacle representation to ensure that obstacle constraints can be created and evaluated in real time while maintaining safety. Persistent feasibility is achieved by prescribing a minimum sensor horizon and a minimum duration for the planned trajectories. A system decomposition approach is used to improve the tractability of computing the FRS, allowing RTD to create more complex plans at runtime. RTD is compared in simulation with Rapidly-Exploring Random Trees and Nonlinear Model-Predictive Control. RTD is also demonstrated in randomly-crafted environments on two hardware platforms: a differential-drive Segway, and a car-like Rover. The proposed method is safe and persistently feasible across thousands of simulations and dozens of real-world hardware demos.",
                        "Citation Paper Authors": "Authors:Shreyas Kousik, Sean Vaskov, Fan Bu, Matthew Johnson-Roberson, Ram Vasudevan"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1909.05004v4": {
            "Paper Title": "Predicting optimal value functions by interpolating reward functions in\n  scalarized multi-objective reinforcement learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.01641v1": {
            "Paper Title": "Efficient Exploration in Constrained Environments with Goal-Oriented\n  Reference Path",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.12313v2": {
            "Paper Title": "On Local Computation for Optimization in Multi-Agent Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.01540v1": {
            "Paper Title": "On the Effectiveness of Virtual Reality-based Training for Robotic Setup",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.11303v3": {
            "Paper Title": "C-3PO: Cyclic-Three-Phase Optimization for Human-Robot Motion\n  Retargeting based on Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.11948v2": {
            "Paper Title": "Features for Ground Texture Based Localization -- A Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.01369v1": {
            "Paper Title": "Traversing the Reality Gap via Simulator Tuning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.00924v2": {
            "Paper Title": "MOZARD: Multi-Modal Localization for Autonomous Vehicles in Urban\n  Outdoor Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.01238v1": {
            "Paper Title": "Robot Mindreading and the Problem of Trust",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.04854v2": {
            "Paper Title": "Deep Imitation Learning of Sequential Fabric Smoothing From an\n  Algorithmic Supervisor",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.13439v2": {
            "Paper Title": "Learning to Manipulate Deformable Objects without Demonstrations",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": "show better performance than prior off-policy algorithms such\nDeep Deterministic Policy Gradient (DDPG) ",
                    "Citation Text": "Timothy P. Lillicrap, Jonathan J. Hunt, Alexand er\nPritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David\nSilver, and Daan Wierstra. Continuous control with deep\nreinforcement learning. arXiv e-prints arXiv:1509.02971 ,\n2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1509.02971",
                        "Citation Paper Title": "Title:Continuous control with deep reinforcement learning",
                        "Citation Paper Abstract": "Abstract:We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.",
                        "Citation Paper Authors": "Authors:Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1909.05227v2": {
            "Paper Title": "On-Demand Trajectory Predictions for Interaction Aware Highway Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.07887v2": {
            "Paper Title": "Inferring and Learning Multi-Robot Policies by Observing an Expert",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.07718v2": {
            "Paper Title": "A Sequential Composition Framework for Coordinating Multi-Robot\n  Behaviors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.08198v2": {
            "Paper Title": "Safety Considerations in Deep Control Policies with Safety Barrier\n  Certificates Under Uncertainty",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.00946v1": {
            "Paper Title": "A Self-Supervised Learning Approach to Rapid Path Planning for Car-Like\n  Vehicles Maneuvering in Urban Environment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.03510v2": {
            "Paper Title": "Autonomous quadrotor obstacle avoidance based on dueling double deep\n  recurrent Q-learning with monocular vision",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": "train con-\nvolutional neural networks(CNN) to predict distance-to-collision from the\non-board monocular camera. The proposed CNN is trained on the datasets\nannotated with real-distance labels, which are obtained by the Ultrasonic\nand Infra-Red distance sensors. Moreover, Gandhi et al. ",
                    "Citation Text": "D. Gandhi, L. Pinto, A. Gupta, Learning to \ry by crashing, in: 2017\nIEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS), IEEE, 2017, pp. 3948{3955.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.05588",
                        "Citation Paper Title": "Title:Learning to Fly by Crashing",
                        "Citation Paper Abstract": "Abstract:How do you learn to navigate an Unmanned Aerial Vehicle (UAV) and avoid obstacles? One approach is to use a small dataset collected by human experts: however, high capacity learning algorithms tend to overfit when trained with little data. An alternative is to use simulation. But the gap between simulation and real world remains large especially for perception problems. The reason most research avoids using large-scale real data is the fear of crashes! In this paper, we propose to bite the bullet and collect a dataset of crashes itself! We build a drone whose sole purpose is to crash into objects: it samples naive trajectories and crashes into random objects. We crash our drone 11,500 times to create one of the biggest UAV crash dataset. This dataset captures the different ways in which a UAV can crash. We use all this negative flying data in conjunction with positive data sampled from the same trajectories to learn a simple yet powerful policy for UAV navigation. We show that this simple self-supervised model is quite effective in navigating the UAV even in extremely cluttered environments with dynamic obstacles including humans. For supplementary video see: this https URL",
                        "Citation Paper Authors": "Authors:Dhiraj Gandhi, Lerrel Pinto, Abhinav Gupta"
                    },
                    "Keywords": [
                        "obstacle",
                        "avoidance",
                        ",",
                        "deep",
                        "vehicle",
                        "aerial",
                        "Unmanned"
                    ]
                }
            ]
        },
        "http://arxiv.org/abs/2003.00699v1": {
            "Paper Title": "Planning to Build Soma Blocks Using a Dual-arm Robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.00695v1": {
            "Paper Title": "Efficient Latent Representations using Multiple Tasks for Autonomous\n  Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.02333v2": {
            "Paper Title": "Occ-Traj120: Occupancy Maps with Associated Trajectories",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.00676v1": {
            "Paper Title": "Design and Implementation of A Novel Precision Irrigation Robot Based on\n  An Intelligent Path Planning Algorithm",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.00667v1": {
            "Paper Title": "MVP: Unified Motion and Visual Self-Supervised Learning for Large-Scale\n  Robotic Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.09272v3": {
            "Paper Title": "Learning 3D-aware Egocentric Spatial-Temporal Interaction via Graph\n  Convolutional Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.00658v1": {
            "Paper Title": "Socially-Aware Robot Planning via Bandit Human Feedback",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.02919v3": {
            "Paper Title": "EVDodgeNet: Deep Dynamic Obstacle Dodging with Event Cameras",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.06063v3": {
            "Paper Title": "Probabilistic Trajectory Segmentation by Means of Hierarchical Dirichlet\n  Process Switching Linear Dynamical Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.00504v1": {
            "Paper Title": "MonoPair: Monocular 3D Object Detection Using Pairwise Spatial\n  Relationships",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.00472v1": {
            "Paper Title": "Optimal Oscillation Damping Control of cable-Suspended Aerial\n  Manipulator with a Single IMU Sensor",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.00467v1": {
            "Paper Title": "NeuroTac: A Neuromorphic Optical Tactile Sensor applied to Texture\n  Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.08267v3": {
            "Paper Title": "An NMPC Approach using Convex Inner Approximations for Online Motion\n  Planning with Guaranteed Collision Avoidance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.00385v1": {
            "Paper Title": "Vision-based Robot Manipulation Learning via Human Demonstrations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.00344v1": {
            "Paper Title": "An Evaluation of Knowledge Graph Embeddings for Autonomous Driving Data:\n  Experience and Practice",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.00790v1": {
            "Paper Title": "Towards Identifying and closing Gaps in Assurance of autonomous Road\n  vehicleS -- a collection of Technical Notes Part 2",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.00789v1": {
            "Paper Title": "Towards Identifying and closing Gaps in Assurance of autonomous Road\n  vehicleS -- a collection of Technical Notes Part 1",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.05274v3": {
            "Paper Title": "Efficient Exploration via State Marginal Matching",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.12731v1": {
            "Paper Title": "Linear Features Observation Model for Autonomous Vehicle Localization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.02210v2": {
            "Paper Title": "RINS-W: Robust Inertial Navigation System on Wheels",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.12609v1": {
            "Paper Title": "SCALE-Net: Scalable Vehicle Trajectory Prediction Network under Random\n  Number of Interacting Vehicles via Edge-enhanced Graph Convolutional Neural\n  Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.00848v1": {
            "Paper Title": "Mixed Reinforcement Learning with Additive Stochastic Uncertainty",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.12354v4": {
            "Paper Title": "Realtime Simulation of Thin-Shell Deformable Materials using CNN-Based\n  Mesh Embedding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.05650v2": {
            "Paper Title": "Control of the Final-Phase of Closed-Loop Visual Grasping using\n  Image-Based Visual Servoing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.12408v1": {
            "Paper Title": "High Precision In-Pipe Robot Localization with Reciprocal Sensor Fusion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.12336v1": {
            "Paper Title": "Hallucinative Topological Memory for Zero-Shot Visual Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.12207v1": {
            "Paper Title": "Assembly robots with optimized control stiffness through reinforcement\n  learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.12078v1": {
            "Paper Title": "Training Adversarial Agents to Exploit Weaknesses in Deep Control\n  Policies",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.08105v2": {
            "Paper Title": "Split Deep Q-Learning for Robust Object Singulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.05842v3": {
            "Paper Title": "Real-time Monocular Visual Odometry for Turbid and Dynamic Underwater\n  Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.11708v1": {
            "Paper Title": "Generalized Hindsight for Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.11637v1": {
            "Paper Title": "Learning Navigation Costs from Demonstration in Partially Observable\n  Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.02634v1": {
            "Paper Title": "Dimensionality Reduction of Movement Primitives in Parameter Space",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.11559v1": {
            "Paper Title": "PointTrackNet: An End-to-End Network For 3-D Object Detection and\n  Tracking From Point Clouds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.11326v1": {
            "Paper Title": "Fail-safe Flight of a Fully-Actuated Quadcopter in a Single Motor\n  Failure",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.09820v2": {
            "Paper Title": "Deep Reinforcement Learning with Linear Quadratic Regulator Regions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.02638v1": {
            "Paper Title": "Metric-Based Imitation Learning Between Two Dissimilar Anthropomorphic\n  Robotic Arms",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.02636v1": {
            "Paper Title": "Scalable Multi-Task Imitation Learning with Autonomous Improvement",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.11089v1": {
            "Paper Title": "Rewriting History with Inverse RL: Hindsight Inference for Policy\n  Improvement",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.10853v1": {
            "Paper Title": "Learning Machines from Simulation to Real World",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.10552v1": {
            "Paper Title": "Optimisation of Body-ground Contact for Augmenting Whole-Body\n  Loco-manipulation of Quadruped Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.00541v2": {
            "Paper Title": "Real-Time Semantic Stereo Matching",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.10158v1": {
            "Paper Title": "Robot Perception of Static and Dynamic Objects with an Autonomous Floor\n  Scrubber",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.08584v2": {
            "Paper Title": "Dynamic SLAM: The Need For Speed",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.05321v2": {
            "Paper Title": "IRIS: Implicit Reinforcement without Interaction at Scale for Learning\n  Control from Offline Robot Manipulation Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.09458v2": {
            "Paper Title": "Long-Range Indoor Navigation with PRM-RL",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.09710v1": {
            "Paper Title": "Actively Mapping Industrial Structures with Information Gain-Based\n  Planning on a Quadruped Robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.09676v1": {
            "Paper Title": "Guided Constrained Policy Optimization for Dynamic Quadrupedal Robot\n  Locomotion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.11251v1": {
            "Paper Title": "Back to the Future: Joint Aware Temporal Deep Learning 3D Human Pose\n  Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.09554v1": {
            "Paper Title": "Particle Filter Based Monocular Human Tracking with a 3D Cardbox Model\n  and a Novel Deterministic Resampling Strategy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.10939v1": {
            "Paper Title": "Online Semantic Exploration of Indoor Maps",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.06025v2": {
            "Paper Title": "Interaction-aware Decision Making with Adaptive Strategies under Merging\n  Scenarios",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.09195v1": {
            "Paper Title": "Nonlinearity Compensation in a Multi-DoF Shoulder Sensing Exosuit for\n  Real-Time Teleoperation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.09003v1": {
            "Paper Title": "Complete Endomorphisms in Computer Vision",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.08602v1": {
            "Paper Title": "A Hybrid Systems-based Hierarchical Control Architecture for\n  Heterogeneous Field Robot Teams",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.02243v2": {
            "Paper Title": "Design, Modeling, and Control of Norma: a Slider & Pendulum-Driven\n  Spherical Robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.08417v1": {
            "Paper Title": "Table-Top Scene Analysis Using Knowledge-Supervised MCMC",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.08402v1": {
            "Paper Title": "A Generalizable Knowledge Framework for Semantic Indoor Mapping Based on\n  Markov Logic Networks and Data Driven MCMC",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.08394v1": {
            "Paper Title": "MonoLayout: Amodal scene layout from a single image",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.05201v2": {
            "Paper Title": "Deep compositional robotic planners that follow natural language\n  commands",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.08210v1": {
            "Paper Title": "A Structured Approach to Trustworthy Autonomous/Cognitive Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.11635v1": {
            "Paper Title": "Sim2Real Transfer for Reinforcement Learning without Dynamics\n  Randomization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.07890v1": {
            "Paper Title": "Informative Path Planning for Mobile Sensing with Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.07870v1": {
            "Paper Title": "Online Parameter Estimation for Safety-Critical Systems with Gaussian\n  Processes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.00493v5": {
            "Paper Title": "Event-Triggered Diffusion Kalman Filters",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.07368v1": {
            "Paper Title": "D2C 2.0: Decoupled Data-Based Approach for Learning to Control\n  Stochastic Nonlinear Systems via Model-Free ILQR",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.05837v2": {
            "Paper Title": "Human Action Recognition and Assessment via Deep Neural Network\n  Self-Organization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.06598v1": {
            "Paper Title": "Nonlinear MPC with Motor Failure Identification and Recovery for Safe\n  and Aggressive Multicopter Flight",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.10712v3": {
            "Paper Title": "RoadTrack: Realtime Tracking of Road Agents in Dense and Heterogeneous\n  Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.00049v3": {
            "Paper Title": "GraphRQI: Classifying Driver Behaviors Using Graph Spectrums",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.06417v1": {
            "Paper Title": "Designing Interaction for Multi-agent Cooperative System in an Office\n  Environment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.06344v1": {
            "Paper Title": "Learning Pregrasp Manipulation of Objects from Ungraspable Poses",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.06302v1": {
            "Paper Title": "Applying Depth-Sensing to Automated Surgical Manipulation with a da\n  Vinci Robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.01657v2": {
            "Paper Title": "Dynamics-Aware Unsupervised Discovery of Skills",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.06241v1": {
            "Paper Title": "Social-WaGDAT: Interaction-aware Trajectory Prediction via Wasserstein\n  Graph Double-Attention Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.02364v2": {
            "Paper Title": "Learning to Move with Affordance Maps",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.08225v4": {
            "Paper Title": "Dynamical Distance Learning for Semi-Supervised and Unsupervised Skill\n  Discovery",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.05214v4": {
            "Paper Title": "A Survey of Simultaneous Localization and Mapping with an Envision in 6G\n  Wireless Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06675v3": {
            "Paper Title": "MAD-TN: A Tool for Measuring Fluency in Human-Robot Collaboration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.11200v3": {
            "Paper Title": "TuneNet: One-Shot Residual Tuning for System Identification and\n  Sim-to-Real Robot Task Transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.05721v1": {
            "Paper Title": "A New Exocentric Metaphor for Complex Path Following to Control a UAV\n  Using Mixed Reality",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.04920v2": {
            "Paper Title": "Robust Vision-based Obstacle Avoidance for Micro Aerial Vehicles in\n  Dynamic Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.04911v1": {
            "Paper Title": "Ensemble of Sparse Gaussian Process Experts for Implicit Surface Mapping\n  with Streaming Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.04671v1": {
            "Paper Title": "Can I Trust You? A User Study of Robot Mediation of a Support Group",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.03749v2": {
            "Paper Title": "DFKI Cabin Simulator: A Test Platform for Visual In-Cabin Monitoring\n  Functions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.04109v1": {
            "Paper Title": "On Reward Shaping for Mobile Robot Navigation: A Reinforcement Learning\n  and SLAM Based Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.11699v2": {
            "Paper Title": "Multi-Vehicle Mixed-Reality Reinforcement Learning for Autonomous\n  Multi-Lane Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.04076v1": {
            "Paper Title": "Unsupervised Learning of Audio Perception for Robotics Applications:\n  Learning to Project Data to T-SNE/UMAP space",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.09165v2": {
            "Paper Title": "MeteorNet: Deep Learning on Dynamic 3D Point Cloud Sequences",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.03042v1": {
            "Paper Title": "Bayesian Residual Policy Optimization: Scalable Bayesian Reinforcement\n  Learning with Clairvoyant Experts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.03038v1": {
            "Paper Title": "DenseCAvoid: Real-time Navigation in Dense Crowds using Anticipatory\n  Behaviors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.02715v1": {
            "Paper Title": "Free Space of Rigid Objects: Caging, Path Non-Existence, and Narrow\n  Passage Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.03233v2": {
            "Paper Title": "RSL-Net: Localising in Satellite Images From a Radar on the Ground",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.05149v3": {
            "Paper Title": "STRATA: A Unified Framework for Task Assignments in Large Teams of\n  Heterogeneous Agents",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.02474v1": {
            "Paper Title": "Design of a Fully Actuated Robotic Hand With Multiple Gelsight Tactile\n  Sensors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.02424v1": {
            "Paper Title": "Reliability Validation of Learning Enabled Vehicle Tracking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.02360v1": {
            "Paper Title": "Scalable and Probabilistically Complete Planning for Robotic Spatial\n  Extrusion",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.09430v2": {
            "Paper Title": "Adversarial Skill Networks: Unsupervised Robot Skill Learning from Video",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.02089v1": {
            "Paper Title": "Soft Hindsight Experience Replay",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.07193v3": {
            "Paper Title": "Rolling in the Deep -- Hybrid Locomotion for Wheeled-Legged Robots using\n  Online Trajectory Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.11777v2": {
            "Paper Title": "A Review of Personality in Human Robot Interactions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.01965v1": {
            "Paper Title": "Learning Probabilistic Intersection Traffic Models for Trajectory\n  Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.01921v1": {
            "Paper Title": "Autonomous Navigation in Unknown Environments using Sparse Kernel-based\n  Occupancy Mapping",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.01631v1": {
            "Paper Title": "Toward Optimal FDM Toolpath Planning with Monte Carlo Tree Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.01428v1": {
            "Paper Title": "Learning Task-Driven Control Policies via Information Bottlenecks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.01155v1": {
            "Paper Title": "Simultaneous Enhancement and Super-Resolution of Underwater Imagery for\n  Improved Visual Perception",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.09760v3": {
            "Paper Title": "Not Only Look But Observe: Variational Observation Model of Scene-Level\n  3D Multi-Object Understanding for Probabilistic SLAM",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.11737v2": {
            "Paper Title": "AU-AIR: A Multi-modal Unmanned Aerial Vehicle Dataset for Low Altitude\n  Traffic Surveillance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.00515v1": {
            "Paper Title": "Shapeshifter: A Multi-Agent, Multi-Modal Robotic Platform for\n  Exploration of Titan",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.02286v2": {
            "Paper Title": "A Generic Synchronous Dataflow Architecture to Rapidly Prototype and\n  Deploy Robot Controllers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.00216v1": {
            "Paper Title": "Leveraging Uncertainties for Deep Multi-modal Object Detection in\n  Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.08499v3": {
            "Paper Title": "A Large Scale Event-based Detection Dataset for Automotive",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.11597v1": {
            "Paper Title": "Path Planning in Dynamic Environments using Generative RNNs and Monte\n  Carlo Tree Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.11196v1": {
            "Paper Title": "Model-free vision-based shaping of deformable plastic materials",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.11159v1": {
            "Paper Title": "Universally Safe Swerve Manoeuvres for Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.07642v3": {
            "Paper Title": "Fully distributed cooperation for networked uncertain mobile\n  manipulators",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.04840v2": {
            "Paper Title": "A Deep Learning Approach to Grasping the Invisible",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.10773v1": {
            "Paper Title": "Virtual KITTI 2",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.04514v2": {
            "Paper Title": "DOB-Net: Actively Rejecting Unknown Excessive Time-Varying Disturbances",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.04297v2": {
            "Paper Title": "Online Simultaneous Semi-Parametric Dynamics Model Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.10386v1": {
            "Paper Title": "Taking Recoveries to Task: Recovery-Driven Development for Recipe-based\n  Robot Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.10249v1": {
            "Paper Title": "Online LiDAR-SLAM for Legged Robots with Robust Registration and\n  Deep-Learned Loop Closure",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.10220v1": {
            "Paper Title": "Learning to Catch Piglets in Flight",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.10208v1": {
            "Paper Title": "Towards Learning Multi-agent Negotiations via Self-Play",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.13486v2": {
            "Paper Title": "Predicting Responses to a Robot's Future Motion using Generative\n  Recurrent Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.03895v1": {
            "Paper Title": "Hierarchical Multi-Process Fusion for Visual Place Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.02090v2": {
            "Paper Title": "Open-Sourced Reinforcement Learning Environments for Surgical Robotics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.09550v1": {
            "Paper Title": "Experimental Evaluation of Human Motion Prediction: Toward Safe and\n  Efficient Human Robot Collaboration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.09336v1": {
            "Paper Title": "Learning Constraints from Locally-Optimal Demonstrations under Cost\n  Function Uncertainty",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.09199v1": {
            "Paper Title": "A 3D Reactive Navigation Algorithm for Mobile Robots by Using\n  Tentacle-Based Sampling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.09183v1": {
            "Paper Title": "An Overview of Fingerprint-Based Authentication: Liveness Detection and\n  Beyond",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.09181v1": {
            "Paper Title": "End-to-End Vision-Based Adaptive Cruise Control (ACC) Using Deep\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.09084v1": {
            "Paper Title": "What went wrong?: Identification of Everyday Object Manipulation\n  Anomalies",
            "Sentences": [
                {
                    "Sentence ID": 45,
                    "Sentence": "\u000fLSTM-based identi\fcation procedure\nThe LSTM-based method is applied by using the following parameters:\nAdam Algorithm ",
                    "Citation Text": "D. P. Kingma, J. Ba, Adam: A method for stochastic optimization, CoRR\nabs/1412.6980. arXiv:1412.6980 .\nURL http://arxiv.org/abs/1412.6980",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.6980",
                        "Citation Paper Title": "Title:Adam: A Method for Stochastic Optimization",
                        "Citation Paper Abstract": "Abstract:We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
                        "Citation Paper Authors": "Authors:Diederik P. Kingma, Jimmy Ba"
                    },
                    "Keywords": [
                        "Everyday",
                        "Execution",
                        "Robotics",
                        ",",
                        "in",
                        "Safety",
                        "Object",
                        "Manipulation"
                    ]
                }
            ]
        },
        "http://arxiv.org/abs/1910.10885v2": {
            "Paper Title": "Robust Model Predictive Shielding for Safe Reinforcement Learning with\n  Stochastic Dynamics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.08620v1": {
            "Paper Title": "Trajectory Planning for Connected and Automated Vehicles: Cruising, Lane\n  Changing, and Platooning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.08098v1": {
            "Paper Title": "Learning to Correct 3D Reconstructions from Multiple Views",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.08539v1": {
            "Paper Title": "Automatic Differentiation and Continuous Sensitivity Analysis of Rigid\n  Body Dynamics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.09981v1": {
            "Paper Title": "Designing a Socially Assistive Robot for Long-Term In-Home Use for\n  Children with Autism Spectrum Disorders",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.07679v1": {
            "Paper Title": "Stochastic Finite State Control of POMDPs with LTL Specifications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.07481v1": {
            "Paper Title": "Joint Learning of Instance and Semantic Segmentation for Robotic\n  Pick-and-Place with Heavy Occlusions in Clutter",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.07475v1": {
            "Paper Title": "Instance Segmentation of Visible and Occluded Regions for Finding and\n  Picking Target from a Pile of Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.07343v1": {
            "Paper Title": "Lyceum: An efficient and scalable ecosystem for robot learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.07210v1": {
            "Paper Title": "Extent-Compatible Control Barrier Functions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.09167v3": {
            "Paper Title": "Sensor Aware Lidar Odometry",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.03735v1": {
            "Paper Title": "Real-Time Object Detection and Recognition on Low-Compute Humanoid\n  Robots using Deep Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.06940v1": {
            "Paper Title": "Reinforcement Learning with Probabilistically Complete Exploration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.07231v2": {
            "Paper Title": "DeepTIO: A Deep Thermal-Inertial Odometry with Visual Hallucination",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.03752v4": {
            "Paper Title": "Masking by Moving: Learning Distraction-Free Radar Odometry from Pose\n  Information",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.04869v2": {
            "Paper Title": "A Lightweight and Accurate Localization Algorithm Using Multiple\n  Inertial Measurement Units",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.06175v1": {
            "Paper Title": "Spatiotemporal Camera-LiDAR Calibration: A Targetless and Structureless\n  Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.06066v1": {
            "Paper Title": "Tracking of Micro Unmanned Aerial Vehicles: A Comparative Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.05752v1": {
            "Paper Title": "Probabilistic 3D Multilabel Real-time Mapping for Multi-object\n  Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.05703v1": {
            "Paper Title": "A Markerless Deep Learning-based 6 Degrees of Freedom PoseEstimation for\n  with Mobile Robots using RGB Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.05406v2": {
            "Paper Title": "3D Object Segmentation for Shelf Bin Picking by Humanoid with Deep\n  Learning and Occupancy Voxel Grid Map",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.05673v1": {
            "Paper Title": "Probabilistic 3D Multi-Object Tracking for Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.04313v2": {
            "Paper Title": "Convex Controller Synthesis for Robot Contact",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.05514v1": {
            "Paper Title": "The Penetration of Internet of Things in Robotics: Towards a Web of\n  Robotic Things",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.05462v1": {
            "Paper Title": "Offline Grid-Based Coverage path planning for guards in games",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.05215v1": {
            "Paper Title": "Direct Visual-Inertial Ego-Motion Estimation via Iterated Extended\n  Kalman Filter",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.05161v1": {
            "Paper Title": "Pose-Assisted Multi-Camera Collaboration for Active Object Tracking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.06347v1": {
            "Paper Title": "Tethered Aerial Visual Assistance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.04931v1": {
            "Paper Title": "Parameterized and GPU-Parallelized Real-Time Model Predictive Control\n  for High Degree of Freedom Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.04637v1": {
            "Paper Title": "Companion Unmanned Aerial Vehicles: A Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.03615v2": {
            "Paper Title": "Universal Flying Objects: Modular Multirotor System for Flight of Rigid\n  Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.04509v1": {
            "Paper Title": "Examining the Effects of Emotional Valence and Arousal on Takeover\n  Performance in Conditionally Automated Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.04061v1": {
            "Paper Title": "Deep Learning based Pedestrian Inertial Navigation: Methods, Dataset and\n  On-Device Inference",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.03864v1": {
            "Paper Title": "Learning to drive via Apprenticeship Learning and Deep Reinforcement\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.03855v1": {
            "Paper Title": "Hyperparameters optimization for Deep Learning based emotion prediction\n  for Human Robot Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.05863v1": {
            "Paper Title": "Establishing Human-Robot Trust through Music-Driven Robotic Emotion\n  Prosody and Gesture",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.03676v1": {
            "Paper Title": "Learning Topometric Semantic Maps from Occupancy Grids",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.01293v4": {
            "Paper Title": "Scene recognition based on DNN and game theory with its applications in\n  human-robot interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.03359v1": {
            "Paper Title": "Deep Interactive Reinforcement Learning for Path Following of Autonomous\n  Underwater Vehicle",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.03343v1": {
            "Paper Title": "RTM3D: Real-time Monocular 3D Detection from Object Keypoints for\n  Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.08336v2": {
            "Paper Title": "Video Object Segmentation-based Visual Servo Control and Object Depth\n  Estimation on a Mobile Robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.04397v1": {
            "Paper Title": "SMT-based Robot Transition Repair",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.11884v4": {
            "Paper Title": "Identifying Emotions from Walking using Affective and Deep Features",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.03205v1": {
            "Paper Title": "Camera-Based Adaptive Trajectory Guidance via Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.03075v1": {
            "Paper Title": "Pivot calibration concept for sensor attached mobile c-arms",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.03870v3": {
            "Paper Title": "An informative path planning framework for UAV-based terrain monitoring",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.05856v1": {
            "Paper Title": "Domain Independent Unsupervised Learning to grasp the Novel Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.02906v1": {
            "Paper Title": "Multirobot Coverage of Linear Modular Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.08190v2": {
            "Paper Title": "Control What You Can: Intrinsically Motivated Task-Planning Agent",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.12555v3": {
            "Paper Title": "Ensemble Bayesian Decision Making with Redundant Deep Perceptual Control\n  Policies",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.03435v1": {
            "Paper Title": "Design and Control of a Variable Aerial Cable Towed System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.03539v1": {
            "Paper Title": "A rasterized ray-tracer pipeline for real-time, multi-device sonar\n  simulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.03847v2": {
            "Paper Title": "Danger-aware Adaptive Composition of DRL Agents for Self-navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.02354v1": {
            "Paper Title": "VisionNet: A Drivable-space-based Interactive Motion Prediction Network\n  for Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.01921v1": {
            "Paper Title": "A water-obstacle separation and refinement network for unmanned surface\n  vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.00991v1": {
            "Paper Title": "Human-robot co-manipulation of extended objects: Data-driven models and\n  control from analysis of human-human dyads",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.08252v2": {
            "Paper Title": "Inverse Statics Optimization for Compound Tensegrity Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.00762v1": {
            "Paper Title": "Deep Unsupervised Common Representation Learning for LiDAR and Camera\n  Data using Double Siamese Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.00605v1": {
            "Paper Title": "Zero-Shot Reinforcement Learning with Deep Attention Convolutional\n  Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.00449v1": {
            "Paper Title": "Continuous-Discrete Reinforcement Learning for Hybrid Control in\n  Robotics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.00358v1": {
            "Paper Title": "Motion Generation Interface of ROS to PODO Software Framework for\n  Wheeled Huamanoid Robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.00356v1": {
            "Paper Title": "Fast Perception, Planning, and Execution for a Robotic Butler: Wheeled\n  Humanoid M-Hubo",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.11215v2": {
            "Paper Title": "RoboNet: Large-Scale Multi-Robot Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.05881v2": {
            "Paper Title": "Can User-Centered Reinforcement Learning Allow a Robot to Attract\n  Passersby without Causing Discomfort?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.00135v1": {
            "Paper Title": "Development of a Connected and Automated Vehicle Longitudinal Control\n  Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.13470v2": {
            "Paper Title": "GraspNet: A Large-Scale Clustered and Densely Annotated Dataset for\n  Object Grasping",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.00048v1": {
            "Paper Title": "MIR-Vehicle: Cost-Effective Research Platform for Autonomous Vehicle\n  Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.13360v1": {
            "Paper Title": "Morphology-Agnostic Visual Robotic Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.12907v1": {
            "Paper Title": "Gait Library Synthesis for Quadruped Robots via Augmented Random Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.12828v1": {
            "Paper Title": "ICSTrace: A Malicious IP Traceback Model for Attacking Data of\n  Industrial Control System",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.12773v1": {
            "Paper Title": "Learning Predictive Models From Observation and Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.12726v1": {
            "Paper Title": "SLOAM: Semantic Lidar Odometry and Mapping for Forest Inventory",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.08097v2": {
            "Paper Title": "Conflict Detection and Resolution in Table Top Scenarios for Human-Robot\n  Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.12639v2": {
            "Paper Title": "MAMPS: Safe Multi-Agent Reinforcement Learning via Model Predictive\n  Shielding",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.12367v1": {
            "Paper Title": "Illumination Robust Loop Closure Detection with the Constraint of Pose",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.12294v1": {
            "Paper Title": "Learning by Cheating",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.12109v1": {
            "Paper Title": "Augmented-Reality-Based Visualization of Navigation Data of Mobile\n  Robots on the Microsoft Hololens -- Possibilities and Limitations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.12101v1": {
            "Paper Title": "A 3D-Deep-Learning-based Augmented Reality Calibration Method for\n  Robotic Environments using Depth Sensor Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.13461v1": {
            "Paper Title": "3D Sensing of a Moving Object with a Nodding 2D LIDAR and Reconfigurable\n  Mirrors",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.11932v1": {
            "Paper Title": "Skeleton Extraction from 3D Point Clouds by Decomposing the Object into\n  Parts",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.11912v1": {
            "Paper Title": "Quasi-Newton Trust Region Policy Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.11774v1": {
            "Paper Title": "Autonomous Removal of Perspective Distortion for Robotic Elevator Button\n  Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.11749v1": {
            "Paper Title": "Invariant Cubature Kalman Filter for Monocular Visual Inertial Odometry\n  with Line Features",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.11981v3": {
            "Paper Title": "Regularizing Trajectory Optimization with Denoising Autoencoders",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.11651v1": {
            "Paper Title": "Extending Multi-Object Tracking systems to better exploit appearance and\n  3D information",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.10675v2": {
            "Paper Title": "A Multimodal Target-Source Classifier with Attention Branches to\n  Understand Ambiguous Instructions for Fetching Daily Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.11521v1": {
            "Paper Title": "Focusing and Diffusion: Bidirectional Attentive Graph Convolutional\n  Networks for Skeleton-based Action Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.12204v1": {
            "Paper Title": "Federated Imitation Learning: A Novel Framework for Cloud Robotic\n  Systems with Heterogeneous Sensor Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.11121v1": {
            "Paper Title": "Learning to Navigate Using Mid-Level Visual Priors",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.11032v1": {
            "Paper Title": "Towards Practical Multi-Object Manipulation using Relational\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.04967v2": {
            "Paper Title": "Diverse Trajectory Forecasting with Determinantal Point Processes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.10609v1": {
            "Paper Title": "One-Shot Imitation Filming of Human Motion Videos",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.10146v1": {
            "Paper Title": "Optimizing Collision Avoidance in Dense Airspace using Deep\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.01973v2": {
            "Paper Title": "Learning Latent Plans from Play",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.09316v2": {
            "Paper Title": "P$^2$GNet: Pose-Guided Point Cloud Generating Networks for 6-DoF Object\n  Pose Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.09539v1": {
            "Paper Title": "Interactive Open-Ended Learning for 3D Object Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.09503v1": {
            "Paper Title": "Multi-Robot Path Planning Via Genetic Programming",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.09288v1": {
            "Paper Title": "Online Path Generation and Navigation for Swarms of UAVs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.09260v1": {
            "Paper Title": "Deep Reinforcement Learning for Motion Planning of Mobile Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.08487v2": {
            "Paper Title": "FuseSeg: LiDAR Point Cloud Segmentation Fusing Multi-Modal Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.08352v3": {
            "Paper Title": "GRIP: Generative Robust Inference and Perception for Semantic Robot\n  Manipulation in Adversarial Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06682v2": {
            "Paper Title": "Learning to Collaborate from Simulation for Robot-Assisted Dressing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.08444v1": {
            "Paper Title": "Relational Mimic for Visual Adversarial Imitation Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.08116v1": {
            "Paper Title": "When Your Robot Breaks: Active Learning During Plant Failure",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.08718v1": {
            "Paper Title": "Poisson Multi-Bernoulli Mixtures for Sets of Trajectories",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.08601v1": {
            "Paper Title": "Kalman Filter Tuning with Bayesian Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.07834v1": {
            "Paper Title": "Design and Implementation of Linked Planning Domain Definition Language",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.07670v1": {
            "Paper Title": "To Follow or not to Follow: Selective Imitation Learning from\n  Observations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.05209v3": {
            "Paper Title": "A Survey of Recent Scalability Improvements for Semidefinite Programming\n  with Applications in Machine Learning, Control, and Robotics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.11639v3": {
            "Paper Title": "ROBEL: Robotics Benchmarks for Learning with Low-Cost Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.09660v3": {
            "Paper Title": "Informative Path Planning for Active Field Mapping under Localization\n  Uncertainty",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.02598v2": {
            "Paper Title": "DDM: Fast Near-Optimal Multi-Robot Path Planning using Diversified-Path\n  and Optimal Sub-Problem Solution Database Heuristics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.10252v4": {
            "Paper Title": "Efficient Probabilistic Collision Detection for Non-Gaussian Noise\n  Distributions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.06837v1": {
            "Paper Title": "Follow Pedro! An Infrared-based Person-Follower using Nonlinear\n  Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.06704v1": {
            "Paper Title": "Hierarchical Deep Stereo Matching on High-resolution Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.06602v1": {
            "Paper Title": "That and There: Judging the Intent of Pointing Actions with Robotic Arms",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.14453v2": {
            "Paper Title": "LiDAR-Flow: Dense Scene Flow Estimation from Sparse LiDAR and Stereo\n  Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.06449v1": {
            "Paper Title": "Solving Visual Object Ambiguities when Pointing: An Unsupervised\n  Learning Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.06268v1": {
            "Paper Title": "Inferring Distributions Over Depth from a Single Image",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": "popularize the problem of inferring scene depth maps from\na single image, making use of handcrafted features. Eigen\net al. ",
                    "Citation Text": "D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction from a\nsingle image using a multi-scale deep network. In NIPS , 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.2283",
                        "Citation Paper Title": "Title:Depth Map Prediction from a Single Image using a Multi-Scale Deep Network",
                        "Citation Paper Abstract": "Abstract:Predicting depth is an essential component in understanding the 3D geometry of a scene. While for stereo images local correspondence suffices for estimation, finding depth relations from a single image is less straightforward, requiring integration of both global and local information from various cues. Moreover, the task is inherently ambiguous, with a large source of uncertainty coming from the overall scale. In this paper, we present a new method that addresses this task by employing two deep network stacks: one that makes a coarse global prediction based on the entire image, and another that refines this prediction locally. We also apply a scale-invariant error to help measure depth relations rather than scale. By leveraging the raw datasets as large sources of training data, our method achieves state-of-the-art results on both NYU Depth and KITTI, and matches detailed depth boundaries without the need for superpixelation.",
                        "Citation Paper Authors": "Authors:David Eigen, Christian Puhrsch, Rob Fergus"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1912.05629v1": {
            "Paper Title": "Large-scale Kernel Methods and Applications to Lifelong Robot Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.05604v1": {
            "Paper Title": "A Billion Ways to Grasp: An Evaluation of Grasp Sampling Schemes on a\n  Dense, Physics-based Grasp Data Set",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.05440v1": {
            "Paper Title": "Self-Driving Car Steering Angle Prediction Based on Image Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.05220v1": {
            "Paper Title": "Lane Detection For Prototype Autonomous Vehicle",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.05099v1": {
            "Paper Title": "RoboCoDraw: Robotic Avatar Drawing with GAN-based Style Transfer and\n  Time-efficient Path Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.04976v1": {
            "Paper Title": "Learning to Optimally Segment Point Clouds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.02496v1": {
            "Paper Title": "Architecting Safe Automated Driving with Legacy Platforms",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.04235v1": {
            "Paper Title": "An Interactive Indoor Drone Assistant",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.03992v1": {
            "Paper Title": "Environment reconstruction on depth images using Generative Adversarial\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.10232v2": {
            "Paper Title": "Bottom-Up Meta-Policy Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.09683v2": {
            "Paper Title": "From semantics to execution: Integrating action planning with\n  reinforcement learning for robotic causal problem-solving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.09674v3": {
            "Paper Title": "Controlling Assistive Robots with Learned Latent Actions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.00997v2": {
            "Paper Title": "Multiple Futures Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.03820v3": {
            "Paper Title": "Real-time Soft Body 3D Proprioception via Deep Vision-based Sensing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.03227v1": {
            "Paper Title": "Self-Supervised Visual Terrain Classification from Unsupervised Acoustic\n  Feature Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.12125v1": {
            "Paper Title": "Large-scale 6D Object Pose Estimation Dataset for Industrial Bin-Picking",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.00177v2": {
            "Paper Title": "Urban Driving with Conditional Imitation Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.02752v1": {
            "Paper Title": "Reorienting Objects in 3D Space Using Pivoting",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.02241v1": {
            "Paper Title": "Learning from Interventions using Hierarchical Policies for Safe\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.02149v1": {
            "Paper Title": "Dynamic Hilbert Maps: Real-Time Occupancy Predictions in Changing\n  Environment",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.01992v1": {
            "Paper Title": "Research on dynamic target detection and tracking system of hexapod\n  robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.01870v1": {
            "Paper Title": "A Fully-Integrated Sensing and Control System for High-Accuracy Mobile\n  Robotic Building Construction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.02693v1": {
            "Paper Title": "Robotic Surveillance Based on the Meeting Time of Random Walks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.01704v1": {
            "Paper Title": "Robustness-Driven Exploration with Probabilistic Metric Temporal Logic",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.01292v1": {
            "Paper Title": "Internally-Balanced Magnetic Mechanisms Using Magnetic Spring for\n  Producing Large Amplified Clamping Force",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.01221v1": {
            "Paper Title": "Evaluation of Smartphone IMUs for Small Mobile Search and Rescue Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.02652v1": {
            "Paper Title": "Autonomous Robot Swarms for Off-World Construction and Resource Mining",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.01136v1": {
            "Paper Title": "Human Whole-Body Dynamics Estimation for Enhancing Physical Human-Robot\n  Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.01064v1": {
            "Paper Title": "A Keyframe-based Continuous Visual SLAM for RGB-D Cameras via\n  Nonparametric Joint Geometric and Appearance Representation",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": "is a semi-dense method, we followed the point\nselection approach proposed in Direct Sparse Odometry\n(DSO) ",
                    "Citation Text": "Jakob Engel, Vladlen Koltun, and Daniel Cremers. Direct\nsparse odometry. IEEE Transactions on Pattern Analysis and\nMachine Intelligence , 40(3):611\u2013625, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.02565",
                        "Citation Paper Title": "Title:Direct Sparse Odometry",
                        "Citation Paper Abstract": "Abstract:We propose a novel direct sparse visual odometry formulation. It combines a fully direct probabilistic model (minimizing a photometric error) with consistent, joint optimization of all model parameters, including geometry -- represented as inverse depth in a reference frame -- and camera motion. This is achieved in real time by omitting the smoothness prior used in other direct methods and instead sampling pixels evenly throughout the images. Since our method does not depend on keypoint detectors or descriptors, it can naturally sample pixels from across all image regions that have intensity gradient, including edges or smooth intensity variations on mostly white walls. The proposed model integrates a full photometric calibration, accounting for exposure time, lens vignetting, and non-linear response functions. We thoroughly evaluate our method on three different datasets comprising several hours of video. The experiments show that the presented approach significantly outperforms state-of-the-art direct and indirect methods in a variety of real-world settings, both in terms of tracking accuracy and robustness.",
                        "Citation Paper Authors": "Authors:Jakob Engel, Vladlen Koltun, Daniel Cremers"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1912.00951v1": {
            "Paper Title": "Augmented Reality for Human-Swarm Interaction in a Swarm-Robotic\n  Chemistry Simulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.00745v1": {
            "Paper Title": "Surface Following using Deep Reinforcement Learning and a\n  GelSightTactile Sensor",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.01715v1": {
            "Paper Title": "Human-Robot Collaboration via Deep Reinforcement Learning of Real-World\n  Interactions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.00603v1": {
            "Paper Title": "Online Multi-Target Tracking for Maneuvering Vehicles in Dynamic Road\n  Context",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.05023v1": {
            "Paper Title": "A Robust Stereo Camera Localization Method with Prior LiDAR Map\n  Constrains",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.03961v2": {
            "Paper Title": "DCAD: Decentralized Collision Avoidance with Dynamics Constraints for\n  Agile Quadrotor Swarms",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.03822v4": {
            "Paper Title": "Graph Policy Gradients for Large Scale Robot Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.00438v1": {
            "Paper Title": "RST-MODNet: Real-time Spatio-temporal Moving Object Detection for\n  Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.00260v1": {
            "Paper Title": "Transferable Force-Torque Dynamics Model for Peg-in-hole Task",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.00253v1": {
            "Paper Title": "Idle Time Optimization for Target Assignment and Path Finding in\n  Sortation Centers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.02759v1": {
            "Paper Title": "Collaborative SLAM based on Wifi Fingerprint Similarity and Motion\n  Information",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.10425v2": {
            "Paper Title": "Improving Visual Feature Extraction in Glacial Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.00045v1": {
            "Paper Title": "Mechanism for Embossing Braille Characters on Paper: Conceptual Design",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.12937v1": {
            "Paper Title": "Road Curb Detection Using A Novel Tensor Voting Algorithm",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.10312v2": {
            "Paper Title": "How to improve CNN-based 6-DoF camera pose estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.10304v2": {
            "Paper Title": "Where to Look Next: Unsupervised Active Visual Exploration on 360\u00b0\n  Input",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.12610v1": {
            "Paper Title": "DeepGoal: Learning to Drive with driving intention from Human Control\n  Demonstration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.12482v1": {
            "Paper Title": "Designing the Next Generation of Intelligent Personal Robotic Assistants\n  for the Physically Impaired",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.12470v1": {
            "Paper Title": "LeRoP: A Learning-Based Modular Robot Photography Framework",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.12063v1": {
            "Paper Title": "Following Social Groups: Socially Compliant Autonomous Navigation in\n  Dense Crowds",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.12007v1": {
            "Paper Title": "Weakly-Supervised Road Affordances Inference and Learning in Scenes\n  without Traffic Signs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.02022v2": {
            "Paper Title": "Chasing Ghosts: Instruction Following as Bayesian State Tracking",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.11744v1": {
            "Paper Title": "Imitation Learning of Robot Policies by Combining Language, Vision and\n  Demonstration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.10752v1": {
            "Paper Title": "Fast and Incremental Loop Closure Detection Using Proximity Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.01201v2": {
            "Paper Title": "Habitat: A Platform for Embodied AI Research",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.10627v1": {
            "Paper Title": "The Reconfigurable Aerial Robotic Chain: Shape and Motion Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.09233v2": {
            "Paper Title": "Contextual Reinforcement Learning of Visuo-tactile Multi-fingered\n  Grasping Policies",
            "Sentences": [
                {
                    "Sentence ID": 37,
                    "Sentence": ". Manuelli et al. also leverage\nkeypoint representation to learn an agnostic representation\nfor a class of objects where a classical controller is written\nto accomplish a pick-and-place task ",
                    "Citation Text": "L. Manuelli, W. Gao, P. R. Florence, and R. Tedrake, \u201ckpam: Keypoint\naffordances for category-level robotic manipulation,\u201d arXiv preprint\narXiv:1903.06684 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.06684",
                        "Citation Paper Title": "Title:kPAM: KeyPoint Affordances for Category-Level Robotic Manipulation",
                        "Citation Paper Abstract": "Abstract:We would like robots to achieve purposeful manipulation by placing any instance from a category of objects into a desired set of goal states. Existing manipulation pipelines typically specify the desired configuration as a target 6-DOF pose and rely on explicitly estimating the pose of the manipulated objects. However, representing an object with a parameterized transformation defined on a fixed template cannot capture large intra-category shape variation, and specifying a target pose at a category level can be physically infeasible or fail to accomplish the task -- e.g. knowing the pose and size of a coffee mug relative to some canonical mug is not sufficient to successfully hang it on a rack by its handle. Hence we propose a novel formulation of category-level manipulation that uses semantic 3D keypoints as the object representation. This keypoint representation enables a simple and interpretable specification of the manipulation target as geometric costs and constraints on the keypoints, which flexibly generalizes existing pose-based manipulation methods. Using this formulation, we factor the manipulation policy into instance segmentation, 3D keypoint detection, optimization-based robot action planning and local dense-geometry-based action execution. This factorization allows us to leverage advances in these sub-problems and combine them into a general and effective perception-to-action manipulation pipeline. Our pipeline is robust to large intra-category shape variation and topology changes as the keypoint representation ignores task-irrelevant geometric details. Extensive hardware experiments demonstrate our method can reliably accomplish tasks with never-before seen objects in a category, such as placing shoes and mugs with significant shape variation into category level target configurations.",
                        "Citation Paper Authors": "Authors:Lucas Manuelli, Wei Gao, Peter Florence, Russ Tedrake"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1906.01624v3": {
            "Paper Title": "Off-Policy Evaluation via Off-Policy Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.10274v1": {
            "Paper Title": "Titan: A Parallel Asynchronous Library for Multi-Agent and Soft-Body\n  Robotics using NVIDIA CUDA",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.10231v1": {
            "Paper Title": "Design and Experiments with a Robot-Driven Underwater Holographic\n  Microscope for Low-Cost In Situ Particle Measurements",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.10090v1": {
            "Paper Title": "Learning End-To-End Scene Flow by Distilling Single Tasks Knowledge",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.10079v1": {
            "Paper Title": "RoboSherlock: Cognition-enabled Robot Perception for Everyday\n  Manipulation Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.03110v2": {
            "Paper Title": "Making High-Performance Robots Safe and Easy to Use for an Introduction\n  to Computing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.11620v1": {
            "Paper Title": "Teaching Perception",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.09782v1": {
            "Paper Title": "Verbal Programming of Robot Behavior",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.05546v2": {
            "Paper Title": "Learning to Control Self-Assembling Morphologies: A Study of\n  Generalization via Modularity",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.09676v1": {
            "Paper Title": "Third-Person Visual Imitation Learning via Decoupled Hierarchical\n  Controller",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.09476v1": {
            "Paper Title": "Incremental Learning of Motion Primitives for Pedestrian Trajectory\n  Prediction at Intersections",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.09430v1": {
            "Paper Title": "Visual Tactile Fusion Object Clustering",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.09391v1": {
            "Paper Title": "Accelerating Reinforcement Learning with Suboptimal Guidance",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.10782v6": {
            "Paper Title": "High-quality Instance-aware Semantic 3D Map Using RGB-D Camera",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.09313v1": {
            "Paper Title": "Magnetic-Assisted Initialization for Infrastructure-free Mobile Robot\n  Localization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.09280v1": {
            "Paper Title": "Integrated Motion Planner for Real-time Aerial Videography with a Drone\n  in a Dense Environment",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.05395v3": {
            "Paper Title": "FuseMODNet: Real-Time Camera and LiDAR based Moving Object Detection for\n  robust low-light Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.09054v1": {
            "Paper Title": "Robust Lane Marking Detection Algorithm Using Drivable Area Segmentation\n  and Extended SLT",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.02718v2": {
            "Paper Title": "Model Adaption Object Detection System for Robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.08666v1": {
            "Paper Title": "Evaluating task-agnostic exploration for fixed-batch learning of\n  arbitrary future tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.08453v1": {
            "Paper Title": "Planning with Goal-Conditioned Policies",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.07983v1": {
            "Paper Title": "Task-Based Hybrid Shared Control for Training Through Forceful\n  Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.07980v1": {
            "Paper Title": "Simultaneous Mapping and Target Driven Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.07759v1": {
            "Paper Title": "A gamified simulator and physical platform for self-driving algorithm\n  training and validation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.07602v1": {
            "Paper Title": "The inD Dataset: A Drone Dataset of Naturalistic Road User Trajectories\n  at German Intersections",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.13875v2": {
            "Paper Title": "Expressive Inverse Kinematics Solving in Real-time for Virtual and\n  Robotic Interactive Characters",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.07505v1": {
            "Paper Title": "A Hierarchical Framework to Generate Robust Biped Locomotion Based on\n  Divergent Component of Motion",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.07432v1": {
            "Paper Title": "Fast 2D Map Matching Based on Area Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.07383v1": {
            "Paper Title": "Towards Robust RGB-D Human Mesh Recovery",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.04417v4": {
            "Paper Title": "Imitation Learning from Observations by Minimizing Inverse Dynamics\n  Disagreement",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.05071v2": {
            "Paper Title": "Experience-Embedded Visual Foresight",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.07246v1": {
            "Paper Title": "IKEA Furniture Assembly Environment for Long-Horizon Complex\n  Manipulation Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.08581v1": {
            "Paper Title": "A Configuration-Space Decomposition Scheme for Learning-based Collision\n  Checking",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.06882v1": {
            "Paper Title": "Adaptive Leader-Follower Formation Control and Obstacle Avoidance via\n  Deep Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.06832v1": {
            "Paper Title": "Data-efficient Co-Adaptation of Morphology and Behaviour with Deep\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.09707v3": {
            "Paper Title": "Collision Detection for Agents in Multi-Agent Pathfinding",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.09959v3": {
            "Paper Title": "Towards More Sample Efficiency in Reinforcement Learning with Data\n  Augmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.08528v1": {
            "Paper Title": "Haptic Sketches on the Arm for Manipulation in Virtual Reality",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.05884v1": {
            "Paper Title": "Robots Assembling Machines: Learning from the World Robot Summit 2018\n  Assembly Challenge",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.05787v1": {
            "Paper Title": "Visual-Inertial Localization for Skid-Steering Robots with Kinematic\n  Constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.05761v1": {
            "Paper Title": "Predicting Unobserved Space For Planning via Depth Map Augmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.06144v1": {
            "Paper Title": "A Penetration Metric for Deforming Tetrahedra using Object Norm",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.05185v1": {
            "Paper Title": "Pose estimation and bin picking for deformable products",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.04930v1": {
            "Paper Title": "HMTNet:3D Hand Pose Estimation from Single Depth Image Based on Hand\n  Morphological Topology",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.12031v2": {
            "Paper Title": "Deep Learning and Control Algorithms of Direct Perception for Autonomous\n  Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.04676v1": {
            "Paper Title": "Prediction of Bottleneck Points for Manipulation Planning in Cluttered\n  Environment using a 3D Convolutional Neural Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.04643v1": {
            "Paper Title": "Framing Effects on Privacy Concerns about a Home Telepresence Robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.05758v3": {
            "Paper Title": "Learning to Navigate from Simulation via Spatial and Semantic\n  Information Synthesis with Noise Model Embedding",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.04521v1": {
            "Paper Title": "Tool Substitution with Shape and Material Reasoning Using Dual Neural\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.04175v1": {
            "Paper Title": "Multi-Agent Connected Autonomous Driving using Deep Reinforcement\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.04069v1": {
            "Paper Title": "Generative Autoregressive Networks for 3D Dancing Move Synthesis from\n  Music",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.04052v1": {
            "Paper Title": "Scaling Robot Supervision to Hundreds of Hours with RoboTurk: Robotic\n  Manipulation Dataset through Human Reasoning and Dexterity",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.04024v1": {
            "Paper Title": "MAME : Model-Agnostic Meta-Exploration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.09251v2": {
            "Paper Title": "Contact-Aided Invariant Extended Kalman Filtering for Robot State\n  Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.03955v1": {
            "Paper Title": "Distributed Recursive Filtering for Spatially Interconnected Systems\n  with Randomly Occurred Missing Measurements",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.03848v1": {
            "Paper Title": "Embedded Neural Networks for Robot Autonomy",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.03801v1": {
            "Paper Title": "Human Driver Behavior Prediction based on UrbanFlow",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.03799v1": {
            "Paper Title": "Hierarchical Reinforcement Learning Method for Autonomous Vehicle\n  Behavior Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.03369v1": {
            "Paper Title": "LiDAR Enhanced Structure-from-Motion",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.03262v1": {
            "Paper Title": "ROSY: An elegant language to teach the pure reactive nature of robot\n  programming",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.02725v1": {
            "Paper Title": "Benchmark for Skill Learning from Demonstration: Impact of User\n  Experience, Task Complexity, and Start Configuration on Performance",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.02620v1": {
            "Paper Title": "Argoverse: 3D Tracking and Forecasting with Rich Maps",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.02543v1": {
            "Paper Title": "Rapid Uncertainty Propagation and Chance-Constrained Path Planning for\n  Small Unmanned Aerial Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.08582v1": {
            "Paper Title": "End to end collision avoidance based on optical flow and neural networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.02320v1": {
            "Paper Title": "Nonverbal Robot Feedback for Human Teachers",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": "is challenging; humans\nmay have trouble providing useful demonstrations [7, 8] and knowing when to stop teaching ",
                    "Citation Text": "A. Sena, Y . Zhao, and M. J. Howard. Teaching human teachers to teach robot learners. In IEEE Interna-\ntional Conference on Robotics and Automation (ICRA) , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.04218v1",
                        "Citation Paper Title": "Title:Quantifying Teaching Behaviour in Robot Learning from Demonstration",
                        "Citation Paper Abstract": "Abstract:Learning from demonstration allows for rapid deployment of robot manipulators to a great many tasks, by relying on a person showing the robot what to do rather than programming it. While this approach provides many opportunities, measuring, evaluating and improving the person's teaching ability has remained largely unexplored in robot manipulation research. To this end, a model for learning from demonstration is presented here which incorporates the teacher's understanding of, and influence on, the learner. The proposed model is used to clarify the teacher's objectives during learning from demonstration, providing new views on how teaching failures and efficiency can be defined. The benefit of this approach is shown in two experiments (N=30 and N=36, respectively), which highlight the difficulty teachers have in providing effective demonstrations, and show how ~169-180% improvement in teaching efficiency can be achieved through evaluation and feedback shaped by the proposed framework, relative to unguided teaching.",
                        "Citation Paper Authors": "Authors:Aran Sena, Matthew J Howard"
                    },
                    "Keywords": [
                        "algorithmic",
                        ",",
                        "teaching",
                        "learning",
                        "feedback",
                        "demonstrations",
                        "from",
                        "robot"
                    ]
                }
            ]
        },
        "http://arxiv.org/abs/1911.02268v1": {
            "Paper Title": "Robot navigation and target capturing using nature-inspired approaches\n  in a dynamic environment",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06560v2": {
            "Paper Title": "Unclogging Our Arteries: Using Human-Inspired Signals to Disambiguate\n  Navigational Intentions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.01774v1": {
            "Paper Title": "Efficient Multi-robot Exploration via Multi-head Attention-based\n  Cooperation Strategy",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.01562v1": {
            "Paper Title": "DeepRacer: Educational Autonomous Racing Platform for Experimentation\n  with Sim2Real Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.01532v1": {
            "Paper Title": "Real-time Funnel Generation for Restricted Motion Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.01103v1": {
            "Paper Title": "Learning One-Shot Imitation from Humans without Humans",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": ".\nThis method allows the algorithm operating on these ran-\ndomised scenes to become invariant to domain differences\nthat appear in the real world. Rather than directly operating\non randomised images, RCAN ",
                    "Citation Text": "S. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan,\nJ. Ibarz, S. Levine, R. Hadsell, and K. Bousmalis, \u201cSim-to-real via sim-\nto-sim: Data-ef\ufb01cient robotic grasping via randomized-to-canonical\nadaptation networks,\u201d in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition , 2019, pp. 12 627\u201312 637.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.07252",
                        "Citation Paper Title": "Title:Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks",
                        "Citation Paper Abstract": "Abstract:Real world data, especially in the domain of robotics, is notoriously costly to collect. One way to circumvent this can be to leverage the power of simulation to produce large amounts of labelled data. However, training models on simulated images does not readily transfer to real-world ones. Using domain adaptation methods to cross this \"reality gap\" requires a large amount of unlabelled real-world data, whilst domain randomization alone can waste modeling power. In this paper, we present Randomized-to-Canonical Adaptation Networks (RCANs), a novel approach to crossing the visual reality gap that uses no real-world data. Our method learns to translate randomized rendered images into their equivalent non-randomized, canonical versions. This in turn allows for real images to also be translated into canonical sim images. We demonstrate the effectiveness of this sim-to-real approach by training a vision-based closed-loop grasping reinforcement learning agent in simulation, and then transferring it to the real world to attain 70% zero-shot grasp success on unseen objects, a result that almost doubles the success of learning the same task directly on domain randomization alone. Additionally, by joint finetuning in the real-world with only 5,000 real-world grasps, our method achieves 91%, attaining comparable performance to a state-of-the-art system trained with 580,000 real-world grasps, resulting in a reduction of real-world data by more than 99%.",
                        "Citation Paper Authors": "Authors:Stephen James, Paul Wohlhart, Mrinal Kalakrishnan, Dmitry Kalashnikov, Alex Irpan, Julian Ibarz, Sergey Levine, Raia Hadsell, Konstantinos Bousmalis"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1904.12306v2": {
            "Paper Title": "STANCE: Locomotion Adaptation over Soft Terrain",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.01082v1": {
            "Paper Title": "Technical Report: Co-learning of geometry and semantics for online 3D\n  mapping",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06527v3": {
            "Paper Title": "Towards Effective Human-AI Teams: The Case of Collaborative Packing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.07224v1": {
            "Paper Title": "Learning from Trajectories via Subgoal Discovery",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.00635v1": {
            "Paper Title": "Automatic Calibration of Dual-LiDARs Using Two Poles Stickered with\n  Retro-Reflective Tape",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.04102v2": {
            "Paper Title": "LIC-Fusion: LiDAR-Inertial-Camera Odometry",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.00238v1": {
            "Paper Title": "Situated GAIL: Multitask imitation using task-conditioned adversarial\n  inverse reinforcement learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.00165v1": {
            "Paper Title": "A2: Extracting Cyclic Switchings from DOB-nets for Rejecting Excessive\n  Disturbances",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.14540v1": {
            "Paper Title": "Team NCTU: Toward AI-Driving for Autonomous Surface Vehicles -- From\n  Duckietown to RobotX",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.14218v1": {
            "Paper Title": "S4G: Amodal Single-view Single-Shot SE(3) Grasp Detection in Cluttered\n  Scenes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.01207v1": {
            "Paper Title": "Autonomous Vehicles Meet the Physical World: RSS, Variability,\n  Uncertainty, and Proving Safety (Expanded Version)",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.14103v1": {
            "Paper Title": "CALC2.0: Combining Appearance, Semantic and Geometric Information for\n  Robust and Efficient Visual Loop Closure",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.13942v1": {
            "Paper Title": "Motion-Nets: 6D Tracking of Unknown Objects in Unseen Environments using\n  RGB",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.13880v1": {
            "Paper Title": "Path Planning Games",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.13726v1": {
            "Paper Title": "Safe Exploration for Interactive Machine Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.11977v2": {
            "Paper Title": "KETO: Learning Keypoint Representations for Tool Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.13399v1": {
            "Paper Title": "Robust Model-free Reinforcement Learning with Multi-objective Bayesian\n  Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.06684v2": {
            "Paper Title": "kPAM: KeyPoint Affordances for Category-Level Robotic Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.13317v1": {
            "Paper Title": "Distributed and Consistent Multi-Image Feature Matching via QuickMatch",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.13174v1": {
            "Paper Title": "Autonomous UAV Landing System Based on Visual Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.13000v1": {
            "Paper Title": "Human-centered Control of a Growing Soft Robot for Object Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.07613v2": {
            "Paper Title": "Learning from My Partner's Actions: Roles in Decentralized Robot Teams",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.12731v1": {
            "Paper Title": "Low-Cost GPS-Aided LiDAR State Estimation and Map Building",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.12728v1": {
            "Paper Title": "Real-Time, Environmentally-Robust 3D LiDAR Localization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.12453v1": {
            "Paper Title": "Asynchronous Methods for Model-Based Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.12294v1": {
            "Paper Title": "Robots as Actors in a Film: No War, A Robot Story",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.12284v1": {
            "Paper Title": "Task-Informed Fidelity Management for Speeding Up Robotics Simulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.03032v4": {
            "Paper Title": "A Holistic Visual Place Recognition Approach using Lightweight CNNs for\n  Significant ViewPoint and Appearance Changes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.11968v1": {
            "Paper Title": "Driving Datasets Literature Review",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.11956v1": {
            "Paper Title": "Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.01911v1": {
            "Paper Title": "BlenderProc",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.11459v1": {
            "Paper Title": "A Robot's Expressive Language Affects Human Strategy and Perceptions in\n  a Competitive Game",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.11432v1": {
            "Paper Title": "HRL4IN: Hierarchical Reinforcement Learning for Interactive Navigation\n  with Mobile Manipulators",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.11296v1": {
            "Paper Title": "Identifying Unknown Instances for Autonomous Driving",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": ", covering a wide range of rare objects. This is, however, still closed-set\nrecognition with weak labels. In ",
                    "Citation Text": "A. Osep, P. V oigtlaender, M. Weber, J. Luiten, and B. Leibe. 4d generic video object proposals. CoRR ,\nabs/1901.09260, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.09260",
                        "Citation Paper Title": "Title:4D Generic Video Object Proposals",
                        "Citation Paper Abstract": "Abstract:Many high-level video understanding methods require input in the form of object proposals. Currently, such proposals are predominantly generated with the help of networks that were trained for detecting and segmenting a set of known object classes, which limits their applicability to cases where all objects of interest are represented in the training set. This is a restriction for automotive scenarios, where unknown objects can frequently occur. We propose an approach that can reliably extract spatio-temporal object proposals for both known and unknown object categories from stereo video. Our 4D Generic Video Tubes (4D-GVT) method leverages motion cues, stereo data, and object instance segmentation to compute a compact set of video-object proposals that precisely localizes object candidates and their contours in 3D space and time. We show that given only a small amount of labeled data, our 4D-GVT proposal generator generalizes well to real-world scenarios, in which unknown categories appear. It outperforms other approaches that try to detect as many objects as possible by increasing the number of classes in the training set to several thousand.",
                        "Citation Paper Authors": "Authors:Aljosa Osep, Paul Voigtlaender, Mark Weber, Jonathon Luiten, Bastian Leibe"
                    },
                    "Keywords": [
                        "Open-Set",
                        ",",
                        "Driving",
                        "Autonomous",
                        "Instance",
                        "Perception",
                        "Segmentation"
                    ]
                }
            ]
        },
        "http://arxiv.org/abs/1910.11116v1": {
            "Paper Title": "Depth Camera Based Particle Filter for Robotic Osteotomy Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.11683v1": {
            "Paper Title": "Task-Motion Planning for Navigation in Belief Space",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.10985v1": {
            "Paper Title": "Learning Hierarchical Control for Robust In-Hand Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.10949v1": {
            "Paper Title": "ROBO: Robust, Fully Neural Object Detection for Robot Soccer",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.10852v1": {
            "Paper Title": "Computing Robust Inverse Kinematics Under Uncertainty",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.10754v1": {
            "Paper Title": "Learning Q-network for Active Information Acquisition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.10750v1": {
            "Paper Title": "6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.11670v1": {
            "Paper Title": "Contextual Imagined Goals for Self-Supervised Robotic Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.10258v1": {
            "Paper Title": "Robot-Friendly Cities",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.10253v1": {
            "Paper Title": "Towards Robotic Things in Society",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.10034v1": {
            "Paper Title": "Language-guided Semantic Mapping and Mobile Manipulation in Partially\n  Observable Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.10620v1": {
            "Paper Title": "Learning Humanoid Robot Running Skills through Proximal Policy\n  Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.09667v1": {
            "Paper Title": "Combining Benefits from Trajectory Optimization and Deep Reinforcement\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.09664v1": {
            "Paper Title": "Learning to Map Natural Language Instructions to Physical Quadcopter\n  Control using Simulated Flight",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.09503v2": {
            "Paper Title": "Model-free Deep Reinforcement Learning for Urban Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.09636v1": {
            "Paper Title": "Real-Time Multi-Diver Tracking and Re-identification for Underwater\n  Human-Robot Collaboration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.09471v1": {
            "Paper Title": "Modelling Generalized Forces with Reinforcement Learning for Sim-to-Real\n  Transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.08949v1": {
            "Paper Title": "Electric Sheep Team Description Paper Humanoid League Kid-Size 2019",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.08811v1": {
            "Paper Title": "Active 6D Multi-Object Pose Estimation in Cluttered Scenarios with Deep\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.08521v1": {
            "Paper Title": "Fast Local Planning and Mapping in Unknown Off-Road Terrain",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.08233v1": {
            "Paper Title": "Spatially-Aware Graph Neural Networks for Relational Behavior\n  Forecasting from Sensor Data",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "parametrizes the\noutput space as insertion areas where vehicle could go,\npredicting an estimated time of arrival and a spatial offset. ",
                    "Citation Text": "N. Djuric, V . Radosavljevic, H. Cui, T. Nguyen, F.-C. Chou, T.-H.\nLin, and J. Schneider, \u201cMotion prediction of traf\ufb01c actors for au-\ntonomous driving using deep convolutional networks,\u201d arXiv preprint\narXiv:1808.05819 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.05819",
                        "Citation Paper Title": "Title:Uncertainty-aware Short-term Motion Prediction of Traffic Actors for Autonomous Driving",
                        "Citation Paper Abstract": "Abstract:We address one of the crucial aspects necessary for safe and efficient operations of autonomous vehicles, namely predicting future state of traffic actors in the autonomous vehicle's surroundings. We introduce a deep learning-based approach that takes into account a current world state and produces raster images of each actor's vicinity. The rasters are then used as inputs to deep convolutional models to infer future movement of actors while also accounting for and capturing inherent uncertainty of the prediction task. Extensive experiments on real-world data strongly suggest benefits of the proposed approach. Moreover, following completion of the offline tests the system was successfully tested onboard self-driving vehicles.",
                        "Citation Paper Authors": "Authors:Nemanja Djuric, Vladan Radosavljevic, Henggang Cui, Thi Nguyen, Fang-Chieh Chou, Tsung-Han Lin, Nitin Singh, Jeff Schneider"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1910.08184v1": {
            "Paper Title": "Map-Predictive Motion Planning in Unknown Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.08181v1": {
            "Paper Title": "Online Learning in Planar Pushing with Combined Prediction Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.08102v1": {
            "Paper Title": "Probabilistic Trajectory Prediction for Autonomous Vehicles with\n  Attentive Recurrent Neural Process",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.08041v1": {
            "Paper Title": "Discrete Residual Flow for Probabilistic Pedestrian Behavior Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.07948v1": {
            "Paper Title": "Self-supervised 3D Shape and Viewpoint Estimation from Single Images for\n  Robotics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.07615v1": {
            "Paper Title": "Conditional Driving from Natural Language Instructions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.09236v3": {
            "Paper Title": "A Planning Framework for Persistent, Multi-UAV Coverage with Global\n  Deconfliction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.07224v1": {
            "Paper Title": "Teacher algorithms for curriculum learning of Deep RL in continuously\n  parameterized environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.02445v3": {
            "Paper Title": "Enhanced Human-Machine Interaction by Combining Proximity Sensing with\n  Global Perception",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.11898v2": {
            "Paper Title": "Perceptual Attention-based Predictive Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.07113v1": {
            "Paper Title": "Solving Rubik's Cube with a Robot Hand",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.07093v1": {
            "Paper Title": "Explainable Semantic Mapping for First Responders",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.06988v1": {
            "Paper Title": "Autonomous Aerial Cinematography In Unstructured Environments With\n  Learned Artistic Decision-Making",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.06632v1": {
            "Paper Title": "Multi-Frame GAN: Image Enhancement for Stereo Visual Odometry in Low\n  Light",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.06607v1": {
            "Paper Title": "Stereo-based Multi-motion Visual Odometry for Mobile Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.12612v2": {
            "Paper Title": "Learning Navigation Subroutines from Egocentric Videos",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.06530v1": {
            "Paper Title": "Concurrent Flow-Based Localization and Mapping in Time-Invariant Flow\n  Fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.06460v1": {
            "Paper Title": "A Feedback Motion Plan for Vehicles with Bounded Curvature Constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.06451v1": {
            "Paper Title": "Forward Kinematics Kernel for Improved Proxy Collision Checking",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.06425v1": {
            "Paper Title": "Real-time Data Driven Precision Estimator for RAVEN-II Surgical Robot\n  End Effector Position",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.03135v2": {
            "Paper Title": "DexPilot: Vision Based Teleoperation of Dexterous Robotic Hand-Arm\n  System",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.02550v2": {
            "Paper Title": "ClearGrasp: 3D Shape Estimation of Transparent Objects for Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.06031v1": {
            "Paper Title": "Imitating by generating: deep generative models for imitation of\n  interactive tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.06786v1": {
            "Paper Title": "Trajectory Advancement for Robot Stand-up with Human Assistance",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.06001v1": {
            "Paper Title": "Federated Transfer Reinforcement Learning for Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.04957v3": {
            "Paper Title": "Vision-and-Dialog Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.05636v1": {
            "Paper Title": "Trajectory optimization for a class of robots belonging to Constrained\n  Collaborative Mobile Agents (CCMA) family",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.05624v1": {
            "Paper Title": "A Research Platform for Multi-Robot Dialogue with Humans",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.05527v1": {
            "Paper Title": "Regularizing Model-Based Planning with Energy-Based Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.00640v2": {
            "Paper Title": "Deep Imitation Learning for Autonomous Driving in Generic Urban\n  Scenarios with Enhanced Safety",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.05449v1": {
            "Paper Title": "MultiPath: Multiple Probabilistic Anchor Trajectory Hypotheses for\n  Behavior Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.05005v1": {
            "Paper Title": "Learning from demonstration with model-based Gaussian process",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06153v2": {
            "Paper Title": "HJB Optimal Feedback Control with Deep Differential Value Functions and\n  Action Constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.04998v1": {
            "Paper Title": "Bayesian Optimization Meets Riemannian Manifolds in Robot Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.12989v2": {
            "Paper Title": "SURREAL-System: Fully-Integrated Stack for Distributed Deep\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.04953v1": {
            "Paper Title": "Scene-level Pose Estimation for Multiple Instances of Densely Packed\n  Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.04950v1": {
            "Paper Title": "Trajectory Planning for Autonomous Parking in Complex Environments: A\n  Tunnel-based Optimal Control Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.08076v3": {
            "Paper Title": "An Evaluation of Bayesian Methods for Bathymetry-based Localization of\n  Autonomous Underwater Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.06734v1": {
            "Paper Title": "Self Driving RC Car using Behavioral Cloning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.04803v1": {
            "Paper Title": "Autonomous Driving using Safe Reinforcement Learning by Incorporating a\n  Regret-based Human Lane-Changing Decision Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.04668v1": {
            "Paper Title": "AlignNet-3D: Fast Point Cloud Registration of Partially Observed Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.10522v2": {
            "Paper Title": "Intuitive Neuromyoelectric Control of a Dexterous Bionic Arm Using a\n  Modified Kalman Filter",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01557v2": {
            "Paper Title": "CyPhyHouse: A Programming, Simulation, and Deployment Toolchain for\n  Heterogeneous Distributed Coordination",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.04494v1": {
            "Paper Title": "Concepts for End-to-end Augmented Reality based Human-Robot Interaction\n  Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.04754v1": {
            "Paper Title": "A Generative Approach Towards Improved Robotic Detection of Marine\n  Litter",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.04365v1": {
            "Paper Title": "Asking Easy Questions: A User-Friendly Approach to Active Reward\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.04326v1": {
            "Paper Title": "Unconstrained Road Marking Recognition with Generative Adversarial\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.04787v2": {
            "Paper Title": "MAT: Multi-Fingered Adaptive Tactile Grasping via Deep Reinforcement\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.04142v1": {
            "Paper Title": "Imagined Value Gradients: Model-Based Policy Optimization with\n  Transferable Latent Dynamics Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.04537v1": {
            "Paper Title": "Defensive Escort Teams via Multi-Agent Deep Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.03973v1": {
            "Paper Title": "Towards Learning to Detect and Predict Contact Events on Vision-based\n  Tactile Sensors",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.03858v1": {
            "Paper Title": "Intention Recognition of Pedestrians and Cyclists by 2D Pose Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.03854v1": {
            "Paper Title": "Multimodal representation models for prediction and control from partial\n  information",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.03829v1": {
            "Paper Title": "Autonomous Multirobot Technologies for Mars Mining Base Construction and\n  Operation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.03701v1": {
            "Paper Title": "Learned Critical Probabilistic Roadmaps for Robotic Motion Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.03620v1": {
            "Paper Title": "Receding Horizon Curiosity",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.03568v1": {
            "Paper Title": "Object-centric Forward Modeling for Model Predictive Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.03516v1": {
            "Paper Title": "Advanced Autonomy on a Low-Cost Educational Drone Platform",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.03477v1": {
            "Paper Title": "Learning Parametric Constraints in High Dimensions from Demonstrations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.03764v2": {
            "Paper Title": "PAC: A Novel Self-Adaptive Neuro-Fuzzy Controller for Micro Aerial\n  Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.03358v1": {
            "Paper Title": "Deep Value Model Predictive Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.03336v1": {
            "Paper Title": "Improving Map Re-localization with Deep 'Movable' Objects Segmentation\n  on 3D LiDAR Point Clouds",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.02646v2": {
            "Paper Title": "Riemannian Motion Policy Fusion through Learnable Lyapunov Function\n  Reshaping",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.03159v1": {
            "Paper Title": "xYOLO: A Model For Real-Time Object Detection In Humanoid Soccer On\n  Low-End Hardware",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.03157v1": {
            "Paper Title": "Model-based Behavioral Cloning with Future Image Similarity Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.06786v2": {
            "Paper Title": "Curious iLQR: Resolving Uncertainty in Model-based RL",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.05224v2": {
            "Paper Title": "Multi-Agent Manipulation via Locomotion using Hierarchical Sim2Real",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.02835v1": {
            "Paper Title": "A Learnable Safety Measure",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.02818v1": {
            "Paper Title": "Learning Navigation by Visual Localization and Trajectory Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.02812v1": {
            "Paper Title": "Policies Modulating Trajectory Generators",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.05127v2": {
            "Paper Title": "Kernel Trajectory Maps for Multi-Modal Probabilistic Motion Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.02738v1": {
            "Paper Title": "An Interactive Control Approach to 3D Shape Reconstruction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.09006v2": {
            "Paper Title": "On the Limitations of Representing Functions on Sets",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.02564v1": {
            "Paper Title": "Action-conditioned Benchmarking of Robotic Video Prediction Models: a\n  Comparative Study",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": ". All these systems have been widely\nadapted in the \ufb01eld of video prediction.\nIn\ufb02uential work on video prediction by Mathieu et al. ",
                    "Citation Text": "Michael Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction beyond mean square error. 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.05440",
                        "Citation Paper Title": "Title:Deep multi-scale video prediction beyond mean square error",
                        "Citation Paper Abstract": "Abstract:Learning to predict future images from a video sequence involves the construction of an internal representation that models the image evolution accurately, and therefore, to some degree, its content and dynamics. This is why pixel-space video prediction may be viewed as a promising avenue for unsupervised feature learning. In addition, while optical flow has been a very studied problem in computer vision for a long time, future frame prediction is rarely approached. Still, many vision applications could benefit from the knowledge of the next frames of videos, that does not require the complexity of tracking every pixel trajectories. In this work, we train a convolutional network to generate future frames given an input sequence. To deal with the inherently blurry predictions obtained from the standard Mean Squared Error (MSE) loss function, we propose three different and complementary feature learning strategies: a multi-scale architecture, an adversarial training method, and an image gradient difference loss function. We compare our predictions to different published results based on recurrent neural networks on the UCF101 dataset",
                        "Citation Paper Authors": "Authors:Michael Mathieu, Camille Couprie, Yann LeCun"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1910.02527v1": {
            "Paper Title": "3D Scene Graph: A Structure for Unified Semantics, 3D Space, and Camera",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.03613v2": {
            "Paper Title": "Data Efficient Reinforcement Learning for Legged Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.02461v1": {
            "Paper Title": "Risk-Aware Reasoning for Autonomous Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.13627v2": {
            "Paper Title": "Disentangled Relational Representations for Explaining and Learning from\n  Demonstration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.02291v1": {
            "Paper Title": "Cascaded Gaussian Processes for Data-efficient Robot Dynamics Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.02201v1": {
            "Paper Title": "Early Estimation of User's Intention of Tele-Operation Using Object\n  Affordance and Hand Motion in a Dual First-Person Vision",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.02165v1": {
            "Paper Title": "SLAM-based Integrity Monitoring Using GPS and Fish-eye Camera",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.02106v1": {
            "Paper Title": "Direct Visual-Inertial Odometry with Semi-Dense Mapping",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.02066v1": {
            "Paper Title": "Higher Order Function Networks for View Planning and Multi-View\n  Reconstruction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01821v1": {
            "Paper Title": "Motion Planning through Demonstration to Deal with Complex Motions in\n  Assembly Process",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01701v1": {
            "Paper Title": "Low-cost LIDAR based Vehicle Pose Estimation and Tracking",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.04874v1": {
            "Paper Title": "A Stereo Algorithm for Thin Obstacles and Reflective Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01300v1": {
            "Paper Title": "Resilience in multi-robot target tracking through reconfiguration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01240v1": {
            "Paper Title": "Deep Reinforcement Learning for Single-Shot Diagnosis and Adaptation in\n  Damaged Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01907v1": {
            "Paper Title": "Attacking Vision-based Perception in End-to-End Autonomous Driving\n  Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01078v1": {
            "Paper Title": "ROS Rescue : Fault Tolerance System for Robot Operating System",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.04407v2": {
            "Paper Title": "Self-Driving Cars: A Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.04695v1": {
            "Paper Title": "GLADAS: Gesture Learning for Advanced Driver Assistance Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.00528v1": {
            "Paper Title": "Augmenting learning using symmetry in a biologically-inspired domain",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.00343v1": {
            "Paper Title": "Autonomous Bimanual Functional Regrasping of Novel Object Class\n  Instances",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01019v1": {
            "Paper Title": "Area Graph: Generation of Topological Maps using the Voronoi Diagram",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01012v1": {
            "Paper Title": "Thread Homeostasis: Real-Time Anomalous Behavior Detection for\n  Safety-Critical Software",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.06544v4": {
            "Paper Title": "Deep Imitative Models for Flexible Inference, Planning, and Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.03167v2": {
            "Paper Title": "HomebrewedDB: RGB-D Dataset for 6D Pose Estimation of 3D Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.13870v1": {
            "Paper Title": "Learning Compact Models for Planning with Exogenous Processes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.08601v2": {
            "Paper Title": "Object-RPE: Dense 3D Reconstruction and Pose Estimation with\n  Convolutional Neural Networks for Warehouse Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.01296v3": {
            "Paper Title": "PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.13582v1": {
            "Paper Title": "Dynamic Interaction-Aware Scene Understanding for Reinforcement Learning\n  in Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.13551v1": {
            "Paper Title": "Enhancing Object Detection in Adverse Conditions using Thermal Imaging",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.13493v1": {
            "Paper Title": "Robust Data Association for Object-level Semantic SLAM",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.03299v2": {
            "Paper Title": "PyramNet: Point Cloud Pyramid Attention Network and Graph Embedding\n  Module for Classification and Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.11213v2": {
            "Paper Title": "Probabilistic Data Association via Mixture Models for Robust Semantic\n  SLAM",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.04862v1": {
            "Paper Title": "Vision-Based Autonomous Vehicle Control using the Two-Point Visual\n  Driver Control Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.13072v1": {
            "Paper Title": "Regression Planning Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.00399v1": {
            "Paper Title": "Safe Reinforcement Learning on Autonomous Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.07497v3": {
            "Paper Title": "Harmonious Sampling for Mobile Manipulation Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.12460v1": {
            "Paper Title": "Leveraging Multimodal Haptic Sensory Data for Robust Cutting",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.12436v1": {
            "Paper Title": "Autonomous Control of a Tendon-driven Robotic Limb with Elastic Elements\n  Reveals that Added Elasticity can Enhance Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.00671v3": {
            "Paper Title": "PCN: Point Completion Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.12324v1": {
            "Paper Title": "Learning Generalizable Locomotion Skills with Hierarchical Reinforcement\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.12271v1": {
            "Paper Title": "RLBench: The Robot Learning Benchmark & Learning Environment",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.12262v1": {
            "Paper Title": "Exercising with an \"Iron Man\": Design for a Robot Exercise Coach for\n  Persons with Dementia",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.11888v1": {
            "Paper Title": "Resolving Marker Pose Ambiguity by Robust Rotation Averaging with Clique\n  Constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.11652v1": {
            "Paper Title": "Deep Dynamics Models for Learning Dexterous Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.11280v1": {
            "Paper Title": "Human-in-the-loop Robotic Manipulation Planning for Collaborative\n  Assembly",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.11226v1": {
            "Paper Title": "Minimal Work: A Grasp Quality Metric for Deformable Hollow Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.11221v1": {
            "Paper Title": "Contact-Aware Controller Design for Complementarity Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.11125v1": {
            "Paper Title": "Leveraging the Template and Anchor Framework for Safe, Online Robotic\n  Gait Design",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.10823v1": {
            "Paper Title": "Software architecture for YOLO, a creativity-stimulating robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.10809v1": {
            "Paper Title": "Towards S-NAMO: Socially-aware Navigation Among Movable Obstacles",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.10650v1": {
            "Paper Title": "Non-monotonic Logical Reasoning Guiding Deep Learning for Explainable\n  Visual Question Answering",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.10582v1": {
            "Paper Title": "Kalman Filtering with Gaussian Processes Measurement Noise",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.10400v1": {
            "Paper Title": "Robot Navigation in Crowds by Graph Convolutional Networks with\n  Attention Learned from Human Gaze",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06561v2": {
            "Paper Title": "Commitments in Human-Robot Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.10270v1": {
            "Paper Title": "Pose Estimation for Texture-less Shiny Objects in a Single RGB Image\n  Using Synthetic Training Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.10111v1": {
            "Paper Title": "Optimal Reduced-order Modeling of Bipedal Locomotion",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.08804v2": {
            "Paper Title": "Finding Locomanipulation Plans Quickly in the Locomotion Constrained\n  Manifold",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.10034v1": {
            "Paper Title": "In-hand Sliding Regrasp with Spring-Sliding Compliance",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.12536v4": {
            "Paper Title": "A Quaternion-based Certifiably Optimal Solution to the Wahba Problem\n  with Outliers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.05930v2": {
            "Paper Title": "Cross-View Policy Learning for Street Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.11353v2": {
            "Paper Title": "Feedback Control for Autonomous Riding of Hovershoes by a Cassie Bipedal\n  Robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.09880v1": {
            "Paper Title": "Language-guided Adaptive Perception with Hierarchical Symbolic\n  Representations for Mobile Manipulators",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.03227v4": {
            "Paper Title": "Pixel-Attentive Policy Gradient for Multi-Fingered Grasping in Cluttered\n  Scenes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.09490v3": {
            "Paper Title": "Aztarna, a footprinting tool for robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.11842v2": {
            "Paper Title": "Self-Imitation Learning of Locomotion Movements through Termination\n  Curriculum",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.06725v3": {
            "Paper Title": "Mutual Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.09669v1": {
            "Paper Title": "Building a Library of Tactile Skills Based on FingerVision",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.00572v1": {
            "Paper Title": "Map as The Hidden Sensor: Fast Odometry-Based Global Localization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06363v2": {
            "Paper Title": "Sample Complexity of Probabilistic Roadmaps via $\u03b5$-nets",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.10980v1": {
            "Paper Title": "PST900: RGB-Thermal Calibration, Dataset and Segmentation Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.09536v1": {
            "Paper Title": "Object grasping planning for the situation when soft and rigid objects\n  are mixed together",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.08862v1": {
            "Paper Title": "Assembly of randomly placed parts realized by using only one robot arm\n  with a general parallel-jaw gripper",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.08812v1": {
            "Paper Title": "Flexible Disaster Response of Tomorrow -- Final Presentation and\n  Evaluation of the CENTAURO System",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.08792v1": {
            "Paper Title": "Agent Prioritization for Autonomous Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.08537v1": {
            "Paper Title": "Visual Measurement Integrity Monitoring for UAV Localization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.04360v4": {
            "Paper Title": "Hybrid system identification using switching density networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.08291v1": {
            "Paper Title": "SalsaNet: Fast Road and Vehicle Segmentation in LiDAR Point Clouds for\n  Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.08162v1": {
            "Paper Title": "Nailed It: Autonomous Roofing with a Nailgun-Equipped Octocopter",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.01503v2": {
            "Paper Title": "Creating Navigable Space from Sparse Noisy Map Points",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.11306v2": {
            "Paper Title": "Vehicular Multi-object Tracking with Persistent Detector Failures",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.08059v1": {
            "Paper Title": "Risk Assessment and Planning with Bidirectional Reachability for\n  Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.07966v1": {
            "Paper Title": "Design of the Anatomically Correct, Biomechatronic Hand",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.07953v1": {
            "Paper Title": "What Are You Looking at? Detecting Human Intention in Gaze based\n  Human-Robot Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06585v2": {
            "Paper Title": "Deep Robotic Prediction with hierarchical RGB-D Fusion",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.07867v1": {
            "Paper Title": "TruPercept: Trust Modelling for Autonomous Vehicle Cooperative\n  Perception from Synthetic Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.07745v1": {
            "Paper Title": "Adversarial Feature Training for Generalizable Robotic Visuomotor\n  Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.01401v3": {
            "Paper Title": "Unsupervised Emergence of Egocentric Spatial Structure from Sensorimotor\n  Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.07671v1": {
            "Paper Title": "Spatio-Semantic ConvNet-Based Visual Place Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.08202v4": {
            "Paper Title": "Person Following by Autonomous Robots: A Categorical Overview",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.07592v1": {
            "Paper Title": "Real-time Multi-target Path Prediction and Planning for Autonomous\n  Driving aided by FCN",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.07545v1": {
            "Paper Title": "Real-Time Variational Fisheye Stereo without Rectification and\n  Undistortion",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.07507v1": {
            "Paper Title": "Scene Compliant Trajectory Forecast with Agent-Centric Spatio-Temporal\n  Grids",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.09810v3": {
            "Paper Title": "Deep Transfer Learning of Pick Points on Fabric for Robot Bed-Making",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.07496v1": {
            "Paper Title": "Source Seeking in Unknown Environments with Convex Obstacles",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.07476v1": {
            "Paper Title": "Experimental Validation of Stable Coordination for Multi-Robot Systems\n  with Limited Fields of View using a PortableMulti-Robot Testbed",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.07471v1": {
            "Paper Title": "Multimodal Dataset of Human-Robot Hugging Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.06553v2": {
            "Paper Title": "Neuroflight: Next Generation Flight Control Firmware",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": "which showed PPO to out\nperform Deep Deterministic Policy Gradient (DDPG) ",
                    "Citation Text": "T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y . Tassa,\nD. Silver, and D. Wierstra, \u201cContinuous control with deep reinforcement\nlearning,\u201d arXiv preprint arXiv:1509.02971 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1509.02971",
                        "Citation Paper Title": "Title:Continuous control with deep reinforcement learning",
                        "Citation Paper Abstract": "Abstract:We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.",
                        "Citation Paper Authors": "Authors:Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1909.07247v1": {
            "Paper Title": "Surfing on an uncertain edge: Precision cutting of soft tissue using\n  torque-based medium classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.04361v3": {
            "Paper Title": "Estimating Pedestrian Moving State Based on Single 2D Body Pose",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.07201v1": {
            "Paper Title": "MuPNet: Multi-modal Predictive Coding Network for Place Recognition by\n  Unsupervised Learning of Joint Visuo-Tactile Latent Representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.09592v1": {
            "Paper Title": "Fault-Diagnosing SLAM for Varying Scale Change Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.07050v1": {
            "Paper Title": "A Single Multi-Task Deep Neural Network with Post-Processing for Object\n  Detection with Reasoning and Robotic Grasp Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.12920v3": {
            "Paper Title": "Bayesian Grasp: Robotic visual stable grasp based on prior tactile\n  knowledge",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06998v1": {
            "Paper Title": "Real-time 3-D Mapping with Estimating Acoustic Materials",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.03568v2": {
            "Paper Title": "Active Robot-Assisted Feeding with a General-Purpose Mobile Manipulator:\n  Design, Evaluation, and Lessons Learned",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06980v1": {
            "Paper Title": "kPAM-SC: Generalizable Manipulation Planning using KeyPoint Affordance\n  and Shape Completion",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06933v1": {
            "Paper Title": "Self-Supervised Correspondence in Visuomotor Policy Learning",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": ", or using no visual\ntraining and instead using only generic pre-trained visual features ",
                    "Citation Text": "P . Sermanet, K. Xu, and S. Levine, \u201cUnsupervised perceptual rewards for\nimitation learning,\u201d Robotics: Science and Systems (RSS) , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.06699",
                        "Citation Paper Title": "Title:Unsupervised Perceptual Rewards for Imitation Learning",
                        "Citation Paper Abstract": "Abstract:Reward function design and exploration time are arguably the biggest obstacles to the deployment of reinforcement learning (RL) agents in the real world. In many real-world tasks, designing a reward function takes considerable hand engineering and often requires additional sensors to be installed just to measure whether the task has been executed successfully. Furthermore, many interesting tasks consist of multiple implicit intermediate steps that must be executed in sequence. Even when the final outcome can be measured, it does not necessarily provide feedback on these intermediate steps. To address these issues, we propose leveraging the abstraction power of intermediate visual representations learned by deep models to quickly infer perceptual reward functions from small numbers of demonstrations. We present a method that is able to identify key intermediate steps of a task from only a handful of demonstration sequences, and automatically identify the most discriminative features for identifying these steps. This method makes use of the features in a pre-trained deep model, but does not require any explicit specification of sub-goals. The resulting reward functions can then be used by an RL agent to learn to perform the task in real-world settings. To evaluate the learned reward, we present qualitative results on two real-world tasks and a quantitative evaluation against a human-designed reward function. We also show that our method can be used to learn a real-world door opening skill using a real robot, even when the demonstration used for reward learning is provided by a human using their own hand. To our knowledge, these are the first results showing that complex robotic manipulation skills can be learned directly and without supervised labels from a video of a human performing the task. Supplementary material and data are available at this https URL",
                        "Citation Paper Authors": "Authors:Pierre Sermanet, Kelvin Xu, Sergey Levine"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1904.11042v2": {
            "Paper Title": "Physical Adversarial Textures that Fool Visual Object Tracking",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.05711v2": {
            "Paper Title": "Optimal Routing Schedules for Robots Operating in Aisle-Structures",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.04331v2": {
            "Paper Title": "Visual Area Coverage with Attitude-Dependent Camera Footprints by\n  Particle Harvesting",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.00895v2": {
            "Paper Title": "Federated Imitation Learning: A Privacy Considered Imitation Learning\n  Framework for Cloud Robotic Systems with Heterogeneous Sensor Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06736v1": {
            "Paper Title": "Identifying Multiple Interaction Events from Tactile Data during\n  Robot-Human Object Transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.09594v1": {
            "Paper Title": "Mining Minimal Map-Segments for Visual Place Classifiers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06670v1": {
            "Paper Title": "Delivering Cognitive Behavioral Therapy Using A Conversational\n  SocialRobot",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.11004v1": {
            "Paper Title": "Fuzzy Knowledge-Based Architecture for Learning and Interaction in\n  Social Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06529v1": {
            "Paper Title": "Solving Service Robot Tasks: UT Austin Villa@Home 2019 Team Report",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06508v1": {
            "Paper Title": "Building Second-Order Mental Models for Human-Robot Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06493v1": {
            "Paper Title": "Flight Controller Synthesis Via Deep Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06418v1": {
            "Paper Title": "Towards A Robot Explanation System: A Survey and Our Approach to State\n  Summarization, Storage and Querying, and Human Interface",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.07814v2": {
            "Paper Title": "Large-scale 3D Mapping of Subarctic Forests",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.05731v2": {
            "Paper Title": "A Reinforcement Learning Framework for Sequencing Multi-Robot Behaviors",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.08052v1": {
            "Paper Title": "Towards an Adaptive Robot for Sports and Rehabilitation Coaching",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06174v1": {
            "Paper Title": "Petri Net Machines for Human-Agent Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.04349v3": {
            "Paper Title": "FreiHAND: A Dataset for Markerless Capture of Hand Pose and Shape from\n  Single RGB Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06087v1": {
            "Paper Title": "Human Following for Wheeled Robot with Monocular Pan-tilt Camera",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06077v1": {
            "Paper Title": "Enabling Humans to Plan Inspection Paths Using a Virtual Reality\n  Interface",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06034v1": {
            "Paper Title": "Deep Learned Path Planning via Randomized Reward-Linked-Goals and\n  Potential Space Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.08508v1": {
            "Paper Title": "Using Synthetic Data and Deep Networks to Recognize Primitive Shapes for\n  Object Grasping",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.05829v1": {
            "Paper Title": "Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks\n  via Visual Subgoal Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.05777v1": {
            "Paper Title": "Robots that Take Advantage of Human Trust",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.06440v4": {
            "Paper Title": "Robots that Sync and Swarm: A Proof of Concept in ROS 2",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.08391v2": {
            "Paper Title": "Learning Object-Action Relations from Bimanual Human Demonstration Using\n  Graph Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.05426v1": {
            "Paper Title": "Tactile-Based Insertion for Dense Box-Packing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.05160v1": {
            "Paper Title": "Trust and Cognitive Load During Human-Robot Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.05089v1": {
            "Paper Title": "Adaptable Human Intention and Trajectory Prediction for Human-Robot\n  Collaboration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.11766v1": {
            "Paper Title": "An Automated Vehicle (AV) like Me? The Impact of Personality\n  Similarities and Differences between Humans and AVs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.04993v1": {
            "Paper Title": "Four-Arm Manipulation via Feet Interfaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.04905v1": {
            "Paper Title": "SwarmMesh: A Distributed Data Structure for Cooperative Multi-Robot\n  Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.03772v2": {
            "Paper Title": "A Survey on Reproducibility by Evaluating Deep Reinforcement Learning\n  Algorithms on Real-World Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.04838v1": {
            "Paper Title": "3D traffic flow model for UAVs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.04768v1": {
            "Paper Title": "Human-robot Collaborative Navigation Search using Social Reward Sources",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.10203v3": {
            "Paper Title": "3D-LaneNet: End-to-End 3D Multiple Lane Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.04312v1": {
            "Paper Title": "Learning Actions from Human Demonstration Video for Robotic Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.04306v1": {
            "Paper Title": "Bayesian Relational Memory for Semantic Visual Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.04250v1": {
            "Paper Title": "Real-time Scalable Dense Surfel Mapping",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.11866v8": {
            "Paper Title": "From Pixels to Buildings: End-to-end Probabilistic Deep Networks for\n  Large-scale Semantic Mapping",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.05664v1": {
            "Paper Title": "Multimodal Attention Branch Network for Perspective-Free Sentence\n  Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.04163v1": {
            "Paper Title": "MLOD: A multi-view 3D object detection based on robust feature fusion\n  method",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.02935v3": {
            "Paper Title": "An optimization framework for simulation and kinematic control of\n  Constrained Collaborative Mobile Agents (CCMA) system",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.05659v1": {
            "Paper Title": "Estimating Fingertip Forces, Torques, and Local Curvatures from\n  Fingernail Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.03985v1": {
            "Paper Title": "Certified Grasping",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.01177v2": {
            "Paper Title": "PanopticFusion: Online Volumetric Semantic Mapping at the Level of Stuff\n  and Things",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.03687v1": {
            "Paper Title": "Trunk Pitch Oscillations for Joint Load Redistribution in Humans and\n  Humanoid Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.03669v1": {
            "Paper Title": "DensePoint: Learning Densely Contextual Representation for Efficient\n  Point Cloud Processing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.03584v1": {
            "Paper Title": "Reality as a simulation of reality: robot illusions, fundamental limits,\n  and a physical demonstration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.05134v1": {
            "Paper Title": "Robot Risk-Awareness by Formal Risk Reasoning and Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.03462v1": {
            "Paper Title": "Deep Workpiece Region Segmentation for Bin Picking",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.03049v2": {
            "Paper Title": "Loosely Coupled Payload Transport System with Robot Replacement",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.03304v1": {
            "Paper Title": "Robotic Coverage for Continuous Mapping Ahead of a Moving Vehicle",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.02350v2": {
            "Paper Title": "Robot-Assisted Feeding: Generalizing Skewering Strategies across Food\n  Items on a Realistic Plate",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.02966v1": {
            "Paper Title": "Robust Barrier Functions for a Fully Autonomous, Remotely Accessible\n  Swarm-Robotics Testbed",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.02933v1": {
            "Paper Title": "AR-based interaction for safe human-robot collaborative manufacturing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.02778v1": {
            "Paper Title": "Automatic Failure Recovery for End-User Programs on Service Mobile\n  Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.02721v1": {
            "Paper Title": "Real-time Joint Motion Analysis and Instrument Tracking for\n  Robot-Assisted Orthopaedic Surgery",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.06884v2": {
            "Paper Title": "Deep Reinforcement Learning Based Robot Arm Manipulation with Efficient\n  Training Data through Simulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.02531v1": {
            "Paper Title": "Robot Motion Risk Reasoning Framework",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.02288v1": {
            "Paper Title": "An Optimal Assistive Control Strategy based on User's Motor Goal\n  Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.01140v2": {
            "Paper Title": "Learning Object Bounding Boxes for 3D Instance Segmentation on Point\n  Clouds",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.02198v1": {
            "Paper Title": "Efficient Optimal Planning in non-FIFO Time-Dependent Flow Fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.02129v1": {
            "Paper Title": "Towards Precise Robotic Grasping by Probabilistic Post-grasp\n  Displacement Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.08770v2": {
            "Paper Title": "Inferring Occluded Geometry Improves Performance when Retrieving an\n  Object from Dense Clutter",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.01957v1": {
            "Paper Title": "Dispersion of Mobile Robots in the Global Communication Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.01768v1": {
            "Paper Title": "Learning to gesticulate by observation using a deep generative approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.01575v1": {
            "Paper Title": "Learning sparse representations in reinforcement learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.01423v1": {
            "Paper Title": "Exploration Without Global Consistency Using Local Volume Consolidation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.05015v12": {
            "Paper Title": "Deep Reinforcement Learning for Unmanned Aerial Vehicle-Assisted\n  Vehicular Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.10175v2": {
            "Paper Title": "Robust Trajectory Tracking Control for Underactuated Autonomous\n  Underwater Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.01304v2": {
            "Paper Title": "Vision-Based Autonomous UAV Navigation and Landing for Urban Search and\n  Rescue",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.05329v1": {
            "Paper Title": "Robot Capability and Intention in Trust-based Decisions across Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.08641v2": {
            "Paper Title": "An Optimal Task Allocation Strategy for Heterogeneous Multi-Robot\n  Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.00792v1": {
            "Paper Title": "Conditional Vehicle Trajectories Prediction in CARLA Urban Environment",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.00732v1": {
            "Paper Title": "Hierarchical Control for Bipedal Locomotion using Central Pattern\n  Generators and Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.00713v1": {
            "Paper Title": "Estimation of Absolute Scale in Monocular SLAM Using Synthetic Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.01342v1": {
            "Paper Title": "Building Small-Satellites to Live Through the Kessler Effect",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.00450v1": {
            "Paper Title": "Model-free Visual Control for Continuum Robot Manipulators via\n  Orientation Adaptation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.00342v1": {
            "Paper Title": "On Maximizing Lateral Clearance of an Autonomous Vehicle in Urban\n  Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.00193v1": {
            "Paper Title": "Functional advantages of an adaptive Theory of Mind for robotics: a\n  review of current architectures",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.00192v1": {
            "Paper Title": "Combined Task and Motion Planning for a Dual-arm Robot to Use a Suction\n  Cup Tool",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.09090v2": {
            "Paper Title": "Entropic Risk Measure in Policy Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.04298v2": {
            "Paper Title": "Deep Learning for Spacecraft Pose Estimation from Photorealistic\n  Rendering",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.02082v3": {
            "Paper Title": "ReFusion: 3D Reconstruction in Dynamic Environments for RGB-D Cameras\n  Exploiting Residuals",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.02285v2": {
            "Paper Title": "Lambda-Field: A Continuous Counterpart of the Bayesian Occupancy Grid\n  for Risk Assessment",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.10925v4": {
            "Paper Title": "FA-Harris: A Fast and Asynchronous Corner Detector for Event Cameras",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.10125v1": {
            "Paper Title": "Proactive Intention Recognition for Joint Human-Robot Search and Rescue\n  Missions through Monte-Carlo Planning in POMDP Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.00606v4": {
            "Paper Title": "General Support-Effective Decomposition for Multi-Directional 3D\n  Printing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.09445v1": {
            "Paper Title": "High Performance Visual Object Tracking with Unified Convolutional\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.01462v2": {
            "Paper Title": "Modeling and Analysis of Non-unique Behaviors in Multiple Frictional\n  Impacts",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.01350v2": {
            "Paper Title": "Rapidly-Exploring Quotient-Space Trees: Motion Planning using Sequential\n  Simplifications",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.10161v3": {
            "Paper Title": "A Novel Multi-layer Framework for Tiny Obstacle Discovery",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.05993v3": {
            "Paper Title": "The Trajectron: Probabilistic Multi-Agent Trajectory Modeling With\n  Dynamic Spatiotemporal Graphs",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": ".\nEach person is modeled as an LSTM with all other\npedestrian hidden states being incorporated with a\nglobal pooling module. Pooled data as well as encoded\ntrajectories are then fed into a Generative Adversarial\nNetwork (GAN) ",
                    "Citation Text": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In Conf. on\nNeural Information Processing Systems , 2014. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.2661",
                        "Citation Paper Title": "Title:Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1908.08891v1": {
            "Paper Title": "Flexible Trinocular: Non-rigid Multi-Camera-IMU Dense Reconstruction for\n  UAV Navigation and Mapping",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.08704v1": {
            "Paper Title": "Sequential Adversarial Learning for Self-Supervised Deep Visual Odometry",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.08659v1": {
            "Paper Title": "A Comparison of Action Spaces for Learning Manipulation Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.08244v1": {
            "Paper Title": "Object detection on aerial imagery using CenterNet",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.11020v3": {
            "Paper Title": "DPOD: 6D Pose Object Detector and Refiner",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.01248v3": {
            "Paper Title": "Asymmetric Dual-Arm Task Execution using an Extended Relative Jacobian",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.09009v1": {
            "Paper Title": "Development of a Robotic System for Automatic Wheel Removal and Fitting",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.06422v1": {
            "Paper Title": "Scene Classification in Indoor Environments for Robots using Context\n  Based Word Embeddings",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.10520v2": {
            "Paper Title": "6-DOF GraspNet: Variational Grasp Generation for Object Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.08377v2": {
            "Paper Title": "Gaze Training by Modulated Dropout Improves Imitation Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.01416v3": {
            "Paper Title": "SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR\n  Sequences",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": "surpasses the amount and di-\nversity of labeled data compared to Cityscapes .\nAlso in point cloud-based interpretation, e.g., semantic\nsegmentation, RGB-D based datasets enabled tremendous\nprogress. ShapeNet ",
                    "Citation Text": "Angel X. Chang, Thomas Funkhouser, Leonidas J. Guibas,\nPat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,\nManolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi,\nand Fisher Yu. ShapeNet: An Information-Rich 3D Model\nRepository. Technical Report arXiv:1512.03012 [cs.GR],\nStanford University and Princeton University and Toyota\nTechnological Institute at Chicago, 2015. 2, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.03012",
                        "Citation Paper Title": "Title:ShapeNet: An Information-Rich 3D Model Repository",
                        "Citation Paper Abstract": "Abstract:We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.",
                        "Citation Paper Authors": "Authors:Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, Fisher Yu"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1908.05552v1": {
            "Paper Title": "Learning Interactive Behaviors for Musculoskeletal Robots Using Bayesian\n  Interaction Primitives",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.05402v1": {
            "Paper Title": "Shield Synthesis for Real: Enforcing Safety in Cyber-Physical Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.05250v1": {
            "Paper Title": "Swimming locomotion of Soft Robotic Snakes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.04692v1": {
            "Paper Title": "General Hand Guidance Framework using Microsoft HoloLens",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.04558v1": {
            "Paper Title": "Loop Closure Detection in Closed Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.03409v2": {
            "Paper Title": "Transferable Representation Learning in Vision-and-Language Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.04354v1": {
            "Paper Title": "Learning to Detect Collisions for Continuum Manipulators without a Prior\n  Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.06979v3": {
            "Paper Title": "Learning Socially Appropriate Robot Approaching Behavior Toward Groups\n  using Deep Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.04087v1": {
            "Paper Title": "Fast Adaptation with Meta-Reinforcement Learning for Trust Modelling in\n  Human-Robot Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.04293v1": {
            "Paper Title": "Deep Dexterous Grasping of Novel Objects from a Single View",
            "Sentences": []
        },
        "http://arxiv.org/abs/1709.07174v6": {
            "Paper Title": "Agile Autonomous Driving using End-to-End Deep Imitation Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.06144v2": {
            "Paper Title": "Feedback MPC for Torque-Controlled Legged Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.03364v1": {
            "Paper Title": "Deep Learning based Wearable Assistive System for Visually Impaired\n  People",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.03269v1": {
            "Paper Title": "Neural-Learning Trajectory Tracking Control of Flexible-Joint Robot\n  Manipulators with Unknown Dynamics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.03166v1": {
            "Paper Title": "Disturbance Estimation and Rejection for High-Precision Multirotor\n  Position Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.02641v3": {
            "Paper Title": "An Extensible Interactive Interface for Agent Design",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.02999v1": {
            "Paper Title": "Learning Vision-based Flight in Drone Swarms by Imitation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.03440v1": {
            "Paper Title": "Learning to Grasp from 2.5D images: a Deep Reinforcement Learning\n  Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.08229v2": {
            "Paper Title": "Optimal Continuous State POMDP Planning with Semantic Observations: A\n  Variational Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.02850v1": {
            "Paper Title": "Dynamic Autonomous Surface Vehicle Controls Under Changing Environmental\n  Forces",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.02827v1": {
            "Paper Title": "Riverine Coverage with an Autonomous Surface Vehicle over Known\n  Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.02813v1": {
            "Paper Title": "Meander Based River Coverage by an Autonomous Surface Vehicle",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.02704v1": {
            "Paper Title": "Unified Simulation and Test Platform for Control Systems of Unmanned\n  Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.02432v1": {
            "Paper Title": "DronePick: Object Picking and Delivery Teleoperation with the Drone\n  Controlled by a Wearable Tactile Display",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.01046v2": {
            "Paper Title": "Adaptive Stress Testing with Reward Augmentation for Autonomous Vehicle\n  Validation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.02274v5": {
            "Paper Title": "Episodic Curiosity through Reachability",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.04391v1": {
            "Paper Title": "Local Supports Global: Deep Camera Relocalization with Sequence\n  Enhancement",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.01896v1": {
            "Paper Title": "Representing Robot Task Plans as Robust Logical-Dynamical Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.01885v1": {
            "Paper Title": "Towards Active Robotic Vision in Agriculture: A Deep Learning Approach\n  to Visual Servoing in Occluded and Unstructured Protected Cropping\n  Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.01618v1": {
            "Paper Title": "Speech Driven Backchannel Generation using Deep Q-Network for Enhancing\n  Engagement in Human-Robot Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.01504v1": {
            "Paper Title": "Part Segmentation for Highly Accurate Deformable Tracking in Occlusions\n  via Fully Convolutional Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.03173v2": {
            "Paper Title": "Ego-Pose Estimation and Forecasting as Real-Time PD Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.01367v1": {
            "Paper Title": "Unsupervised Learning of Depth and Deep Representation for Visual\n  Odometry from Monocular Videos in a Metric Space",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.01094v1": {
            "Paper Title": "Requirements-driven Test Generation for Autonomous Vehicles with Machine\n  Learning Components",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.05841v2": {
            "Paper Title": "Deep Reinforcement Learning for Industrial Insertion Tasks with Visual\n  Inputs and Natural Rewards",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.06427v2": {
            "Paper Title": "A Convex-Combinatorial Model for Planar Caging",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.00858v1": {
            "Paper Title": "Distilling Knowledge From a Deep Pose Regressor Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.00641v1": {
            "Paper Title": "Online Motion Planning Over Multiple Homotopy Classes with Gaussian\n  Process Inference",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.13153v2": {
            "Paper Title": "Grounding Language Attributes to Objects using Bayesian Eigenobjects",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.07813v4": {
            "Paper Title": "Learning Real-World Robot Policies by Dreaming",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.00191v1": {
            "Paper Title": "DEDUCE: Diverse scEne Detection methods in Unseen Challenging\n  Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.00178v1": {
            "Paper Title": "Scalable Place Recognition Under Appearance Change for Autonomous\n  Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.02489v2": {
            "Paper Title": "GQ-STN: Optimizing One-Shot Grasp Detection based on Robustness\n  Classifier",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.00524v1": {
            "Paper Title": "Deep Sensor Fusion for Real-Time Odometry Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.01028v2": {
            "Paper Title": "IVOA: Introspective Vision for Obstacle Avoidance",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.07249v4": {
            "Paper Title": "Learning Local RGB-to-CAD Correspondences for Object Pose Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.13445v1": {
            "Paper Title": "Trajectory Advancement during Human-Robot Collaboration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.03629v2": {
            "Paper Title": "Movable-Object-Aware Visual SLAM via Weakly Supervised Semantic\n  Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.10171v2": {
            "Paper Title": "Driving Decision and Control for Autonomous Lane Change based on Deep\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.12699v1": {
            "Paper Title": "Towards Dynamic Simulation Guided Optimal Design of Tumbling Microrobots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.13122v1": {
            "Paper Title": "Learning Stabilizable Nonlinear Dynamics with Contraction-Based\n  Regularization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.11683v2": {
            "Paper Title": "Outlier-Robust Spatial Perception: Hardness, General-Purpose Algorithms,\n  and Guarantees",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.02319v2": {
            "Paper Title": "Towards a Robust Aerial Cinematography Platform: Localizing and Tracking\n  Moving Targets in Unstructured Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.00649v3": {
            "Paper Title": "Robust Tracking with Model Mismatch for Fast and Safe Planning: an SOS\n  Optimization Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.01631v2": {
            "Paper Title": "Conditional Generative Neural System for Probabilistic Trajectory\n  Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.13098v1": {
            "Paper Title": "Making Sense of Vision and Touch: Learning Multimodal Representations\n  for Contact-Rich Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.07747v4": {
            "Paper Title": "Lookup Table-Based Consensus Algorithm for Real-Time Longitudinal Motion\n  Control of Connected and Automated Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.10313v3": {
            "Paper Title": "DensePeds: Pedestrian Tracking in Dense Crowds Using Front-RVO and\n  Sparse Features",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.10782v2": {
            "Paper Title": "A Framework for Monitoring Human Physiological Response during Human\n  Robot Collaborative Task",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.08960v2": {
            "Paper Title": "Short-Term Prediction and Multi-Camera Fusion on Semantic Grids",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.02223v2": {
            "Paper Title": "Learning Physics-Based Manipulation in Clutter: Combining Image-Based\n  Generalization and Look-Ahead Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.11388v1": {
            "Paper Title": "Learning to Solve a Rubik's Cube with a Dexterous Hand",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.11208v1": {
            "Paper Title": "Prediction of Highway Lane Changes Based on Prototype Trajectories",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.11035v1": {
            "Paper Title": "Robot Learning of Shifting Objects for Grasping in Cluttered\n  Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.11025v1": {
            "Paper Title": "Towards Generalizing Sensorimotor Control Across Weather Conditions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.11021v1": {
            "Paper Title": "Experimentation on the motion of an obstacle avoiding robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.10170v1": {
            "Paper Title": "Generic Prediction Architecture Considering both Rational and Irrational\n  Driving Behaviors",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.10029v1": {
            "Paper Title": "Hidden Markov Models derived from Behavior Trees",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.10008v1": {
            "Paper Title": "Incremental Class Discovery for Semantic Segmentation with RGBD Sensing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.09873v1": {
            "Paper Title": "An extended framework for characterizing social robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.09673v1": {
            "Paper Title": "Multilevel Monte-Carlo for Solving POMDPs Online",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.09656v1": {
            "Paper Title": "Grasping Using Tactile Sensing and Deep Calibration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.02795v4": {
            "Paper Title": "Generalized Lazy Search for Robot Motion Planning: Interleaving Search\n  and Edge Evaluation via Event-based Toggles",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.13074v2": {
            "Paper Title": "Cooperative Localization under Limited Connectivity",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.08981v1": {
            "Paper Title": "Alice's Adventures in the Markovian World",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.07252v3": {
            "Paper Title": "Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via\n  Randomized-to-Canonical Adaptation Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.11486v2": {
            "Paper Title": "A test bed for measuring UAV servo reliability",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.08752v1": {
            "Paper Title": "RobustTP: End-to-End Trajectory Prediction for Heterogeneous Road-Agents\n  in Dense Traffic with Noisy Sensor Inputs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.08749v1": {
            "Paper Title": "Generating Optimal Grasps Under A Stress-Minimizing Metric",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.08541v1": {
            "Paper Title": "Resilient Sensor Architecture Design and Tradespace Analysis for\n  Autonomous Vehicle Localization and Mapping",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.09007v2": {
            "Paper Title": "DeepLocalization: Landmark-based Self-Localization with Deep Neural\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.08388v1": {
            "Paper Title": "Robust Real-time RGB-D Visual Odometry in Dynamic Environments via Rigid\n  Motion Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.00063v2": {
            "Paper Title": "Flight Recovery of MAVs with Compromised IMU",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.07958v1": {
            "Paper Title": "Transfer Learning Across Simulated Robots With Different Sensors",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.00821v2": {
            "Paper Title": "Visual-based Autonomous Driving Deployment from a Stochastic and\n  Uncertainty-aware Perspective",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.07876v1": {
            "Paper Title": "Towards Learning Efficient Maneuver Sets for Kinodynamic Motion Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.02531v2": {
            "Paper Title": "Combining Optimal Control and Learning for Visual Navigation in Novel\n  Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.09682v2": {
            "Paper Title": "Finding plans subject to stipulations on what information they divulge",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.07647v1": {
            "Paper Title": "Fly Safe: Aerial Swarm Robotics using Force Field Particle Swarm\n  Optimisation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.03589v2": {
            "Paper Title": "NeurAll: Towards a Unified Model for Visual Perception in Automated\n  Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.00208v2": {
            "Paper Title": "RGB and LiDAR fusion based 3D Semantic Segmentation for Autonomous\n  Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.07521v1": {
            "Paper Title": "Stochastic Optimization for Trajectory Planning with Heteroscedastic\n  Gaussian Processes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.07469v1": {
            "Paper Title": "Edge Detection for Event Cameras using Intra-pixel-area Events",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.07433v1": {
            "Paper Title": "Towards Blockchain-based Multi-Agent Robotic Systems: Analysis,\n  Classification and Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.08380v2": {
            "Paper Title": "2D Linear Time-Variant Controller for Human's Intention Detection for\n  Reach-to-Grasp Trajectories in Novel Scenes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.07160v1": {
            "Paper Title": "EnforceNet: Monocular Camera Localization in Large Scale Indoor Sparse\n  LiDAR Point Cloud",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.07045v1": {
            "Paper Title": "Pedestrian Tracking by Probabilistic Data Association and Correspondence\n  Embeddings",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.06795v1": {
            "Paper Title": "Efficient Autonomy Validation in Simulation with Adaptive Stress Testing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.07866v2": {
            "Paper Title": "Reinforcement Learning without Ground-Truth State",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.06337v1": {
            "Paper Title": "Energy-efficient Path Planning for Ground Robots by Combining Air and\n  Ground Measurements",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.00269v2": {
            "Paper Title": "On Training Flexible Robots using Deep Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.06053v1": {
            "Paper Title": "Learning better generative models for dexterous, single-view grasping of\n  novel objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.05945v1": {
            "Paper Title": "NH-TTC: A gradient-based framework for generalized anticipatory\n  collision avoidance",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.05910v1": {
            "Paper Title": "Coverage Sampling Planner for UAV-enabled Environmental Exploration and\n  Field Mapping",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.04799v2": {
            "Paper Title": "RL-RRT: Kinodynamic Motion Planning via Learning Reachability Estimators\n  from RL Policies",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.05738v1": {
            "Paper Title": "Learning a Curve Guardian for Motorcycles",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.09558v1": {
            "Paper Title": "System-Level Development of a User-Integrated Semi-Autonomous Lawn\n  Mowing System: Problem Overview, Basic Requirements, and Proposed\n  Architecture",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.01804v2": {
            "Paper Title": "Robot Localization in Floor Plans Using a Room Layout Edge Extraction\n  Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.05375v1": {
            "Paper Title": "Online Inference and Detection of Curbs in Partially Occluded Scenes\n  with Sparse LIDAR",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.05364v1": {
            "Paper Title": "Performance Boundary Identification for the Evaluation of Automated\n  Vehicles using Gaussian Process Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.05310v1": {
            "Paper Title": "Aerial Animal Biometrics: Individual Friesian Cattle Recovery and Visual\n  Identification via an Autonomous UAV with Onboard Deep Inference",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.05300v1": {
            "Paper Title": "Learning Safe Unlabeled Multi-Robot Planning with Motion Constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.05281v2": {
            "Paper Title": "Automatic 3D Mapping for Tree Diameter Measurements in Inventory\n  Operations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.04762v2": {
            "Paper Title": "Active Domain Randomization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.04904v1": {
            "Paper Title": "A Resource-Aware Approach to Collaborative Loop Closure Detection with\n  Provable Performance Guarantees",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.04796v1": {
            "Paper Title": "Bayesian Optimization in Variational Latent Spaces with Dynamic\n  Compression",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.04761v1": {
            "Paper Title": "Towards Affordance Prediction with Vision via Task Oriented Grasp\n  Quality Metrics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.04759v1": {
            "Paper Title": "Toward a Procedural Fruit Tree Rendering Framework for Image Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.05850v1": {
            "Paper Title": "Exploiting Causality for Selective Belief Filtering in Dynamic Bayesian\n  Networks (Extended Abstract)",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.04569v1": {
            "Paper Title": "Generating All the Roads to Rome: Road Layout Randomization for Improved\n  Road Marking Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.04490v1": {
            "Paper Title": "Deep Lagrangian Networks: Using Physics as Model Prior for Deep Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.00078v2": {
            "Paper Title": "Autonomous Visual Assistance for Robot Operations Using a Tethered UAV",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.04057v2": {
            "Paper Title": "Lidar-based Object Classification with Explicit Occlusion Modeling",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.04457v1": {
            "Paper Title": "Partially Observable Planning and Learning for Systems with Non-Uniform\n  Dynamics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.04396v1": {
            "Paper Title": "Informative Path Planning with Local Penalization for Decentralized and\n  Asynchronous Swarm Robotic Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.05352v3": {
            "Paper Title": "Path Planning of an Autonomous Mobile Robot in a Dynamic Environment\n  using Modified Bat Swarm Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.04008v1": {
            "Paper Title": "Decentralized Gaussian Mixture Fusion through Unified Quotient\n  Approximations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.02184v2": {
            "Paper Title": "Dynamic Regret Convergence Analysis and an Adaptive Regularization\n  Algorithm for On-Policy Robot Imitation Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.03880v1": {
            "Paper Title": "Swarm Engineering Through Quantitative Measurement of Swarm Robotic\n  Principles in a 10,000 Robot Swarm",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.09541v3": {
            "Paper Title": "Towards Assistive Robotic Pick and Place in Open World Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.03817v1": {
            "Paper Title": "Towards the Internet of Robotic Things: Analysis, Architecture,\n  Components and Challenges",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.03716v1": {
            "Paper Title": "Modeling and Simulation of the Quadrtor Delivery System",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.03424v1": {
            "Paper Title": "Segway DRIVE Benchmark: Place Recognition and SLAM Data Collected by A\n  Fleet of Delivery Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.07012v2": {
            "Paper Title": "Understanding of Object Manipulation Actions Using Human Multi-Modal\n  Sensory Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.03116v1": {
            "Paper Title": "Intrinsic Motivation Driven Intuitive Physics Learning using Deep\n  Reinforcement Learning with Intrinsic Reward Normalization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.00324v4": {
            "Paper Title": "Meeting in a Polygon by Anonymous Oblivious Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.02824v1": {
            "Paper Title": "Visual Appearance Analysis of Forest Scenes for Monocular SLAM",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.09802v3": {
            "Paper Title": "Generalized Multiple Correlation Coefficient as a Similarity\n  Measurements between Trajectories",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.02426v1": {
            "Paper Title": "Multimodal Uncertainty Reduction for Intention Recognition in\n  Human-Robot Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.01984v1": {
            "Paper Title": "Cooperative Schedule-Driven Intersection Control with Connected and\n  Autonomous Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.01879v1": {
            "Paper Title": "Learning to Predict Robot Keypoints Using Artificially Generated Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.01839v1": {
            "Paper Title": "Intrinsic Calibration of Depth Cameras for Mobile Robots using a Radial\n  Laser Scanner",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.01593v3": {
            "Paper Title": "Improving Semantic Segmentation via Video Propagation and Label\n  Relaxation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.01713v1": {
            "Paper Title": "End-to-end Decentralized Multi-robot Navigation in Unknown Complex\n  Environments via Deep Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.02493v2": {
            "Paper Title": "Discontinuity-Sensitive Optimal Control Learning by Mixture of Experts",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.09271v4": {
            "Paper Title": "Infinite Grid Exploration by Disoriented Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.01180v1": {
            "Paper Title": "Conservative Q-Improvement: Reinforcement Learning for an Interpretable\n  Decision-Tree Policy",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.00930v1": {
            "Paper Title": "A Joint Optimization Approach of LiDAR-Camera Fusion for Accurate Dense\n  3D Reconstructions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.00921v1": {
            "Paper Title": "Active Learning within Constrained Environments through Imitation of an\n  Expert Questioner",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.07705v2": {
            "Paper Title": "Multi-Objective Autonomous Braking System using Naturalistic Dataset",
            "Sentences": []
        },
        "http://arxiv.org/abs/1707.00893v4": {
            "Paper Title": "Optimization Beyond the Convolution: Generalizing Spatial Relations with\n  End-to-End Metric Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.00734v1": {
            "Paper Title": "Learning Objectness from Sonar Images for Class-Independent Object\n  Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.11264v2": {
            "Paper Title": "Real Time Lidar and Radar High-Level Fusion for Obstacle Detection and\n  Tracking with evaluation on a ground truth",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.00553v1": {
            "Paper Title": "Model-free Friction Observers for Flexible Joint Robots with Torque\n  Measurements",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.00222v3": {
            "Paper Title": "Model-Free Active Input-Output Feedback Linearization of a Single-Link\n  Flexible Joint Manipulator: An Improved ADRC Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.08173v2": {
            "Paper Title": "Service-Oriented Software Architecture for Cloud Robotics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.00408v1": {
            "Paper Title": "GarmNet: Improving Global with Local Perception for Robotic Laundry\n  Folding",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.00276v1": {
            "Paper Title": "Stereo relative pose from line and point feature triplets",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.11614v2": {
            "Paper Title": "Methodology of Designing Multi-agent Robot Control Systems Utilising\n  Hierarchical Petri Nets",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.12279v1": {
            "Paper Title": "Motion Prediction with Recurrent Neural Network Dynamical Models and\n  Trajectory Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.11435v2": {
            "Paper Title": "DeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial\n  Odometry using 3D Geometric Constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.00749v1": {
            "Paper Title": "Deep Multi-Task Learning for Anomalous Driving Detection Using CAN Bus\n  Scalar Sensor Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.11909v1": {
            "Paper Title": "Comparing Semi-Parametric Model Learning Algorithms for Dynamic Model\n  Estimation in Robotics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.11858v1": {
            "Paper Title": "Robotic Supervised Autonomy: A Review",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.11750v1": {
            "Paper Title": "A Constant-Factor Approximation Algorithm for Online Coverage Path\n  Planning with Energy Constraint",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.11747v1": {
            "Paper Title": "Raven: Open Surgical Robotic Platforms",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.09948v2": {
            "Paper Title": "Autonomous Identification and Goal-Directed Invocation of\n  Event-Predictive Behavioral Primitives",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.11548v1": {
            "Paper Title": "Generative grasp synthesis from demonstration using parametric mixtures",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.11452v1": {
            "Paper Title": "Traffic Management Strategies for Multi-Robotic Rigid Payload Transport\n  Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.11419v1": {
            "Paper Title": "Automatic Coverage Selection for Surface-Based Visual Localization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.10687v3": {
            "Paper Title": "High-Precision Localization Using Ground Texture",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.11176v1": {
            "Paper Title": "PyRep: Bringing V-REP to Deep Robot Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.11114v1": {
            "Paper Title": "From Multi-modal Property Dataset to Robot-centric Conceptual Knowledge\n  About Household Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.11021v1": {
            "Paper Title": "Cooperation-Aware Reinforcement Learning for Merging in Dense Traffic",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.10783v1": {
            "Paper Title": "OLAE-ICP: Robust and fast alignment of geometric features with the\n  optimal linear attitude estimator",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.12171v1": {
            "Paper Title": "Gesture Recognition in RGB Videos UsingHuman Body Keypoints and Dynamic\n  Time Warping",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.10182v1": {
            "Paper Title": "Planning Robot Motion using Deep Visual Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.10099v1": {
            "Paper Title": "DynoPlan: Combining Motion Planning and Deep Neural Network based\n  Controllers for Safe HRL",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.09941v1": {
            "Paper Title": "Learning Generalisable Coupling Terms for Obstacle Avoidance via\n  Low-dimensional Geometric Descriptors",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.09868v1": {
            "Paper Title": "Pose Estimation for Non-Cooperative Rendezvous Using Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.09785v1": {
            "Paper Title": "An Efficient B-spline-Based Kinodynamic Replanning Framework for\n  Quadrotors",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.08834v2": {
            "Paper Title": "Deep Learning in the Automotive Industry: Recent Advances and\n  Application Examples",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.10513v1": {
            "Paper Title": "The Role of Compute in Autonomous Aerial Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.09105v4": {
            "Paper Title": "SOLAR: Deep Structured Representations for Model-Based Reinforcement\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.10641v1": {
            "Paper Title": "Micro Air Vehicle Link (MAVLink) in a Nutshell: A Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.09371v1": {
            "Paper Title": "Wind Gust Detection using Physical Sensors in Quadcopters",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.03328v2": {
            "Paper Title": "Modeling Supervisor Safe Sets for Improving Collaboration in Human-Robot\n  Teams",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.09205v1": {
            "Paper Title": "Continual Reinforcement Learning with Diversity Exploration and\n  Adversarial Self-Correction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.07400v2": {
            "Paper Title": "Scheduled Intrinsic Drive: A Hierarchical Take on Intrinsically\n  Motivated Exploration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.08989v1": {
            "Paper Title": "Data-Efficient Learning for Sim-to-Real Robotic Grasping using Deep\n  Point Cloud Prediction Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.08945v1": {
            "Paper Title": "Rules of the Road: Predicting Driving Behavior with a Convolutional\n  Model of Semantic Interactions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.08928v1": {
            "Paper Title": "Learning Reward Functions by Integrating Human Demonstrations and\n  Preferences",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.08716v1": {
            "Paper Title": "Deep-Learning-Based Aerial Image Classification for Emergency Response\n  Applications Using Unmanned Aerial Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.08649v1": {
            "Paper Title": "Exploring Model-based Planning with Policy Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.08070v2": {
            "Paper Title": "Monocular 3D Object Detection and Box Fitting Trained End-to-End Using\n  Intersection-over-Union Loss",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.08494v1": {
            "Paper Title": "Object Placement on Cluttered Surfaces: A Nested Local Search Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.08291v1": {
            "Paper Title": "Multi-Agent Pathfinding: Definitions, Variants, and Benchmarks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.11103v3": {
            "Paper Title": "Learning to Walk via Deep Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.08236v1": {
            "Paper Title": "PyRobot: An Open-source Robotics Framework for Research and Benchmarking",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.08085v1": {
            "Paper Title": "PLANE: An Extensible Open Source Framework for modeling the Internet of\n  Drones",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.01116v3": {
            "Paper Title": "Object Sorting Using a Global Texture-Shape 3D Feature Descriptor",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.13566v2": {
            "Paper Title": "Recent Advances in Imitation Learning from Observation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.07838v1": {
            "Paper Title": "RadGrad: Active learning with loss gradients",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.07795v1": {
            "Paper Title": "Characterizing the Uncertainty of Jointly Distributed Poses in the Lie\n  Algebra",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.05728v2": {
            "Paper Title": "Technical Report: Safe, Aggressive Quadrotor Flight via\n  Reachability-based Trajectory Design",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.11046v3": {
            "Paper Title": "GCNv2: Efficient Correspondence Prediction for Real-Time SLAM",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.06866v1": {
            "Paper Title": "Embracing Contact: Pushing Multiple Objects with Robot's Forearm",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.06830v1": {
            "Paper Title": "Understanding Natural Language Instructions for Fetching Daily Objects\n  Using GAN-Based Multimodal Target-Source Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.06775v1": {
            "Paper Title": "Usability Squared: Principles for doing good systems research in\n  robotics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.01035v2": {
            "Paper Title": "Real-Time Planning with Multi-Fidelity Models for Agile Flights in\n  Unknown Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.08871v2": {
            "Paper Title": "Pose consensus based on dual quaternion algebra with application to\n  decentralized formation control of mobile manipulators",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.06322v1": {
            "Paper Title": "Connecting Touch and Vision via Cross-Modal Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.06113v1": {
            "Paper Title": "A Survey on Deep Learning Architectures for Image-based Depth\n  Reconstruction",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": "Re\ufb01nement XL1\n1\u000e 2:67 1:40 1:61 2:88 1:51 1:74 2:67 1:40 1:61 2:88 1:51 1:74 0:5 GPU @ 2.5 Ghz\n(C/C++)\nEdgeStereo ",
                    "Citation Text": "X. Song, X. Zhao, H. Hu, and L. Fang, \u201cEdgeStereo: A Context\nIntegrated Residual Pyramid Network for Stereo Matching,\u201d\nACCV , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.05196",
                        "Citation Paper Title": "Title:EdgeStereo: A Context Integrated Residual Pyramid Network for Stereo Matching",
                        "Citation Paper Abstract": "Abstract:Recent convolutional neural networks, especially end-to-end disparity estimation models, achieve remarkable performance on stereo matching task. However, existed methods, even with the complicated cascade structure, may fail in the regions of non-textures, boundaries and tiny details. Focus on these problems, we propose a multi-task network EdgeStereo that is composed of a backbone disparity network and an edge sub-network. Given a binocular image pair, our model enables end-to-end prediction of both disparity map and edge map. Basically, we design a context pyramid to encode multi-scale context information in disparity branch, followed by a compact residual pyramid for cascaded refinement. To further preserve subtle details, our EdgeStereo model integrates edge cues by feature embedding and edge-aware smoothness loss regularization. Comparative results demonstrates that stereo matching and edge detection can help each other in the unified model. Furthermore, our method achieves state-of-art performance on both KITTI Stereo and Scene Flow benchmarks, which proves the effectiveness of our design.",
                        "Citation Paper Authors": "Authors:Xiao Song, Xu Zhao, Hanwen Hu, Liangji Fang"
                    },
                    "Keywords": []
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "Raw disparity XL5\n1\u000e 4:31 1:71 2:14 4:62 1:86 2:32 4:31 1:71 2:14 4:62 1:86 2:32 0:41 Nvidia GTX Titan\nXp\nZhong et al. ",
                    "Citation Text": "Y. Zhong, Y. Dai, and H. Li, \u201cSelf-supervised learning for\nstereo matching with self-improving ability,\u201d arXiv preprint\narXiv:1709.00930 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.00930",
                        "Citation Paper Title": "Title:Self-Supervised Learning for Stereo Matching with Self-Improving Ability",
                        "Citation Paper Abstract": "Abstract:Exiting deep-learning based dense stereo matching methods often rely on ground-truth disparity maps as the training signals, which are however not always available in many situations. In this paper, we design a simple convolutional neural network architecture that is able to learn to compute dense disparity maps directly from the stereo inputs. Training is performed in an end-to-end fashion without the need of ground-truth disparity maps. The idea is to use image warping error (instead of disparity-map residuals) as the loss function to drive the learning process, aiming to find a depth-map that minimizes the warping error. While this is a simple concept well-known in stereo matching, to make it work in a deep-learning framework, many non-trivial challenges must be overcome, and in this work we provide effective solutions. Our network is self-adaptive to different unseen imageries as well as to different camera settings. Experiments on KITTI and Middlebury stereo benchmark datasets show that our method outperforms many state-of-the-art stereo matching methods with a margin, and at the same time significantly faster.",
                        "Citation Paper Authors": "Authors:Yiran Zhong, Yuchao Dai, Hongdong Li"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1906.05794v1": {
            "Paper Title": "Egocentric affordance detection with the one-shot geometry-driven\n  Interaction Tensor",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.05498v1": {
            "Paper Title": "Understanding Human Context in 3D Scenes by Learning Spatial Affordances\n  with Virtual Skeleton Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.05253v1": {
            "Paper Title": "Search on the Replay Buffer: Bridging Planning and Reinforcement\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.11873v1": {
            "Paper Title": "VolMap: A Real-time Model for Semantic Segmentation of a LiDAR\n  surrounding view",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.05717v1": {
            "Paper Title": "Unsupervised Monocular Depth and Ego-motion Learning with Structure and\n  Semantics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.05194v1": {
            "Paper Title": "Active Learning of Dynamics for Data-Driven Control Using Koopman\n  Operators",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.05070v1": {
            "Paper Title": "Identification of Motor Parameters on Coupled Joints",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.11899v1": {
            "Paper Title": "Lidar based Detection and Classification of Pedestrians and Vehicles\n  Using Machine Learning Methods",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.03853v2": {
            "Paper Title": "DensePhysNet: Learning Dense Physical Object Representations via\n  Multi-step Dynamic Interactions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.04452v1": {
            "Paper Title": "Continual Reinforcement Learning deployed in Real-life using Policy\n  Distillation and Sim2Real Transfer",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.04312v1": {
            "Paper Title": "Online Object Representations with Contrastive Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.04161v1": {
            "Paper Title": "Self-Supervised Exploration via Disagreement",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.03710v1": {
            "Paper Title": "Curiosity-Driven Multi-Criteria Hindsight Experience Replay",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.03582v1": {
            "Paper Title": "Simplified Kinematics of Continuum Robot Equilibrium Modulation via\n  Moment Coupling Effects and Model Calibration",
            "Sentences": []
        },
        "http://arxiv.org/abs/1709.04906v4": {
            "Paper Title": "On the interaction between Autonomous Mobility-on-Demand systems and the\n  power network: models and coordination algorithms",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.03098v1": {
            "Paper Title": "Multi-modal Active Learning From Human Data: A Deep Reinforcement\n  Learning Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.02995v1": {
            "Paper Title": "Object-Agnostic Suction Grasp Affordance Detection in Dense Cluster\n  Using Self-Supervised Learning.docx",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.02815v1": {
            "Paper Title": "Intention-aware Long Horizon Trajectory Prediction of Surrounding\n  Vehicles using Dual LSTM Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.02650v1": {
            "Paper Title": "Visual-Inertial Navigation: A Concise Review",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.02318v1": {
            "Paper Title": "Highly Parallelized Data-driven MPC for Minimal Intervention Shared\n  Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.02275v1": {
            "Paper Title": "Continuous Control for Automated Lane Change Behavior Based on Deep\n  Deterministic Policy Gradient Algorithm",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.02003v1": {
            "Paper Title": "Machine Learning and System Identification for Estimation in Physical\n  Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.01957v1": {
            "Paper Title": "Maximizing Energy Battery Efficiency in Swarm Robotics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.01936v1": {
            "Paper Title": "Comparison Study of Well-Known Inverted Pendulum Models for Balance\n  Recovery in Humanoid Robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.11160v2": {
            "Paper Title": "ColCOS$\u03a6$: A Multiple Pheromone Communication System for Swarm\n  Robotics and Social Insects Research",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.01868v1": {
            "Paper Title": "A Survey of Behavior Learning Applications in Robotics -- State of the\n  Art and Perspectives",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.01728v1": {
            "Paper Title": "BayesSim: adaptive domain randomization via probabilistic inference for\n  robotics simulators",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.01532v1": {
            "Paper Title": "Closed-Loop Control of a Delta-Wing Unmanned Aerial-Aquatic Vehicle",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.01299v1": {
            "Paper Title": "Grid-based Localization Stack for Inspection Drones towards Automation\n  of Large Scale Warehouse Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.00214v2": {
            "Paper Title": "Exploration of Finite 2D Square Grid by a Metamorphic Robotic System",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.01258v1": {
            "Paper Title": "Knowledge is Never Enough: Towards Web Aided Deep Open World Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.09366v2": {
            "Paper Title": "Macro Action Reinforcement Learning with Sequence Disentanglement using\n  Variational Autoencoder",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.01108v1": {
            "Paper Title": "Socially Inspired Communication in Swarm Robotics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.02898v3": {
            "Paper Title": "Nutty-based Robot Animation -- Principles and Practices",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.00825v1": {
            "Paper Title": "Self-supervised Body Image Acquisition Using a Deep Neural Network for\n  Sensorimotor Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.06388v2": {
            "Paper Title": "MAVBench: Micro Aerial Vehicle Benchmarking",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.00052v1": {
            "Paper Title": "Training Detection-Range-Frugal Cooperative Collision Avoidance Models\n  for Quadcopters via Neuroevolution",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.13531v1": {
            "Paper Title": "Graduated Fidelity Lattices for Motion Planning under Uncertainty",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.12875v2": {
            "Paper Title": "Fast Reciprocal Collision Avoidance Under Measurement Uncertainty",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.09988v2": {
            "Paper Title": "Decentralized Informative Path Planning with Exploration-Exploitation\n  Balance for Swarm Robotic Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.11822v2": {
            "Paper Title": "Robotic bees: Algorithms for collision detection and prevention",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.00199v2": {
            "Paper Title": "Cyber-Physical Testbed for Human-Robot Collaborative Task Planning and\n  Execution",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.12973v1": {
            "Paper Title": "Partial Computing Offloading Assisted Cloud Point Registration in\n  Multi-robot SLAM",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.12942v1": {
            "Paper Title": "A Realtime Autonomous Robot Navigation Framework for Human like\n  High-level Interaction and Task Planning in Global Dynamic Environment",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.12853v1": {
            "Paper Title": "RoNIN: Robust Neural Inertial Navigation in the Wild: Benchmark,\n  Evaluations, and New Methods",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.07189v2": {
            "Paper Title": "Reinforcement Learning with Probabilistic Guarantees for Autonomous\n  Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.02749v2": {
            "Paper Title": "Deep Visual MPC-Policy Learning for Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.12197v1": {
            "Paper Title": "LeTS-Drive: Driving in a Crowd by Learning from Tree Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.12079v1": {
            "Paper Title": "Probabilistic Category-Level Pose Estimation via Segmentation and\n  Predicted-Shape Priors",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.12061v1": {
            "Paper Title": "On Evaluating the Effectiveness of the HoneyBot: A Case Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.11734v1": {
            "Paper Title": "Fast human motion prediction for human-robot collaboration with wearable\n  interfaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.11602v1": {
            "Paper Title": "Differentiable Algorithm Networks for Composable Robot Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.11191v1": {
            "Paper Title": "Development of an Intuitive Foot-Machine Interface for Robotic Surgery",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.11129v1": {
            "Paper Title": "On Motion Control and Machine Learning for Robotic Assembly",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.04236v2": {
            "Paper Title": "Strategy Synthesis in POMDPs via Game-Based Abstractions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.08379v3": {
            "Paper Title": "LSwarm: Efficient Collision Avoidance for Large Swarms with Coverage\n  Constraints in Complex Urban Scenes",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": ". These ap-\nproaches estimate the future positions of obstacles using their\nobserved velocities. Velocity Obstacle (VO) ",
                    "Citation Text": "Fiorini, P., & Shiller, Z. (1998). \u201dMotion Planning in Dynamic\nEnvironments Using Velocity Obstacles\u201d. The International Journal\nof Robotics Research, 17(7), 760772.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2309.06293",
                        "Citation Paper Title": "Title:Reference Frames and Black Hole Thermodynamics",
                        "Citation Paper Abstract": "Abstract:In the context of the absolute parallelism formulation of General Relativity, and because of the fact that the scalar curvature can be written in purely torsional terms, it was known for a long time that a surface term based solely on the torsion tensor appears in the action. It was subsequently suggested that this term might play the role of the Gibbons-Hawking-York boundary term which, in turn, is associated to the free energy in the path integral approach, and then, to the black hole entropy by standard thermodynamic arguments. We show that the identification of the two boundary terms is rather incomplete, and that it strongly depends on the choice of the tetrad (frame) field used to reproduce a given metric. By considering variations of the tetrad field not necessarily subjected to Dirichlet-like conditions on the boundary surface, we find a class of frames adapted to the Schwarzschild spacetime in which the Gibbons-Hawking-York/torsion link is actually established, and conducing to the right black hole entropy without the need of any background subtraction. Remarkably, these frames are also responsible for the correct value of the gravitational energy as computed from the teleparallel energy-momentum pseudo-current.",
                        "Citation Paper Authors": "Authors:Franco Fiorini, P. A. Gonz\u00e1lez, Yerko V\u00e1squez"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1905.10814v1": {
            "Paper Title": "Operation and Imitation under Safety-Aware Shared Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.10762v1": {
            "Paper Title": "A Staged Approach to Evolving Real-world UAV Controllers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.07601v3": {
            "Paper Title": "Relation-Shape Convolutional Neural Network for Point Cloud Analysis",
            "Sentences": [
                {
                    "Sentence ID": 50,
                    "Sentence": "2k,nor 81.9 85.1 82.4 79.0 87.7 77.3 90.8 71.8 91.0 85.9 83.7 95.3 71.6 94.1 81.3 58.7 76.4 82.6\nSyncCNN ",
                    "Citation Text": "L. Yi, H. Su, X. Guo, and L. J. Guibas. SyncSpecCNN:\nSynchronized spectral CNN for 3D shape segmentation. In\nCVPR , pages 6584\u20136592, 2017. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.00606",
                        "Citation Paper Title": "Title:SyncSpecCNN: Synchronized Spectral CNN for 3D Shape Segmentation",
                        "Citation Paper Abstract": "Abstract:In this paper, we study the problem of semantic annotation on 3D models that are represented as shape graphs. A functional view is taken to represent localized information on graphs, so that annotations such as part segment or keypoint are nothing but 0-1 indicator vertex functions. Compared with images that are 2D grids, shape graphs are irregular and non-isomorphic data structures. To enable the prediction of vertex functions on them by convolutional neural networks, we resort to spectral CNN method that enables weight sharing by parameterizing kernels in the spectral domain spanned by graph laplacian eigenbases. Under this setting, our network, named SyncSpecCNN, strive to overcome two key challenges: how to share coefficients and conduct multi-scale analysis in different parts of the graph for a single shape, and how to share information across related but different shapes that may be represented by very different graphs. Towards these goals, we introduce a spectral parameterization of dilated convolutional kernels and a spectral transformer network. Experimentally we tested our SyncSpecCNN on various tasks, including 3D shape part segmentation and 3D keypoint prediction. State-of-the-art performance has been achieved on all benchmark datasets.",
                        "Citation Paper Authors": "Authors:Li Yi, Hao Su, Xingwen Guo, Leonidas Guibas"
                    },
                    "Keywords": []
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "captures similar lo-\ncal shapes by learning point relation in a high-dimensional\nfeature space, yet this relation could be unreliable in some\ncases. Wang et al. ",
                    "Citation Text": "S. Wang, S. Suo, W. Ma, A. Pokrovsky, and R. Urtasun.\nDeep parametric continuous convolutional neural networks.\nInCVPR , pages 2589\u20132597, 2018. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.06742",
                        "Citation Paper Title": "Title:Deep Parametric Continuous Convolutional Neural Networks",
                        "Citation Paper Abstract": "Abstract:Standard convolutional neural networks assume a grid structured input is available and exploit discrete convolutions as their fundamental building blocks. This limits their applicability to many real-world applications. In this paper we propose Parametric Continuous Convolution, a new learnable operator that operates over non-grid structured data. The key idea is to exploit parameterized kernel functions that span the full continuous vector space. This generalization allows us to learn over arbitrary data structures as long as their support relationship is computable. Our experiments show significant improvement over the state-of-the-art in point cloud segmentation of indoor and outdoor scenes, and lidar motion estimation of driving scenes.",
                        "Citation Paper Authors": "Authors:Shenlong Wang, Simon Suo, Wei-Chiu Ma, Andrei Pokrovsky, Raquel Urtasun"
                    },
                    "Keywords": []
                }
            ]
        },
        "http://arxiv.org/abs/1905.09342v2": {
            "Paper Title": "Reachable Space Characterization of Markov Decision Processes with Time\n  Variability",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.10610v1": {
            "Paper Title": "Reasoning on Grasp-Action Affordances",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.07434v2": {
            "Paper Title": "A Coordinated Search Strategy for Multiple Solitary Robots: An Extension",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.10654v3": {
            "Paper Title": "Failure-Scenario Maker for Rule-Based Agent using Multi-agent\n  Adversarial Reinforcement Learning and its Application to Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.02111v2": {
            "Paper Title": "Multidimensional Capacitive Sensing for Robot-Assisted Dressing and\n  Bathing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.09934v1": {
            "Paper Title": "Mechatronic Design of a Dribbling System for RoboCup Small Size Robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.10117v1": {
            "Paper Title": "Robust Semantic Segmentation in Adverse Weather Conditions by means of\n  Sensor Data Fusion",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.04829v2": {
            "Paper Title": "Spatial Uncertainty Sampling for End-to-End Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.09949v1": {
            "Paper Title": "Scene Induced Multi-Modal Trajectory Forecasting via Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.09634v1": {
            "Paper Title": "Robust Point Cloud Based Reconstruction of Large-Scale Outdoor Scenes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.01129v3": {
            "Paper Title": "OIL: Observational Imitation Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.09533v1": {
            "Paper Title": "Incorporating Human Domain Knowledge in 3D LiDAR-based Semantic\n  Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.08159v2": {
            "Paper Title": "3D Object Recognition with Ensemble Learning --- A Study of Point\n  Cloud-Based Deep Learning Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.08453v2": {
            "Paper Title": "Towards Safety-Aware Computing System Design in Autonomous Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.09304v1": {
            "Paper Title": "PoseRBPF: A Rao-Blackwellized Particle Filter for 6D Object Pose\n  Tracking",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.09046v1": {
            "Paper Title": "A Deep Reinforcement Learning Driving Policy for Autonomous Road\n  Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.08956v1": {
            "Paper Title": "Globally Optimal Joint Search of Topology and Trajectory for Planar\n  Linkages",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.08926v1": {
            "Paper Title": "Hierarchical Reinforcement Learning for Quadruped Locomotion",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.08878v1": {
            "Paper Title": "Look Who's Talking Now: Implications of AV's Explanations on Driver's\n  Trust, AV Preference, Anxiety and Mental Workload",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.01365v6": {
            "Paper Title": "A Comparative Study of Nonlinear MPC and Differential-Flatness-Based\n  Control for Quadrotor Agile Flight",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.13702v2": {
            "Paper Title": "DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to\n  Reality",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": "achieve real-world in\nhand manipulation using reinforcement learning, but on a platform that cannot mimic the dexterity of\na human hand. Sievers et al. ",
                    "Citation Text": "Leon Sievers, Johannes Pitz, and Berthold B\u00e4uml. Learning purely tactile in-hand manipulation\nwith a torque-controlled hand. 2022. doi: 10.48550/ARXIV .2204.03698. URL https:\n//arxiv.org/abs/2204.03698 . 18",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.03698",
                        "Citation Paper Title": "Title:Learning Purely Tactile In-Hand Manipulation with a Torque-Controlled Hand",
                        "Citation Paper Abstract": "Abstract:We show that a purely tactile dextrous in-hand manipulation task with continuous regrasping, requiring permanent force closure, can be learned from scratch and executed robustly on a torque-controlled humanoid robotic hand. The task is rotating a cube without dropping it, but in contrast to OpenAI's seminal cube manipulation task, the palm faces downwards and no cameras but only the hand's position and torque sensing are used. Although the task seems simple, it combines for the first time all the challenges in execution as well as learning that are important for using in-hand manipulation in real-world applications. We efficiently train in a precisely modeled and identified rigid body simulation with off-policy deep reinforcement learning, significantly sped up by a domain adapted curriculum, leading to a moderate 600 CPU hours of training time. The resulting policy is robustly transferred to the real humanoid DLR Hand-II, e.g., reaching more than 46 full 2${\\pi}$ rotations of the cube in a single run and allowing for disturbances like different cube sizes, hand orientation, or pulling a finger.",
                        "Citation Paper Authors": "Authors:Leon Sievers, Johannes Pitz, Berthold B\u00e4uml"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "to\nlearn a parametric stochastic policy \u03c0\u03b8(actor), mapping from observations o\u2208 O to actions a\u2208 A.\nPPO additionally learns a function V\u03c0\n\u03d5(s, o)(critic) to approximate the on-policy value function.\nFollowing Pinto et al. ",
                    "Citation Text": "Lerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wojciech Zaremba, and Pieter Abbeel.\nAsymmetric actor critic for image-based robot learning. CoRR , 2017. URL http://arxiv.\norg/abs/1710.06542 . 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.06542",
                        "Citation Paper Title": "Title:Asymmetric Actor Critic for Image-Based Robot Learning",
                        "Citation Paper Abstract": "Abstract:Deep reinforcement learning (RL) has proven a powerful technique in many sequential decision making domains. However, Robotics poses many challenges for RL, most notably training on a physical system can be expensive and dangerous, which has sparked significant interest in learning control policies using a physics simulator. While several recent works have shown promising results in transferring policies trained in simulation to the real world, they often do not fully utilize the advantage of working with a simulator. In this work, we exploit the full state observability in the simulator to train better policies which take as input only partial observations (RGBD images). We do this by employing an actor-critic training algorithm in which the critic is trained on full states while the actor (or policy) gets rendered images as input. We show experimentally on a range of simulated tasks that using these asymmetric inputs significantly improves performance. Finally, we combine this method with domain randomization and show real robot experiments for several tasks like picking, pushing, and moving a block. We achieve this simulation to real world transfer without training on any real world data.",
                        "Citation Paper Authors": "Authors:Lerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wojciech Zaremba, Pieter Abbeel"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": ", but those results have\nonly been in simulation.\nWhile the NLP and computer vision communities have reproduced and extended the successes of\nlarge-scale models like GPT-3 ",
                    "Citation Text": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.\nURL https://arxiv.org/abs/2005.14165 . 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.14165",
                        "Citation Paper Title": "Title:Language Models are Few-Shot Learners",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                        "Citation Paper Authors": "Authors:Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.04718v2": {
            "Paper Title": "On the Application of Efficient Neural Mapping to Real-Time Indoor\n  Localisation for Unmanned Ground Vehicles",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": ", joint prediction of pose and \nvisual odometry between image pairs ",
                    "Citation Text": "A. Valada, N. Radwan and W. Burgard, \"Deep auxiliary learning for \nvisual localization and odometry,\" in 2018 IEEE international \nconference on robotics and automation (ICRA) , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.03642",
                        "Citation Paper Title": "Title:Deep Auxiliary Learning for Visual Localization and Odometry",
                        "Citation Paper Abstract": "Abstract:Localization is an indispensable component of a robot's autonomy stack that enables it to determine where it is in the environment, essentially making it a precursor for any action execution or planning. Although convolutional neural networks have shown promising results for visual localization, they are still grossly outperformed by state-of-the-art local feature-based techniques. In this work, we propose VLocNet, a new convolutional neural network architecture for 6-DoF global pose regression and odometry estimation from consecutive monocular images. Our multitask model incorporates hard parameter sharing, thus being compact and enabling real-time inference, in addition to being end-to-end trainable. We propose a novel loss function that utilizes auxiliary learning to leverage relative pose information during training, thereby constraining the search space to obtain consistent pose estimates. We evaluate our proposed VLocNet on indoor as well as outdoor datasets and show that even our single task model exceeds the performance of state-of-the-art deep architectures for global localization, while achieving competitive performance for visual odometry estimation. Furthermore, we present extensive experimental evaluations utilizing our proposed Geometric Consistency Loss that show the effectiveness of multitask learning and demonstrate that our model is the first deep learning technique to be on par with, and in some cases outperforms state-of-the-art SIFT-based approaches.",
                        "Citation Paper Authors": "Authors:Abhinav Valada, Noha Radwan, Wolfram Burgard"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": ", in which a \ndataset of five outdoor scenes each comprising between 200 and \n3000 training images was used to train a Googlenet ",
                    "Citation Text": "C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. \nErhan, V. Vanhoucke and A. Rabinovich, \"Going deeper with \nconvolutions,\" in Proceedings of the IEEE conference on computer \nvision and pattern recognition , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.4842",
                        "Citation Paper Title": "Title:Going Deeper with Convolutions",
                        "Citation Paper Abstract": "Abstract:We propose a deep convolutional neural network architecture codenamed \"Inception\", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.",
                        "Citation Paper Authors": "Authors:Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "; \n2. 2D \u2013 3D Descriptor Matching approaches compute features \nfor keypoints in captured images, which are subsequently matched to pre-computed descriptors contained within a 3D \nmap ",
                    "Citation Text": "E. Brachmann, A. Krull, S. Nowozin, J. Shotton, F. Michel, S. \nGumhold and C. Rother, \"Dsac-differentiable ransac for camera \nlocalization,\" in Proceedings of the IEEE conference on computer \nvision and pattern recognition , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.05705",
                        "Citation Paper Title": "Title:DSAC - Differentiable RANSAC for Camera Localization",
                        "Citation Paper Abstract": "Abstract:RANSAC is an important algorithm in robust optimization and a central building block for many computer vision applications. In recent years, traditionally hand-crafted pipelines have been replaced by deep learning pipelines, which can be trained in an end-to-end fashion. However, RANSAC has so far not been used as part of such deep learning pipelines, because its hypothesis selection procedure is non-differentiable. In this work, we present two different ways to overcome this limitation. The most promising approach is inspired by reinforcement learning, namely to replace the deterministic hypothesis selection by a probabilistic selection for which we can derive the expected loss w.r.t. to all learnable parameters. We call this approach DSAC, the differentiable counterpart of RANSAC. We apply DSAC to the problem of camera localization, where deep learning has so far failed to improve on traditional approaches. We demonstrate that by directly minimizing the expected loss of the output camera poses, robustly estimated by RANSAC, we achieve an increase in accuracy. In the future, any deep learning pipeline can use DSAC as a robust optimization component.",
                        "Citation Paper Authors": "Authors:Eric Brachmann, Alexander Krull, Sebastian Nowozin, Jamie Shotton, Frank Michel, Stefan Gumhold, Carsten Rother"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.10981v4": {
            "Paper Title": "Adaptive Sampling-based Particle Filter for Visual-inertial Gimbal in\n  the Wild",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": ", then the warped parts are blended to\nsynthesize the stabilized image. The framework by J. Choi et\nal. ",
                    "Citation Text": "J. Choi, J. Park, I. S & Kweon. (2021). Self-Supervised Real-time\nVideo Stabilization. arXiv preprint arXiv:2111.05980.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.05980",
                        "Citation Paper Title": "Title:Self-Supervised Real-time Video Stabilization",
                        "Citation Paper Abstract": "Abstract:Videos are a popular media form, where online video streaming has recently gathered much popularity. In this work, we propose a novel method of real-time video stabilization - transforming a shaky video to a stabilized video as if it were stabilized via gimbals in real-time. Our framework is trainable in a self-supervised manner, which does not require data captured with special hardware setups (i.e., two cameras on a stereo rig or additional motion sensors). Our framework consists of a transformation estimator between given frames for global stability adjustments, followed by scene parallax reduction module via spatially smoothed optical flow for further stability. Then, a margin inpainting module fills in the missing margin regions created during stabilization to reduce the amount of post-cropping. These sequential steps reduce distortion and margin cropping to a minimum while enhancing stability. Hence, our approach outperforms state-of-the-art real-time video stabilization methods as well as offline methods that require camera trajectory optimization. Our method procedure takes approximately 24.3 ms yielding 41 fps regardless of resolution (e.g., 480p or 1080p).",
                        "Citation Paper Authors": "Authors:Jinsoo Choi, Jaesik Park, In So Kweon"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": ". To tackle the problem of video\nstabilization in dynamic scenes, a dense warping field as\nscene representation was trained from consecutive video\nframes by Liu et al. ",
                    "Citation Text": "Yu-Lun Liu, et al. \u201cHybrid neural fusion for full-frame video stabi-\nlization.\u201d Proceedings of the IEEE/CVF International Conference on\nComputer Vision. 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.06205",
                        "Citation Paper Title": "Title:Hybrid Neural Fusion for Full-frame Video Stabilization",
                        "Citation Paper Abstract": "Abstract:Existing video stabilization methods often generate visible distortion or require aggressive cropping of frame boundaries, resulting in smaller field of views. In this work, we present a frame synthesis algorithm to achieve full-frame video stabilization. We first estimate dense warp fields from neighboring frames and then synthesize the stabilized frame by fusing the warped contents. Our core technical novelty lies in the learning-based hybrid-space fusion that alleviates artifacts caused by optical flow inaccuracy and fast-moving objects. We validate the effectiveness of our method on the NUS, selfie, and DeepStab video datasets. Extensive experiment results demonstrate the merits of our approach over prior video stabilization methods.",
                        "Citation Paper Authors": "Authors:Yu-Lun Liu, Wei-Sheng Lai, Ming-Hsuan Yang, Yung-Yu Chuang, Jia-Bin Huang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.09419v3": {
            "Paper Title": "Line Coverage with Multiple Robots: Algorithms and Experiments",
            "Sentences": [
                {
                    "Sentence ID": 53,
                    "Sentence": ". These clusters form a set of subgraphs,20 40 60 80 100204060\nLength of the road network (km)Cost difference (%)MD-MEM\nSRLC-4 ",
                    "Citation Text": "R. van Bevern, C. Komusiewicz, and M. Sorge, \u201cA parameterized\napproximation algorithm for the mixed and windy capacitated arc\nrouting problem: Theory and experiments,\u201d Networks , vol. 70, no. 3,\npp. 262\u2013278, Oct. 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.05620",
                        "Citation Paper Title": "Title:A parameterized approximation algorithm for the mixed and windy Capacitated Arc Routing Problem: theory and experiments",
                        "Citation Paper Abstract": "Abstract:We prove that any polynomial-time $\\alpha(n)$-approximation algorithm for the $n$-vertex metric asymmetric Traveling Salesperson Problem yields a polynomial-time $O(\\alpha(C))$-approximation algorithm for the mixed and windy Capacitated Arc Routing Problem, where $C$ is the number of weakly connected components in the subgraph induced by the positive-demand arcs---a small number in many applications. In conjunction with known results, we obtain constant-factor approximations for $C\\in O(\\log n)$ and $O(\\log C/\\log\\log C)$-approximations in general. Experiments show that our algorithm, together with several heuristic enhancements, outperforms many previous polynomial-time heuristics. Finally, since the solution quality achievable in polynomial time appears to mainly depend on $C$ and since $C=1$ in almost all benchmark instances, we propose the Ob benchmark set, simulating cities that are divided into several components by a river.",
                        "Citation Paper Authors": "Authors:Ren\u00e9 van Bevern, Christian Komusiewicz, Manuel Sorge"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.02647v3": {
            "Paper Title": "Neural Grasp Distance Fields for Robot Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.09141v2": {
            "Paper Title": "\"Guess what I'm doing\": Extending legibility to sequential decision\n  tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.13393v3": {
            "Paper Title": "TransKD: Transformer Knowledge Distillation for Efficient Semantic\n  Segmentation",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": ". * denotes that\nthe efficient segmentation model uses pre-trained weights. \u201c-\u201d\ndenotes that the corresponding information is missing in the\nrespective paper.\nNetwork #Params (M) GFLOPs mIoU (%)\nLarge Semantic Segmentation Models\nPSPNet ",
                    "Citation Text": "H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \u201cPyramid scene parsing\nnetwork,\u201d in CVPR , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.01105",
                        "Citation Paper Title": "Title:Pyramid Scene Parsing Network",
                        "Citation Paper Abstract": "Abstract:Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes.",
                        "Citation Paper Authors": "Authors:Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": ". SegFormer B0 achieves a speed of 47.6Frames Per\nSecond (FPS) when running at a resolution of 512\u00d71024 and\ntherefore suits well for real-time applications ",
                    "Citation Text": "E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo,\n\u201cSegFormer: Simple and efficient design for semantic segmentation with\ntransformers,\u201d in NeurIPS , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.15203",
                        "Citation Paper Title": "Title:SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers",
                        "Citation Paper Abstract": "Abstract:We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perception (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5x smaller and 2.2% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code will be released at: this http URL.",
                        "Citation Paper Authors": "Authors:Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo"
                    }
                },
                {
                    "Sentence ID": 79,
                    "Sentence": "reach high mIoU scores, they are computationally intensive,\nwhich often disqualifies their deployment in mobile and real-\ntime systems. Accurate segmenters like HANet ",
                    "Citation Text": "S. Choi, J. T. Kim, and J. Choo, \u201cCars can\u2019t fly up in the sky: Improv-\ning urban-scene segmentation via height-driven attention networks,\u201d in\nCVPR , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.05128",
                        "Citation Paper Title": "Title:Cars Can't Fly up in the Sky: Improving Urban-Scene Segmentation via Height-driven Attention Networks",
                        "Citation Paper Abstract": "Abstract:This paper exploits the intrinsic features of urban-scene images and proposes a general add-on module, called height-driven attention networks (HANet), for improving semantic segmentation for urban-scene images. It emphasizes informative features or classes selectively according to the vertical position of a pixel. The pixel-wise class distributions are significantly different from each other among horizontally segmented sections in the urban-scene images. Likewise, urban-scene images have their own distinct characteristics, but most semantic segmentation networks do not reflect such unique attributes in the architecture. The proposed network architecture incorporates the capability exploiting the attributes to handle the urban scene dataset effectively. We validate the consistent performance (mIoU) increase of various semantic segmentation models on two datasets when HANet is adopted. This extensive quantitative analysis demonstrates that adding our module to existing models is easy and cost-effective. Our method achieves a new state-of-the-art performance on the Cityscapes benchmark with a large margin among ResNet-101 based segmentation models. Also, we show that the proposed model is coherent with the facts observed in the urban scene by visualizing and interpreting the attention map. Our code and trained models are publicly available at this https URL",
                        "Citation Paper Authors": "Authors:Sungha Choi, Joanne T. Kim, Jaegul Choo"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": ". * denotes at the position of\npatch embeddings.\nNetwork #Params (M) GFLOPs mIoU (%)\nTeacher (B2) 27.36 113.84 76.49\nStudent (B0) 3.72 13.67 55.86\n+Pre-train 3.72 13.67 69.75\n+KD ",
                    "Citation Text": "G. Hinton, O. Vinyals, and J. Dean, \u201cDistilling the knowledge in a neural\nnetwork,\u201d arXiv preprint arXiv:1503.02531 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1503.02531",
                        "Citation Paper Title": "Title:Distilling the Knowledge in a Neural Network",
                        "Citation Paper Abstract": "Abstract:A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.",
                        "Citation Paper Authors": "Authors:Geoffrey Hinton, Oriol Vinyals, Jeff Dean"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.03087v3": {
            "Paper Title": "Iterative Vision-and-Language Navigation",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": "on IR2R-CE Train. The best performing\ncheckpoint on Val-Unseen is accepted as the final model. We\nuse the Progress Monitor ",
                    "Citation Text": "Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan AlRegib, Zsolt\nKira, Richard Socher, and Caiming Xiong. Self-monitoring\nnavigation agent via auxiliary progress estimation. In Interna-tional Conference on Learning Representations (ICLR) , 2019.\n6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.03035",
                        "Citation Paper Title": "Title:Self-Monitoring Navigation Agent via Auxiliary Progress Estimation",
                        "Citation Paper Abstract": "Abstract:The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at this https URL .",
                        "Citation Paper Authors": "Authors:Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan AlRegib, Zsolt Kira, Richard Socher, Caiming Xiong"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "ported to IR2R-CE, then fine-\ntune the best-performing EnvDrop Val-Unseen checkpoint\nusing DAgger ",
                    "Citation Text": "St\u00e9phane Ross, Geoffrey Gordon, and Drew Bagnell. A\nreduction of imitation learning and structured prediction to no-\nregret online learning. In Artificial Intelligence and Statistics\n(AISTATS) , 2011. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1011.0686",
                        "Citation Paper Title": "Title:A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning",
                        "Citation Paper Abstract": "Abstract:Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem.",
                        "Citation Paper Authors": "Authors:Stephane Ross, Geoffrey J. Gordon, J. Andrew Bagnell"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "used to train the\nCMA model. We initially train using teacher forcing on\nthe augmented EnvDrop ",
                    "Citation Text": "Hao Tan, Licheng Yu, and Mohit Bansal. Learning to navigate\nunseen environments: Back translation with environmental\ndropout. North American Chapter of the Association for\nComputational Linguistics (NAACL) , 2019. 3, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.04195",
                        "Citation Paper Title": "Title:Learning to Navigate Unseen Environments: Back Translation with Environmental Dropout",
                        "Citation Paper Abstract": "Abstract:A grand goal in AI is to build a robot that can accurately navigate based on natural language instructions, which requires the agent to perceive the scene, understand and ground language, and act in the real-world environment. One key challenge here is to learn to navigate in new environments that are unseen during training. Most of the existing approaches perform dramatically worse in unseen environments as compared to seen ones. In this paper, we present a generalizable navigational agent. Our agent is trained in two stages. The first stage is training via mixed imitation and reinforcement learning, combining the benefits from both off-policy and on-policy optimization. The second stage is fine-tuning via newly-introduced 'unseen' triplets (environment, path, instruction). To generate these unseen triplets, we propose a simple but effective 'environmental dropout' method to mimic unseen environments, which overcomes the problem of limited seen environment variability. Next, we apply semi-supervised learning (via back-translation) on these dropped-out environments to generate new paths and instructions. Empirically, we show that our agent is substantially better at generalizability when fine-tuned with these triplets, outperforming the state-of-art approaches by a large margin on the private unseen test set of the Room-to-Room task, and achieving the top rank on the leaderboard.",
                        "Citation Paper Authors": "Authors:Hao Tan, Licheng Yu, Mohit Bansal"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ", we use the inverse pinhole\ncamera projection model to unproject depth measurements\nto 3D pointclouds. We also unproject egocentric semantics\u2014\nboth ground-truth ",
                    "Citation Text": "Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Hal-\nber, Matthias Niessner, Manolis Savva, Shuran Song, Andy\nZeng, and Yinda Zhang. Matterport3d: learning from RGB-D\ndata in indoor environments. In 3D Vision , 2017. Matter-\nPort3D dataset license available at: http://kaldir.vc.\nin.tum.de/matterport/MP_TOS.pdf . 2, 3, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.06158",
                        "Citation Paper Title": "Title:Matterport3D: Learning from RGB-D Data in Indoor Environments",
                        "Citation Paper Abstract": "Abstract:Access to large, diverse RGB-D datasets is critical for training RGB-D scene understanding algorithms. However, existing datasets still cover only a limited number of views or a restricted scale of spaces. In this paper, we introduce Matterport3D, a large-scale RGB-D dataset containing 10,800 panoramic views from 194,400 RGB-D images of 90 building-scale scenes. Annotations are provided with surface reconstructions, camera poses, and 2D and 3D semantic segmentations. The precise global alignment and comprehensive, diverse panoramic set of views over entire buildings enable a variety of supervised and self-supervised computer vision tasks, including keypoint matching, view overlap prediction, normal prediction from color, semantic segmentation, and region classification.",
                        "Citation Paper Authors": "Authors:Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Nie\u00dfner, Manolis Savva, Shuran Song, Andy Zeng, Yinda Zhang"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "where cells\nare either 0 (empty) or 1 (occupied), and semantic maps\ncontain one-hot vectors of thirteen common labels in R2R\nenvironments ",
                    "Citation Text": "Vincent Cartillier, Zhile Ren, Neha Jain, Stefan Lee, Irfan\nEssa, and Dhruv Batra. Semantic mapnet: Building allocen-\ntric semantic maps and representations from egocentric views.\nInAAAI Conference on Artificial Intelligence , 2021. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.01191",
                        "Citation Paper Title": "Title:Semantic MapNet: Building Allocentric Semantic Maps and Representations from Egocentric Views",
                        "Citation Paper Abstract": "Abstract:We study the task of semantic mapping - specifically, an embodied agent (a robot or an egocentric AI assistant) is given a tour of a new environment and asked to build an allocentric top-down semantic map (\"what is where?\") from egocentric observations of an RGB-D camera with known pose (via localization sensors). Towards this goal, we present SemanticMapNet (SMNet), which consists of: (1) an Egocentric Visual Encoder that encodes each egocentric RGB-D frame, (2) a Feature Projector that projects egocentric features to appropriate locations on a floor-plan, (3) a Spatial Memory Tensor of size floor-plan length x width x feature-dims that learns to accumulate projected egocentric features, and (4) a Map Decoder that uses the memory tensor to produce semantic top-down maps. SMNet combines the strengths of (known) projective camera geometry and neural representation learning. On the task of semantic mapping in the Matterport3D dataset, SMNet significantly outperforms competitive baselines by 4.01-16.81% (absolute) on mean-IoU and 3.81-19.69% (absolute) on Boundary-F1 metrics. Moreover, we show how to use the neural episodic memories and spatio-semantic allocentric representations build by SMNet for subsequent tasks in the same space - navigating to objects seen during the tour(\"Find chair\") or answering questions about the space (\"How many chairs did you see in the house?\"). Project page: this https URL.",
                        "Citation Paper Authors": "Authors:Vincent Cartillier, Zhile Ren, Neha Jain, Stefan Lee, Irfan Essa, Dhruv Batra"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": ",\nin response to language instructions requires contending\nwith the real, continuous world. Existing work has trans-\nferred policies for discrete VLN to the physical world by\nmanually curating a discrete representation of the world\nmap as a navigation graph ",
                    "Citation Text": "Peter Anderson, Ayush Shrivastava, Joanne Truong, Arjun\nMajumdar, Devi Parikh, Dhruv Batra, and Stefan Lee. Sim-\nto-real transfer for vision-and-language navigation. In Con-\nference on Robot Learning (CoRL) , 2020. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.03807",
                        "Citation Paper Title": "Title:Sim-to-Real Transfer for Vision-and-Language Navigation",
                        "Citation Paper Abstract": "Abstract:We study the challenging problem of releasing a robot in a previously unseen environment, and having it follow unconstrained natural language navigation instructions. Recent work on the task of Vision-and-Language Navigation (VLN) has achieved significant progress in simulation. To assess the implications of this work for robotics, we transfer a VLN agent trained in simulation to a physical robot. To bridge the gap between the high-level discrete action space learned by the VLN agent, and the robot's low-level continuous action space, we propose a subgoal model to identify nearby waypoints, and use domain randomization to mitigate visual domain differences. For accurate sim and real comparisons in parallel environments, we annotate a 325m2 office space with 1.3km of navigation instructions, and create a digitized replica in simulation. We find that sim-to-real transfer to an environment not seen in training is successful if an occupancy map and navigation graph can be collected and annotated in advance (success rate of 46.8% vs. 55.9% in sim), but much more challenging in the hardest setting with no prior mapping at all (success rate of 22.5%).",
                        "Citation Paper Authors": "Authors:Peter Anderson, Ayush Shrivastava, Joanne Truong, Arjun Majumdar, Devi Parikh, Dhruv Batra, Stefan Lee"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.16058v2": {
            "Paper Title": "Goal Exploration Augmentation via Pre-trained Skills for Sparse-Reward\n  Long-Horizon Goal-Conditioned Reinforcement Learning",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": "explores the goal space at first,\nthen encode those goals into discrete latent vectors Zvia a trained VQ-VAE ",
                    "Citation Text": "Van Den Oord, A., Vinyals, O., et al.: Neural discrete representation\nlearning. Advances in neural information processing systems 30(2017)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.00937",
                        "Citation Paper Title": "Title:Neural Discrete Representation Learning",
                        "Citation Paper Abstract": "Abstract:Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of \"posterior collapse\" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.",
                        "Citation Paper Authors": "Authors:Aaron van den Oord, Oriol Vinyals, Koray Kavukcuoglu"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "for a\nfair comparison. To obtain the pre-trained skills for PointMaze andAntMaze ,\nwe use a multi-layer perceptron trained with the TRPO ",
                    "Citation Text": "Schulman, J., Levine, S., Abbeel, P., Jordan, M., Moritz, P.: Trust region\npolicy optimization. In: International Conference on Machine Learning,\npp. 1889\u20131897 (2015). PMLR",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1502.05477",
                        "Citation Paper Title": "Title:Trust Region Policy Optimization",
                        "Citation Paper Abstract": "Abstract:We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.",
                        "Citation Paper Authors": "Authors:John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "below in terms of goals for simplicity. With Eq. 1a,\nSNN4HRL ",
                    "Citation Text": "Florensa, C., Duan, Y., Abbeel, P.: Stochastic neural networks for hier-\narchical reinforcement learning. arXiv preprint arXiv:1704.03012 (2017)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.03012",
                        "Citation Paper Title": "Title:Stochastic Neural Networks for Hierarchical Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:Deep reinforcement learning has achieved many impressive results in recent years. However, tasks with sparse rewards or long horizons continue to pose significant challenges. To tackle these important problems, we propose a general framework that first learns useful skills in a pre-training environment, and then leverages the acquired skills for learning faster in downstream tasks. Our approach brings together some of the strengths of intrinsic motivation and hierarchical methods: the learning of useful skill is guided by a single proxy reward, the design of which requires very minimal domain knowledge about the downstream tasks. Then a high-level policy is trained on top of these skills, providing a significant improvement of the exploration and allowing to tackle sparse rewards in the downstream tasks. To efficiently pre-train a large span of skills, we use Stochastic Neural Networks combined with an information-theoretic regularizer. Our experiments show that this combination is effective in learning a wide span of interpretable skills in a sample-efficient way, and can significantly boost the learning performance uniformly across a wide range of downstream tasks.",
                        "Citation Paper Authors": "Authors:Carlos Florensa, Yan Duan, Pieter Abbeel"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": ", several classical or SOTA baselines were\ntested on the same environments used in our work. As OMEGA beats those\nbaselines with a huge margin, e.g., OMEGA is around 100 and 10 times faster\nthan the best performer, PPO+SR ",
                    "Citation Text": "Trott, A., Zheng, S., Xiong, C., Socher, R.: Keeping your distance: Solv-\ning sparse reward tasks using self-balancing shaped rewards. Advances in\nNeural Information Processing Systems 32(2019)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.01417",
                        "Citation Paper Title": "Title:Keeping Your Distance: Solving Sparse Reward Tasks Using Self-Balancing Shaped Rewards",
                        "Citation Paper Abstract": "Abstract:While using shaped rewards can be beneficial when solving sparse reward tasks, their successful application often requires careful engineering and is problem specific. For instance, in tasks where the agent must achieve some goal state, simple distance-to-goal reward shaping often fails, as it renders learning vulnerable to local optima. We introduce a simple and effective model-free method to learn from shaped distance-to-goal rewards on tasks where success depends on reaching a goal state. Our method introduces an auxiliary distance-based reward based on pairs of rollouts to encourage diverse exploration. This approach effectively prevents learning dynamics from stabilizing around local optima induced by the naive distance-to-goal reward shaping and enables policies to efficiently solve sparse reward tasks. Our augmented objective does not require any additional reward engineering or domain expertise to implement and converges to the original sparse objective as the agent learns to solve the task. We demonstrate that our method successfully solves a variety of hard-exploration tasks (including maze navigation and 3D construction in a Minecraft environment), where naive distance-based reward shaping otherwise fails, and intrinsic curiosity and reward relabeling strategies exhibit poor performance.",
                        "Citation Paper Authors": "Authors:Alexander Trott, Stephan Zheng, Caiming Xiong, Richard Socher"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "learn skills by fixing the distribution of latent\nvectors and minimizing the conditional entropy H(Z|G). DADS ",
                    "Citation Text": "Sharma, A., Gu, S., Levine, S., Kumar, V., Hausman, K.: Dynamics-aware\nunsupervised discovery of skills. arXiv preprint arXiv:1907.01657 (2019)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.01657",
                        "Citation Paper Title": "Title:Dynamics-Aware Unsupervised Discovery of Skills",
                        "Citation Paper Abstract": "Abstract:Conventionally, model-based reinforcement learning (MBRL) aims to learn a global model for the dynamics of the environment. A good model can potentially enable planning algorithms to generate a large variety of behaviors and solve diverse tasks. However, learning an accurate model for complex dynamical systems is difficult, and even then, the model might not generalize well outside the distribution of states on which it was trained. In this work, we combine model-based learning with model-free learning of primitives that make model-based planning easy. To that end, we aim to answer the question: how can we discover skills whose outcomes are easy to predict? We propose an unsupervised learning algorithm, Dynamics-Aware Discovery of Skills (DADS), which simultaneously discovers predictable behaviors and learns their dynamics. Our method can leverage continuous skill spaces, theoretically, allowing us to learn infinitely many behaviors even for high-dimensional state-spaces. We demonstrate that zero-shot planning in the learned latent space significantly outperforms standard MBRL and model-free goal-conditioned RL, can handle sparse-reward tasks, and substantially improves over prior hierarchical RL methods for unsupervised skill discovery.",
                        "Citation Paper Authors": "Authors:Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, Karol Hausman"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.11357v3": {
            "Paper Title": "POLTER: Policy Trajectory Ensemble Regularization for Unsupervised\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.01795v3": {
            "Paper Title": "Dynamic Modeling of Branched Robots using Modular Composition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.00944v2": {
            "Paper Title": "Piezoelectric Soft Robot Inchworm Motion by Tuning Ground Friction\n  through Robot Shape: Quasi-Static Modeling and Experimental Validation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.03323v3": {
            "Paper Title": "Receding Horizon Planning with Rule Hierarchies for Autonomous Vehicles",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": "offer approaches to\ntune the contribution of competing criteria in the total re-\nward/cost function by harnessing data. Various recent works\nuse data to learn such reward functions for autonomous\nvehicles by learning a trajectory scoring function ",
                    "Citation Text": "T. Phan-Minh, F. Howington, T.-S. Chu, S. U. Lee, M. S. Tomov,\nN. Li, C. Dicle, S. Findler, F. Suarez-Ruiz, R. Beaudoin, et al. ,\n\u201cDriving in real life with inverse reinforcement learning,\u201d arXiv\npreprint arXiv:2206.03004 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.03004",
                        "Citation Paper Title": "Title:Driving in Real Life with Inverse Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:In this paper, we introduce the first learning-based planner to drive a car in dense, urban traffic using Inverse Reinforcement Learning (IRL). Our planner, DriveIRL, generates a diverse set of trajectory proposals, filters these trajectories with a lightweight and interpretable safety filter, and then uses a learned model to score each remaining trajectory. The best trajectory is then tracked by the low-level controller of our self-driving vehicle. We train our trajectory scoring model on a 500+ hour real-world dataset of expert driving demonstrations in Las Vegas within the maximum entropy IRL framework. DriveIRL's benefits include: a simple design due to only learning the trajectory scoring function, relatively interpretable features, and strong real-world performance. We validated DriveIRL on the Las Vegas Strip and demonstrated fully autonomous driving in heavy traffic, including scenarios involving cut-ins, abrupt braking by the lead vehicle, and hotel pickup/dropoff zones. Our dataset will be made public to help further research in this area.",
                        "Citation Paper Authors": "Authors:Tung Phan-Minh, Forbes Howington, Ting-Sheng Chu, Sang Uk Lee, Momchil S. Tomov, Nanxiang Li, Caglayan Dicle, Samuel Findler, Francisco Suarez-Ruiz, Robert Beaudoin, Bo Yang, Sammy Omari, Eric M. Wolff"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.05580v2": {
            "Paper Title": "Risk-aware Meta-level Decision Making for Exploration Under Uncertainty",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.08642v2": {
            "Paper Title": "POAR: Efficient Policy Optimization via Online Abstract State\n  Representation Learning",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": ". Since SRL is usually conducted in an\nunsupervised manner, it demands advanced priors of the tasks.\nTwo mainstream works prove to address SRL effectively, in which we compre-\nhensively review in our former survey ",
                    "Citation Text": "Lesort, T., D\u00b4 \u0131az-Rodr\u00b4 \u0131guez, N., Goudou, J.-F., Filliat, D.: State representation\nlearning for control: An overview. Neural Networks 108, 379\u2013392 (2018)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.04181",
                        "Citation Paper Title": "Title:State Representation Learning for Control: An Overview",
                        "Citation Paper Abstract": "Abstract:Representation learning algorithms are designed to learn abstract features that characterize data. State representation learning (SRL) focuses on a particular kind of representation learning where learned features are in low dimension, evolve through time, and are influenced by actions of an agent. The representation is learned to capture the variation in the environment generated by the agent's actions; this kind of representation is particularly suitable for robotics and control scenarios. In particular, the low dimension characteristic of the representation helps to overcome the curse of dimensionality, provides easier interpretation and utilization by humans and can help improve performance and speed in policy learning algorithms such as reinforcement learning.\nThis survey aims at covering the state-of-the-art on state representation learning in the most recent years. It reviews different SRL methods that involve interaction with the environment, their implementations and their applications in robotics control tasks (simulated or real). In particular, it highlights how generic learning objectives are differently exploited in the reviewed algorithms. Finally, it discusses evaluation methods to assess the representation learned and summarizes current and future lines of research.",
                        "Citation Paper Authors": "Authors:Timoth\u00e9e Lesort, Natalia D\u00edaz-Rodr\u00edguez, Jean-Fran\u00e7ois Goudou, David Filliat"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": ". To do so, POAR employs a demonstration-based prior to optimize\nthe maximum mean discrepancy (MMD) ",
                    "Citation Text": "Gretton, A., Borgwardt, K.M., Rasch, M.J., Sch\u00a8 olkopf, B., Smola, A.: A ker-\nnel two-sample test. The Journal of Machine Learning Research 13(1), 723\u2013773\n(2012)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:0805.2368",
                        "Citation Paper Title": "Title:A Kernel Method for the Two-Sample Problem",
                        "Citation Paper Abstract": "Abstract:  We propose a framework for analyzing and comparing distributions, allowing us to design statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS). We present two tests based on large deviation bounds for the test statistic, while a third is based on the asymptotic distribution of this statistic. The test statistic can be computed in quadratic time, although efficient linear time approximations are available. Several classical metrics on distributions are recovered when the function space used to compute the difference in expectations is allowed to be more general (eg. a Banach space). We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.",
                        "Citation Paper Authors": "Authors:Arthur Gretton, Karsten Borgwardt, Malte J. Rasch, Bernhard Scholkopf, Alexander J. Smola"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "thus introduce some robotic priors that only pertain to tasks involving robots\nin real-world function. Since manually designing priors are tedious, ",
                    "Citation Text": "He, T., Zhang, Y., Ren, K., Liu, M., Wang, C., Zhang, W., Yang, Y., Li, D.:\nReinforcement learning with automated auxiliary loss search. Advances in Neural\n17Information Processing Systems 35, 1820\u20131834 (2022)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.06041",
                        "Citation Paper Title": "Title:Reinforcement Learning with Automated Auxiliary Loss Search",
                        "Citation Paper Abstract": "Abstract:A good state representation is crucial to solving complicated reinforcement learning (RL) challenges. Many recent works focus on designing auxiliary losses for learning informative representations. Unfortunately, these handcrafted objectives rely heavily on expert knowledge and may be sub-optimal. In this paper, we propose a principled and universal method for learning better representations with auxiliary loss functions, named Automated Auxiliary Loss Search (A2LS), which automatically searches for top-performing auxiliary loss functions for RL. Specifically, based on the collected trajectory data, we define a general auxiliary loss space of size $7.5 \\times 10^{20}$ and explore the space with an efficient evolutionary search strategy. Empirical results show that the discovered auxiliary loss (namely, A2-winner) significantly improves the performance on both high-dimensional (image) and low-dimensional (vector) unseen tasks with much higher efficiency, showing promising generalization ability to different settings and even different benchmark domains. We conduct a statistical analysis to reveal the relations between patterns of auxiliary losses and RL performance.",
                        "Citation Paper Authors": "Authors:Tairan He, Yuge Zhang, Kan Ren, Minghuan Liu, Che Wang, Weinan Zhang, Yuqing Yang, Dongsheng Li"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "proposed to learn the environment dynamics via predicting future states coupled\nwith reward prediction. However, such system dynamic models are highly non-linear\nand stochastic that often lead the learned latent space to collapse ",
                    "Citation Text": "Schwarzer, M., Anand, A., Goel, R., Hjelm, R.D., Courville, A., Bachman, P.:\nData-efficient reinforcement learning with self-predictive representations. arXiv\npreprint arXiv:2007.05929 (2020)\n15",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.05929",
                        "Citation Paper Title": "Title:Data-Efficient Reinforcement Learning with Self-Predictive Representations",
                        "Citation Paper Abstract": "Abstract:While deep reinforcement learning excels at solving tasks where large amounts of data can be collected through virtually unlimited interaction with the environment, learning from limited interaction remains a key challenge. We posit that an agent can learn more efficiently if we augment reward maximization with self-supervised objectives based on structure in its visual input and sequential interaction with the environment. Our method, Self-Predictive Representations(SPR), trains an agent to predict its own latent state representations multiple steps into the future. We compute target representations for future states using an encoder which is an exponential moving average of the agent's parameters and we make predictions using a learned transition model. On its own, this future prediction objective outperforms prior methods for sample-efficient deep RL from pixels. We further improve performance by adding data augmentation to the future prediction loss, which forces the agent's representations to be consistent across multiple views of an observation. Our full self-supervised objective, which combines future prediction and data augmentation, achieves a median human-normalized score of 0.415 on Atari in a setting limited to 100k steps of environment interaction, which represents a 55% relative improvement over the previous state-of-the-art. Notably, even in this limited data regime, SPR exceeds expert human scores on 7 out of 26 games. The code associated with this work is available at this https URL",
                        "Citation Paper Authors": "Authors:Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, Philip Bachman"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2007.00245v3": {
            "Paper Title": "Fighting Failures with FIRE: Failure Identification to Reduce Expert\n  Burden in Intervention-Based Learning",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": "with a UR10 arm and a two-fingered gripper. A timestep in each environment is 0.1 s, while, at the level of\nsimulation, each action is repeated 10 times, as is common in robotics learning environments ",
                    "Citation Text": "M. Plappert, M. Andrychowicz, A. Ray, B. McGrew, B. Baker, G. Powell, J. Schneider, J. Tobin,\nM. Chociej, P. Welinder, V. Kumar, and W. Zaremba, \u201cMulti-Goal Reinforcement Learning: Challenging\nRobotics Environments and Request for Research,\u201d arXiv:1802.09464 [cs] , Mar. 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.09464",
                        "Citation Paper Title": "Title:Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research",
                        "Citation Paper Abstract": "Abstract:The purpose of this technical report is two-fold. First of all, it introduces a suite of challenging continuous control tasks (integrated with OpenAI Gym) based on currently existing robotics hardware. The tasks include pushing, sliding and pick & place with a Fetch robotic arm as well as in-hand object manipulation with a Shadow Dexterous Hand. All tasks have sparse binary rewards and follow a Multi-Goal Reinforcement Learning (RL) framework in which an agent is told what to do using an additional input.\nThe second part of the paper presents a set of concrete research ideas for improving RL algorithms, most of which are related to Multi-Goal RL and Hindsight Experience Replay.",
                        "Citation Paper Authors": "Authors:Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, Vikash Kumar, Wojciech Zaremba"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.03897v4": {
            "Paper Title": "Sim-to-Real Transfer of Compliant Bipedal Locomotion on Torque\n  Sensor-Less Gear-Driven Humanoid",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.06930v3": {
            "Paper Title": "PaintNet: Unstructured Multi-Path Learning from 3D Point Clouds for\n  Robotic Spray Painting",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": ", and robotic grasping, where the output\nis a structured grasp descriptor ",
                    "Citation Text": "P. Ni, W. Zhang, X. Zhu, and Q. Cao, \u201cPointnet++ grasping: Learning\nan end-to-end spatial grasp generation algorithm from sparse point\nclouds,\u201d in IEEE ICRA , 2020, pp. 3619\u20133625.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.09644",
                        "Citation Paper Title": "Title:PointNet++ Grasping: Learning An End-to-end Spatial Grasp Generation Algorithm from Sparse Point Clouds",
                        "Citation Paper Abstract": "Abstract:Grasping for novel objects is important for robot manipulation in unstructured environments. Most of current works require a grasp sampling process to obtain grasp candidates, combined with local feature extractor using deep learning. This pipeline is time-costly, expecially when grasp points are sparse such as at the edge of a bowl. In this paper, we propose an end-to-end approach to directly predict the poses, categories and scores (qualities) of all the grasps. It takes the whole sparse point clouds as the input and requires no sampling or search process. Moreover, to generate training data of multi-object scene, we propose a fast multi-object grasp detection algorithm based on Ferrari Canny metrics. A single-object dataset (79 objects from YCB object set, 23.7k grasps) and a multi-object dataset (20k point clouds with annotations and masks) are generated. A PointNet++ based network combined with multi-mask loss is introduced to deal with different training points. The whole weight size of our network is only about 11.6M, which takes about 102ms for a whole prediction process using a GeForce 840M GPU. Our experiment shows our work get 71.43% success rate and 91.60% completion rate, which performs better than current state-of-art works.",
                        "Citation Paper Authors": "Authors:Peiyuan Ni, Wenguang Zhang, Xiaoxiao Zhu, Qixin Cao"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2207.12544v2": {
            "Paper Title": "End-User Puppeteering of Expressive Movements",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.03628v2": {
            "Paper Title": "GraspCaps: A Capsule Network Approach for Familiar 6DoF Object Grasping",
            "Sentences": [
                {
                    "Sentence ID": 33,
                    "Sentence": "processed\nthe input data by applying a \u03c7-transform on the point set.\nDGCNN ",
                    "Citation Text": "Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma,\nMichael M Bronstein, and Justin M Solomon. Dynamic\ngraph cnn for learning on point clouds. Acm Transactions\nOn Graphics (tog) , 38(5):1\u201312, 2019. 2, 3, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.07829",
                        "Citation Paper Title": "Title:Dynamic Graph CNN for Learning on Point Clouds",
                        "Citation Paper Abstract": "Abstract:Point clouds provide a flexible geometric representation suitable for countless applications in computer graphics; they also comprise the raw output of most 3D data acquisition devices. While hand-designed features on point clouds have long been proposed in graphics and vision, however, the recent overwhelming success of convolutional neural networks (CNNs) for image analysis suggests the value of adapting insight from CNN to the point cloud world. Point clouds inherently lack topological information so designing a model to recover topology can enrich the representation power of point clouds. To this end, we propose a new neural network module dubbed EdgeConv suitable for CNN-based high-level tasks on point clouds including classification and segmentation. EdgeConv acts on graphs dynamically computed in each layer of the network. It is differentiable and can be plugged into existing architectures. Compared to existing modules operating in extrinsic space or treating each point independently, EdgeConv has several appealing properties: It incorporates local neighborhood information; it can be stacked applied to learn global shape properties; and in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding. We show the performance of our model on standard benchmarks including ModelNet40, ShapeNetPart, and S3DIS.",
                        "Citation Paper Authors": "Authors:Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, Justin M. Solomon"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "extended the\nPointNet architecture to generate 6D grasps based on the\ninput point set. Grasp pose detection (GPD) ",
                    "Citation Text": "Andreas ten Pas, Marcus Gualtieri, Kate Saenko, and Robert\nPlatt. Grasp pose detection in point clouds. The International\nJournal of Robotics Research , 36(13-14):1455\u20131473, 2017.\n2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.09911",
                        "Citation Paper Title": "Title:Grasp Pose Detection in Point Clouds",
                        "Citation Paper Abstract": "Abstract:Recently, a number of grasp detection methods have been proposed that can be used to localize robotic grasp configurations directly from sensor data without estimating object pose. The underlying idea is to treat grasp perception analogously to object detection in computer vision. These methods take as input a noisy and partially occluded RGBD image or point cloud and produce as output pose estimates of viable grasps, without assuming a known CAD model of the object. Although these methods generalize grasp knowledge to new objects well, they have not yet been demonstrated to be reliable enough for wide use. Many grasp detection methods achieve grasp success rates (grasp successes as a fraction of the total number of grasp attempts) between 75% and 95% for novel objects presented in isolation or in light clutter. Not only are these success rates too low for practical grasping applications, but the light clutter scenarios that are evaluated often do not reflect the realities of real world grasping. This paper proposes a number of innovations that together result in a significant improvement in grasp detection performance. The specific improvement in performance due to each of our contributions is quantitatively measured either in simulation or on robotic hardware. Ultimately, we report a series of robotic experiments that average a 93% end-to-end grasp success rate for novel objects presented in dense clutter.",
                        "Citation Paper Authors": "Authors:Andreas ten Pas, Marcus Gualtieri, Kate Saenko, Robert Platt"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": ", which makes\nthe insight to split up the PointNet architecture into several\ndistinct modules.\nLater research showed successful results working with\npoint sets by transforming the point set to be processed by\na convolutional neural network. PointCNN ",
                    "Citation Text": "Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan\nDi, and Baoquan Chen. Pointcnn: Convolution on \u03c7-\ntransformed points. Advances in neural information process-\ning systems , 31:820\u2013830, 2018. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.07791",
                        "Citation Paper Title": "Title:PointCNN: Convolution On $\\mathcal{X}$-Transformed Points",
                        "Citation Paper Abstract": "Abstract:We present a simple and general framework for feature learning from point clouds. The key to the success of CNNs is the convolution operator that is capable of leveraging spatially-local correlation in data represented densely in grids (e.g. images). However, point clouds are irregular and unordered, thus directly convolving kernels against features associated with the points, will result in desertion of shape information and variance to point ordering. To address these problems, we propose to learn an $\\mathcal{X}$-transformation from the input points, to simultaneously promote two causes. The first is the weighting of the input features associated with the points, and the second is the permutation of the points into a latent and potentially canonical order. Element-wise product and sum operations of the typical convolution operator are subsequently applied on the $\\mathcal{X}$-transformed features. The proposed method is a generalization of typical CNNs to feature learning from point clouds, thus we call it PointCNN. Experiments show that PointCNN achieves on par or better performance than state-of-the-art methods on multiple challenging benchmark datasets and tasks.",
                        "Citation Paper Authors": "Authors:Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, Baoquan Chen"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ", hence order should not be fully disre-\ngarded. PointNet++ ",
                    "Citation Text": "Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Point-\nnet++: Deep hierarchical feature learning on point sets in a\nmetric space. arXiv preprint arXiv:1706.02413 , 2017. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.02413",
                        "Citation Paper Title": "Title:PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space",
                        "Citation Paper Abstract": "Abstract:Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.",
                        "Citation Paper Authors": "Authors:Charles R. Qi, Li Yi, Hao Su, Leonidas J. Guibas"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "Deep learning-based object grasping methods provide\nenhanced accuracy and adaptability, reduced dependency\non manual engineering, and improved robustness to vari-\nability in real-world scenarios ",
                    "Citation Text": "Rhys Newbury, Morris Gu, Lachlan Chumbley, Arsalan\nMousavian, Clemens Eppner, J \u00a8urgen Leitner, Jeannette\nBohg, Antonio Morales, Tamim Asfour, Danica Kragic, et al.\nDeep learning approaches to grasp synthesis: A review.\nIEEE Transactions on Robotics , 2023. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2207.02556",
                        "Citation Paper Title": "Title:Deep Learning Approaches to Grasp Synthesis: A Review",
                        "Citation Paper Abstract": "Abstract:Grasping is the process of picking up an object by applying forces and torques at a set of contacts. Recent advances in deep-learning methods have allowed rapid progress in robotic object grasping. In this systematic review, we surveyed the publications over the last decade, with a particular interest in grasping an object using all 6 degrees of freedom of the end-effector pose. Our review found four common methodologies for robotic grasping: sampling-based approaches, direct regression, reinforcement learning, and exemplar approaches. Additionally, we found two `supporting methods` around grasping that use deep-learning to support the grasping process, shape approximation, and affordances. We have distilled the publications found in this systematic review (85 papers) into ten key takeaways we consider crucial for future robotic grasping and manipulation research. An online version of the survey is available at this https URL",
                        "Citation Paper Authors": "Authors:Rhys Newbury, Morris Gu, Lachlan Chumbley, Arsalan Mousavian, Clemens Eppner, J\u00fcrgen Leitner, Jeannette Bohg, Antonio Morales, Tamim Asfour, Danica Kragic, Dieter Fox, Akansel Cosgun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2207.08892v3": {
            "Paper Title": "Distributed Differentiable Dynamic Game for Multi-robot Coordination",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": ". Several methods in the\nliterature can be used to solve IOC ",
                    "Citation Text": "W. Jin, Z. Wang, Z. Yang, and S. Mou, \u201cPontryagin differentiable pro-\ngramming: An end-to-end learning and control framework,\u201d Advances\nin Neural Information Processing Systems , vol. 33, pp. 7979\u20137992,\n2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.12970",
                        "Citation Paper Title": "Title:Pontryagin Differentiable Programming: An End-to-End Learning and Control Framework",
                        "Citation Paper Abstract": "Abstract:This paper develops a Pontryagin Differentiable Programming (PDP) methodology, which establishes a unified framework to solve a broad class of learning and control tasks. The PDP distinguishes from existing methods by two novel techniques: first, we differentiate through Pontryagin's Maximum Principle, and this allows to obtain the analytical derivative of a trajectory with respect to tunable parameters within an optimal control system, enabling end-to-end learning of dynamics, policies, or/and control objective functions; and second, we propose an auxiliary control system in the backward pass of the PDP framework, and the output of this auxiliary control system is the analytical derivative of the original system's trajectory with respect to the parameters, which can be iteratively solved using standard control tools. We investigate three learning modes of the PDP: inverse reinforcement learning, system identification, and control/planning. We demonstrate the capability of the PDP in each learning mode on different high-dimensional systems, including multi-link robot arm, 6-DoF maneuvering quadrotor, and 6-DoF rocket powered landing.",
                        "Citation Paper Authors": "Authors:Wanxin Jin, Zhaoran Wang, Zhuoran Yang, Shaoshuai Mou"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.02054v4": {
            "Paper Title": "Placing by Touching: An empirical study on the importance of tactile\n  sensing for precise object placing",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": "uses pointcloud observations for extracting\nmeaningful feature representations for learning to place new\nobjects in stable configurations. More recently, ",
                    "Citation Text": "L. Manuelli, W. Gao, P. Florence, and R. Tedrake, \u201ckpam: Keypoint\naffordances for category-level robotic manipulation,\u201d in The Interna-\ntional Symposium of Robotics Research . Springer, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.06684",
                        "Citation Paper Title": "Title:kPAM: KeyPoint Affordances for Category-Level Robotic Manipulation",
                        "Citation Paper Abstract": "Abstract:We would like robots to achieve purposeful manipulation by placing any instance from a category of objects into a desired set of goal states. Existing manipulation pipelines typically specify the desired configuration as a target 6-DOF pose and rely on explicitly estimating the pose of the manipulated objects. However, representing an object with a parameterized transformation defined on a fixed template cannot capture large intra-category shape variation, and specifying a target pose at a category level can be physically infeasible or fail to accomplish the task -- e.g. knowing the pose and size of a coffee mug relative to some canonical mug is not sufficient to successfully hang it on a rack by its handle. Hence we propose a novel formulation of category-level manipulation that uses semantic 3D keypoints as the object representation. This keypoint representation enables a simple and interpretable specification of the manipulation target as geometric costs and constraints on the keypoints, which flexibly generalizes existing pose-based manipulation methods. Using this formulation, we factor the manipulation policy into instance segmentation, 3D keypoint detection, optimization-based robot action planning and local dense-geometry-based action execution. This factorization allows us to leverage advances in these sub-problems and combine them into a general and effective perception-to-action manipulation pipeline. Our pipeline is robust to large intra-category shape variation and topology changes as the keypoint representation ignores task-irrelevant geometric details. Extensive hardware experiments demonstrate our method can reliably accomplish tasks with never-before seen objects in a category, such as placing shoes and mugs with significant shape variation into category level target configurations.",
                        "Citation Paper Authors": "Authors:Lucas Manuelli, Wei Gao, Peter Florence, Russ Tedrake"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.05859v4": {
            "Paper Title": "Bootstrap Motion Forecasting With Self-Consistent Constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.04838v5": {
            "Paper Title": "CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with\n  Transformers",
            "Sentences": [
                {
                    "Sentence ID": 33,
                    "Sentence": "as the backbone and MLP-decoder\nwith an embedding dimension of 512unless specified, both in-\ntroduced in SegFormer ",
                    "Citation Text": "E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo,\n\u201cSegFormer: Simple and efficient design for semantic segmentation\nwith transformers,\u201d in NeurIPS , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.15203",
                        "Citation Paper Title": "Title:SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers",
                        "Citation Paper Abstract": "Abstract:We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perception (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5x smaller and 2.2% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code will be released at: this http URL.",
                        "Citation Paper Authors": "Authors:Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": ", which are critical in\ndynamic scenes with motion information such as road-driving\nenvironments ",
                    "Citation Text": "J. Zhang, K. Yang, and R. Stiefelhagen, \u201cISSAFE: Improving semantic\nsegmentation in accidents by fusing event-based data,\u201d in IROS , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.08974",
                        "Citation Paper Title": "Title:ISSAFE: Improving Semantic Segmentation in Accidents by Fusing Event-based Data",
                        "Citation Paper Abstract": "Abstract:Ensuring the safety of all traffic participants is a prerequisite for bringing intelligent vehicles closer to practical applications. The assistance system should not only achieve high accuracy under normal conditions, but obtain robust perception against extreme situations. However, traffic accidents that involve object collisions, deformations, overturns, etc., yet unseen in most training sets, will largely harm the performance of existing semantic segmentation models. To tackle this issue, we present a rarely addressed task regarding semantic segmentation in accidental scenarios, along with an accident dataset DADA-seg. It contains 313 various accident sequences with 40 frames each, of which the time windows are located before and during a traffic accident. Every 11th frame is manually annotated for benchmarking the segmentation performance. Furthermore, we propose a novel event-based multi-modal segmentation architecture ISSAFE. Our experiments indicate that event-based data can provide complementary information to stabilize semantic segmentation under adverse conditions by preserving fine-grain motion of fast-moving foreground (crash objects) in accidents. Our approach achieves +8.2% mIoU performance gain on the proposed evaluation set, exceeding more than 10 state-of-the-art segmentation methods. The proposed ISSAFE architecture is demonstrated to be consistently effective for models learned on multiple source databases including Cityscapes, KITTI-360, BDD and ApolloScape.",
                        "Citation Paper Authors": "Authors:Jiaming Zhang, Kailun Yang, Rainer Stiefelhagen"
                    }
                },
                {
                    "Sentence ID": 94,
                    "Sentence": ".\nMethod Modal Daytime mIoU (%) Nighttime mIoU (%)\nFRRN ",
                    "Citation Text": "T. Pohlen, A. Hermans, M. Mathias, and B. Leibe, \u201cFull-resolution\nresidual networks for semantic segmentation in street scenes,\u201d in CVPR ,\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.08323",
                        "Citation Paper Title": "Title:Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes",
                        "Citation Paper Abstract": "Abstract:Semantic image segmentation is an essential component of modern autonomous driving systems, as an accurate understanding of the surrounding scene is crucial to navigation and action planning. Current state-of-the-art approaches in semantic image segmentation rely on pre-trained networks that were initially developed for classifying images as a whole. While these networks exhibit outstanding recognition performance (i.e., what is visible?), they lack localization accuracy (i.e., where precisely is something located?). Therefore, additional processing steps have to be performed in order to obtain pixel-accurate segmentation masks at the full image resolution. To alleviate this problem we propose a novel ResNet-like architecture that exhibits strong localization and recognition performance. We combine multi-scale context with pixel-level accuracy by using two processing streams within our network: One stream carries information at the full image resolution, enabling precise adherence to segment boundaries. The other stream undergoes a sequence of pooling operations to obtain robust features for recognition. The two streams are coupled at the full image resolution using residuals. Without additional processing steps and without pre-training, our approach achieves an intersection-over-union score of 71.8% on the Cityscapes dataset.",
                        "Citation Paper Authors": "Authors:Tobias Pohlen, Alexander Hermans, Markus Mathias, Bastian Leibe"
                    }
                },
                {
                    "Sentence ID": 91,
                    "Sentence": "RGB-T 96.7 79.4 64.7 52.7 32.9 28.4 0.8 16.9 44.4 46.3\nPSTNet ",
                    "Citation Text": "S. S. Shivakumar, N. Rodrigues, A. Zhou, I. D. Miller, V . Kumar,\nand C. J. Taylor, \u201cPST900: RGB-thermal calibration, dataset and\nsegmentation network,\u201d in ICRA , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.10980",
                        "Citation Paper Title": "Title:PST900: RGB-Thermal Calibration, Dataset and Segmentation Network",
                        "Citation Paper Abstract": "Abstract:In this work we propose long wave infrared (LWIR) imagery as a viable supporting modality for semantic segmentation using learning-based techniques. We first address the problem of RGB-thermal camera calibration by proposing a passive calibration target and procedure that is both portable and easy to use. Second, we present PST900, a dataset of 894 synchronized and calibrated RGB and Thermal image pairs with per pixel human annotations across four distinct classes from the DARPA Subterranean Challenge. Lastly, we propose a CNN architecture for fast semantic segmentation that combines both RGB and Thermal imagery in a way that leverages RGB imagery independently. We compare our method against the state-of-the-art and show that our method outperforms them in our dataset.",
                        "Citation Paper Authors": "Authors:Shreyas S. Shivakumar, Neil Rodrigues, Alex Zhou, Ian D. Miller, Vijay Kumar, Camillo J. Taylor"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": ". In particular, with MiT-B4\nand -B5, CMX elevates the mIoU to >52.0%. CMX is also\nbetter than multi-task methods like PAP ",
                    "Citation Text": "Z. Zhang, Z. Cui, C. Xu, Y . Yan, N. Sebe, and J. Yang, \u201cPattern-\naffinitive propagation across depth, surface normal and semantic seg-\nmentation,\u201d in CVPR , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.03525",
                        "Citation Paper Title": "Title:Pattern-Affinitive Propagation across Depth, Surface Normal and Semantic Segmentation",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose a novel Pattern-Affinitive Propagation (PAP) framework to jointly predict depth, surface normal and semantic segmentation. The motivation behind it comes from the statistic observation that pattern-affinitive pairs recur much frequently across different tasks as well as within a task. Thus, we can conduct two types of propagations, cross-task propagation and task-specific propagation, to adaptively diffuse those similar patterns. The former integrates cross-task affinity patterns to adapt to each task therein through the calculation on non-local relationships. Next the latter performs an iterative diffusion in the feature space so that the cross-task affinity patterns can be widely-spread within the task. Accordingly, the learning of each task can be regularized and boosted by the complementary task-level affinities. Extensive experiments demonstrate the effectiveness and the superiority of our method on the joint three tasks. Meanwhile, we achieve the state-of-the-art or competitive results on the three related datasets, NYUD-v2, SUN-RGBD and KITTI.",
                        "Citation Paper Authors": "Authors:Zhenyu Zhang, Zhen Cui, Chunyan Xu, Yan Yan, Nicu Sebe, Jian Yang"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "has 70496 RGB-D images with\n13object categories. Following the data splitting ",
                    "Citation Text": "J. Cao, H. Leng, D. Lischinski, D. Cohen-Or, C. Tu, and Y . Li,\n\u201cShapeConv: Shape-aware convolutional layer for indoor RGB-D se-\nmantic segmentation,\u201d in ICCV , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.10528",
                        "Citation Paper Title": "Title:ShapeConv: Shape-aware Convolutional Layer for Indoor RGB-D Semantic Segmentation",
                        "Citation Paper Abstract": "Abstract:RGB-D semantic segmentation has attracted increasing attention over the past few years. Existing methods mostly employ homogeneous convolution operators to consume the RGB and depth features, ignoring their intrinsic differences. In fact, the RGB values capture the photometric appearance properties in the projected image space, while the depth feature encodes both the shape of a local geometry as well as the base (whereabout) of it in a larger context. Compared with the base, the shape probably is more inherent and has a stronger connection to the semantics, and thus is more critical for segmentation accuracy. Inspired by this observation, we introduce a Shape-aware Convolutional layer (ShapeConv) for processing the depth feature, where the depth feature is firstly decomposed into a shape-component and a base-component, next two learnable weights are introduced to cooperate with them independently, and finally a convolution is applied on the re-weighted combination of these two components. ShapeConv is model-agnostic and can be easily integrated into most CNNs to replace vanilla convolutional layers for semantic segmentation. Extensive experiments on three challenging indoor RGB-D semantic segmentation benchmarks, i.e., NYU-Dv2(-13,-40), SUN RGB-D, and SID, demonstrate the effectiveness of our ShapeConv when employing it over five popular architectures. Moreover, the performance of CNNs with ShapeConv is boosted without introducing any computation and memory increase in the inference phase. The reason is that the learnt weights for balancing the importance between the shape and base components in ShapeConv become constants in the inference phase, and thus can be fused into the following convolution, resulting in a network that is identical to one with vanilla convolutional layers.",
                        "Citation Paper Authors": "Authors:Jinming Cao, Hanchao Leng, Dani Lischinski, Danny Cohen-Or, Changhe Tu, Yangyan Li"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "RGB-D ResNet-50 82.2\nCMX RGB-D MiT-B2 81.6\nCMX RGB-D MiT-B4 82.6\ndramatically improve the mIoU to 56.3%and56.9%, clearly\nstanding out in front of all state-of-the-art approaches. The best\nCMX model even reaches superior results than recent strong\npretraining-based methods ",
                    "Citation Text": "R. Girdhar, M. Singh, N. Ravi, L. van der Maaten, A. Joulin, and\nI. Misra, \u201cOmnivore: A single model for many visual modalities,\u201d in\nCVPR , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.08377",
                        "Citation Paper Title": "Title:Omnivore: A Single Model for Many Visual Modalities",
                        "Citation Paper Abstract": "Abstract:Prior work has studied different visual modalities in isolation and developed separate architectures for recognition of images, videos, and 3D data. Instead, in this paper, we propose a single model which excels at classifying images, videos, and single-view 3D data using exactly the same model parameters. Our 'Omnivore' model leverages the flexibility of transformer-based architectures and is trained jointly on classification tasks from different modalities. Omnivore is simple to train, uses off-the-shelf standard datasets, and performs at-par or better than modality-specific models of the same size. A single Omnivore model obtains 86.0% on ImageNet, 84.1% on Kinetics, and 67.1% on SUN RGB-D. After finetuning, our models outperform prior work on a variety of vision tasks and generalize across modalities. Omnivore's shared visual representation naturally enables cross-modal recognition without access to correspondences between modalities. We hope our results motivate researchers to model visual modalities together.",
                        "Citation Paper Authors": "Authors:Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens van der Maaten, Armand Joulin, Ishan Misra"
                    }
                },
                {
                    "Sentence ID": 62,
                    "Sentence": ",\nareas of {1,2,3,4,6}are used for training and area 5is for\ntesting. The input image is resized to 480\u00d7480.\nScanNetV2 dataset ",
                    "Citation Text": "A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and\nM. Nie\u00dfner, \u201cScanNet: Richly-annotated 3D reconstructions of indoor\nscenes,\u201d in CVPR , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.04405",
                        "Citation Paper Title": "Title:ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes",
                        "Citation Paper Abstract": "Abstract:A key requirement for leveraging supervised deep learning methods is the availability of large, labeled datasets. Unfortunately, in the context of RGB-D scene understanding, very little data is available -- current datasets cover a small range of scene views and have limited semantic annotations. To address this issue, we introduce ScanNet, an RGB-D video dataset containing 2.5M views in 1513 scenes annotated with 3D camera poses, surface reconstructions, and semantic segmentations. To collect this data, we designed an easy-to-use and scalable RGB-D capture system that includes automated surface reconstruction and crowdsourced semantic annotation. We show that using this data helps achieve state-of-the-art performance on several 3D scene understanding tasks, including 3D object classification, semantic voxel labeling, and CAD model retrieval. The dataset is freely available at this http URL.",
                        "Citation Paper Authors": "Authors:Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, Matthias Nie\u00dfner"
                    }
                },
                {
                    "Sentence ID": 61,
                    "Sentence": ", we randomly crop and resize the input to 480\u00d7480.\nStanford2D3D dataset ",
                    "Citation Text": "I. Armeni, S. Sax, A. R. Zamir, and S. Savarese, \u201cJoint 2D-3D-semantic\ndata for indoor scene understanding,\u201d arXiv preprint arXiv:1702.01105 ,\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.01105",
                        "Citation Paper Title": "Title:Joint 2D-3D-Semantic Data for Indoor Scene Understanding",
                        "Citation Paper Abstract": "Abstract:We present a dataset of large-scale indoor spaces that provides a variety of mutually registered modalities from 2D, 2.5D and 3D domains, with instance-level semantic and geometric annotations. The dataset covers over 6,000m2 and contains over 70,000 RGB images, along with the corresponding depths, surface normals, semantic annotations, global XYZ images (all in forms of both regular and 360\u00b0 equirectangular images) as well as camera information. It also includes registered raw and semantically annotated 3D meshes and point clouds. The dataset enables development of joint and cross-modal learning models and potentially unsupervised approaches utilizing the regularities present in large-scale indoor spaces. The dataset is available here: this http URL",
                        "Citation Paper Authors": "Authors:Iro Armeni, Sasha Sax, Amir R. Zamir, Silvio Savarese"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.05077v5": {
            "Paper Title": "Tensor Train for Global Optimization Problems in Robotics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.14372v2": {
            "Paper Title": "Formalizing and Evaluating Requirements of Perception Systems for\n  Automated Vehicles using Spatio-Temporal Perception Logic",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.00319v2": {
            "Paper Title": "Robust Planning for Multi-stage Forceful Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.13684v3": {
            "Paper Title": "The Surface Edge Explorer (SEE): A measurement-direct approach to next\n  best view planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.13112v3": {
            "Paper Title": "Optimization-based Motion Planning for Autonomous Parking Considering\n  Dynamic Obstacle: A Hierarchical Framework",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.06650v2": {
            "Paper Title": "Interpreting Neural Policies with Disentangled Tree Representations",
            "Sentences": [
                {
                    "Sentence ID": 39,
                    "Sentence": ". Additionally, we include advanced continuous-time baselines designed by ordinary\ndifferential equations such as ODE-RNN ",
                    "Citation Text": "Y . Rubanova, R. T. Chen, and D. K. Duvenaud. Latent ordinary differential equations for\nirregularly-sampled time series. Advances in neural information processing systems , 32, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.03907",
                        "Citation Paper Title": "Title:Latent ODEs for Irregularly-Sampled Time Series",
                        "Citation Paper Abstract": "Abstract:Time series with non-uniform intervals occur in many applications, and are difficult to model using standard recurrent neural networks (RNNs). We generalize RNNs to have continuous-time hidden dynamics defined by ordinary differential equations (ODEs), a model we call ODE-RNNs. Furthermore, we use ODE-RNNs to replace the recognition network of the recently-proposed Latent ODE model. Both ODE-RNNs and Latent ODEs can naturally handle arbitrary time gaps between observations, and can explicitly model the probability of observation times using Poisson processes. We show experimentally that these ODE-based models outperform their RNN-based counterparts on irregularly-sampled data.",
                        "Citation Paper Authors": "Authors:Yulia Rubanova, Ricky T. Q. Chen, David Duvenaud"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ". One line of\nresearch analyzes multi-step trajectories from the perspective of options or compositional skills\n[24, 25, 26, 27, 28, 29]. A more fine-grained single-step alternative is to extract policies via imitation\nlearning to interpretable models like decision tree ",
                    "Citation Text": "O. Bastani, Y . Pu, and A. Solar-Lezama. Verifiable reinforcement learning via policy extrac-\ntion. Advances in neural information processing systems , 31, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.08328",
                        "Citation Paper Title": "Title:Verifiable Reinforcement Learning via Policy Extraction",
                        "Citation Paper Abstract": "Abstract:While deep reinforcement learning has successfully solved many challenging control tasks, its real-world applicability has been limited by the inability to ensure the safety of learned policies. We propose an approach to verifiable reinforcement learning by training decision tree policies, which can represent complex policies (since they are nonparametric), yet can be efficiently verified using existing techniques (since they are highly structured). The challenge is that decision tree policies are difficult to train. We propose VIPER, an algorithm that combines ideas from model compression and imitation learning to learn decision tree policies guided by a DNN policy (called the oracle) and its Q-function, and show that it substantially outperforms two baselines. We use VIPER to (i) learn a provably robust decision tree policy for a variant of Atari Pong with a symbolic state space, (ii) learn a decision tree policy for a toy game based on Pong that provably never loses, and (iii) learn a provably stable decision tree policy for cart-pole. In each case, the decision tree policy achieves performance equal to that of the original DNN policy.",
                        "Citation Paper Authors": "Authors:Osbert Bastani, Yewen Pu, Armando Solar-Lezama"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": ". An active line of research focuses on dissecting and analyzing\ntrained neural networks in a generic yet post-hoc manner [17, 18, 19, 20]. Another active line of\nresearch is to study disentangled explanatory factors in learned representation ",
                    "Citation Text": "F. Locatello, S. Bauer, M. Lucic, G. Raetsch, S. Gelly, B. Sch \u00a8olkopf, and O. Bachem. Chal-\nlenging common assumptions in the unsupervised learning of disentangled representations. In\ninternational conference on machine learning , pages 4114\u20134124. PMLR, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.12359",
                        "Citation Paper Title": "Title:Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations",
                        "Citation Paper Abstract": "Abstract:The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than 12000 models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties ``encouraged'' by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.",
                        "Citation Paper Authors": "Authors:Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar R\u00e4tsch, Sylvain Gelly, Bernhard Sch\u00f6lkopf, Olivier Bachem"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": ". Compact repre-\nsentation of features can be learned using discriminative masking ",
                    "Citation Text": "J. Bu, A. Daw, M. Maruf, and A. Karpatne. Learning compact representations of neural net-\nworks using discriminative masking (dam). Advances in Neural Information Processing Sys-\ntems, 34, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.00684",
                        "Citation Paper Title": "Title:Learning Compact Representations of Neural Networks using DiscriminAtive Masking (DAM)",
                        "Citation Paper Abstract": "Abstract:A central goal in deep learning is to learn compact representations of features at every layer of a neural network, which is useful for both unsupervised representation learning and structured network pruning. While there is a growing body of work in structured pruning, current state-of-the-art methods suffer from two key limitations: (i) instability during training, and (ii) need for an additional step of fine-tuning, which is resource-intensive. At the core of these limitations is the lack of a systematic approach that jointly prunes and refines weights during training in a single stage, and does not require any fine-tuning upon convergence to achieve state-of-the-art performance. We present a novel single-stage structured pruning method termed DiscriminAtive Masking (DAM). The key intuition behind DAM is to discriminatively prefer some of the neurons to be refined during the training process, while gradually masking out other neurons. We show that our proposed DAM approach has remarkably good performance over various applications, including dimensionality reduction, recommendation system, graph representation learning, and structured pruning for image classification. We also theoretically show that the learning objective of DAM is directly related to minimizing the L0 norm of the masking layer.",
                        "Citation Paper Authors": "Authors:Jie Bu, Arka Daw, M. Maruf, Anuj Karpatne"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "Compact neural networks. Compact neural networks are ideal in resource constraints situations\nsuch as robotics and by nature easier to interpret due to a smaller number of neurons [2, 4]. Com-\npact networks can be obtained by pruning ",
                    "Citation Text": "C. Baykal, L. Liebenwein, I. Gilitschenski, D. Feldman, and D. Rus. Sensitivity-informed\nprovable pruning of neural networks. SIAM Journal on Mathematics of Data Science , 4(1):\n26\u201345, 2022. doi:10.1137/20M1383239. URL https://doi.org/10.1137/20M1383239 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.05422",
                        "Citation Paper Title": "Title:SiPPing Neural Networks: Sensitivity-informed Provable Pruning of Neural Networks",
                        "Citation Paper Abstract": "Abstract:We introduce a pruning algorithm that provably sparsifies the parameters of a trained model in a way that approximately preserves the model's predictive accuracy. Our algorithm uses a small batch of input points to construct a data-informed importance sampling distribution over the network's parameters, and adaptively mixes a sampling-based and deterministic pruning procedure to discard redundant weights. Our pruning method is simultaneously computationally efficient, provably accurate, and broadly applicable to various network architectures and data distributions. Our empirical comparisons show that our algorithm reliably generates highly compressed networks that incur minimal loss in performance relative to that of the original network. We present experimental results that demonstrate our algorithm's potential to unearth essential network connections that can be trained successfully in isolation, which may be of independent interest.",
                        "Citation Paper Authors": "Authors:Cenk Baykal, Lucas Liebenwein, Igor Gilitschenski, Dan Feldman, Daniela Rus"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.13422v2": {
            "Paper Title": "Generalized Two Color Map Theorem -- Complete Theorem of Robust Gait\n  Plan for a Tilt-rotor",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.05629v2": {
            "Paper Title": "Leveraging Large (Visual) Language Models for Robot 3D Scene\n  Understanding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.01346v8": {
            "Paper Title": "Guaranteed Conformance of Neurosymbolic Models to Natural Constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.03713v3": {
            "Paper Title": "Integration of Riemannian Motion Policy with Whole-Body Control for\n  Collision-Free Legged Locomotion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.07324v4": {
            "Paper Title": "Example When Local Optimal Policies Contain Unstable Control",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": "analyzes the metrics to define the\nsimilarity of tasks, in particular, the structural conditions that\nallow efficient generalization; Reference ",
                    "Citation Text": "M. Igl, G. Farquhar, J. Luketina, W. Boehmer, and S. Whiteson,\n\u201cTransient non-stationarity and generalisation in deep reinforcement\nlearning,\u201d arXiv preprint arXiv:2006.05826 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.05826",
                        "Citation Paper Title": "Title:Transient Non-Stationarity and Generalisation in Deep Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:Non-stationarity can arise in Reinforcement Learning (RL) even in stationary environments. For example, most RL algorithms collect new data throughout training, using a non-stationary behaviour policy. Due to the transience of this non-stationarity, it is often not explicitly addressed in deep RL and a single neural network is continually updated. However, we find evidence that neural networks exhibit a memory effect where these transient non-stationarities can permanently impact the latent representation and adversely affect generalisation performance. Consequently, to improve generalisation of deep RL agents, we propose Iterated Relearning (ITER). ITER augments standard RL training by repeated knowledge transfer of the current policy into a freshly initialised network, which thereby experiences less non-stationarity during training. Experimentally, we show that ITER improves performance on the challenging generalisation benchmarks ProcGen and Multiroom.",
                        "Citation Paper Authors": "Authors:Maximilian Igl, Gregory Farquhar, Jelena Luketina, Wendelin Boehmer, Shimon Whiteson"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": ".\nEfforts from different research lines try to analyze why RL\nstruggles with generalization. For example, Reference ",
                    "Citation Text": "X. Song, Y . Jiang, S. Tu, Y . Du, and B. Neyshabur, \u201cObservational\noverfitting in reinforcement learning,\u201d arXiv preprint arXiv:1912.02975 ,\n2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.02975",
                        "Citation Paper Title": "Title:Observational Overfitting in Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:A major component of overfitting in model-free reinforcement learning (RL) involves the case where the agent may mistakenly correlate reward with certain spurious features from the observations generated by the Markov Decision Process (MDP). We provide a general framework for analyzing this scenario, which we use to design multiple synthetic benchmarks from only modifying the observation space of an MDP. When an agent overfits to different observation spaces even if the underlying MDP dynamics is fixed, we term this observational overfitting. Our experiments expose intriguing properties especially with regards to implicit regularization, and also corroborate results from previous works in RL generalization and supervised learning (SL).",
                        "Citation Paper Authors": "Authors:Xingyou Song, Yiding Jiang, Stephen Tu, Yilun Du, Behnam Neyshabur"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "separates control\nand planning via hierarchical learning to allow free exploration\nin planning while adding constraints on control for stable and\ncompliant response to contacts.\n3) Generalization of RL: Characterizing generalization is\nan on-going topic in RL ",
                    "Citation Text": "C. Packer, K. Gao, J. Kos, P. Kr \u00a8ahenb \u00a8uhl, V . Koltun, and D. Song, \u201cAs-\nsessing generalization in deep reinforcement learning,\u201d arXiv preprint\narXiv:1810.12282 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.12282",
                        "Citation Paper Title": "Title:Assessing Generalization in Deep Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:Deep reinforcement learning (RL) has achieved breakthrough results on many tasks, but agents often fail to generalize beyond the environment they were trained in. As a result, deep RL algorithms that promote generalization are receiving increasing attention. However, works in this area use a wide variety of tasks and experimental setups for evaluation. The literature lacks a controlled assessment of the merits of different generalization schemes. Our aim is to catalyze community-wide progress on generalization in deep RL. To this end, we present a benchmark and experimental protocol, and conduct a systematic empirical study. Our framework contains a diverse set of environments, our methodology covers both in-distribution and out-of-distribution generalization, and our evaluation includes deep RL algorithms that specifically tackle generalization. Our key finding is that `vanilla' deep RL algorithms generalize better than specialized schemes that were proposed specifically to tackle generalization.",
                        "Citation Paper Authors": "Authors:Charles Packer, Katelyn Gao, Jernej Kos, Philipp Kr\u00e4henb\u00fchl, Vladlen Koltun, Dawn Song"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.02371v2": {
            "Paper Title": "Optical Proximity Sensing for Pose Estimation During In-Hand\n  Manipulation",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": "on the proximity sensing modality for\nrobot hands based on the underlying sensing mechanism:\neither acoustic, capacitive, or optical. While we describe\nmany works here, please see Navarro et al. ",
                    "Citation Text": "S. E. Navarro, S. M \u00a8uhlbacher-Karrer, H. Alagi, H. Zangl, K. Koyama,\nB. Hein, C. Duriez, and J. R. Smith, \u201cProximity perception in human-\ncentered robotics: A survey on sensing systems and applications,\u201d\nIEEE Transactions on Robotics , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.07206",
                        "Citation Paper Title": "Title:Proximity Perception in Human-Centered Robotics: A Survey on Sensing Systems and Applications",
                        "Citation Paper Abstract": "Abstract:Proximity perception is a technology that has the potential to play an essential role in the future of robotics. It can fulfill the promise of safe, robust, and autonomous systems in industry and everyday life, alongside humans, as well as in remote locations in space and underwater. In this survey paper, we cover the developments of this field from the early days up to the present, with a focus on human-centered robotics. Here, proximity sensors are typically deployed in two scenarios: first, on the exterior of manipulator arms to support safety and interaction functionality, and second, on the inside of grippers or hands to support grasping and exploration. Starting from this observation, we propose a categorization for the approaches found in the literature. To provide a basis for understanding these approaches, we devote effort to present the technologies and different measuring principles that were developed over the years, also providing a summary in form of a table. Then, we show the diversity of applications that have been presented in the literature. Finally, we give an overview of the most important trends that will shape the future of this domain.",
                        "Citation Paper Authors": "Authors:Stefan Escaida Navarro, Stephan M\u00fchlbacher-Karrer, Hosam Alagi, Hubert Zangl, Keisuke Koyama, Bj\u00f6rn Hein, Christian Duriez, Joshua R. Smith"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.02460v2": {
            "Paper Title": "Electrostatic Brakes Enable Individual Joint Control of Underactuated,\n  Highly Articulated Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.03444v2": {
            "Paper Title": "Fairness and Bias in Robot Learning",
            "Sentences": [
                {
                    "Sentence ID": 214,
                    "Sentence": "G. Marchant, \u201cThe growing gap between emerging technologies and\nlegal-ethical oversight,\u201d The Int. Library of Ethics Law and Technology ,\nvol. 7, pp. 35\u201344, 2011. ",
                    "Citation Text": "M. Finck and A. Biega, \u201cReviving purpose limitation and data minimi-\nsation in personalisation, profiling and decision-making systems,\u201d in\nReviving Purpose Limitation and Data Minimisation in Personalisation,\nProfiling and Decision-Making Systems: Finck, Mich `ele\u2014 uBiega, Asia ,\n2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.06203",
                        "Citation Paper Title": "Title:Reviving Purpose Limitation and Data Minimisation in Data-Driven Systems",
                        "Citation Paper Abstract": "Abstract:This paper determines whether the two core data protection principles of data minimisation and purpose limitation can be meaningfully implemented in data-driven systems. While contemporary data processing practices appear to stand at odds with these principles, we demonstrate that systems could technically use much less data than they currently do. This observation is a starting point for our detailed techno-legal analysis uncovering obstacles that stand in the way of meaningful implementation and compliance as well as exemplifying unexpected trade-offs which emerge where data protection law is applied in practice. Our analysis seeks to inform debates about the impact of data protection on the development of artificial intelligence in the European Union, offering practical action points for data controllers, regulators, and researchers.",
                        "Citation Paper Authors": "Authors:Asia J. Biega, Mich\u00e8le Finck"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.09378v2": {
            "Paper Title": "Outracing Human Racers with Model-based Planning and Control for\n  Time-trial Racing",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": "proposed an approximated\ngeometry expression of path curvature, which was used as\nthe objective function of a nonlinear programming problem.\nKapania et al. ",
                    "Citation Text": "N. R. Kapania, J. Subosits, and J. Christian Gerdes, \u201cA sequential two-\nstep algorithm for fast generation of vehicle racing trajectories,\u201d Journal\nof Dynamic Systems, Measurement, and Control , vol. 138, no. 9, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.00606",
                        "Citation Paper Title": "Title:A Sequential Two-Step Algorithm for Fast Generation of Vehicle Racing Trajectories",
                        "Citation Paper Abstract": "Abstract:The problem of maneuvering a vehicle through a race course in minimum time requires computation of both longitudinal (brake and throttle) and lateral (steering wheel) control inputs. Unfortunately, solving the resulting nonlinear optimal control problem is typically computationally expensive and infeasible for real-time trajectory planning. This paper presents an iterative algorithm that divides the path generation task into two sequential subproblems that are significantly easier to solve. Given an initial path through the race track, the algorithm runs a forward-backward integration scheme to determine the minimum-time longitudinal speed profile, subject to tire friction constraints. With this fixed speed profile, the algorithm updates the vehicle's path by solving a convex optimization problem that minimizes the resulting path curvature while staying within track boundaries and obeying affine, time-varying vehicle dynamics constraints. This two-step process is repeated iteratively until the predicted lap time no longer improves. While providing no guarantees of convergence or a globally optimal solution, the approach performs very well when validated on the Thunderhill Raceway course in Willows, CA. The predicted lap time converges after four to five iterations, with each iteration over the full 4.5 km race course requiring only thirty seconds of computation time on a laptop computer. The resulting trajectory is experimentally driven at the race circuit with an autonomous Audi TTS test vehicle, and the resulting lap time and racing line is comparable to both a nonlinear gradient descent solution and a trajectory recorded from a professional racecar driver. The experimental results indicate that the proposed method is a viable option for online trajectory planning in the near future.",
                        "Citation Paper Authors": "Authors:Nitin R. Kapania, John Subosits, J Christian Gerdes"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.11092v2": {
            "Paper Title": "INVIGORATE: Interactive Visual Grounding and Grasping in Clutter",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.07101v3": {
            "Paper Title": "Delay-aware Robust Control for Safe Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.10493v3": {
            "Paper Title": "Benchmarking Augmentation Methods for Learning Robust Navigation Agents:\n  the Winning Entry of the 2021 iGibson Challenge",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": "A. PointGoal Navigation\nPointGoal Navigation (PointNav), as formally defined by\nAnderson et al. in ",
                    "Citation Text": "P. Anderson, A. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta,\nV . Koltun, J. Kosecka, J. Malik, R. Mottaghi, M. Savva et al. ,\n\u201cOn evaluation of embodied navigation agents,\u201d arXiv preprint\narXiv:1807.06757 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.06757",
                        "Citation Paper Title": "Title:On Evaluation of Embodied Navigation Agents",
                        "Citation Paper Abstract": "Abstract:Skillful mobile operation in three-dimensional environments is a primary topic of study in Artificial Intelligence. The past two years have seen a surge of creative work on navigation. This creative output has produced a plethora of sometimes incompatible task definitions and evaluation protocols. To coordinate ongoing and future research in this area, we have convened a working group to study empirical methodology in navigation research. The present document summarizes the consensus recommendations of this working group. We discuss different problem statements and the role of generalization, present evaluation measures, and provide standard scenarios that can be used for benchmarking.",
                        "Citation Paper Authors": "Authors:Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, Amir R. Zamir"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "incorporate visual priors about the world to confer\nperformance gains over training end-to-end from scratch\nwithout priors. Chaplot et al. ",
                    "Citation Text": "D. S. Chaplot, D. Gandhi, S. Gupta, A. Gupta, and R. Salakhutdi-\nnov, \u201cLearning to explore using active neural slam,\u201d arXiv preprint\narXiv:2004.05155 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.05155",
                        "Citation Paper Title": "Title:Learning to Explore using Active Neural SLAM",
                        "Citation Paper Abstract": "Abstract:This work presents a modular and hierarchical approach to learn policies for exploring 3D environments, called `Active Neural SLAM'. Our approach leverages the strengths of both classical and learning-based methods, by using analytical path planners with learned SLAM module, and global and local policies. The use of learning provides flexibility with respect to input modalities (in the SLAM module), leverages structural regularities of the world (in global policies), and provides robustness to errors in state estimation (in local policies). Such use of learning within each module retains its benefits, while at the same time, hierarchical decomposition and modular training allow us to sidestep the high sample complexities associated with training end-to-end policies. Our experiments in visually and physically realistic simulated 3D environments demonstrate the effectiveness of our approach over past learning and geometry-based approaches. The proposed model can also be easily transferred to the PointGoal task and was the winning entry of the CVPR 2019 Habitat PointGoal Navigation Challenge.",
                        "Citation Paper Authors": "Authors:Devendra Singh Chaplot, Dhiraj Gandhi, Saurabh Gupta, Abhinav Gupta, Ruslan Salakhutdinov"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.08022v2": {
            "Paper Title": "Success Weighted by Completion Time: A Dynamics-Aware Evaluation\n  Criteria for Embodied Navigation",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": "to\nmodel the policy of each of our agents. It has two main\ncomponents; a visual encoder and a policy. The visual\nencoder is a convolutional neural network based on ResNet-\n50 ",
                    "Citation Text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep\nresidual learning for image recognition. CoRR , abs/1512.03385, 2015.\n5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.03385",
                        "Citation Paper Title": "Title:Deep Residual Learning for Image Recognition",
                        "Citation Paper Abstract": "Abstract:Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
                        "Citation Paper Authors": "Authors:Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": ", comprised of 86 high quality 3D scans and\nmeshes curated from the full Gibson dataset ",
                    "Citation Text": "Fei Xia, Amir R. Zamir, Zhiyang He, Alexander Sax, Jitendra Malik,\nand Silvio Savarese. Gibson env: Real-world perception for embodied\nagents. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) , June 2018. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.10654",
                        "Citation Paper Title": "Title:Gibson Env: Real-World Perception for Embodied Agents",
                        "Citation Paper Abstract": "Abstract:Developing visual perception models for active agents and sensorimotor control are cumbersome to be done in the physical world, as existing algorithms are too slow to efficiently learn in real-time and robots are fragile and costly. This has given rise to learning-in-simulation which consequently casts a question on whether the results transfer to real-world. In this paper, we are concerned with the problem of developing real-world perception for active agents, propose Gibson Virtual Environment for this purpose, and showcase sample perceptual tasks learned therein. Gibson is based on virtualizing real spaces, rather than using artificially designed ones, and currently includes over 1400 floor spaces from 572 full buildings. The main characteristics of Gibson are: I. being from the real-world and reflecting its semantic complexity, II. having an internal synthesis mechanism, \"Goggles\", enabling deploying the trained models in real-world without needing further domain adaptation, III. embodiment of agents and making them subject to constraints of physics and space.",
                        "Citation Paper Authors": "Authors:Fei Xia, Amir Zamir, Zhi-Yang He, Alexander Sax, Jitendra Malik, Silvio Savarese"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": ". In this\nwork, we show why SPL is not optimal for evaluating agents\nwith more complex dynamics, and propose a new metric to\nevaluate the navigation performance for such agents.\nEvaluation Metrics. The metric proposed by ",
                    "Citation Text": "Changan Chen, Ziad Al-Halah, and Kristen Grauman. Semantic audio-\nvisual navigation, 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.11583",
                        "Citation Paper Title": "Title:Semantic Audio-Visual Navigation",
                        "Citation Paper Abstract": "Abstract:Recent work on audio-visual navigation assumes a constantly-sounding target and restricts the role of audio to signaling the target's position. We introduce semantic audio-visual navigation, where objects in the environment make sounds consistent with their semantic meaning (e.g., toilet flushing, door creaking) and acoustic events are sporadic or short in duration. We propose a transformer-based model to tackle this new semantic AudioGoal task, incorporating an inferred goal descriptor that captures both spatial and semantic properties of the target. Our model's persistent multimodal memory enables it to reach the goal even long after the acoustic event stops. In support of the new task, we also expand the SoundSpaces audio simulations to provide semantically grounded sounds for an array of objects in Matterport3D. Our method strongly outperforms existing audio-visual navigation methods by learning to associate semantic, acoustic, and visual cues.",
                        "Citation Paper Authors": "Authors:Changan Chen, Ziad Al-Halah, Kristen Grauman"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.05859v5": {
            "Paper Title": "Rapid Exploration for Open-World Navigation with Latent Goal Models",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": ", which serves a\ndual purpose: first, it provides a representation that can aid the generalization capabilities of RL\nalgorithms [40, 41], and second, it serves as a measure of task-relevant uncertainty ",
                    "Citation Text": "A. A. Alemi, I. Fischer, J. V . Dillon, and K. Murphy, \u201cDeep variational information bottleneck,\u201d\narXiv preprint arXiv:1612.00410 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.00410",
                        "Citation Paper Title": "Title:Deep Variational Information Bottleneck",
                        "Citation Paper Abstract": "Abstract:We present a variational approximation to the information bottleneck of Tishby et al. (1999). This variational approach allows us to parameterize the information bottleneck model using a neural network and leverage the reparameterization trick for efficient training. We call this method \"Deep Variational Information Bottleneck\", or Deep VIB. We show that models trained with the VIB objective outperform those that are trained with other forms of regularization, in terms of generalization performance and robustness to adversarial attack.",
                        "Citation Paper Authors": "Authors:Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, Kevin Murphy"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.03893v4": {
            "Paper Title": "Multimodal Fusion of EMG and Vision for Human Grasp Intent Inference in\n  Prosthetic Hand Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.08086v2": {
            "Paper Title": "An Information-state based Approach to the Optimal Output Feedback\n  Control of Nonlinear Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.01919v2": {
            "Paper Title": "Discovering and Exploiting Sparse Rewards in a Learned Behavior Space",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.08091v5": {
            "Paper Title": "Joint State and Input Estimation of Agent Based on Recursive Kalman\n  Filter Given Prior Knowledge",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.11941v2": {
            "Paper Title": "State Augmented Constrained Reinforcement Learning: Overcoming the\n  Limitations of Learning with Rewards",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ". These methods are based on the use\nof Lagrange multipliers as weights. Different primal steps can\nbe used, with policy gradient ",
                    "Citation Text": "C. Tessler, D. J. Mankowitz, and S. Mannor, \u201cReward constrained policy\noptimization,\u201d arXiv preprint arXiv:1805.11074 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.11074",
                        "Citation Paper Title": "Title:Reward Constrained Policy Optimization",
                        "Citation Paper Abstract": "Abstract:Solving tasks in Reinforcement Learning is no easy feat. As the goal of the agent is to maximize the accumulated reward, it often learns to exploit loopholes and misspecifications in the reward signal resulting in unwanted behavior. While constraints may solve this issue, there is no closed form solution for general constraints. In this work we present a novel multi-timescale approach for constrained policy optimization, called `Reward Constrained Policy Optimization' (RCPO), which uses an alternative penalty signal to guide the policy towards a constraint satisfying one. We prove the convergence of our approach and provide empirical evidence of its ability to train constraint satisfying policies.",
                        "Citation Paper Authors": "Authors:Chen Tessler, Daniel J. Mankowitz, Shie Mannor"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.06038v2": {
            "Paper Title": "Co-GAIL: Learning Diverse Strategies for Human-Robot Collaboration",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": "introduces a method to learn diverse and plausible assistive dressing behavior by\nhand-coding human motor capabilities in the model. In contrast, our method does not assume ex-\nplicit domain knowledge and learns collaborative policies from demonstration data. Wang et al. ",
                    "Citation Text": "Z. Wang, J. Merel, S. Reed, G. Wayne, N. de Freitas, and N. Heess. Robust imitation of\ndiverse behaviors. In Proceedings of the 31st International Conference on Neural Information\nProcessing Systems , pages 5326\u20135335, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.02747",
                        "Citation Paper Title": "Title:Robust Imitation of Diverse Behaviors",
                        "Citation Paper Abstract": "Abstract:Deep generative models have recently shown great promise in imitation learning for motor control. Given enough data, even supervised approaches can do one-shot imitation learning; however, they are vulnerable to cascading failures when the agent trajectory diverges from the demonstrations. Compared to purely supervised methods, Generative Adversarial Imitation Learning (GAIL) can learn more robust controllers from fewer demonstrations, but is inherently mode-seeking and more difficult to train. In this paper, we show how to combine the favourable aspects of these two approaches. The base of our model is a new type of variational autoencoder on demonstration trajectories that learns semantic policy embeddings. We show that these embeddings can be learned on a 9 DoF Jaco robot arm in reaching tasks, and then smoothly interpolated with a resulting smooth interpolation of reaching behavior. Leveraging these policy representations, we develop a new version of GAIL that (1) is much more robust than the purely-supervised controller, especially with few demonstrations, and (2) avoids mode collapse, capturing many diverse behaviors when GAIL on its own does not. We demonstrate our approach on learning diverse gaits from demonstration on a 2D biped and a 62 DoF 3D humanoid in the MuJoCo physics environment.",
                        "Citation Paper Authors": "Authors:Ziyu Wang, Josh Merel, Scott Reed, Greg Wayne, Nando de Freitas, Nicolas Heess"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "the robot learns to influence the collaborator\u2019s action by optimizing a long-term reward\nover repeated interactions and estimating their policy per interaction episode through a learned latent\nspace. Hand-engineered collaborative rewards can cause the overfitting of the learned robot policy ",
                    "Citation Text": "M. Lanctot, V . Zambaldi, A. Gruslys, A. Lazaridou, K. Tuyls, J. P \u00b4erolat, D. Silver, and T. Grae-\npel. A unified game-theoretic approach to multiagent reinforcement learning. arXiv preprint\narXiv:1711.00832 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.00832",
                        "Citation Paper Title": "Title:A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:To achieve general intelligence, agents must learn how to interact with others in a shared environment: this is the challenge of multiagent reinforcement learning (MARL). The simplest form is independent reinforcement learning (InRL), where each agent treats its experience as part of its (non-stationary) environment. In this paper, we first observe that policies learned using InRL can overfit to the other agents' policies during training, failing to sufficiently generalize during execution. We introduce a new metric, joint-policy correlation, to quantify this effect. We describe an algorithm for general MARL, based on approximate best responses to mixtures of policies generated using deep reinforcement learning, and empirical game-theoretic analysis to compute meta-strategies for policy selection. The algorithm generalizes previous ones such as InRL, iterated best response, double oracle, and fictitious play. Then, we present a scalable implementation which reduces the memory requirement using decoupled meta-solvers. Finally, we demonstrate the generality of the resulting policies in two partially observable settings: gridworld coordination games and poker.",
                        "Citation Paper Authors": "Authors:Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Perolat, David Silver, Thore Graepel"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "use multi-agent reinforcement learning to model both agents\nand demonstrate how collaborative behavior can arise with the appropriate choice of reward struc-\nture. In ",
                    "Citation Text": "A. Xie, D. P. Losey, R. Tolsma, C. Finn, and D. Sadigh. Learning latent representations to in-\nfluence multi-agent interaction. Proceedings of the 4th Conference on Robot Learning (CoRL) ,\n2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.06619",
                        "Citation Paper Title": "Title:Learning Latent Representations to Influence Multi-Agent Interaction",
                        "Citation Paper Abstract": "Abstract:Seamlessly interacting with humans or robots is hard because these agents are non-stationary. They update their policy in response to the ego agent's behavior, and the ego agent must anticipate these changes to co-adapt. Inspired by humans, we recognize that robots do not need to explicitly model every low-level action another agent will make; instead, we can capture the latent strategy of other agents through high-level representations. We propose a reinforcement learning-based framework for learning latent representations of an agent's policy, where the ego agent identifies the relationship between its behavior and the other agent's future strategy. The ego agent then leverages these latent dynamics to influence the other agent, purposely guiding them towards policies suitable for co-adaptation. Across several simulated domains and a real-world air hockey game, our approach outperforms the alternatives and learns to influence the other agent.",
                        "Citation Paper Authors": "Authors:Annie Xie, Dylan P. Losey, Ryan Tolsma, Chelsea Finn, Dorsa Sadigh"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.10310v2": {
            "Paper Title": "Verifying Safe Transitions between Dynamic Motion Primitives on Legged\n  Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.03816v2": {
            "Paper Title": "A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in\n  Row-Based Crops",
            "Sentences": [
                {
                    "Sentence ID": 52,
                    "Sentence": ". Moreover, we pre-trained the overall segmentation net-\nwork with Cityscapes ",
                    "Citation Text": "M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Be-\nnenson, U. Franke, S. Roth, and B. Schiele, \u201cThe cityscapes dataset\nfor semantic urban scene understanding,\u201d in Proceedings of the IEEE\nconference on computer vision and pattern recognition , 2016, pp. 3213\u2013\n3223.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1604.01685",
                        "Citation Paper Title": "Title:The Cityscapes Dataset for Semantic Urban Scene Understanding",
                        "Citation Paper Abstract": "Abstract:Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes.\nTo address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations; 20000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.",
                        "Citation Paper Authors": "Authors:Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": ", followed by a reduced version of the\nAtrous Spatial Pyramid Pooling module, ",
                    "Citation Text": "L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, \u201cRethinking\natrous convolution for semantic image segmentation,\u201d arXiv preprint\narXiv:1706.05587 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.05587",
                        "Citation Paper Title": "Title:Rethinking Atrous Convolution for Semantic Image Segmentation",
                        "Citation Paper Abstract": "Abstract:In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.",
                        "Citation Paper Authors": "Authors:Liang-Chieh Chen, George Papandreou, Florian Schroff, Hartwig Adam"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.02398v4": {
            "Paper Title": "Correct-by-construction reach-avoid control of partially observable\n  linear stochastic systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.08967v2": {
            "Paper Title": "Multi-task UNet architecture for end-to-end autonomous driving",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "and set to zero, respectively.\nThe lateral indicators \u03b8and\u2206are crucial measures to\nquantify and interpret dynamic effects of CNN\u2019s perception\non path planning in autonomous driving. MTUC with MTRe-\nsUNet performs better than that of ",
                    "Citation Text": "Cudrano, P., et al.: Advances in centerline estimation for autonomous\nlateral control. In: IEEE Intelligent Vehicles Symposium (IV), pp. 1415-\n1422 (2020)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.12685",
                        "Citation Paper Title": "Title:Advances in centerline estimation for autonomous lateral control",
                        "Citation Paper Abstract": "Abstract:The ability of autonomous vehicles to maintain an accurate trajectory within their road lane is crucial for safe operation. This requires detecting the road lines and estimating the car relative pose within its lane. Lateral lines are usually retrieved from camera images. Still, most of the works on line detection are limited to image mask retrieval and do not provide a usable representation in world coordinates. What we propose in this paper is a complete perception pipeline based on monocular vision and able to retrieve all the information required by a vehicle lateral control system: road lines equation, centerline, vehicle heading and lateral displacement. We evaluate our system by acquiring data with accurate geometric ground truth. To act as a benchmark for further research, we make this new dataset publicly available at this http URL.",
                        "Citation Paper Authors": "Authors:Paolo Cudrano, Simone Mentasti, Matteo Matteucci, Mattia Bersani, Stefano Arrigoni, Federico Cheli"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": ") pixels in a batchof ground-truth images, respectively, and \u03c3is the sigmoid\nfunction.\nThe segmentation subnet can thus generate a sequence of\ngray images of predicted lane lines with estimated lane width\nand lane centerline ",
                    "Citation Text": "Lee, D.-H., Liu, J.-L.: End-to-end deep learning of lane detection and\npath prediction for real-time autonomous driving. Signal, Image and\nVideo Processing 17, 199\u2013205 (2023)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.04738",
                        "Citation Paper Title": "Title:End-to-End Deep Learning of Lane Detection and Path Prediction for Real-Time Autonomous Driving",
                        "Citation Paper Abstract": "Abstract:Inspired by the UNet architecture of semantic image segmentation, we propose a lightweight UNet using depthwise separable convolutions (DSUNet) for end-to-end learning of lane detection and path prediction (PP) in autonomous driving. We also design and integrate a PP algorithm with convolutional neural network (CNN) to form a simulation model (CNN-PP) that can be used to assess CNN's performance qualitatively, quantitatively, and dynamically in a host agent car driving along with other agents all in a real-time autonomous manner. DSUNet is 5.16x lighter in model size and 1.61x faster in inference than UNet. DSUNet-PP outperforms UNet-PP in mean average errors of predicted curvature and lateral offset for path planning in dynamic simulation. DSUNet-PP outperforms a modified UNet in lateral error, which is tested in a real car on real road. These results show that DSUNet is efficient and effective for lane detection and path prediction in autonomous driving.",
                        "Citation Paper Authors": "Authors:Der-Hau Lee, Jinn-Liang Liu"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": ". Moreover, the\ncombination of regression and classification losses yields a\nmulti-modal loss that can avoid mode collapse problems when\neach individual task is trained alone ",
                    "Citation Text": "Xu, H., et al.: End-to-end learning of driving models from large-scale\nvideo datasets. In: Proceedings of IEEE conference on computer vision\nand pattern recognition, pp. 2174-2182 (2017)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.01079",
                        "Citation Paper Title": "Title:End-to-end Learning of Driving Models from Large-scale Video Datasets",
                        "Citation Paper Abstract": "Abstract:Robust perception-action models should be learned from training data with diverse visual appearances and realistic behaviors, yet current approaches to deep visuomotor policy learning have been generally limited to in-situ models learned from a single vehicle or a simulation environment. We advocate learning a generic vehicle motion model from large scale crowd-sourced video data, and develop an end-to-end trainable architecture for learning to predict a distribution over future vehicle egomotion from instantaneous monocular camera observations and previous vehicle state. Our model incorporates a novel FCN-LSTM architecture, which can be learned from large-scale crowd-sourced vehicle action data, and leverages available scene segmentation side tasks to improve performance under a privileged learning paradigm.",
                        "Citation Paper Authors": "Authors:Huazhe Xu, Yang Gao, Fisher Yu, Trevor Darrell"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.15169v2": {
            "Paper Title": "Multimotion Visual Odometry (MVO)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.06511v2": {
            "Paper Title": "Geometric analysis of gaits and optimal control for three-link kinematic\n  swimmers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.00852v2": {
            "Paper Title": "CrossMap Transformer: A Crossmodal Masked Path Transformer Using Double\n  Back-Translation for Vision-and-Language Navigation",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": "which limits\nthe modeling between the instruction and physical world.\nHence, many studies ",
                    "Citation Text": "D. Fried, R. Hu, V . Cirik, A. Rohrbach, J. Andreas, L.-P. Morency,\nT. Berg-Kirkpatrick, K. Saenko, D. Klein, and T. Darrell, \u201cSpeaker-\nfollower models for vision-and-language navigation,\u201d in Advances in\nNeural Information Processing Systems , 2018, pp. 3314\u20133325.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.02724",
                        "Citation Paper Title": "Title:Speaker-Follower Models for Vision-and-Language Navigation",
                        "Citation Paper Abstract": "Abstract:Navigation guided by natural language instructions presents a challenging reasoning problem for instruction followers. Natural language instructions typically identify only a few high-level decisions and landmarks rather than complete low-level motor behaviors; much of the missing information must be inferred based on perceptual context. In machine learning settings, this is doubly challenging: it is difficult to collect enough annotated data to enable learning of this reasoning process from scratch, and also difficult to implement the reasoning process using generic sequence models. Here we describe an approach to vision-and-language navigation that addresses both these issues with an embedded speaker model. We use this speaker model to (1) synthesize new instructions for data augmentation and to (2) implement pragmatic reasoning, which evaluates how well candidate action sequences explain an instruction. Both steps are supported by a panoramic action space that reflects the granularity of human-generated instructions. Experiments show that all three components of this approach---speaker-driven data augmentation, pragmatic reasoning and panoramic action space---dramatically improve the performance of a baseline instruction follower, more than doubling the success rate over the best existing approach on a standard benchmark.",
                        "Citation Paper Authors": "Authors:Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, Trevor Darrell"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ". In this line of works, each predicted\ntoken attends only tokens at previous positions. Nonetheless\nvery few transformer architectures are optimized for the VLN\ntask. Yet transformers can be trained faster than LSTMs ",
                    "Citation Text": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in\nAdvances in neural information processing systems , 2017, pp. 5998\u2013\n6008.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": ", achieving state-of-the-art results. Text generation\nhas also been addressed using transformers ",
                    "Citation Text": "L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y . Wang, J. Gao,\nM. Zhou, and H.-W. Hon, \u201cUnified language model pre-training for\nnatural language understanding and generation,\u201d in Advances in Neural\nInformation Processing Systems , 2019, pp. 13 063\u201313 075.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.03197",
                        "Citation Paper Title": "Title:Unified Language Model Pre-training for Natural Language Understanding and Generation",
                        "Citation Paper Abstract": "Abstract:This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, Hsiao-Wuen Hon"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.15714v3": {
            "Paper Title": "Set-Valued Rigid Body Dynamics for Simultaneous, Inelastic, Frictional\n  Impacts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.08199v2": {
            "Paper Title": "On Multi-objective Policy Optimization as a Tool for Reinforcement\n  Learning: Case Studies in Offline RL and Finetuning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.03722v2": {
            "Paper Title": "Full-Body Torque-Level Non-linear Model Predictive Control for Aerial\n  Manipulation",
            "Sentences": [
                {
                    "Sentence ID": 40,
                    "Sentence": "T. A. Howell, B. E. Jackson, and Z. Manchester, \u201cALTRO: A Fast Solver\nfor Constrained Trajectory Optimization,\u201d in IEEE/RSJ Int. Conf. Intell.\nRob. Sys. (IROS) , 2019. ",
                    "Citation Text": "C. Mastalli, W. Merkt, J. Marti-Saumell, J. Sol `a, N. Mansard, and\nS. Vijayakumar, \u201cA Direct-Indirect Hybridization Approach to Control-\nLimited DDP,\u201d CoRR , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.00411",
                        "Citation Paper Title": "Title:A Feasibility-Driven Approach to Control-Limited DDP",
                        "Citation Paper Abstract": "Abstract:Differential dynamic programming (DDP) is a direct single shooting method for trajectory optimization. Its efficiency derives from the exploitation of temporal structure (inherent to optimal control problems) and explicit roll-out/integration of the system dynamics. However, it suffers from numerical instability and, when compared to direct multiple shooting methods, it has limited initialization options (allows initialization of controls, but not of states) and lacks proper handling of control constraints. In this work, we tackle these issues with a feasibility-driven approach that regulates the dynamic feasibility during the numerical optimization and ensures control limits. Our feasibility search emulates the numerical resolution of a direct multiple shooting problem with only dynamics constraints. We show that our approach (named BOX-FDDP) has better numerical convergence than BOX-DDP+ (a single shooting method), and that its convergence rate and runtime performance are competitive with state-of-the-art direct transcription formulations solved using the interior point and active set algorithms available in KNITRO. We further show that BOX-FDDP decreases the dynamic feasibility error monotonically--as in state-of-the-art nonlinear programming algorithms. We demonstrate the benefits of our approach by generating complex and athletic motions for quadruped and humanoid robots. Finally, we highlight that BOX-FDDP is suitable for model predictive control in legged robots.",
                        "Citation Paper Authors": "Authors:Carlos Mastalli, Wolfgang Merkt, Josep Marti-Saumell, Henrique Ferrolho, Joan Sola, Nicolas Mansard, Sethu Vijayakumar"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.13008v3": {
            "Paper Title": "Imitation Learning with Human Eye Gaze via Multi-Objective Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.11467v3": {
            "Paper Title": "Spatio-Temporal Lattice Planning Using Optimal Motion Primitives",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.17001v4": {
            "Paper Title": "Near-field Perception for Low-Speed Vehicle Automation using\n  Surround-view Fisheye Cameras",
            "Sentences": [
                {
                    "Sentence ID": 137,
                    "Sentence": ", where they argue that taking\nthis approach results in a system capable of redeployment into\nnew scenes without fine-tuning or retraining. That said, in\nfuture work, we plan to explore inclusion of relocalization\nand reorganization in neural network frameworks ",
                    "Citation Text": "Y . Cui, R. Chen, W. Chu, L. Chen, D. Tian, Y . Li, and D. Cao, \u201cDeep\nlearning for image and point cloud fusion in autonomous driving: A\nreview,\u201d IEEE Transactions on Intelligent Transportation Systems , pp.\n1\u201318, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.05224",
                        "Citation Paper Title": "Title:Deep Learning for Image and Point Cloud Fusion in Autonomous Driving: A Review",
                        "Citation Paper Abstract": "Abstract:Autonomous vehicles were experiencing rapid development in the past few years. However, achieving full autonomy is not a trivial task, due to the nature of the complex and dynamic driving environment. Therefore, autonomous vehicles are equipped with a suite of different sensors to ensure robust, accurate environmental perception. In particular, the camera-LiDAR fusion is becoming an emerging research theme. However, so far there has been no critical review that focuses on deep-learning-based camera-LiDAR fusion methods. To bridge this gap and motivate future research, this paper devotes to review recent deep-learning-based data fusion approaches that leverage both image and point cloud. This review gives a brief overview of deep learning on image and point cloud data processing. Followed by in-depth reviews of camera-LiDAR fusion methods in depth completion, object detection, semantic segmentation, tracking and online cross-sensor calibration, which are organized based on their respective fusion levels. Furthermore, we compare these methods on publicly available datasets. Finally, we identified gaps and over-looked challenges between current academic researches and real-world applications. Based on these observations, we provide our insights and point out promising research directions.",
                        "Citation Paper Authors": "Authors:Yaodong Cui, Ren Chen, Wenbo Chu, Long Chen, Daxin Tian, Ying Li, Dongpu Cao"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "information on pertinent low-\nspeed, near-field sensing use cases, we give an overview of\nfisheye cameras, and we provide a brief overview of the Wood-\nScape dataset ",
                    "Citation Text": "S. Yogamani, C. Hughes, J. Horgan, G. Sistu, P. Varley, D. O\u2019Dea,\nM. U \u02c7ri\u02c7c\u00b4a\u02c7r, S. Milz, M. Simon, K. Amende et al. , \u201cWoodScape: A\nmulti-task, multi-camera fisheye dataset for autonomous driving,\u201d in\nProceedings of the IEEE/CVF International Conference on Computer\nVision (ICCV) , 2019, pp. 9308\u20139318.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.01489",
                        "Citation Paper Title": "Title:WoodScape: A multi-task, multi-camera fisheye dataset for autonomous driving",
                        "Citation Paper Abstract": "Abstract:Fisheye cameras are commonly employed for obtaining a large field of view in surveillance, augmented reality and in particular automotive applications. In spite of their prevalence, there are few public datasets for detailed evaluation of computer vision algorithms on fisheye images. We release the first extensive fisheye automotive dataset, WoodScape, named after Robert Wood who invented the fisheye camera in 1906. WoodScape comprises of four surround view cameras and nine tasks including segmentation, depth estimation, 3D bounding box detection and soiling detection. Semantic annotation of 40 classes at the instance level is provided for over 10,000 images and annotation for other tasks are provided for over 100,000 images. With WoodScape, we would like to encourage the community to adapt computer vision models for fisheye camera instead of using naive rectification.",
                        "Citation Paper Authors": "Authors:Senthil Yogamani, Ciaran Hughes, Jonathan Horgan, Ganesh Sistu, Padraig Varley, Derek O'Dea, Michal Uricar, Stefan Milz, Martin Simon, Karl Amende, Christian Witt, Hazem Rashed, Sumanth Chennupati, Sanjaya Nayak, Saquib Mansoor, Xavier Perroton, Patrick Perez"
                    }
                },
                {
                    "Sentence ID": 123,
                    "Sentence": "640 x 192 0.141 1.029 5.350 0.216 0.816 0.941 0.976\nMonodepth2 ",
                    "Citation Text": "C. Godard, O. Mac Aodha, M. Firman, and G. J. Brostow, \u201cDigging\ninto self-supervised monocular depth estimation,\u201d in Proceedings of\nthe IEEE/CVF International Conference on Computer Vision (ICCV) ,\n2019, pp. 3828\u20133838.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.01260",
                        "Citation Paper Title": "Title:Digging Into Self-Supervised Monocular Depth Estimation",
                        "Citation Paper Abstract": "Abstract:Per-pixel ground-truth depth data is challenging to acquire at scale. To overcome this limitation, self-supervised learning has emerged as a promising alternative for training models to perform monocular depth estimation. In this paper, we propose a set of improvements, which together result in both quantitatively and qualitatively improved depth maps compared to competing self-supervised methods.\nResearch on self-supervised monocular training usually explores increasingly complex architectures, loss functions, and image formation models, all of which have recently helped to close the gap with fully-supervised methods. We show that a surprisingly simple model, and associated design choices, lead to superior predictions. In particular, we propose (i) a minimum reprojection loss, designed to robustly handle occlusions, (ii) a full-resolution multi-scale sampling method that reduces visual artifacts, and (iii) an auto-masking loss to ignore training pixels that violate camera motion assumptions. We demonstrate the effectiveness of each component in isolation, and show high quality, state-of-the-art results on the KITTI benchmark.",
                        "Citation Paper Authors": "Authors:Cl\u00e9ment Godard, Oisin Mac Aodha, Michael Firman, Gabriel Brostow"
                    }
                },
                {
                    "Sentence ID": 124,
                    "Sentence": "640 x 192 0.115 0.903 4.863 0.193 0.877 0.959 0.981\nPackNet-SfM ",
                    "Citation Text": "V . Guizilini, R. Ambrus, S. Pillai, and A. Gaidon, \u201c3D packing for\nself-supervised monocular depth estimation,\u201d in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR) , 2020, pp. 2485\u20132494.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.02693",
                        "Citation Paper Title": "Title:3D Packing for Self-Supervised Monocular Depth Estimation",
                        "Citation Paper Abstract": "Abstract:Although cameras are ubiquitous, robotic platforms typically rely on active sensors like LiDAR for direct 3D perception. In this work, we propose a novel self-supervised monocular depth estimation method combining geometry with a new deep network, PackNet, learned only from unlabeled monocular videos. Our architecture leverages novel symmetrical packing and unpacking blocks to jointly learn to compress and decompress detail-preserving representations using 3D convolutions. Although self-supervised, our method outperforms other self, semi, and fully supervised methods on the KITTI benchmark. The 3D inductive bias in PackNet enables it to scale with input resolution and number of parameters without overfitting, generalizing better on out-of-domain data such as the NuScenes dataset. Furthermore, it does not require large-scale supervised pretraining on ImageNet and can run in real-time. Finally, we release DDAD (Dense Depth for Automated Driving), a new urban driving dataset with more challenging and accurate depth evaluation, thanks to longer-range and denser ground-truth depth generated from high-density LiDARs mounted on a fleet of self-driving cars operating world-wide.",
                        "Citation Paper Authors": "Authors:Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raventos, Adrien Gaidon"
                    }
                },
                {
                    "Sentence ID": 118,
                    "Sentence": ". It has been\nshown that state-of-the-art single frame attempts at monocular\ndepth estimation typically results in recognition tasks ",
                    "Citation Text": "M. Tatarchenko, S. R. Richter, R. Ranftl, Z. Li, V . Koltun, and\nT. Brox, \u201cWhat do single-view 3d reconstruction networks learn?\u201d in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR) , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.03678",
                        "Citation Paper Title": "Title:What Do Single-view 3D Reconstruction Networks Learn?",
                        "Citation Paper Abstract": "Abstract:Convolutional networks for single-view object reconstruction have shown impressive performance and have become a popular subject of research. All existing techniques are united by the idea of having an encoder-decoder network that performs non-trivial reasoning about the 3D structure of the output space. In this work, we set up two alternative approaches that perform image classification and retrieval respectively. These simple baselines yield better results than state-of-the-art methods, both qualitatively and quantitatively. We show that encoder-decoder methods are statistically indistinguishable from these baselines, thus indicating that the current state of the art in single-view object reconstruction does not actually perform reconstruction but image classification. We identify aspects of popular experimental procedures that elicit this behavior and discuss ways to improve the current state of research.",
                        "Citation Paper Authors": "Authors:Maxim Tatarchenko, Stephan R. Richter, Ren\u00e9 Ranftl, Zhuwen Li, Vladlen Koltun, Thomas Brox"
                    }
                },
                {
                    "Sentence ID": 117,
                    "Sentence": ", learnable in a self-\nsupervised manner through reprojection loss ",
                    "Citation Text": "T. Zhou, M. Brown, N. Snavely, and D. G. Lowe, \u201cUnsupervised\nlearning of depth and ego-motion from video,\u201d in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR) , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.07813",
                        "Citation Paper Title": "Title:Unsupervised Learning of Depth and Ego-Motion from Video",
                        "Citation Paper Abstract": "Abstract:We present an unsupervised learning framework for the task of monocular depth and camera motion estimation from unstructured video sequences. We achieve this by simultaneously training depth and camera pose estimation networks using the task of view synthesis as the supervisory signal. The networks are thus coupled via the view synthesis objective during training, but can be applied independently at test time. Empirical evaluation on the KITTI dataset demonstrates the effectiveness of our approach: 1) monocular depth performing comparably with supervised methods that use either ground-truth pose or depth for training, and 2) pose estimation performing favorably with established SLAM systems under comparable input settings.",
                        "Citation Paper Authors": "Authors:Tinghui Zhou, Matthew Brown, Noah Snavely, David G. Lowe"
                    }
                },
                {
                    "Sentence ID": 66,
                    "Sentence": ". In particular, standard bounding box object\ndetection representation breaks for fisheye images ",
                    "Citation Text": "H. Rashed, E. Mohamed, G. Sistu, V . R. Kumar, C. Eising, A. El-\nSallab, and S. Yogamani, \u201cGeneralized object detection on fisheye cam-\neras for autonomous driving: Dataset, representations and baseline,\u201d in\nProceedings of the IEEE/CVF Winter Conference on Applications of\nComputer Vision (WACV) , 2021, pp. 2272\u20132280.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.02124",
                        "Citation Paper Title": "Title:Generalized Object Detection on Fisheye Cameras for Autonomous Driving: Dataset, Representations and Baseline",
                        "Citation Paper Abstract": "Abstract:Object detection is a comprehensively studied problem in autonomous driving. However, it has been relatively less explored in the case of fisheye cameras. The standard bounding box fails in fisheye cameras due to the strong radial distortion, particularly in the image's periphery. We explore better representations like oriented bounding box, ellipse, and generic polygon for object detection in fisheye images in this work. We use the IoU metric to compare these representations using accurate instance segmentation ground truth. We design a novel curved bounding box model that has optimal properties for fisheye distortion models. We also design a curvature adaptive perimeter sampling method for obtaining polygon vertices, improving relative mAP score by 4.9% compared to uniform sampling. Overall, the proposed polygon model improves mIoU relative accuracy by 40.3%. It is the first detailed study on object detection on fisheye cameras for autonomous driving scenarios to the best of our knowledge. The dataset comprising of 10,000 images along with all the object representations ground truth will be made public to encourage further research. We summarize our work in a short video with qualitative results at this https URL.",
                        "Citation Paper Authors": "Authors:Hazem Rashed, Eslam Mohamed, Ganesh Sistu, Varun Ravi Kumar, Ciaran Eising, Ahmad El-Sallab, Senthil Yogamani"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": ". It should be noted that methods are being proposed to\nsimplify the ISP pipeline for visual perception ",
                    "Citation Text": "M. Buckler, S. Jayasuriya, and A. Sampson, \u201cReconfiguring the imag-\ning pipeline for computer vision,\u201d in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV) , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.04352",
                        "Citation Paper Title": "Title:Reconfiguring the Imaging Pipeline for Computer Vision",
                        "Citation Paper Abstract": "Abstract:Advancements in deep learning have ignited an explosion of research on efficient hardware for embedded computer vision. Hardware vision acceleration, however, does not address the cost of capturing and processing the image data that feeds these algorithms. We examine the role of the image signal processing (ISP) pipeline in computer vision to identify opportunities to reduce computation and save energy. The key insight is that imaging pipelines should be designed to be configurable: to switch between a traditional photography mode and a low-power vision mode that produces lower-quality image data suitable only for computer vision. We use eight computer vision algorithms and a reversible pipeline simulation tool to study the imaging system's impact on vision performance. For both CNN-based and classical vision algorithms, we observe that only two ISP stages, demosaicing and gamma compression, are critical for task performance. We propose a new image sensor design that can compensate for skipping these stages. The sensor design features an adjustable resolution and tunable analog-to-digital converters (ADCs). Our proposed imaging system's vision mode disables the ISP entirely and configures the sensor to produce subsampled, lower-precision image data. This vision mode can save ~75% of the average energy of a baseline photography mode while having only a small impact on vision task accuracy.",
                        "Citation Paper Authors": "Authors:Mark Buckler, Suren Jayasuriya, Adrian Sampson"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.13680v3": {
            "Paper Title": "Learning Practically Feasible Policies for Online 3D Bin Packing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.00383v2": {
            "Paper Title": "Relevant Region Sampling Strategy with Adaptive Heuristic for\n  Asymptotically Optimal Path Planning",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": "uses the greedy\nsearching method to generate the initial solution faster and\naccelerate the convergence speed. But these greedy-based\nmethods often fail to assist the searching procedure without\nanaccurateheuristicestimationmethod.TheAdaptivelyIn-\nformedTrees(AIT*) ",
                    "Citation Text": "Strub, M.P., Gammell, J.D., 2020. Adaptively informed trees (ait*):\nFast asymptotically optimal path planning through adaptive heuris-\ntics, in: 2020 IEEE International Conference on Robotics and Au-\ntomation (ICRA), IEEE. pp. 3191\u20133198.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.06599",
                        "Citation Paper Title": "Title:Adaptively Informed Trees (AIT*): Fast Asymptotically Optimal Path Planning through Adaptive Heuristics",
                        "Citation Paper Abstract": "Abstract:Informed sampling-based planning algorithms exploit problem knowledge for better search performance. This knowledge is often expressed as heuristic estimates of solution cost and used to order the search. The practical improvement of this informed search depends on the accuracy of the heuristic.\nSelecting an appropriate heuristic is difficult. Heuristics applicable to an entire problem domain are often simple to define and inexpensive to evaluate but may not be beneficial for a specific problem instance. Heuristics specific to a problem instance are often difficult to define or expensive to evaluate but can make the search itself trivial.\nThis paper presents Adaptively Informed Trees (AIT*), an almost-surely asymptotically optimal sampling-based planner based on BIT*. AIT* adapts its search to each problem instance by using an asymmetric bidirectional search to simultaneously estimate and exploit a problem-specific heuristic. This allows it to quickly find initial solutions and converge towards the optimum. AIT* solves the tested problems as fast as RRT-Connect while also converging towards the optimum.",
                        "Citation Paper Authors": "Authors:Marlin P. Strub, Jonathan D. Gammell"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": ", an asymptotical version of RRT,\nselects parent vertices based on the cost-to-come value and\nrewires the tree structure. However, RRT* only refines the\ncurrent tree structure locally. In contrast, the RRT# ",
                    "Citation Text": "Arslan,O.,Tsiotras,P.,2013. Useofrelaxationmethodsinsampling-\nbased algorithms for optimal motion planning, in: 2013 IEEE Inter-\nnational Conference on Robotics and Automation, IEEE. pp. 2421\u2013\n2428.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1601.06326",
                        "Citation Paper Title": "Title:Sampling-based Algorithms for Optimal Motion Planning Using Closed-loop Prediction",
                        "Citation Paper Abstract": "Abstract:Motion planning under differential constraints, kinodynamic motion planning, is one of the canonical problems in robotics. Currently, state-of-the-art methods evolve around kinodynamic variants of popular sampling-based algorithms, such as Rapidly-exploring Random Trees (RRTs). However, there are still challenges remaining, for example, how to include complex dynamics while guaranteeing optimality. If the open-loop dynamics are unstable, exploration by random sampling in control space becomes inefficient. We describe a new sampling-based algorithm, called CL-RRT#, which leverages ideas from the RRT# algorithm and a variant of the RRT algorithm that generates trajectories using closed-loop prediction. The idea of planning with closed-loop prediction allows us to handle complex unstable dynamics and avoids the need to find computationally hard steering procedures. The search technique presented in the RRT# algorithm allows us to improve the solution quality by searching over alternative reference trajectories. Numerical simulations using a nonholonomic system demonstrate the benefits of the proposed approach.",
                        "Citation Paper Authors": "Authors:Oktay Arslan, Karl Berntorp, Panagiotis Tsiotras"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ".\nWe utilize the idea of avoiding evaluating every edge\nin the forward-searching, which comes from the BIT*\nalgorithm ",
                    "Citation Text": "Gammell, J.D., Barfoot, T.D., Srinivasa, S.S., 2020. Batch informed\ntrees (bit*): Informed asymptotically optimal anytime search. The\nInternational Journal of Robotics Research 39, 543\u2013567.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.01888",
                        "Citation Paper Title": "Title:Batch Informed Trees (BIT*): Informed Asymptotically Optimal Anytime Search",
                        "Citation Paper Abstract": "Abstract:Path planning in robotics often requires finding high-quality solutions to continuously valued and/or high-dimensional problems. These problems are challenging and most planning algorithms instead solve simplified approximations. Popular approximations include graphs and random samples, as respectively used by informed graph-based searches and anytime sampling-based planners. Informed graph-based searches, such as A*, traditionally use heuristics to search a priori graphs in order of potential solution quality. This makes their search efficient but leaves their performance dependent on the chosen approximation. If its resolution is too low then they may not find a (suitable) solution but if it is too high then they may take a prohibitively long time to do so. Anytime sampling-based planners, such as RRT*, traditionally use random sampling to approximate the problem domain incrementally. This allows them to increase resolution until a suitable solution is found but makes their search dependent on the order of approximation. Arbitrary sequences of random samples approximate the problem domain in every direction simultaneously and but may be prohibitively inefficient at containing a solution. This paper unifies and extends these two approaches to develop Batch Informed Trees (BIT*), an informed, anytime sampling-based planner. BIT* solves continuous path planning problems efficiently by using sampling and heuristics to alternately approximate and search the problem domain. Its search is ordered by potential solution quality, as in A*, and its approximation improves indefinitely with additional computational time, as in RRT*. It is shown analytically to be almost-surely asymptotically optimal and experimentally to outperform existing sampling-based planners, especially on high-dimensional planning problems.",
                        "Citation Paper Authors": "Authors:Jonathan D. Gammell, Timothy D. Barfoot, Siddhartha S. Srinivasa"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "proposestofindtheglobalop-\ntimality in each rewiring stage with dynamic programming.\nDynamic programming is also used in the Fast Marching\nTree (FMT*) method ",
                    "Citation Text": "Janson, L., Schmerling, E., Clark, A., Pavone, M., 2015. Fast\nmarching tree: A fast marching sampling-based method for optimal\nmotion planning in many dimensions. The International journal of\nrobotics research 34, 883\u2013921.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1306.3532",
                        "Citation Paper Title": "Title:Fast Marching Tree: a Fast Marching Sampling-Based Method for Optimal Motion Planning in Many Dimensions",
                        "Citation Paper Abstract": "Abstract:In this paper we present a novel probabilistic sampling-based motion planning algorithm called the Fast Marching Tree algorithm (FMT*). The algorithm is specifically aimed at solving complex motion planning problems in high-dimensional configuration spaces. This algorithm is proven to be asymptotically optimal and is shown to converge to an optimal solution faster than its state-of-the-art counterparts, chiefly PRM* and RRT*. The FMT* algorithm performs a \"lazy\" dynamic programming recursion on a predetermined number of probabilistically-drawn samples to grow a tree of paths, which moves steadily outward in cost-to-arrive space. As a departure from previous analysis approaches that are based on the notion of almost sure convergence, the FMT* algorithm is analyzed under the notion of convergence in probability: the extra mathematical flexibility of this approach allows for convergence rate bounds--the first in the field of optimal sampling-based motion planning. Specifically, for a certain selection of tuning parameters and configuration spaces, we obtain a convergence rate bound of order $O(n^{-1/d+\\rho})$, where $n$ is the number of sampled points, $d$ is the dimension of the configuration space, and $\\rho$ is an arbitrarily small constant. We go on to demonstrate asymptotic optimality for a number of variations on FMT*, namely when the configuration space is sampled non-uniformly, when the cost is not arc length, and when connections are made based on the number of nearest neighbors instead of a fixed connection radius. Numerical experiments over a range of dimensions and obstacle configurations confirm our theoretical and heuristic arguments by showing that FMT*, for a given execution time, returns substantially better solutions than either PRM* or RRT*, especially in high-dimensional configuration spaces and in scenarios where collision-checking is expensive.",
                        "Citation Paper Authors": "Authors:Lucas Janson, Edward Schmerling, Ashley Clark, Marco Pavone"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": ",typicallyuseuniformsamplingtogenerate\nsamples and utilize a tree structure to search the space.\nThe RRT* algorithm ",
                    "Citation Text": "Karaman, S., Frazzoli, E., 2011. Sampling-based algorithms for\noptimal motion planning. The international journal of robotics\nresearch 30, 846\u2013894.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1105.1186",
                        "Citation Paper Title": "Title:Sampling-based Algorithms for Optimal Motion Planning",
                        "Citation Paper Abstract": "Abstract:During the last decade, sampling-based path planning algorithms, such as Probabilistic RoadMaps (PRM) and Rapidly-exploring Random Trees (RRT), have been shown to work well in practice and possess theoretical guarantees such as probabilistic completeness. However, little effort has been devoted to the formal analysis of the quality of the solution returned by such algorithms, e.g., as a function of the number of samples. The purpose of this paper is to fill this gap, by rigorously analyzing the asymptotic behavior of the cost of the solution returned by stochastic sampling-based algorithms as the number of samples increases. A number of negative results are provided, characterizing existing algorithms, e.g., showing that, under mild technical conditions, the cost of the solution returned by broadly used sampling-based algorithms converges almost surely to a non-optimal value. The main contribution of the paper is the introduction of new algorithms, namely, PRM* and RRT*, which are provably asymptotically optimal, i.e., such that the cost of the returned solution converges almost surely to the optimum. Moreover, it is shown that the computational complexity of the new algorithms is within a constant factor of that of their probabilistically complete (but not asymptotically optimal) counterparts. The analysis in this paper hinges on novel connections between stochastic sampling-based path planning algorithms and the theory of random geometric graphs.",
                        "Citation Paper Authors": "Authors:Sertac Karaman, Emilio Frazzoli"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.07140v4": {
            "Paper Title": "Natural Language Specification of Reinforcement Learning Policies\n  through Differentiable Decision Trees",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.06615v6": {
            "Paper Title": "Controlled Gaussian Process Dynamical Models with Application to Robotic\n  Cloth Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.09056v3": {
            "Paper Title": "Joint order assignment and picking station scheduling in KIVA warehouses\n  with multiple stations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.00246v6": {
            "Paper Title": "AdaAfford: Learning to Adapt Manipulation Affordance for 3D Articulated\n  Objects via Few-shot Interactions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.04891v3": {
            "Paper Title": "SQN: Weakly-Supervised Semantic Segmentation of Large-Scale 3D Point\n  Clouds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.14229v4": {
            "Paper Title": "Physics-informed Guided Disentanglement in Generative Networks",
            "Sentences": [
                {
                    "Sentence ID": 96,
                    "Sentence": "datasets having\nsoiling semantic annotation as polygons. Following our training\nstrategy (Fig. 5, bottom), our neural guidance DirtyGAN ",
                    "Citation Text": "M. Uricar, G. Sistu, H. Rashed, A. V obecky, P. Krizek, F. Burger, and\nS. Yogamani, \u201cLet\u2019s get dirty: Gan based data augmentation for soiling\nand adverse weather classi\ufb01cation in autonomous driving,\u201d in WACV ,\n2021. 6, 7, 10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.02249",
                        "Citation Paper Title": "Title:Let's Get Dirty: GAN Based Data Augmentation for Camera Lens Soiling Detection in Autonomous Driving",
                        "Citation Paper Abstract": "Abstract:Wide-angle fisheye cameras are commonly used in automated driving for parking and low-speed navigation tasks. Four of such cameras form a surround-view system that provides a complete and detailed view of the vehicle. These cameras are directly exposed to harsh environmental settings and can get soiled very easily by mud, dust, water, frost. Soiling on the camera lens can severely degrade the visual perception algorithms, and a camera cleaning system triggered by a soiling detection algorithm is increasingly being deployed. While adverse weather conditions, such as rain, are getting attention recently, there is only limited work on general soiling. The main reason is the difficulty in collecting a diverse dataset as it is a relatively rare event. We propose a novel GAN based algorithm for generating unseen patterns of soiled images. Additionally, the proposed method automatically provides the corresponding soiling masks eliminating the manual annotation cost. Augmentation of the generated soiled images for training improves the accuracy of soiling detection tasks significantly by 18% demonstrating its usefulness. The manually annotated soiling dataset and the generated augmentation dataset will be made public. We demonstrate the generalization of our fisheye trained GAN model on the Cityscapes dataset. We provide an empirical evaluation of the degradation of the semantic segmentation algorithm with the soiled data.",
                        "Citation Paper Authors": "Authors:Michal Uricar, Ganesh Sistu, Hazem Rashed, Antonin Vobecky, Varun Ravi Kumar, Pavel Krizek, Fabian Burger, Senthil Yogamani"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.06011v4": {
            "Paper Title": "Teaching Agents how to Map: Spatial Reasoning for Multi-Object\n  Navigation",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": "distinguish map-\nbased and map-less navigation. Recently, many navigation\n1http://multion-challenge.cs.sfu.ca/2021.html\n2https://eval.ai/web/challenges/challenge-page/805/\nleaderboard/2202\n3https://youtu.be/rzHZNATBec8\n4https://github.com/PierreMarza/teaching_agents_how_to_\nmapproblems have been posed as goal-reaching tasks ",
                    "Citation Text": "P. Anderson, A. X. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta,\nV . Koltun, J. Kosecka, J. Malik, R. Mottaghi, M. Savva, and A. R.\nZamir, \u201cOn evaluation of embodied navigation agents,\u201d arXiv , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.06757",
                        "Citation Paper Title": "Title:On Evaluation of Embodied Navigation Agents",
                        "Citation Paper Abstract": "Abstract:Skillful mobile operation in three-dimensional environments is a primary topic of study in Artificial Intelligence. The past two years have seen a surge of creative work on navigation. This creative output has produced a plethora of sometimes incompatible task definitions and evaluation protocols. To coordinate ongoing and future research in this area, we have convened a working group to study empirical methodology in navigation research. The present document summarizes the consensus recommendations of this working group. We discuss different problem statements and the role of generalization, present evaluation measures, and provide standard scenarios that can be used for benchmarking.",
                        "Citation Paper Authors": "Authors:Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, Amir R. Zamir"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": ", all agents are trained with Proximal Policy\nOptimization (PPO) ",
                    "Citation Text": "J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,\n\u201cProximal policy optimization algorithms,\u201d arXiv preprint , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.06347",
                        "Citation Paper Title": "Title:Proximal Policy Optimization Algorithms",
                        "Citation Paper Abstract": "Abstract:We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.",
                        "Citation Paper Authors": "Authors:John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "introduce\nself-supervised auxiliary tasks to speed up the training on\nPointGoal . They augment the base agent from ",
                    "Citation Text": "E. Wijmans, A. Kadian, A. Morcos, S. Lee, I. Essa, D. Parikh,\nM. Savva, and D. Batra, \u201cDd-ppo: Learning near-perfect pointgoal\nnavigators from 2.5 billion frames,\u201d in ICLR , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.00357",
                        "Citation Paper Title": "Title:DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames",
                        "Citation Paper Abstract": "Abstract:We present Decentralized Distributed Proximal Policy Optimization (DD-PPO), a method for distributed reinforcement learning in resource-intensive simulated environments. DD-PPO is distributed (uses multiple machines), decentralized (lacks a centralized server), and synchronous (no computation is ever stale), making it conceptually simple and easy to implement. In our experiments on training virtual robots to navigate in Habitat-Sim, DD-PPO exhibits near-linear scaling -- achieving a speedup of 107x on 128 GPUs over a serial implementation. We leverage this scaling to train an agent for 2.5 Billion steps of experience (the equivalent of 80 years of human experience) -- over 6 months of GPU-time training in under 3 days of wall-clock time with 64 GPUs.\nThis massive-scale training not only sets the state of art on Habitat Autonomous Navigation Challenge 2019, but essentially solves the task --near-perfect autonomous navigation in an unseen environment without access to a map, directly from an RGB-D camera and a GPS+Compass sensor. Fortuitously, error vs computation exhibits a power-law-like distribution; thus, 90% of peak performance is obtained relatively early (at 100 million steps) and relatively cheaply (under 1 day with 8 GPUs). Finally, we show that the scene understanding and navigation policies learned can be transferred to other navigation tasks -- the analog of ImageNet pre-training + task-specific fine-tuning for embodied AI. Our model outperforms ImageNet pre-trained CNNs on these transfer tasks and can serve as a universal resource (all models and code are publicly available).",
                        "Citation Paper Authors": "Authors:Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, Dhruv Batra"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ".\nIn contrast to end-to-end training, other approaches de-compose the agent into sub-modules ",
                    "Citation Text": "D. S. Chaplot, D. Gandhi, S. Gupta, A. Gupta, and R. Salakhutdinov,\n\u201cLearning to explore using active neural slam,\u201d in ICLR , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.05155",
                        "Citation Paper Title": "Title:Learning to Explore using Active Neural SLAM",
                        "Citation Paper Abstract": "Abstract:This work presents a modular and hierarchical approach to learn policies for exploring 3D environments, called `Active Neural SLAM'. Our approach leverages the strengths of both classical and learning-based methods, by using analytical path planners with learned SLAM module, and global and local policies. The use of learning provides flexibility with respect to input modalities (in the SLAM module), leverages structural regularities of the world (in global policies), and provides robustness to errors in state estimation (in local policies). Such use of learning within each module retains its benefits, while at the same time, hierarchical decomposition and modular training allow us to sidestep the high sample complexities associated with training end-to-end policies. Our experiments in visually and physically realistic simulated 3D environments demonstrate the effectiveness of our approach over past learning and geometry-based approaches. The proposed model can also be easily transferred to the PointGoal task and was the winning entry of the CVPR 2019 Habitat PointGoal Navigation Challenge.",
                        "Citation Paper Authors": "Authors:Devendra Singh Chaplot, Dhiraj Gandhi, Saurabh Gupta, Abhinav Gupta, Ruslan Salakhutdinov"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": ". Other work\nreduces assumptions about the necessary structure of the\nenvironment representation by using Transformers ",
                    "Citation Text": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in\nNeurIPS , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ". The\nnature of the goal, its regularities in the environment and\nhow it is communicated to the agent have a signi\ufb01cant\nimpact on required reasoning capacities of the agent ",
                    "Citation Text": "E. Beeching, J. Dibangoye, O. Simonin, and C. Wolf, \u201cDeep rein-\nforcement learning on a budget: 3d control and reasoning without a\nsupercomputer,\u201d in ICPR , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.01806",
                        "Citation Paper Title": "Title:Deep Reinforcement Learning on a Budget: 3D Control and Reasoning Without a Supercomputer",
                        "Citation Paper Abstract": "Abstract:An important goal of research in Deep Reinforcement Learning in mobile robotics is to train agents capable of solving complex tasks, which require a high level of scene understanding and reasoning from an egocentric perspective. When trained from simulations, optimal environments should satisfy a currently unobtainable combination of high-fidelity photographic observations, massive amounts of different environment configurations and fast simulation speeds. In this paper we argue that research on training agents capable of complex reasoning can be simplified by decoupling from the requirement of high fidelity photographic observations. We present a suite of tasks requiring complex reasoning and exploration in continuous, partially observable 3D environments. The objective is to provide challenging scenarios and a robust baseline agent architecture that can be trained on mid-range consumer hardware in under 24h. Our scenarios combine two key advantages: (i) they are based on a simple but highly efficient 3D environment (ViZDoom) which allows high speed simulation (12000fps); (ii) the scenarios provide the user with a range of difficulty settings, in order to identify the limitations of current state of the art algorithms and network architectures. We aim to increase accessibility to the field of Deep-RL by providing baselines for challenging scenarios where new ideas can be iterated on quickly. We argue that the community should be able to address challenging problems in reasoning of mobile agents without the need for a large compute infrastructure.",
                        "Citation Paper Authors": "Authors:Edward Beeching, Christian Wolf, Jilles Dibangoye, Olivier Simonin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.08248v2": {
            "Paper Title": "Active Vapor-Based Robotic Wiper",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.06549v4": {
            "Paper Title": "AdvSim: Generating Safety-Critical Scenarios for Self-Driving Vehicles",
            "Sentences": [
                {
                    "Sentence ID": 52,
                    "Sentence": "shared\nfeature representations between multiple subtasks and pre-\ndicted a cost volume to represent the quality of possible\nlocations in planning. DSDNet ",
                    "Citation Text": "Wenyuan Zeng, Shenlong Wang, Renjie Liao, Yun Chen, Bin\nYang, and Raquel Urtasun. Dsdnet: Deep structured self-\ndriving network. CoRR , abs/2008.06041, 2020. 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.06041",
                        "Citation Paper Title": "Title:DSDNet: Deep Structured self-Driving Network",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose the Deep Structured self-Driving Network (DSDNet), which performs object detection, motion prediction, and motion planning with a single neural network. Towards this goal, we develop a deep structured energy based model which considers the interactions between actors and produces socially consistent multimodal future predictions. Furthermore, DSDNet explicitly exploits the predicted future distributions of actors to plan a safe maneuver by using a structured planning cost. Our sample-based formulation allows us to overcome the difficulty in probabilistic inference of continuous random variables. Experiments on a number of large-scale self driving datasets demonstrate that our model significantly outperforms the state-of-the-art.",
                        "Citation Paper Authors": "Authors:Wenyuan Zeng, Shenlong Wang, Renjie Liao, Yun Chen, Bin Yang, Raquel Urtasun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.09863v3": {
            "Paper Title": "MORE: Simultaneous Multi-View 3D Object Recognition and Pose Estimation",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": "was designed for a more general\nonline learning situation where the model was required to\nlearnnewclassesaftertheinitialtraining,withveryfewex-\namples. This thus required the network to learn to model\nmore rotation and scale in-variance and improved perfor-\nmanceoflearningnewobjects. Orthographicprojectionwas\nusedalongwithMobileNetV2 ",
                    "Citation Text": "M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,\n\u201cMobilenetv2: Invertedresidualsandlinearbottlenecks,\u201din Proceed-\ningsoftheIEEEconferenceoncomputervisionandpatternrecogni-\ntion, 2018, pp. 4510\u20134520.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.04381",
                        "Citation Paper Title": "Title:MobileNetV2: Inverted Residuals and Linear Bottlenecks",
                        "Citation Paper Abstract": "Abstract:In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.\nThe MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters",
                        "Citation Paper Authors": "Authors:Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "showed\nthat a combination of CNNs and LSTMs could be used to\ncreate a more robust descriptor of the object by taking into\naccountmultiplelowlevelfeaturesandthenperformingvot-\ninginordertoimproveperformance. Theyalsousedahigh-\nway network layer to further ensure less loss of information\nbetweenthetwodi\ufb00erenttypesofnetworks. Xuanetal. ",
                    "Citation Text": "Q. Xuan, F. Li, Y. Liu, and Y. Xiang, \u201cMV-C3D: A Spatial Corre-\nlatedMulti-View3DConvolutionalNeuralNetworks,\u201d IEEEAccess ,\nvol. 7, pp. 92528\u201392538, 2019, conference Name: IEEE Access.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.06538",
                        "Citation Paper Title": "Title:MV-C3D: A Spatial Correlated Multi-View 3D Convolutional Neural Networks",
                        "Citation Paper Abstract": "Abstract:As the development of deep neural networks, 3D object recognition is becoming increasingly popular in computer vision community. Many multi-view based methods are proposed to improve the category recognition accuracy. These approaches mainly rely on multi-view images which are rendered with the whole circumference. In real-world applications, however, 3D objects are mostly observed from partial viewpoints in a less range. Therefore, we propose a multi-view based 3D convolutional neural network, which takes only part of contiguous multi-view images as input and can still maintain high accuracy. Moreover, our model takes these view images as a joint variable to better learn spatially correlated features using 3D convolution and 3D max-pooling layers. Experimental results on ModelNet10 and ModelNet40 datasets show that our MV-C3D technique can achieve outstanding performance with multi-view images which are captured from partial angles with less range. The results on 3D rotated real image dataset MIRO further demonstrate that MV-C3D is more adaptable in real-world scenarios. The classification accuracy can be further improved with the increasing number of view images.",
                        "Citation Paper Authors": "Authors:Qi Xuan, Fuxian Li, Yi Liu, Yun Xiang"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": ", where the geometric rela-\ntionsofpointsintheirrespectivecloudwasusedtocreatethe\nRS-CNN. This operation was intended to better model spa-\ntiallayoutthatwouldinturnmakeitbetteratunderstanding\nshapes from the point cloud.\nRecent approaches showed that it is possible to achieve\nsigni\ufb01cantimprovementsinclassi\ufb01cationaccuracybyusing\ncollectionsofrenderedviewsof3Dobjects[1,10]. Inparti-\ncular, Su et al., ",
                    "Citation Text": "H. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller, \u201cMulti-view\nconvolutionalneuralnetworksfor3Dshaperecognition,\u201din Proceed-\nings of the IEEE international conference on computer vision , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1505.00880",
                        "Citation Paper Title": "Title:Multi-view Convolutional Neural Networks for 3D Shape Recognition",
                        "Citation Paper Abstract": "Abstract:A longstanding question in computer vision concerns the representation of 3D shapes for recognition: should 3D shapes be represented with descriptors operating on their native 3D formats, such as voxel grid or polygon mesh, or can they be effectively represented with view-based descriptors? We address this question in the context of learning to recognize 3D shapes from a collection of their rendered views on 2D images. We first present a standard CNN architecture trained to recognize the shapes' rendered views independently of each other, and show that a 3D shape can be recognized even from a single view at an accuracy far higher than using state-of-the-art 3D shape descriptors. Recognition rates further increase when multiple views of the shapes are provided. In addition, we present a novel CNN architecture that combines information from multiple views of a 3D shape into a single and compact shape descriptor offering even better recognition performance. The same architecture can be applied to accurately recognize human hand-drawn sketches of shapes. We conclude that a collection of 2D views can be highly informative for 3D shape recognition and is amenable to emerging CNN architectures and their derivatives.",
                        "Citation Paper Authors": "Authors:Hang Su, Subhransu Maji, Evangelos Kalogerakis, Erik Learned-Miller"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.15242v3": {
            "Paper Title": "ConDA: Unsupervised Domain Adaptation for LiDAR Segmentation via\n  Regularized Domain Concatenation",
            "Sentences": [
                {
                    "Sentence ID": 55,
                    "Sentence": ", where the latter can be generated via\ncon\ufb01dence thresholding ",
                    "Citation Text": "Y . Li, L. Yuan, and N. Vasconcelos, \u201cBidirectional learning for domain\nadaptation of semantic segmentation,\u201d in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pp. 6936\u2013\n6945, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.10620",
                        "Citation Paper Title": "Title:Bidirectional Learning for Domain Adaptation of Semantic Segmentation",
                        "Citation Paper Abstract": "Abstract:Domain adaptation for semantic image segmentation is very necessary since manually labeling large datasets with pixel-level labels is expensive and time consuming. Existing domain adaptation techniques either work on limited datasets, or yield not so good performance compared with supervised learning. In this paper, we propose a novel bidirectional learning framework for domain adaptation of segmentation. Using the bidirectional learning, the image translation model and the segmentation adaptation model can be learned alternatively and promote to each other. Furthermore, we propose a self-supervised learning algorithm to learn a better segmentation adaptation model and in return improve the image translation model. Experiments show that our method is superior to the state-of-the-art methods in domain adaptation of segmentation with a big margin. The source code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Yunsheng Li, Lu Yuan, Nuno Vasconcelos"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "for the source\ndomain. This guided supervision and regularization may\nyield a more adaptable and robust feature learning. It is not\neasy, however, to directly mix domains via interpolation ",
                    "Citation Text": "H. Zhang, M. Cisse, Y . N. Dauphin, and D. Lopez-Paz, \u201cmixup:\nBeyond empirical risk minimization,\u201d in International Conference on\nLearning Representations , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.09412",
                        "Citation Paper Title": "Title:mixup: Beyond Empirical Risk Minimization",
                        "Citation Paper Abstract": "Abstract:Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.",
                        "Citation Paper Authors": "Authors:Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz"
                    }
                },
                {
                    "Sentence ID": 71,
                    "Sentence": "as our backbone. Since his much\nsmaller than w, we only downsample the height at later\nstages. The segmentation head combines the upsampled\noutputs from the last four stages for multi-scale feature\naggregations. Compared to previous RV networks ",
                    "Citation Text": "C. Xu, B. Wu, Z. Wang, W. Zhan, P. Vajda, K. Keutzer, and\nM. Tomizuka., \u201cSqueezesegv3: Spatially-adaptive convolution for ef\ufb01-\ncient point-cloud segmentation,\u201d in European Conference on Computer\nVision , pp. 1\u201319, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.01803",
                        "Citation Paper Title": "Title:SqueezeSegV3: Spatially-Adaptive Convolution for Efficient Point-Cloud Segmentation",
                        "Citation Paper Abstract": "Abstract:LiDAR point-cloud segmentation is an important problem for many applications. For large-scale point cloud segmentation, the \\textit{de facto} method is to project a 3D point cloud to get a 2D LiDAR image and use convolutions to process it. Despite the similarity between regular RGB and LiDAR images, we discover that the feature distribution of LiDAR images changes drastically at different image locations. Using standard convolutions to process such LiDAR images is problematic, as convolution filters pick up local features that are only active in specific regions in the image. As a result, the capacity of the network is under-utilized and the segmentation performance decreases. To fix this, we propose Spatially-Adaptive Convolution (SAC) to adopt different filters for different locations according to the input image. SAC can be computed efficiently since it can be implemented as a series of element-wise multiplications, im2col, and standard convolution. It is a general framework such that several previous methods can be seen as special cases of SAC. Using SAC, we build SqueezeSegV3 for LiDAR point-cloud segmentation and outperform all previous published methods by at least 3.7% mIoU on the SemanticKITTI benchmark with comparable inference speed.",
                        "Citation Paper Authors": "Authors:Chenfeng Xu, Bichen Wu, Zining Wang, Wei Zhan, Peter Vajda, Kurt Keutzer, Masayoshi Tomizuka"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": ". However, such methods\nsuffer from high computational costs and tend to be sensitive\nto hyperparameters and target domain changes ",
                    "Citation Text": "Z. Luo, Z. Cai, C. Zhou, G. Zhang, H. Zhao, S. Yi, S. Lu, H. Li,\nS. Zhang, and Z. Liu, \u201cUnsupervised domain adaptive 3d detection\nwith multi-level consistency,\u201d in Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision , pp. 8866\u20138875, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.11355",
                        "Citation Paper Title": "Title:Unsupervised Domain Adaptive 3D Detection with Multi-Level Consistency",
                        "Citation Paper Abstract": "Abstract:Deep learning-based 3D object detection has achieved unprecedented success with the advent of large-scale autonomous driving datasets. However, drastic performance degradation remains a critical challenge for cross-domain deployment. In addition, existing 3D domain adaptive detection methods often assume prior access to the target domain annotations, which is rarely feasible in the real world. To address this challenge, we study a more realistic setting, unsupervised 3D domain adaptive detection, which only utilizes source domain annotations. 1) We first comprehensively investigate the major underlying factors of the domain gap in 3D detection. Our key insight is that geometric mismatch is the key factor of domain shift. 2) Then, we propose a novel and unified framework, Multi-Level Consistency Network (MLC-Net), which employs a teacher-student paradigm to generate adaptive and reliable pseudo-targets. MLC-Net exploits point-, instance- and neural statistics-level consistency to facilitate cross-domain transfer. Extensive experiments demonstrate that MLC-Net outperforms existing state-of-the-art methods (including those using additional target domain information) on standard benchmarks. Notably, our approach is detector-agnostic, which achieves consistent gains on both single- and two-stage 3D detectors.",
                        "Citation Paper Authors": "Authors:Zhipeng Luo, Zhongang Cai, Changqing Zhou, Gongjie Zhang, Haiyu Zhao, Shuai Yi, Shijian Lu, Hongsheng Li, Shanghang Zhang, Ziwei Liu"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "to implicitly search for domain-invariant features via\ndistance measurements at different levels, i.e., input-level ",
                    "Citation Text": "J. Hoffman, E. Tzeng, T. Park, J.-Y . Zhu, P. Isola, K. Saenko, A. Efros,\nand T. Darrell, \u201cCycada: Cycle-consistent adversarial domain adapta-\ntion,\u201d in International Conference on Machine Learning , pp. 1989\u2013\n1998, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.03213",
                        "Citation Paper Title": "Title:CyCADA: Cycle-Consistent Adversarial Domain Adaptation",
                        "Citation Paper Abstract": "Abstract:Domain adaptation is critical for success in new, unseen environments. Adversarial adaptation models applied in feature spaces discover domain invariant representations, but are difficult to visualize and sometimes fail to capture pixel-level and low-level domain shifts. Recent work has shown that generative adversarial networks combined with cycle-consistency constraints are surprisingly effective at mapping images between domains, even without the use of aligned image pairs. We propose a novel discriminatively-trained Cycle-Consistent Adversarial Domain Adaptation model. CyCADA adapts representations at both the pixel-level and feature-level, enforces cycle-consistency while leveraging a task loss, and does not require aligned pairs. Our model can be applied in a variety of visual recognition and prediction settings. We show new state-of-the-art results across multiple adaptation tasks, including digit classification and semantic segmentation of road scenes demonstrating transfer from synthetic to real world domains.",
                        "Citation Paper Authors": "Authors:Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei A. Efros, Trevor Darrell"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.06080v3": {
            "Paper Title": "Pareto-optimal lane-changing motion planning in mixed traffic",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.09822v3": {
            "Paper Title": "Bayesian Controller Fusion: Leveraging Control Priors in Deep\n  Reinforcement Learning for Robotics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.07131v2": {
            "Paper Title": "Inequality Constrained Trajectory Optimization with A Hybrid\n  Multiple-shooting iLQR",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.01460v2": {
            "Paper Title": "Geometry-aware Bayesian Optimization in Robotics using Riemannian\n  Mat\u00e9rn Kernels",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "as the acquisition function. Each optimization process is repeated 30times with 5random initial\nsamples. Our implementations employ GPyTorch ",
                    "Citation Text": "J. R. Gardner, G. Pleiss, D. Bindel, K. Q. Weinberger, and A. G. Wilson. GPyTorch: Blackbox\nMatrix-Matrix Gaussian Process Inference with GPU Acceleration. In Advances in Neural\nInformation Processing Systems , 2018. URL:https://gpytorch.ai/ . Cited on page 7.\n9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.11165",
                        "Citation Paper Title": "Title:GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration",
                        "Citation Paper Abstract": "Abstract:Despite advances in scalable models, the inference tools used for Gaussian processes (GPs) have yet to fully capitalize on developments in computing hardware. We present an efficient and general approach to GP inference based on Blackbox Matrix-Matrix multiplication (BBMM). BBMM inference uses a modified batched version of the conjugate gradients algorithm to derive all terms for training and inference in a single call. BBMM reduces the asymptotic complexity of exact GP inference from $O(n^3)$ to $O(n^2)$. Adapting this algorithm to scalable approximations and complex GP models simply requires a routine for efficient matrix-matrix multiplication with the kernel and its derivative. In addition, BBMM uses a specialized preconditioner to substantially speed up convergence. In experiments we show that BBMM effectively uses GPU hardware to dramatically accelerate both exact GP inference and scalable approximations. Additionally, we provide GPyTorch, a software platform for scalable GP inference via BBMM, built on PyTorch.",
                        "Citation Paper Authors": "Authors:Jacob R. Gardner, Geoff Pleiss, David Bindel, Kilian Q. Weinberger, Andrew Gordon Wilson"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.01894v2": {
            "Paper Title": "Combining Physics and Deep Learning to learn Continuous-Time Dynamics\n  Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.07415v6": {
            "Paper Title": "ES-ENAS: Efficient Evolutionary Optimization for Large Hybrid Search\n  Spaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.01906v5": {
            "Paper Title": "A trained humanoid robot can perform human-like crossmodal social\n  attention and conflict resolution",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.12097v3": {
            "Paper Title": "Integrated Task and Motion Planning for Safe Legged Navigation in\n  Partially Observable Environments",
            "Sentences": [
                {
                    "Sentence ID": 22,
                    "Sentence": ". Reactive methods for motion planning with formal\nguarantees are widely studied with the methods of safety\nbarrier certi\ufb01cates ",
                    "Citation Text": "L. Wang, A. D. Ames, and M. Egerstedt, \u201cSafe certi\ufb01cate-based ma-\nneuvers for teams of quadrotors using differential \ufb02atness,\u201d in IEEE\nInternational Conference on Robotics and Automation , 2017, pp. 3293\u2013\n3298.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.01075",
                        "Citation Paper Title": "Title:Safe Certificate-Based Maneuvers for Teams of Quadrotors Using Differential Flatness",
                        "Citation Paper Abstract": "Abstract:Safety Barrier Certificates that ensure collision-free maneuvers for teams of differential flatness-based quadrotors are presented in this paper. Synthesized with control barrier functions, the certificates are used to modify the nominal trajectory in a minimally invasive way to avoid collisions. The proposed collision avoidance strategy complements existing flight control and planning algorithms by providing trajectory modifications with provable safety guarantees. The effectiveness of this strategy is supported both by the theoretical results and experimental validation on a team of five quadrotors.",
                        "Citation Paper Authors": "Authors:Li Wang, Aaron D. Ames, Magnus Egerstedt"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.02823v3": {
            "Paper Title": "Learning Visual-Audio Representations for Voice-Controlled Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.01220v2": {
            "Paper Title": "Trust-Aware Planning: Modeling Trust Evolution in Iterated Human-Robot\n  Interaction",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": "has been used to estimate trust in multi-\nrobot scenarios. The estimated trust is utilized to decide between\nmanual or autonomous control mode of robots ",
                    "Citation Text": "Yue Wang, Laura R Humphrey, Zhanrui Liao, and Huanfei Zheng. 2018. Trust-\nbased multi-robot symbolic motion planning with a human-in-the-loop. ACM\nTransactions on Interactive Intelligent Systems (TiiS) 8, 4 (2018), 1\u201333.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.05120",
                        "Citation Paper Title": "Title:Trust-based Multi-Robot Symbolic Motion Planning with a Human-in-the-Loop",
                        "Citation Paper Abstract": "Abstract:Symbolic motion planning for robots is the process of specifying and planning robot tasks in a discrete space, then carrying them out in a continuous space in a manner that preserves the discrete-level task specifications. Despite progress in symbolic motion planning, many challenges remain, including addressing scalability for multi-robot systems and improving solutions by incorporating human intelligence. In this paper, distributed symbolic motion planning for multi-robot systems is developed to address scalability. More specifically, compositional reasoning approaches are developed to decompose the global planning problem, and atomic propositions for observation, communication, and control are proposed to address inter-robot collision avoidance. To improve solution quality and adaptability, a dynamic, quantitative, and probabilistic human-to-robot trust model is developed to aid this decomposition. Furthermore, a trust-based real-time switching framework is proposed to switch between autonomous and manual motion planning for tradeoffs between task safety and efficiency. Deadlock- and livelock-free algorithms are designed to guarantee reachability of goals with a human-in-the-loop. A set of non-trivial multi-robot simulations with direct human input and trust evaluation are provided demonstrating the successful implementation of the trust-based multi-robot symbolic motion planning methods.",
                        "Citation Paper Authors": "Authors:Yue Wang, Laura R. Humphrey, Zhanrui Liao, Huanfei Zheng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.11138v2": {
            "Paper Title": "Distinguishing Engagement Facets: An Essential Component for AI-based\n  Interactive Healthcare",
            "Sentences": [
                {
                    "Sentence ID": 37,
                    "Sentence": "used person-independent edge features and Kernel Prin-\ncipal Component Analysis (KPCA) within a deep learning\nframework to detect online learners\u2019 engagement using\nfacial expressions. ",
                    "Citation Text": "F. Del Duchetto, P. Baxter, M. Hanheide, Are\nyou still with me? continuous engagement assess-ment from a robot\u2019s point of view, arXiv preprint\narXiv:2001.03515 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2001.03515",
                        "Citation Paper Title": "Title:Are you still with me? Continuous Engagement Assessment from a Robot's Point of View",
                        "Citation Paper Abstract": "Abstract:Continuously measuring the engagement of users with a robot in a Human-Robot Interaction (HRI) setting paves the way towards in-situ reinforcement learning, improve metrics of interaction quality, and can guide interaction design and behaviour optimisation. However, engagement is often considered very multi-faceted and difficult to capture in a workable and generic computational model that can serve as an overall measure of engagement. Building upon the intuitive ways humans successfully can assess situation for a degree of engagement when they see it, we propose a novel regression model (utilising CNN and LSTM networks) enabling robots to compute a single scalar engagement during interactions with humans from standard video streams, obtained from the point of view of an interacting robot. The model is based on a long-term dataset from an autonomous tour guide robot deployed in a public museum, with continuous annotation of a numeric engagement assessment by three independent coders. We show that this model not only can predict engagement very well in our own application domain but show its successful transfer to an entirely different dataset (with different tasks, environment, camera, robot and people). The trained model and the software is available to the HRI community as a tool to measure engagement in a variety of settings.",
                        "Citation Paper Authors": "Authors:Francesco Del Duchetto, Paul Baxter, Marc Hanheide"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "focusing on not-engaged/engaged\nand not-engaged/normally-engaged/very-engaged state\ndistinction. Frank et al. ",
                    "Citation Text": "M. Frank, G. Tofighi, H. Gu, R. Fruchter, En-\ngagement detection in meetings, arXiv preprint\narXiv:1608.08711 (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1608.08711",
                        "Citation Paper Title": "Title:Engagement Detection in Meetings",
                        "Citation Paper Abstract": "Abstract:Group meetings are frequent business events aimed to develop and conduct project work, such as Big Room design and construction project meetings. To be effective in these meetings, participants need to have an engaged mental state. The mental state of participants however, is hidden from other participants, and thereby difficult to evaluate. Mental state is understood as an inner process of thinking and feeling, that is formed of a conglomerate of mental representations and propositional attitudes. There is a need to create transparency of these hidden states to understand, evaluate and influence them. Facilitators need to evaluate the meeting situation and adjust for higher engagement and productivity. This paper presents a framework that defines a spectrum of engagement states and an array of classifiers aimed to detect the engagement state of participants in real time. The Engagement Framework integrates multi-modal information from 2D and 3D imaging and sound. Engagement is detected and evaluated at participants and aggregated at group level. We use empirical data collected at the lab of Konica Minolta, Inc. to test initial applications of this framework. The paper presents examples of the tested engagement classifiers, which are based on research in psychology, communication, and human computer interaction. Their accuracy is illustrated in dyadic interaction for engagement detection. In closing we discuss the potential extension to complex group collaboration settings and future feedback implementations.",
                        "Citation Paper Authors": "Authors:Maria Frank, Ghassem Tofighi, Haisong Gu, Renate Fruchter"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.13338v2": {
            "Paper Title": "Solving Challenging Control Problems Using Two-Staged Deep Reinforcement\n  Learning",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": "for more\ndetails.\nA. Rocket Navigation\nOur \ufb01rst environment is Rocket Navigation , where the task\nis to control the rocket so that it can reach the target location,\nthe moon. This environment is inspired by the OpenAI\ngym ",
                    "Citation Text": "G. Brockman, V . Cheung, L. Pettersson, J. Schneider, J. Schul-\nman, J. Tang, and W. Zaremba, \u201cOpenai gym,\u201d arXiv preprint\narXiv:1606.01540 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.01540",
                        "Citation Paper Title": "Title:OpenAI Gym",
                        "Citation Paper Abstract": "Abstract:OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.",
                        "Citation Paper Authors": "Authors:Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.13201v3": {
            "Paper Title": "DROID: Driver-centric Risk Object Identification",
            "Sentences": [
                {
                    "Sentence ID": 107,
                    "Sentence": ". The details of each graph are discussed below.\nNote that we choose the interaction modeling proposed\nin ",
                    "Citation Text": "C. Li, Y. Meng, S. H. Chan, and Y.-T. Chen, \u201cLearning 3D-aware\nEgocentric Spatial-Temporal Interaction via Graph Convolutional\nNetworks,\u201d in IEEE International Conference on Robotics and Au-\ntomation , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.09272",
                        "Citation Paper Title": "Title:Learning 3D-aware Egocentric Spatial-Temporal Interaction via Graph Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:To enable intelligent automated driving systems, a promising strategy is to understand how human drives and interacts with road users in complicated driving situations. In this paper, we propose a 3D-aware egocentric spatial-temporal interaction framework for automated driving applications. Graph convolution networks (GCN) is devised for interaction modeling. We introduce three novel concepts into GCN. First, we decompose egocentric interactions into ego-thing and ego-stuff interaction, modeled by two GCNs. In both GCNs, ego nodes are introduced to encode the interaction between thing objects (e.g., car and pedestrian), and interaction between stuff objects (e.g., lane marking and traffic light). Second, objects' 3D locations are explicitly incorporated into GCN to better model egocentric interactions. Third, to implement ego-stuff interaction in GCN, we propose a MaskAlign operation to extract features for irregular objects.\nWe validate the proposed framework on tactical driver behavior recognition. Extensive experiments are conducted using Honda Research Institute Driving Dataset, the largest dataset with diverse tactical driver behavior annotations. Our framework demonstrates substantial performance boost over baselines on the two experimental settings by 3.9% and 6.0%, respectively. Furthermore, we visualize the learned affinity matrices, which encode ego-thing and ego-stuff interactions, to showcase the proposed framework can capture interactions effectively.",
                        "Citation Paper Authors": "Authors:Chengxi Li, Yue Meng, Stanley H. Chan, Yi-Ting Chen"
                    }
                },
                {
                    "Sentence ID": 124,
                    "Sentence": "to in\ufb02ate\n2D convolution into 3D convolution, and \ufb01ne-tune it (i.e.,\nI3D) on the Kinetics action recognition dataset ",
                    "Citation Text": "W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijaya-\nnarasimhan, F. Viola, T. Green, T. Back, P . Natsev, M. Suleyman,\nand A. Zisserman, \u201cThe Kinetics Human Action Video Dataset,\u201d\ninarXiv preprint arXiv:1705.06950 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.06950",
                        "Citation Paper Title": "Title:The Kinetics Human Action Video Dataset",
                        "Citation Paper Abstract": "Abstract:We describe the DeepMind Kinetics human action video dataset. The dataset contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions are human focussed and cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands. We describe the statistics of the dataset, how it was collected, and give some baseline performance figures for neural network architectures trained and tested for human action classification on this dataset. We also carry out a preliminary analysis of whether imbalance in the dataset leads to bias in the classifiers.",
                        "Citation Paper Authors": "Authors:Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, Andrew Zisserman"
                    }
                },
                {
                    "Sentence ID": 122,
                    "Sentence": "is utilized to associate detected objects into tracks.\nNew objects are identi\ufb01ed when the detection cannot be\nassociated with an existing track.\nWe adopt Inception-v3 ",
                    "Citation Text": "C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna,\n\u201cRethinking the Inception Architecture for Computer Vision,\u201d in\nIEEE Conference on Computer Vision and Pattern Recognition , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.00567",
                        "Citation Paper Title": "Title:Rethinking the Inception Architecture for Computer Vision",
                        "Citation Paper Abstract": "Abstract:Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.",
                        "Citation Paper Authors": "Authors:Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": ".\nThis component incorporates the status, attributes, and dy-\nnamics of relevant traf\ufb01c participants of a traf\ufb01c environ-\nment. Speci\ufb01cally, given TRGB images, we apply object\ndetection ",
                    "Citation Text": "K. He, G. Gkioxari, P . Dollar, and R. Girshick, \u201cMask R-CNN,\u201d in\nIEEE Conference on Computer Vision and Pattern Recognition , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.06870",
                        "Citation Paper Title": "Title:Mask R-CNN",
                        "Citation Paper Abstract": "Abstract:We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: this https URL",
                        "Citation Paper Authors": "Authors:Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, Ross Girshick"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.10421v2": {
            "Paper Title": "Robotic Contact Juggling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.02100v2": {
            "Paper Title": "Object and Relation Centric Representations for Push Effect Prediction",
            "Sentences": [
                {
                    "Sentence ID": 70,
                    "Sentence": "showed that from input images, objects can be decomposed into convex\nhulls. In addition, they showed that these convex hulls could be used for\nphysics simulation. Similarly, Pashevich et al. ",
                    "Citation Text": "A. Pashevich, I. Kalevatykh, I. Laptev, C. Schmid, Learning visual poli-\ncies for building 3d shape categories, arXiv preprint arXiv:2004.07950\n(2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.07950",
                        "Citation Paper Title": "Title:Learning visual policies for building 3D shape categories",
                        "Citation Paper Abstract": "Abstract:Manipulation and assembly tasks require non-trivial planning of actions depending on the environment and the final goal. Previous work in this domain often assembles particular instances of objects from known sets of primitives. In contrast, we aim to handle varying sets of primitives and to construct different objects of a shape category. Given a single object instance of a category, e.g. an arch, and a binary shape classifier, we learn a visual policy to assemble other instances of the same category. In particular, we propose a disassembly procedure and learn a state policy that discovers new object instances and their assembly plans in state space. We then render simulated states in the observation space and learn a heatmap representation to predict alternative actions from a given input image. To validate our approach, we first demonstrate its efficiency for building object categories in state space. We then show the success of our visual policies for building arches from different primitives. Moreover, we demonstrate (i) the reactive ability of our method to re-assemble objects using additional primitives and (ii) the robust performance of our policy for unseen primitives resembling building blocks used during training. Our visual assembly policies are trained with no real images and reach up to 95% success rate when evaluated on a real robot.",
                        "Citation Paper Authors": "Authors:Alexander Pashevich, Igor Kalevatykh, Ivan Laptev, Cordelia Schmid"
                    }
                },
                {
                    "Sentence ID": 63,
                    "Sentence": "used recurrent neural networks to predict the center\nof mass from object mask and interaction experience. Xu et al. ",
                    "Citation Text": "Z. Xu, J. Wu, A. Zeng, J. B. Tenenbaum, S. Song, Densephysnet: Learn-\ning dense physical object representations via multi-step dynamic inter-\nactions, in: Robotics: Science and Systems (RSS), 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.03853",
                        "Citation Paper Title": "Title:DensePhysNet: Learning Dense Physical Object Representations via Multi-step Dynamic Interactions",
                        "Citation Paper Abstract": "Abstract:We study the problem of learning physical object representations for robot manipulation. Understanding object physics is critical for successful object manipulation, but also challenging because physical object properties can rarely be inferred from the object's static appearance. In this paper, we propose DensePhysNet, a system that actively executes a sequence of dynamic interactions (e.g., sliding and colliding), and uses a deep predictive model over its visual observations to learn dense, pixel-wise representations that reflect the physical properties of observed objects. Our experiments in both simulation and real settings demonstrate that the learned representations carry rich physical information, and can directly be used to decode physical object properties such as friction and mass. The use of dense representation enables DensePhysNet to generalize well to novel scenes with more objects than in training. With knowledge of object physics, the learned representation also leads to more accurate and efficient manipulation in downstream tasks than the state-of-the-art.",
                        "Citation Paper Authors": "Authors:Zhenjia Xu, Jiajun Wu, Andy Zeng, Joshua B. Tenenbaum, Shuran Song"
                    }
                },
                {
                    "Sentence ID": 60,
                    "Sentence": "proposed a deep approach for \fnding\nthe parameters of a simulation engine that predicts the future positions of the\nobjects that slide on various tilted surfaces. Zheng et al. ",
                    "Citation Text": "D. Zheng, V. Luo, J. Wu, J. B. Tenenbaum, Unsupervised learning of\nlatent physical properties using perception-prediction networks, arXiv\npreprint arXiv:1807.09244 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.09244",
                        "Citation Paper Title": "Title:Unsupervised Learning of Latent Physical Properties Using Perception-Prediction Networks",
                        "Citation Paper Abstract": "Abstract:We propose a framework for the completely unsupervised learning of latent object properties from their interactions: the perception-prediction network (PPN). Consisting of a perception module that extracts representations of latent object properties and a prediction module that uses those extracted properties to simulate system dynamics, the PPN can be trained in an end-to-end fashion purely from samples of object dynamics. The representations of latent object properties learned by PPNs not only are sufficient to accurately simulate the dynamics of systems comprised of previously unseen objects, but also can be translated directly into human-interpretable properties (e.g., mass, coefficient of restitution) in an entirely unsupervised manner. Crucially, PPNs also generalize to novel scenarios: their gradient-based training can be applied to many dynamical systems and their graph-based structure functions over systems comprised of different numbers of objects. Our results demonstrate the efficacy of graph-based neural architectures in object-centric inference and prediction tasks, and our model has the potential to discover relevant object properties in systems that are not yet well understood.",
                        "Citation Paper Authors": "Authors:David Zheng, Vinson Luo, Jiajun Wu, Joshua B. Tenenbaum"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": "learned object-\ncentric forward models for planning and control. Their model takes object\nbounding boxes as input and learns future state prediction from object em-\nbeddings generated by CNNs. Tung et al. ",
                    "Citation Text": "H.-Y. F. Tung, Z. Xian, M. Prabhudesai, S. Lal, K. Fragkiadaki,\n3d-oes: Viewpoint-invariant object-factorized environment simulators,\narXiv preprint arXiv:2011.06464 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.06464",
                        "Citation Paper Title": "Title:3D-OES: Viewpoint-Invariant Object-Factorized Environment Simulators",
                        "Citation Paper Abstract": "Abstract:We propose an action-conditioned dynamics model that predicts scene changes caused by object and agent interactions in a viewpoint-invariant 3D neural scene representation space, inferred from RGB-D videos. In this 3D feature space, objects do not interfere with one another and their appearance persists over time and across viewpoints. This permits our model to predict future scenes long in the future by simply \"moving\" 3D object features based on cumulative object motion predictions. Object motion predictions are computed by a graph neural network that operates over the object features extracted from the 3D neural scene representation. Our model's simulations can be decoded by a neural renderer into2D image views from any desired viewpoint, which aids the interpretability of our latent 3D simulation space. We show our model generalizes well its predictions across varying number and appearances of interacting objects as well as across camera viewpoints, outperforming existing 2D and 3D dynamics models. We further demonstrate sim-to-real transfer of the learnt dynamics by applying our model trained solely in simulation to model-based control for pushing objects to desired locations under clutter on a real robotic setup",
                        "Citation Paper Authors": "Authors:Hsiao-Yu Fish Tung, Zhou Xian, Mihir Prabhudesai, Shamit Lal, Katerina Fragkiadaki"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "to predict\nthe future image frames using only the current image frame and actions of\n8the robot. Byravan et al. ",
                    "Citation Text": "A. Byravan, D. Fox, SE3-Nets: Learning rigid body motion using deep\nneural networks, in: International Conference on Robotics and Automa-\ntion, 2017, pp. 173{180.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.02378",
                        "Citation Paper Title": "Title:SE3-Nets: Learning Rigid Body Motion using Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:We introduce SE3-Nets, which are deep neural networks designed to model and learn rigid body motion from raw point cloud data. Based only on sequences of depth images along with action vectors and point wise data associations, SE3-Nets learn to segment effected object parts and predict their motion resulting from the applied force. Rather than learning point wise flow vectors, SE3-Nets predict SE3 transformations for different parts of the scene. Using simulated depth data of a table top scene and a robot manipulator, we show that the structure underlying SE3-Nets enables them to generate a far more consistent prediction of object motion than traditional flow based networks. Additional experiments with a depth camera observing a Baxter robot pushing objects on a table show that SE3-Nets also work well on real data.",
                        "Citation Paper Authors": "Authors:Arunkumar Byravan, Dieter Fox"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": "investigated how changing ob-\nject shapes a\u000bects low-level object motion trajectories and modeled it using\nCNNs and LSTMs.\nIn the context of end-to-end learning, Agrawal et al. ",
                    "Citation Text": "P. Agrawal, A. V. Nair, P. Abbeel, J. Malik, S. Levine, Learning to poke\nby poking: Experiential learning of intuitive physics, in: Advances in\nNeural Information Processing Systems, 2016, pp. 5074{5082.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.07419",
                        "Citation Paper Title": "Title:Learning to Poke by Poking: Experiential Learning of Intuitive Physics",
                        "Citation Paper Abstract": "Abstract:We investigate an experiential learning paradigm for acquiring an internal model of intuitive physics. Our model is evaluated on a real-world robotic manipulation task that requires displacing objects to target locations by poking. The robot gathered over 400 hours of experience by executing more than 100K pokes on different objects. We propose a novel approach based on deep neural networks for modeling the dynamics of robot's interactions directly from images, by jointly estimating forward and inverse models of dynamics. The inverse model objective provides supervision to construct informative visual features, which the forward model can then predict and in turn regularize the feature space for the inverse model. The interplay between these two objectives creates useful, accurate models that can then be used for multi-step decision making. This formulation has the additional benefit that it is possible to learn forward models in an abstract feature space and thus alleviate the need of predicting pixels. Our experiments show that this joint modeling approach outperforms alternative methods.",
                        "Citation Paper Authors": "Authors:Pulkit Agrawal, Ashvin Nair, Pieter Abbeel, Jitendra Malik, Sergey Levine"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "proposed hybrid\n7network models which encode object information directly from images via\nCNNs and predict the next states of the objects via GNNs. Lately, these\nnetworks have been extended to handle even more complex environments.\nSanchez-Gonzales et al. ",
                    "Citation Text": "A. Sanchez-Gonzalez, J. Godwin, T. Pfa\u000b, R. Ying, J. Leskovec,\nP. Battaglia, Learning to simulate complex physics with graph networks,\nin: International Conference on Machine Learning, PMLR, 2020, pp.\n8459{8468.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.09405",
                        "Citation Paper Title": "Title:Learning to Simulate Complex Physics with Graph Networks",
                        "Citation Paper Abstract": "Abstract:Here we present a machine learning framework and model implementation that can learn to simulate a wide variety of challenging physical domains, involving fluids, rigid solids, and deformable materials interacting with one another. Our framework---which we term \"Graph Network-based Simulators\" (GNS)---represents the state of a physical system with particles, expressed as nodes in a graph, and computes dynamics via learned message-passing. Our results show that our model can generalize from single-timestep predictions with thousands of particles during training, to different initial conditions, thousands of timesteps, and at least an order of magnitude more particles at test time. Our model was robust to hyperparameter choices across various evaluation metrics: the main determinants of long-term performance were the number of message-passing steps, and mitigating the accumulation of error by corrupting the training data with noise. Our GNS framework advances the state-of-the-art in learned physical simulation, and holds promise for solving a wide range of complex forward and inverse problems.",
                        "Citation Paper Authors": "Authors:Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, Peter W. Battaglia"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "used image and detected the location of objects to predict the\nlatent representation of the next time step. This latent representation was\nthen decoded to create the image expected to be observed in the next time\nstep. Watters et al. ",
                    "Citation Text": "N. Watters, D. Zoran, T. Weber, P. Battaglia, R. Pascanu, A. Tacchetti,\nVisual interaction networks: Learning a physics simulator from video,\nin: Advances in neural information processing systems, 2017, pp. 4539{\n4547.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.01433",
                        "Citation Paper Title": "Title:Visual Interaction Networks",
                        "Citation Paper Abstract": "Abstract:From just a glance, humans can make rich predictions about the future state of a wide range of physical systems. On the other hand, modern approaches from engineering, robotics, and graphics are often restricted to narrow domains and require direct measurements of the underlying states. We introduce the Visual Interaction Network, a general-purpose model for learning the dynamics of a physical system from raw visual observations. Our model consists of a perceptual front-end based on convolutional neural networks and a dynamics predictor based on interaction networks. Through joint training, the perceptual front-end learns to parse a dynamic visual scene into a set of factored latent object representations. The dynamics predictor learns to roll these states forward in time by computing their interactions and dynamics, producing a predicted physical trajectory of arbitrary length. We found that from just six input video frames the Visual Interaction Network can generate accurate future trajectories of hundreds of time steps on a wide range of physical systems. Our model can also be applied to scenes with invisible objects, inferring their future states from their effects on the visible objects, and can implicitly infer the unknown mass of objects. Our results demonstrate that the perceptual module and the object-based dynamics predictor module can induce factored latent representations that support accurate dynamical predictions. This work opens new opportunities for model-based decision-making and planning from raw sensory observations in complex physical environments.",
                        "Citation Paper Authors": "Authors:Nicholas Watters, Andrea Tacchetti, Theophane Weber, Razvan Pascanu, Peter Battaglia, Daniel Zoran"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "employed CNNs to predict\nmovements of objects in static images in response to applied external forces.\nFragkiadaki et al. ",
                    "Citation Text": "K. Fragkiadaki, P. Agrawal, S. Levine, J. Malik, Learning visual predic-\ntive models of physics for playing billiards, in: International Conference\non Learning Representations, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.07404",
                        "Citation Paper Title": "Title:Learning Visual Predictive Models of Physics for Playing Billiards",
                        "Citation Paper Abstract": "Abstract:The ability to plan and execute goal specific actions in varied, unexpected settings is a central requirement of intelligent agents. In this paper, we explore how an agent can be equipped with an internal model of the dynamics of the external world, and how it can use this model to plan novel actions by running multiple internal simulations (\"visual imagination\"). Our models directly process raw visual input, and use a novel object-centric prediction formulation based on visual glimpses centered on objects (fixations) to enforce translational invariance of the learned physical laws. The agent gathers training data through random interaction with a collection of different environments, and the resulting model can then be used to plan goal-directed actions in novel environments that the agent has not seen before. We demonstrate that our agent can accurately plan actions for playing a simulated billiards game, which requires pushing a ball into a target position or into collision with another ball.",
                        "Citation Paper Authors": "Authors:Katerina Fragkiadaki, Pulkit Agrawal, Sergey Levine, Jitendra Malik"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": ".\nA speci\fc topic of interest within modeling physics with deep learning is\nmotion prediction from images, which has gained increasing attention over\nthe last few years. Mottaghi et al. ",
                    "Citation Text": "R. Mottaghi, H. Bagherinezhad, M. Rastegari, A. Farhadi, Newtonian\nscene understanding: Unfolding the dynamics of objects in static im-\n42ages, in: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 2016, pp. 3521{3529.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.04048",
                        "Citation Paper Title": "Title:Newtonian Image Understanding: Unfolding the Dynamics of Objects in Static Images",
                        "Citation Paper Abstract": "Abstract:In this paper, we study the challenging problem of predicting the dynamics of objects in static images. Given a query object in an image, our goal is to provide a physical understanding of the object in terms of the forces acting upon it and its long term motion as response to those forces. Direct and explicit estimation of the forces and the motion of objects from a single image is extremely challenging. We define intermediate physical abstractions called Newtonian scenarios and introduce Newtonian Neural Network ($N^3$) that learns to map a single image to a state in a Newtonian scenario. Our experimental evaluations show that our method can reliably predict dynamics of a query object from a single image. In addition, our approach can provide physical reasoning that supports the predicted dynamics in terms of velocity and force vectors. To spur research in this direction we compiled Visual Newtonian Dynamics (VIND) dataset that includes 6806 videos aligned with Newtonian scenarios represented using game engines, and 4516 still images with their ground truth dynamics.",
                        "Citation Paper Authors": "Authors:Roozbeh Mottaghi, Hessam Bagherinezhad, Mohammad Rastegari, Ali Farhadi"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "trained a deep\nnetwork to predict the stability of the block towers given their raw images\nobtained from a simulator. Groth et al. ",
                    "Citation Text": "O. Groth, F. B. Fuchs, I. Posner, A. Vedaldi, Shapestacks: Learn-\ning vision-based physical intuition for generalised object stacking, in:\nProceedings of the European Conference on Computer Vision (ECCV),\n2018, pp. 702{717.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.08018",
                        "Citation Paper Title": "Title:ShapeStacks: Learning Vision-Based Physical Intuition for Generalised Object Stacking",
                        "Citation Paper Abstract": "Abstract:Physical intuition is pivotal for intelligent agents to perform complex tasks. In this paper we investigate the passive acquisition of an intuitive understanding of physical principles as well as the active utilisation of this intuition in the context of generalised object stacking. To this end, we provide: a simulation-based dataset featuring 20,000 stack configurations composed of a variety of elementary geometric primitives richly annotated regarding semantics and structural stability. We train visual classifiers for binary stability prediction on the ShapeStacks data and scrutinise their learned physical intuition. Due to the richness of the training data our approach also generalises favourably to real-world scenarios achieving state-of-the-art stability prediction on a publicly available benchmark of block towers. We then leverage the physical intuition learned by our model to actively construct stable stacks and observe the emergence of an intuitive notion of stackability - an inherent object affordance - induced by the active stacking task. Our approach performs well even in challenging conditions where it considerably exceeds the stack height observed during training or in cases where initially unstable structures must be stabilised via counterbalancing.",
                        "Citation Paper Authors": "Authors:Oliver Groth, Fabian B. Fuchs, Ingmar Posner, Andrea Vedaldi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.02758v2": {
            "Paper Title": "Mismatched No More: Joint Model-Policy Optimization for Model-Based RL",
            "Sentences": [
                {
                    "Sentence ID": 22,
                    "Sentence": "contains complex contact dynamics that are challenging to model. MBPO performs poorly on these tasks, often\nworse than model-free SAC ",
                    "Citation Text": "Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018). Soft Actor-Critic: Off-Policy Maximum Entropy\nDeep Reinforcement Learning with a Stochastic Actor. In International Conference on Machine Learning .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.01290",
                        "Citation Paper Title": "Title:Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
                        "Citation Paper Abstract": "Abstract:Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
                        "Citation Paper Authors": "Authors:Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ". While our\naim is the same as these prior methods, our approach will not require differentiating through unrolled\nmodel updates or optimization procedures.\nTable 1: Lower bounds for model-based RL.\nLuo et al. ",
                    "Citation Text": "Luo, Y ., Xu, H., Li, Y ., Tian, Y ., Darrell, T., and Ma, T. (2019). Algorithmic framework for model-based\ndeep reinforcement learning with theoretical guarantees. In ICLR (Poster) .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.03858",
                        "Citation Paper Title": "Title:Algorithmic Framework for Model-based Deep Reinforcement Learning with Theoretical Guarantees",
                        "Citation Paper Abstract": "Abstract:Model-based reinforcement learning (RL) is considered to be a promising approach to reduce the sample complexity that hinders model-free RL. However, the theoretical understanding of such methods has been rather limited. This paper introduces a novel algorithmic framework for designing and analyzing model-based RL algorithms with theoretical guarantees. We design a meta-algorithm with a theoretical guarantee of monotone improvement to a local maximum of the expected reward. The meta-algorithm iteratively builds a lower bound of the expected reward based on the estimated dynamical model and sample trajectories, and then maximizes the lower bound jointly over the policy and the model. The framework extends the optimism-in-face-of-uncertainty principle to non-linear dynamical models in a way that requires \\textit{no explicit} uncertainty quantification. Instantiating our framework with simplification gives a variant of model-based RL algorithms Stochastic Lower Bounds Optimization (SLBO). Experiments demonstrate that SLBO achieves state-of-the-art performance when only one million or fewer samples are permitted on a range of continuous control benchmark tasks.",
                        "Citation Paper Authors": "Authors:Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, Tengyu Ma"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.12924v3": {
            "Paper Title": "Joint stereo 3D object detection and implicit surface reconstruction",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": "points. Not using masks leads to inferior performances. We\nbelieve such background points can introduce noise to the Ha\nMethodAOS whenAP2D= 100 :00\nEasy Moderate Hard\nEgo-Net ",
                    "Citation Text": "S. Li, Z. Yan, H. Li, and K.-T. Cheng, \u201cExploring intermediate repre-\nsentation for monocular vehicle pose estimation,\u201d in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2021, pp. 1873\u20131883.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.08464",
                        "Citation Paper Title": "Title:Exploring intermediate representation for monocular vehicle pose estimation",
                        "Citation Paper Abstract": "Abstract:We present a new learning-based framework to recover vehicle pose in SO(3) from a single RGB image. In contrast to previous works that map from local appearance to observation angles, we explore a progressive approach by extracting meaningful Intermediate Geometrical Representations (IGRs) to estimate egocentric vehicle orientation. This approach features a deep model that transforms perceived intensities to IGRs, which are mapped to a 3D representation encoding object orientation in the camera coordinate system. Core problems are what IGRs to use and how to learn them more effectively. We answer the former question by designing IGRs based on an interpolated cuboid that derives from primitive 3D annotation readily. The latter question motivates us to incorporate geometry knowledge with a new loss function based on a projective invariant. This loss function allows unlabeled data to be used in the training stage to improve representation learning. Without additional labels, our system outperforms previous monocular RGB-based methods for joint vehicle detection and pose estimation on the KITTI benchmark, achieving performance even comparable to stereo methods. Code and pre-trained models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Shichao Li, Zengqiang Yan, Hongyang Li, Kwang-Ting Cheng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.04323v2": {
            "Paper Title": "Learning structured approximations of combinatorial optimization\n  problems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.01654v2": {
            "Paper Title": "Comparison of modern open-source visual SLAM approaches",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.12882v3": {
            "Paper Title": "MAVFI: An End-to-End Fault Analysis Framework with Anomaly Detection and\n  Recovery for Micro Aerial Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.10284v5": {
            "Paper Title": "Reinforcement Learning Based Temporal Logic Control with Soft\n  Constraints Using Limit-deterministic Generalized Buchi Automata",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.14180v3": {
            "Paper Title": "AeCoM: An Aerial Continuum Manipulator with Precise Kinematic Modeling\n  for Variable Loading and Tendon-slacking Prevention",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.05366v5": {
            "Paper Title": "Rearrangement on Lattices with Pick-n-Swaps: Optimality Structures and\n  Efficient Algorithms",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.04928v4": {
            "Paper Title": "Trajectory Optimization with Optimization-Based Dynamics",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "and a derivative-free method\nthat generates gradients via randomized smoothing for iLQR\nis also proposed ",
                    "Citation Text": "H. J. T. Suh, T. Pang, and R. Tedrake, \u201cBundled gradients through\ncontact via randomized smoothing,\u201d IEEE Robotics and Automation\nLetters , vol. 7, no. 2, pp. 4000\u20134007, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.05143",
                        "Citation Paper Title": "Title:Bundled Gradients through Contact via Randomized Smoothing",
                        "Citation Paper Abstract": "Abstract:The empirical success of derivative-free methods in reinforcement learning for planning through contact seems at odds with the perceived fragility of classical gradient-based optimization methods in these domains. What is causing this gap, and how might we use the answer to improve gradient-based methods? We believe a stochastic formulation of dynamics is one crucial ingredient. We use tools from randomized smoothing to analyze sampling-based approximations of the gradient, and formalize such approximations through the gradient bundle. We show that using the gradient bundle in lieu of the gradient mitigates fast-changing gradients of non-smooth contact dynamics modeled by the implicit time-stepping, or the penalty method. Finally, we apply the gradient bundle to optimal control using iLQR, introducing a novel algorithm which improves convergence over using exact gradients. Combining our algorithm with a convex implicit time-stepping formulation of contact, we show that we can tractably tackle planning-through-contact problems in manipulation.",
                        "Citation Paper Authors": "Authors:H.J. Terry Suh, Tao Pang, Russ Tedrake"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "simulation. Implicit dynamics have also been\ntrained to represent non-smooth dynamics ",
                    "Citation Text": "S. Pfrommer, M. Halm, and M. Posa, \u201cContactNets: Learning dis-\ncontinuous contact dynamics with smooth, implicit representations,\u201d in\nConference on Robot Learning , 2021, pp. 2279\u20132291.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.11193",
                        "Citation Paper Title": "Title:ContactNets: Learning Discontinuous Contact Dynamics with Smooth, Implicit Representations",
                        "Citation Paper Abstract": "Abstract:Common methods for learning robot dynamics assume motion is continuous, causing unrealistic model predictions for systems undergoing discontinuous impact and stiction behavior. In this work, we resolve this conflict with a smooth, implicit encoding of the structure inherent to contact-induced discontinuities. Our method, ContactNets, learns parameterizations of inter-body signed distance and contact-frame Jacobians, a representation that is compatible with many simulation, control, and planning environments for robotics. We furthermore circumvent the need to differentiate through stiff or non-smooth dynamics with a novel loss function inspired by the principles of complementarity and maximum dissipation. Our method can predict realistic impact, non-penetration, and stiction when trained on 60 seconds of real-world data.",
                        "Citation Paper Authors": "Authors:Samuel Pfrommer, Mathew Halm, Michael Posa"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.05616v3": {
            "Paper Title": "Fast Contact-Implicit Model-Predictive Control",
            "Sentences": [
                {
                    "Sentence ID": 22,
                    "Sentence": ". Approaches that utilize both force-based\nMPC and whole-body control have also demonstrated agile\nlocomotion ",
                    "Citation Text": "D. Kim, J. Di Carlo, B. Katz, G. Bledt, and S. Kim,\n\u201cHighly dynamic quadruped locomotion via whole-\nbody impulse control and model predictive control,\u201d\narXiv:1909.06586 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.06586",
                        "Citation Paper Title": "Title:Highly Dynamic Quadruped Locomotion via Whole-Body Impulse Control and Model Predictive Control",
                        "Citation Paper Abstract": "Abstract:Dynamic legged locomotion is a challenging topic because of the lack of established control schemes which can handle aerial phases, short stance times, and high-speed leg swings. In this paper, we propose a controller combining whole-body control (WBC) and model predictive control (MPC). In our framework, MPC finds an optimal reaction force profile over a longer time horizon with a simple model, and WBC computes joint torque, position, and velocity commands based on the reaction forces computed from MPC. Unlike existing WBCs, which attempt to track commanded body trajectories, our controller is focused more on the reaction force command, which allows it to accomplish high speed dynamic locomotion with aerial phases. The newly devised WBC is integrated with MPC and tested on the Mini-Cheetah quadruped robot. To demonstrate the robustness and versatility, the controller is tested on six different gaits in a number of different environments, including outdoors and on a treadmill, reaching a top speed of 3.7 m/s.",
                        "Citation Paper Authors": "Authors:Donghyun Kim, Jared Di Carlo, Benjamin Katz, Gerardo Bledt, Sangbae Kim"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": ". Integrating hardware design and con-\ntroller synthesis has also recently enabled small humanoids to\nperform agile acrobatic maneuvers in simulation ",
                    "Citation Text": "M. Chignoli, D. Kim, E. Stanger-Jones, and S. Kim,\n\u201cThe MIT humanoid robot: Design, motion planning, and\ncontrol for acrobatic behaviors,\u201d IEEE-RAS International\nConference on Humanoid Robots (Humanoids) , pp. 1\u20138,\n2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.09025",
                        "Citation Paper Title": "Title:The MIT Humanoid Robot: Design, Motion Planning, and Control For Acrobatic Behaviors",
                        "Citation Paper Abstract": "Abstract:Demonstrating acrobatic behavior of a humanoid robot such as flips and spinning jumps requires systematic approaches across hardware design, motion planning, and control. In this paper, we present a new humanoid robot design, an actuator-aware kino-dynamic motion planner, and a landing controller as part of a practical system design for highly dynamic motion control of the humanoid robot. To achieve the impulsive motions, we develop two new proprioceptive actuators and experimentally evaluate their performance using our custom-designed dynamometer. The actuator's torque, velocity, and power limits are reflected in our kino-dynamic motion planner by approximating the configuration-dependent reaction force limits and in our dynamics simulator by including actuator dynamics along with the robot's full-body dynamics. For the landing control, we effectively integrate model-predictive control and whole-body impulse control by connecting them in a dynamically consistent way to accomplish both the long-time horizon optimal control and high-bandwidth full-body dynamics-based feedback. Actuators' torque output over the entire motion are validated based on the velocity-torque model including battery voltage droop and back-EMF voltage. With the carefully designed hardware and control framework, we successfully demonstrate dynamic behaviors such as back flips, front flips, and spinning jumps in our realistic dynamics simulation.",
                        "Citation Paper Authors": "Authors:Matthew Chignoli, Donghyun Kim, Elijah Stanger-Jones, Sangbae Kim"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.14888v2": {
            "Paper Title": "Social influence under uncertainty in interaction with peers, robots and\n  computers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.06242v3": {
            "Paper Title": "Formulating Event-based Image Reconstruction as a Linear Inverse Problem\n  with Deep Regularization using Optical Flow",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": ") may forego\nthis assumption given suf\ufb01cient high quality data, which may\nbe dif\ufb01cult to obtain. The stationary camera problem may be\nsolved by combining events and grayscale frames ",
                    "Citation Text": "C. Scheerlinck, N. Barnes, and R. Mahony, \u201cContinuous-time\nintensity estimation using event cameras,\u201d in Asian Conf. Comput.\nVis. (ACCV) , 2018, pp. 308\u2013324.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.00386",
                        "Citation Paper Title": "Title:Continuous-time Intensity Estimation Using Event Cameras",
                        "Citation Paper Abstract": "Abstract:Event cameras provide asynchronous, data-driven measurements of local temporal contrast over a large dynamic range with extremely high temporal resolution. Conventional cameras capture low-frequency reference intensity information. These two sensor modalities provide complementary information. We propose a computationally efficient, asynchronous filter that continuously fuses image frames and events into a single high-temporal-resolution, high-dynamic-range image state. In absence of conventional image frames, the filter can be run on events only. We present experimental results on high-speed, high-dynamic-range sequences, as well as on new ground truth datasets we generate to demonstrate the proposed algorithm outperforms existing state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Cedric Scheerlinck, Nick Barnes, Robert Mahony"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.06407v2": {
            "Paper Title": "Neural Networks with Physics-Informed Architectures and Constraints for\n  Dynamical Systems Modeling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.13976v2": {
            "Paper Title": "Gaussian Belief Space Path Planning for Minimum Sensing Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.09456v2": {
            "Paper Title": "Compositional Learning-based Planning for Vision POMDPs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.04450v4": {
            "Paper Title": "Scene Editing as Teleoperation: A Case Study in 6DoF Kit Assembly",
            "Sentences": [
                {
                    "Sentence ID": 52,
                    "Sentence": "and\nintegrate it with our system. Planning the grasp and a set of\nsequential 6DoF robot actions for general 6DoF kitting tasks\nwould also be an interesting future direction, where the robot\nmight need to plan a place-driven grasp ",
                    "Citation Text": "K. Fang, Y . Zhu, A. Garg, A. Kurenkov, V . Mehta, F.-F. Li, and\nS. Savarese, \u201cLearning task-oriented grasping for tool manipula-\ntion from simulated self-supervision,\u201d The International Journal of\nRobotics Research , vol. 39, no. 2-3, pp. 202\u2013216, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.09266",
                        "Citation Paper Title": "Title:Learning Task-Oriented Grasping for Tool Manipulation from Simulated Self-Supervision",
                        "Citation Paper Abstract": "Abstract:Tool manipulation is vital for facilitating robots to complete challenging task goals. It requires reasoning about the desired effect of the task and thus properly grasping and manipulating the tool to achieve the task. Task-agnostic grasping optimizes for grasp robustness while ignoring crucial task-specific constraints. In this paper, we propose the Task-Oriented Grasping Network (TOG-Net) to jointly optimize both task-oriented grasping of a tool and the manipulation policy for that tool. The training process of the model is based on large-scale simulated self-supervision with procedurally generated tool objects. We perform both simulated and real-world experiments on two tool-based manipulation tasks: sweeping and hammering. Our model achieves overall 71.1% task success rate for sweeping and 80.0% task success rate for hammering. Supplementary material is available at: this http URL",
                        "Citation Paper Authors": "Authors:Kuan Fang, Yuke Zhu, Animesh Garg, Andrey Kurenkov, Viraj Mehta, Li Fei-Fei, Silvio Savarese"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "representation\nVpartial with voxel size 0 :89 mm. This volume is then fed\ninto our shape-completion network SCqto obtain the shape-\ncompleted 3D volume Vcompleted .SCqfollows a 3D encoder\u2013\ndecoder style architecture with skip connections ",
                    "Citation Text": "Z. Xu, Z. He, J. Wu, and S. Song, \u201cLearning 3d dynamic scene\nrepresentations for robot manipulation,\u201d in Conference on Robotic\nLearning (CoRL) , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.01968",
                        "Citation Paper Title": "Title:Learning 3D Dynamic Scene Representations for Robot Manipulation",
                        "Citation Paper Abstract": "Abstract:3D scene representation for robot manipulation should capture three key object properties: permanency -- objects that become occluded over time continue to exist; amodal completeness -- objects have 3D occupancy, even if only partial observations are available; spatiotemporal continuity -- the movement of each object is continuous over space and time. In this paper, we introduce 3D Dynamic Scene Representation (DSR), a 3D volumetric scene representation that simultaneously discovers, tracks, reconstructs objects, and predicts their dynamics while capturing all three properties. We further propose DSR-Net, which learns to aggregate visual observations over multiple interactions to gradually build and refine DSR. Our model achieves state-of-the-art performance in modeling 3D scene dynamics with DSR on both simulated and real data. Combined with model predictive control, DSR-Net enables accurate planning in downstream robotic manipulation tasks such as planar pushing. Video is available at this https URL.",
                        "Citation Paper Authors": "Authors:Zhenjia Xu, Zhanpeng He, Jiajun Wu, Shuran Song"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.02246v2": {
            "Paper Title": "Safer Motion Planning of Steerable Needles via a Shaft-to-Tissue Force\n  Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.10392v3": {
            "Paper Title": "A generalized stacked reinforcement learning method for sampled systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.05437v2": {
            "Paper Title": "Autonomous Racing using a Hybrid Imitation-Reinforcement Learning\n  Architecture",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": "have employed reinforcement\nlearning for autonomous driving without the absolute intention\nof racing. Some of the recent works ",
                    "Citation Text": "F. Fuchs, Y . Song, E. Kaufmann, D. Scaramuzza, and P. D \u00a8urr, \u201cSuper-\nHuman Performance in Gran Turismo Sport Using Deep Reinforcement\nLearning,\u201d IEEE Robotics and Automation Letters , vol. 6, no. 3, pp.\n4257\u20134264, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.07971",
                        "Citation Paper Title": "Title:Super-Human Performance in Gran Turismo Sport Using Deep Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:Autonomous car racing is a major challenge in robotics. It raises fundamental problems for classical approaches such as planning minimum-time trajectories under uncertain dynamics and controlling the car at the limits of its handling. Besides, the requirement of minimizing the lap time, which is a sparse objective, and the difficulty of collecting training data from human experts have also hindered researchers from directly applying learning-based approaches to solve the problem. In the present work, we propose a learning-based system for autonomous car racing by leveraging a high-fidelity physical car simulation, a course-progress proxy reward, and deep reinforcement learning. We deploy our system in Gran Turismo Sport, a world-leading car simulator known for its realistic physics simulation of different race cars and tracks, which is even used to recruit human race car drivers. Our trained policy achieves autonomous racing performance that goes beyond what had been achieved so far by the built-in AI, and, at the same time, outperforms the fastest driver in a dataset of over 50,000 human players.",
                        "Citation Paper Authors": "Authors:Florian Fuchs, Yunlong Song, Elia Kaufmann, Davide Scaramuzza, Peter Duerr"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "trained a CNN\nmodel to steer an autonomous vehicle by implicitly detecting\nthe road lanes and analyzed its feature extraction capabilities.More recently, ",
                    "Citation Text": "T. V . Samak, C. V . Samak, and S. Kandhasamy, \u201cRobust Behavioral\nCloning for Autonomous Vehicles Using End-to-End Imitation Learn-\ning,\u201d SAE International Journal of Connected and Automated Vehicles ,\nvol. 4, no. 3, pp. 279\u2013295, aug 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.04767",
                        "Citation Paper Title": "Title:Robust Behavioral Cloning for Autonomous Vehicles using End-to-End Imitation Learning",
                        "Citation Paper Abstract": "Abstract:In this work, we present a lightweight pipeline for robust behavioral cloning of a human driver using end-to-end imitation learning. The proposed pipeline was employed to train and deploy three distinct driving behavior models onto a simulated vehicle. The training phase comprised of data collection, balancing, augmentation, preprocessing and training a neural network, following which, the trained model was deployed onto the ego vehicle to predict steering commands based on the feed from an onboard camera. A novel coupled control law was formulated to generate longitudinal control commands on-the-go based on the predicted steering angle and other parameters such as actual speed of the ego vehicle and the prescribed constraints for speed and steering. We analyzed computational efficiency of the pipeline and evaluated robustness of the trained models through exhaustive experimentation during the deployment phase. We also compared our approach against state-of-the-art implementation in order to comment on its validity.",
                        "Citation Paper Authors": "Authors:Tanmay Vilas Samak, Chinmay Vilas Samak, Sivanathan Kandhasamy"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "adopted a convolutional neural network (CNN) to learn off-\nroad obstacle avoidance behavior based on stereo camera feed.\nBuilding on top of the prior works, ",
                    "Citation Text": "M. Bojarski, D. D. Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal,\nL. D. Jackel, M. Monfort, U. Muller, J. Zhang, X. Zhang, J. Zhao, and\nK. Zieba, \u201cEnd to End Learning for Self-Driving Cars,\u201d 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1604.07316",
                        "Citation Paper Title": "Title:End to End Learning for Self-Driving Cars",
                        "Citation Paper Abstract": "Abstract:We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads.\nThe system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads.\nCompared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps.\nWe used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS).",
                        "Citation Paper Authors": "Authors:Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin Zhang, Jake Zhao, Karol Zieba"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.03569v2": {
            "Paper Title": "LiDARTouch: Monocular metric depth estimation with a few-beam LiDAR",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.05084v4": {
            "Paper Title": "Winding Through: Crowd Navigation via Topological Invariance",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": ",\nmaking it amenable for operation in crowded spaces.\nA. Baselines\nWe evaluate our MPC against two baselines from the\nliterature:\nCADRL ",
                    "Citation Text": "M. Everett, Y . F. Chen, and J. P. How. Motion planning\namong dynamic, decision-making agents with deep re-\ninforcement learning. In Proceedings of the IEEE/RSJ\nInternational Conference on Intelligent Robots and Sys-\ntems (IROS) , pages 3052\u20133059, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.01956",
                        "Citation Paper Title": "Title:Motion Planning Among Dynamic, Decision-Making Agents with Deep Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:Robots that navigate among pedestrians use collision avoidance algorithms to enable safe and efficient operation. Recent works present deep reinforcement learning as a framework to model the complex interactions and cooperation. However, they are implemented using key assumptions about other agents' behavior that deviate from reality as the number of agents in the environment increases. This work extends our previous approach to develop an algorithm that learns collision avoidance among a variety of types of dynamic agents without assuming they follow any particular behavior rules. This work also introduces a strategy using LSTM that enables the algorithm to use observations of an arbitrary number of other agents, instead of previous methods that have a fixed observation size. The proposed algorithm outperforms our previous approach in simulation as the number of agents increases, and the algorithm is demonstrated on a fully autonomous robotic vehicle traveling at human walking speed, without the use of a 3D Lidar.",
                        "Citation Paper Authors": "Authors:Michael Everett, Yu Fan Chen, Jonathan P. How"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "is a crowd simulator that is often used in the\nevaluation of crowd navigation algorithms [5, 6, 9, 21, 21, 24,\n25]. We employ the ORCA con\ufb01guration of Chen et al. ",
                    "Citation Text": "C. Chen, Y . Liu, S. Kreiss, and A. Alahi. Crowd-robot\ninteraction: Crowd-aware robot navigation with attention-\nbased deep reinforcement learning. In Proceedings of\nthe IEEE International Conference on Robotics and\nAutomation (ICRA) , pages 6015\u20136022, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.08835",
                        "Citation Paper Title": "Title:Crowd-Robot Interaction: Crowd-aware Robot Navigation with Attention-based Deep Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:Mobility in an effective and socially-compliant manner is an essential yet challenging task for robots operating in crowded spaces. Recent works have shown the power of deep reinforcement learning techniques to learn socially cooperative policies. However, their cooperation ability deteriorates as the crowd grows since they typically relax the problem as a one-way Human-Robot interaction problem. In this work, we want to go beyond first-order Human-Robot interaction and more explicitly model Crowd-Robot Interaction (CRI). We propose to (i) rethink pairwise interactions with a self-attention mechanism, and (ii) jointly model Human-Robot as well as Human-Human interactions in the deep reinforcement learning framework. Our model captures the Human-Human interactions occurring in dense crowds that indirectly affects the robot's anticipation capability. Our proposed attentive pooling mechanism learns the collective importance of neighboring humans with respect to their future states. Various experiments demonstrate that our model can anticipate human dynamics and navigate in crowds with time efficiency, outperforming state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Changan Chen, Yuejiang Liu, Sven Kreiss, Alexandre Alahi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.12081v3": {
            "Paper Title": "Off-policy Reinforcement Learning with Optimistic Exploration and\n  Distribution Correction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.02791v2": {
            "Paper Title": "Motion Planning Transformers: A Motion Planning Framework for Mobile\n  Robots",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": "as edge connectors, while for SST nodes were con-\nstructed by propagating wheel velocity and steering angle.\nThe parameters for SST were set as reported in ",
                    "Citation Text": "Y . Li, Z. Little\ufb01eld, and K. E. Bekris, \u201cAsymptotically optimal\nsampling-based kinodynamic planning,\u201d The Int. Journal of Robot.\nRes., no. 5, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1407.2896",
                        "Citation Paper Title": "Title:Asymptotically Optimal Sampling-based Kinodynamic Planning",
                        "Citation Paper Abstract": "Abstract:Sampling-based algorithms are viewed as practical solutions for high-dimensional motion planning. Recent progress has taken advantage of random geometric graph theory to show how asymptotic optimality can also be achieved with these methods. Achieving this desirable property for systems with dynamics requires solving a two-point boundary value problem (BVP) in the state space of the underlying dynamical system. It is difficult, however, if not impractical, to generate a BVP solver for a variety of important dynamical models of robots or physically simulated ones. Thus, an open challenge was whether it was even possible to achieve optimality guarantees when planning for systems without access to a BVP solver. This work resolves the above question and describes how to achieve asymptotic optimality for kinodynamic planning using incremental sampling-based planners by introducing a new rigorous framework. Two new methods, Stable Sparse-RRT (SST) and SST*, result from this analysis, which are asymptotically near-optimal and optimal, respectively. The techniques are shown to converge fast to high-quality paths, while they maintain only a sparse set of samples, which makes them computationally efficient. The good performance of the planners is confirmed by experimental results using dynamical systems benchmarks, as well as physically simulated robots.",
                        "Citation Paper Authors": "Authors:Yanbo Li, Zakary Littlefield, Kostas E. Bekris"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": ". We used \ufb01xed vectors to\nencode the position and used the following for testing,\nPE(j;2i) = sin\u0012j\n100002i=d\u0013\n(1)\nPE(j;2i+ 1) = cos\u0012j\n10000(2i+1)=d\u0013\n(2)\nj2f0;1;:::;H l\u00001;:::i F\u0003Wl+jF;:::;H l\u0003Wl\u00001g,\nandi2f0;1;2:::;d\u00001gsimilar to ",
                    "Citation Text": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in\nAdvances in Neural Information Processing Systems , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": ". A\ncommon solution is to add learned or \ufb01xed vectors to encode\nthe position of each input ",
                    "Citation Text": "J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y . N. Dauphin,\n\u201cConvolutional sequence to sequence learning,\u201d in Int. Conf. on\nMachine Learning . JMLR.org, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.03122",
                        "Citation Paper Title": "Title:Convolutional Sequence to Sequence Learning",
                        "Citation Paper Abstract": "Abstract:The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.",
                        "Citation Paper Authors": "Authors:Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N. Dauphin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.01840v3": {
            "Paper Title": "Multi-robot task allocation for safe planning against stochastic hazard\n  dynamics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.12111v3": {
            "Paper Title": "Robust Task Scheduling for Heterogeneous Robot Teams under Capability\n  Uncertainty",
            "Sentences": [
                {
                    "Sentence ID": 57,
                    "Sentence": ". It is widely accepted by the \ufb01nance\ncommunity and appears with a growing frequency in recent\nrobotic applications for risk-aware single-robot control ",
                    "Citation Text": "R. Balasubramanian, L. Zhou, P. Tokekar, and P. Sujit, \u201cRisk-aware\nsubmodular optimization for multi-objective travelling salesperson prob-\nlem,\u201d arXiv preprint arXiv:2011.01095 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.01095",
                        "Citation Paper Title": "Title:Risk-Aware Submodular Optimization for Multi-objective Travelling Salesperson Problem",
                        "Citation Paper Abstract": "Abstract:We introduce a risk-aware multi-objective Traveling Salesperson Problem (TSP) variant, where the robot tour cost and tour reward have to be optimized simultaneously. The robot obtains reward along the edges in the graph. We study the case where the rewards and the costs exhibit diminishing marginal gains, i.e., are submodular. Unlike prior work, we focus on the scenario where the costs and the rewards are uncertain and seek to maximize the Conditional-Value-at-Risk (CVaR) metric of the submodular function. We propose a risk-aware greedy algorithm (RAGA) to find a bounded-approximation algorithm. The approximation algorithm runs in polynomial time and is within a constant factor of the optimal and an additive term that depends on the optimal solution. We use the submodular function's curvature to improve approximation results further and verify the algorithm's performance through simulations.",
                        "Citation Paper Authors": "Authors:Rishab Balasubramanian, Lifeng Zhou, Pratap Tokekar, P.B. Sujit"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.01002v3": {
            "Paper Title": "Mixed Observable RRT: Multi-Agent Mission-Planning in Partially\n  Observable Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.04508v2": {
            "Paper Title": "Predictive Visual Tracking: A New Benchmark and Baseline Approach",
            "Sentences": [
                {
                    "Sentence ID": 37,
                    "Sentence": "M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, \u201cMo-\nbilenetv2: Inverted Residuals and Linear Bottlenecks,\u201d in Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR) , 2018, pp. 4510\u20134520. ",
                    "Citation Text": "M. Li, Y .-X. Wang, and D. Ramanan, \u201cTowards Streaming Perception,\u201d\ninProceedings of the European Conference on Computer Vision\n(ECCV) , 2020, pp. 473\u2013488.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.10420",
                        "Citation Paper Title": "Title:Towards Streaming Perception",
                        "Citation Paper Abstract": "Abstract:Embodied perception refers to the ability of an autonomous agent to perceive its environment so that it can (re)act. The responsiveness of the agent is largely governed by latency of its processing pipeline. While past work has studied the algorithmic trade-off between latency and accuracy, there has not been a clear metric to compare different methods along the Pareto optimal latency-accuracy curve. We point out a discrepancy between standard offline evaluation and real-time applications: by the time an algorithm finishes processing a particular frame, the surrounding world has changed. To these ends, we present an approach that coherently integrates latency and accuracy into a single metric for real-time online perception, which we refer to as \"streaming accuracy\". The key insight behind this metric is to jointly evaluate the output of the entire perception stack at every time instant, forcing the stack to consider the amount of streaming data that should be ignored while computation is occurring. More broadly, building upon this metric, we introduce a meta-benchmark that systematically converts any single-frame task into a streaming perception task. We focus on the illustrative tasks of object detection and instance segmentation in urban video streams, and contribute a novel dataset with high-quality and temporally-dense annotations. Our proposed solutions and their empirical analysis demonstrate a number of surprising conclusions: (1) there exists an optimal \"sweet spot\" that maximizes streaming accuracy along the Pareto optimal latency-accuracy curve, (2) asynchronous tracking and future forecasting naturally emerge as internal representations that enable streaming perception, and (3) dynamic scheduling can be used to overcome temporal aliasing, yielding the paradoxical result that latency is sometimes minimized by sitting idle and \"doing nothing\".",
                        "Citation Paper Authors": "Authors:Mengtian Li, Yu-Xiong Wang, Deva Ramanan"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "M. Kristan, J. Matas, A. Leonardis, M. Felsberg, R. P\ufb02ugfelder,\nJoni-Kristian, et al. , \u201cThe Seventh Visual Object Tracking VOT2019\nChallenge Results,\u201d in Proceedings of the IEEE/CVF International\nConference on Computer Vision Workshop (ICCVW) , 2019, pp. 2206\u2013\n2241. ",
                    "Citation Text": "D. Du, Y . Qi, H. Yu, Y . Yang, K. Duan, G. Li, W. Zhang, Q. Huang,\nand Q. Tian, \u201cThe unmanned aerial vehicle benchmark: object de-\ntection and tracking,\u201d in Proceedings of the European Conference on\nComputer Vision (ECCV) , 2018, Conference Proceedings, pp. 370\u2013\n386.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.00518",
                        "Citation Paper Title": "Title:The Unmanned Aerial Vehicle Benchmark: Object Detection and Tracking",
                        "Citation Paper Abstract": "Abstract:With the advantage of high mobility, Unmanned Aerial Vehicles (UAVs) are used to fuel numerous important applications in computer vision, delivering more efficiency and convenience than surveillance cameras with fixed camera angle, scale and view. However, very limited UAV datasets are proposed, and they focus only on a specific task such as visual tracking or object detection in relatively constrained scenarios. Consequently, it is of great importance to develop an unconstrained UAV benchmark to boost related researches. In this paper, we construct a new UAV benchmark focusing on complex scenarios with new level challenges. Selected from 10 hours raw videos, about 80,000 representative frames are fully annotated with bounding boxes as well as up to 14 kinds of attributes (e.g., weather condition, flying altitude, camera view, vehicle category, and occlusion) for three fundamental computer vision tasks: object detection, single object tracking, and multiple object tracking. Then, a detailed quantitative study is performed using most recent state-of-the-art algorithms for each task. Experimental results show that the current state-of-the-art methods perform relative worse on our dataset, due to the new challenges appeared in UAV based real scenes, e.g., high density, small object, and camera motion. To our knowledge, our work is the first time to explore such issues in unconstrained scenes comprehensively.",
                        "Citation Paper Authors": "Authors:Dawei Du, Yuankai Qi, Hongyang Yu, Yifan Yang, Kaiwen Duan, Guorong Li, Weigang Zhang, Qingming Huang, Qi Tian"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "Y . Li, C. Fu, Z. Huang, Y . Zhang, and J. Pan, \u201cKey\ufb01lter-Aware Real-\nTime UA V Object Tracking,\u201d in Proceedings of the IEEE International\nConference on Robotics and Automation (ICRA) , 2020, pp. 193\u2013199. ",
                    "Citation Text": "F. Li, C. Fu, F. Lin, Y . Li, and P. Lu, \u201cTraining-Set Distillation\nfor Real-Time UA V Object Tracking,\u201d in Proceedings of the IEEE\nInternational Conference on Robotics and Automation (ICRA) , 2020,\npp. 9715\u20139721.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.05326",
                        "Citation Paper Title": "Title:Training-Set Distillation for Real-Time UAV Object Tracking",
                        "Citation Paper Abstract": "Abstract:Correlation filter (CF) has recently exhibited promising performance in visual object tracking for unmanned aerial vehicle (UAV). Such online learning method heavily depends on the quality of the training-set, yet complicated aerial scenarios like occlusion or out of view can reduce its reliability. In this work, a novel time slot-based distillation approach is proposed to efficiently and effectively optimize the training-set's quality on the fly. A cooperative energy minimization function is established to score the historical samples adaptively. To accelerate the scoring process, frames with high confident tracking results are employed as the keyframes to divide the tracking process into multiple time slots. After the establishment of a new slot, the weighted fusion of the previous samples generates one key-sample, in order to reduce the number of samples to be scored. Besides, when the current time slot exceeds the maximum frame number, which can be scored, the sample with the lowest score will be discarded. Consequently, the training-set can be efficiently and reliably distilled. Comprehensive tests on two well-known UAV benchmarks prove the effectiveness of our method with real-time speed on a single CPU.",
                        "Citation Paper Authors": "Authors:Fan Li, Changhong Fu, Fuling Lin, Yiming Li, Peng Lu"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "Y . Li, C. Fu, F. Ding, Z. Huang, and J. Pan, \u201cAugmented Memory\nfor Correlation Filters in Real-Time UA V Tracking,\u201d in Proceedings\nof the IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS) , 2020, pp. 1559\u20131566. ",
                    "Citation Text": "Y . Li, C. Fu, Z. Huang, Y . Zhang, and J. Pan, \u201cKey\ufb01lter-Aware Real-\nTime UA V Object Tracking,\u201d in Proceedings of the IEEE International\nConference on Robotics and Automation (ICRA) , 2020, pp. 193\u2013199.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.05218",
                        "Citation Paper Title": "Title:Keyfilter-Aware Real-Time UAV Object Tracking",
                        "Citation Paper Abstract": "Abstract:Correlation filter-based tracking has been widely applied in unmanned aerial vehicle (UAV) with high efficiency. However, it has two imperfections, i.e., boundary effect and filter corruption. Several methods enlarging the search area can mitigate boundary effect, yet introducing undesired background distraction. Existing frame-by-frame context learning strategies for repressing background distraction nevertheless lower the tracking speed. Inspired by keyframe-based simultaneous localization and mapping, keyfilter is proposed in visual tracking for the first time, in order to handle the above issues efficiently and effectively. Keyfilters generated by periodically selected keyframes learn the context intermittently and are used to restrain the learning of filters, so that 1) context awareness can be transmitted to all the filters via keyfilter restriction, and 2) filter corruption can be repressed. Compared to the state-of-the-art results, our tracker performs better on two challenging benchmarks, with enough speed for UAV real-time applications.",
                        "Citation Paper Authors": "Authors:Yiming Li, Changhong Fu, Ziyuan Huang, Yinqiang Zhang, Jia Pan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.00210v3": {
            "Paper Title": "EventPoint: Self-Supervised Interest Point Detection and Description for\n  Event-based Camera",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": "on event-stream is de-\nsigned by tuning throughout of Time-surface and refactor-\ning the conventional Harris algorithm. In ",
                    "Citation Text": "Jacques Manderscheid, Amos Sironi, Nicolas Bourdis, Da-\nvide Migliore, and Vincent Lepetit. Speed invariant time\nsurface for learning to detect corner points with event-based\ncameras. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 10245\u2013\n10254, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.11332",
                        "Citation Paper Title": "Title:Speed Invariant Time Surface for Learning to Detect Corner Points with Event-Based Cameras",
                        "Citation Paper Abstract": "Abstract:We propose a learning approach to corner detection for event-based cameras that is stable even under fast and abrupt motions. Event-based cameras offer high temporal resolution, power efficiency, and high dynamic range. However, the properties of event-based data are very different compared to standard intensity images, and simple extensions of corner detection methods designed for these images do not perform well on event-based data. We first introduce an efficient way to compute a time surface that is invariant to the speed of the objects. We then show that we can train a Random Forest to recognize events generated by a moving corner from our time surface. Random Forests are also extremely efficient, and therefore a good choice to deal with the high capture frequency of event-based cameras ---our implementation processes up to 1.6Mev/s on a single CPU. Thanks to our time surface formulation and this learning approach, our method is significantly more robust to abrupt changes of direction of the corners compared to previous ones. Our method also naturally assigns a confidence score for the corners, which can be useful for postprocessing. Moreover, we introduce a high-resolution dataset suitable for quantitative evaluation and comparison of corner detection methods for event-based cameras. We call our approach SILC, for Speed Invariant Learned Corners, and compare it to the state-of-the-art with extensive experiments, showing better performance.",
                        "Citation Paper Authors": "Authors:Jacques Manderscheid, Amos Sironi, Nicolas Bourdis, Davide Migliore, Vincent Lepetit"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "proposes a siamese network to\nlearn the detector and descriptor, the interest points\u2019 posi-\ntions are learned in a regression manner. R2D2 ",
                    "Citation Text": "Jerome Revaud, Philippe Weinzaepfel, C \u00b4esar De Souza, Noe\nPion, Gabriela Csurka, Yohann Cabon, and Martin Humen-\nberger. R2d2: repeatable and reliable detector and descriptor.\narXiv preprint arXiv:1906.06195 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.06195",
                        "Citation Paper Title": "Title:R2D2: Repeatable and Reliable Detector and Descriptor",
                        "Citation Paper Abstract": "Abstract:Interest point detection and local feature description are fundamental steps in many computer vision applications. Classical methods for these tasks are based on a detect-then-describe paradigm where separate handcrafted methods are used to first identify repeatable keypoints and then represent them with a local descriptor. Neural networks trained with metric learning losses have recently caught up with these techniques, focusing on learning repeatable saliency maps for keypoint detection and learning descriptors at the detected keypoint locations. In this work, we argue that salient regions are not necessarily discriminative, and therefore can harm the performance of the description. Furthermore, we claim that descriptors should be learned only in regions for which matching can be performed with high confidence. We thus propose to jointly learn keypoint detection and description together with a predictor of the local descriptor discriminativeness. This allows us to avoid ambiguous areas and leads to reliable keypoint detections and descriptions. Our detection-and-description approach, trained with self-supervision, can simultaneously output sparse, repeatable and reliable keypoints that outperforms state-of-the-art detectors and descriptors on the HPatches dataset. It also establishes a record on the recently released Aachen Day-Night localization dataset.",
                        "Citation Paper Authors": "Authors:Jerome Revaud, Philippe Weinzaepfel, C\u00e9sar De Souza, Noe Pion, Gabriela Csurka, Yohann Cabon, Martin Humenberger"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "proposes a self-supervised learning\nmethod using pseudo-ground-truth correspondences gener-\nated by homographic transformation. It designs a dual-head\nnetwork for interested point detection and description sep-\narately. Unsuperpoint ",
                    "Citation Text": "Peter Hviid Christiansen, Mikkel Fly Kragh, Yury Brodskiy,\nand Henrik Karstoft. Unsuperpoint: End-to-end unsuper-\nvised interest point detector and descriptor. arXiv preprint\narXiv:1907.04011 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.04011",
                        "Citation Paper Title": "Title:UnsuperPoint: End-to-end Unsupervised Interest Point Detector and Descriptor",
                        "Citation Paper Abstract": "Abstract:It is hard to create consistent ground truth data for interest points in natural images, since interest points are hard to define clearly and consistently for a human annotator. This makes interest point detectors non-trivial to build. In this work, we introduce an unsupervised deep learning-based interest point detector and descriptor. Using a self-supervised approach, we utilize a siamese network and a novel loss function that enables interest point scores and positions to be learned automatically. The resulting interest point detector and descriptor is UnsuperPoint. We use regression of point positions to 1) make UnsuperPoint end-to-end trainable and 2) to incorporate non-maximum suppression in the model. Unlike most trainable detectors, it requires no generation of pseudo ground truth points, no structure-from-motion-generated representations and the model is learned from only one round of training. Furthermore, we introduce a novel loss function to regularize network predictions to be uniformly distributed. UnsuperPoint runs in real-time with 323 frames per second (fps) at a resolution of $224\\times320$ and 90 fps at $480\\times640$. It is comparable or better than state-of-the-art performance when measured for speed, repeatability, localization, matching score and homography estimation on the HPatch dataset.",
                        "Citation Paper Authors": "Authors:Peter Hviid Christiansen, Mikkel Fly Kragh, Yury Brodskiy, Henrik Karstoft"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.05969v3": {
            "Paper Title": "Dynamics-Regulated Kinematic Policy for Egocentric Pose Estimation",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": "use a kinematics-based approach where they\nconstruct a motion graph from the training data and recover the pose sequence by solving the optimal\npose path. Ng et al. ",
                    "Citation Text": "Evonne Ng, Donglai Xiang, Hanbyul Joo, and Kristen Grauman. You2me: Inferring body pose in\negocentric video via \ufb01rst and second person interactions. CoRR , abs/1904.09882, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.09882",
                        "Citation Paper Title": "Title:You2Me: Inferring Body Pose in Egocentric Video via First and Second Person Interactions",
                        "Citation Paper Abstract": "Abstract:The body pose of a person wearing a camera is of great interest for applications in augmented reality, healthcare, and robotics, yet much of the person's body is out of view for a typical wearable camera. We propose a learning-based approach to estimate the camera wearer's 3D body pose from egocentric video sequences. Our key insight is to leverage interactions with another person---whose body pose we can directly observe---as a signal inherently linked to the body pose of the first-person subject. We show that since interactions between individuals often induce a well-ordered series of back-and-forth responses, it is possible to learn a temporal model of the interlinked poses even though one party is largely out of view. We demonstrate our idea on a variety of domains with dyadic interaction and show the substantial impact on egocentric body pose estimation, which improves the state of the art. Video results are available at this http URL",
                        "Citation Paper Authors": "Authors:Evonne Ng, Donglai Xiang, Hanbyul Joo, Kristen Grauman"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.12612v3": {
            "Paper Title": "Towards Disturbance-Free Visual Mobile Manipulation",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": ", while\nself-supervised/unsupervised auxiliary tasks use existing in-\nformation as signals, such as auto-encoders [49, 28, 103],\nforward ",
                    "Citation Text": "Karol Gregor, Danilo Jimenez Rezende, Frederic Besse,\nYan Wu, Hamza Merzic, and A \u00a8aron van den Oord. Shaping\nbelief states with generative environment models for RL.\nInAdvances in Neural Information Processing Systems 32:\nAnnual Conference on Neural Information Processing Sys-\ntems 2019, NeurIPS 2019, December 8-14, 2019, Vancou-\nver, BC, Canada , 2019. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.09237",
                        "Citation Paper Title": "Title:Shaping Belief States with Generative Environment Models for RL",
                        "Citation Paper Abstract": "Abstract:When agents interact with a complex environment, they must form and maintain beliefs about the relevant aspects of that environment. We propose a way to efficiently train expressive generative models in complex environments. We show that a predictive algorithm with an expressive generative model can form stable belief-states in visually rich and dynamic 3D environments. More precisely, we show that the learned representation captures the layout of the environment as well as the position and orientation of the agent. Our experiments show that the model substantially improves data-efficiency on a number of reinforcement learning (RL) tasks compared with strong model-free baseline agents. We find that predicting multiple steps into the future (overshooting), in combination with an expressive generative model, is critical for stable representations to emerge. In practice, using expressive generative models in RL is computationally expensive and we propose a scheme to reduce this computational burden, allowing us to build agents that are competitive with model-free baselines.",
                        "Citation Paper Authors": "Authors:Karol Gregor, Danilo Jimenez Rezende, Frederic Besse, Yan Wu, Hamza Merzic, Aaron van den Oord"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.12673v2": {
            "Paper Title": "Adaptively Calibrated Critic Estimates for Deep Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.12177v2": {
            "Paper Title": "Imitation Learning: Progress, Taxonomies and Challenges",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": "Detailed classified recent IfO methods\nImitating Latent Policies\nfrom Observation ",
                    "Citation Text": "Ashley Edwards, Himanshu Sahni, Yannick Schroecker, and Charles Isbell. 2019. Imitating latent policies from\nobservation. In International Conference on Machine Learning . PMLR, 1755\u20131763.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.07914",
                        "Citation Paper Title": "Title:Imitating Latent Policies from Observation",
                        "Citation Paper Abstract": "Abstract:In this paper, we describe a novel approach to imitation learning that infers latent policies directly from state observations. We introduce a method that characterizes the causal effects of latent actions on observations while simultaneously predicting their likelihood. We then outline an action alignment procedure that leverages a small amount of environment interactions to determine a mapping between the latent and real-world actions. We show that this corrected labeling can be used for imitating the observed behavior, even though no expert actions are given. We evaluate our approach within classic control environments and a platform game and demonstrate that it performs better than standard approaches. Code for this work is available at this https URL.",
                        "Citation Paper Authors": "Authors:Ashley D. Edwards, Himanshu Sahni, Yannick Schroecker, Charles L. Isbell"
                    }
                },
                {
                    "Sentence ID": 72,
                    "Sentence": "Using distance between observations to predict and\npenalize the actions\nIfO survey ",
                    "Citation Text": "Faraz Torabi, Garrett Warnell, and Peter Stone. 2019. Recent advances in imitation learning from observation. arXiv\npreprint arXiv:1905.13566 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.13566",
                        "Citation Paper Title": "Title:Recent Advances in Imitation Learning from Observation",
                        "Citation Paper Abstract": "Abstract:Imitation learning is the process by which one agent tries to learn how to perform a certain task using information generated by another, often more-expert agent performing that same task. Conventionally, the imitator has access to both state and action information generated by an expert performing the task (e.g., the expert may provide a kinesthetic demonstration of object placement using a robotic arm). However, requiring the action information prevents imitation learning from a large number of existing valuable learning resources such as online videos of humans performing tasks. To overcome this issue, the specific problem of imitation from observation (IfO) has recently garnered a great deal of attention, in which the imitator only has access to the state information (e.g., video frames) generated by the expert. In this paper, we provide a literature review of methods developed for IfO, and then point out some open research problems and potential future work.",
                        "Citation Paper Authors": "Authors:Faraz Torabi, Garrett Warnell, Peter Stone"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": "Extracting features from unlabeled and\nunaligned gameplay footage\nZero-Shot Visual Imitation ",
                    "Citation Text": "Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Fred Shentu, Evan Shelhamer,\nJitendra Malik, Alexei A. Efros, and Trevor Darrell. 2018. Zero-Shot Visual Imitation. In 2018 IEEE/CVF Conference\nOctober 2022.20 Boyuan et. al.\non Computer Vision and Pattern Recognition Workshops (CVPRW) . IEEE, Salt Lake City, UT, USA, 2131\u201321313. https:\n//doi.org/10.1109/CVPRW.2018.00278",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.08606",
                        "Citation Paper Title": "Title:Zero-Shot Visual Imitation",
                        "Citation Paper Abstract": "Abstract:The current dominant paradigm for imitation learning relies on strong supervision of expert actions to learn both 'what' and 'how' to imitate. We pursue an alternative paradigm wherein an agent first explores the world without any expert supervision and then distills its experience into a goal-conditioned skill policy with a novel forward consistency loss. In our framework, the role of the expert is only to communicate the goals (i.e., what to imitate) during inference. The learned policy is then employed to mimic the expert (i.e., how to imitate) after seeing just a sequence of images demonstrating the desired task. Our method is 'zero-shot' in the sense that the agent never has access to expert actions during training or for the task demonstration at inference. We evaluate our zero-shot imitator in two real-world settings: complex rope manipulation with a Baxter robot and navigation in previously unseen office environments with a TurtleBot. Through further experiments in VizDoom simulation, we provide evidence that better mechanisms for exploration lead to learning a more capable policy which in turn improves end task performance. Videos, models, and more details are available at this https URL",
                        "Citation Paper Authors": "Authors:Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu, Evan Shelhamer, Jitendra Malik, Alexei A. Efros, Trevor Darrell"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "are all making improvement on GAIL by combining with other method like hindsight\nrelabeling and Deep Deterministic Policy Gradient (DDPG) ",
                    "Citation Text": "Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan\nWierstra. 2019. Continuous control with deep reinforcement learning. arXiv:1509.02971 [cs.LG]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1509.02971",
                        "Citation Paper Title": "Title:Continuous control with deep reinforcement learning",
                        "Citation Paper Abstract": "Abstract:We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.",
                        "Citation Paper Authors": "Authors:Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": ".\nOn the other hand, recent IRL methods mainly focus on the topics such as: extending GAIL with\nother methods or problem settings ",
                    "Citation Text": "Yiming Ding, Carlos Florensa, Mariano Phielipp, and Pieter Abbeel. 2019. Goal-conditioned imitation learning. arXiv\npreprint arXiv:1906.05838 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.05838",
                        "Citation Paper Title": "Title:Goal-conditioned Imitation Learning",
                        "Citation Paper Abstract": "Abstract:Designing rewards for Reinforcement Learning (RL) is challenging because it needs to convey the desired task, be efficient to optimize, and be easy to compute. The latter is particularly problematic when applying RL to robotics, where detecting whether the desired configuration is reached might require considerable supervision and instrumentation. Furthermore, we are often interested in being able to reach a wide range of configurations, hence setting up a different reward every time might be unpractical. Methods like Hindsight Experience Replay (HER) have recently shown promise to learn policies able to reach many goals, without the need of a reward. Unfortunately, without tricks like resetting to points along the trajectory, HER might require many samples to discover how to reach certain areas of the state-space. In this work we investigate different approaches to incorporate demonstrations to drastically speed up the convergence to a policy able to reach any goal, also surpassing the performance of an agent trained with other Imitation Learning algorithms. Furthermore, we show our method can also be used when the available expert trajectories do not contain the actions, which can leverage kinesthetic or third person demonstration. The code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Yiming Ding, Carlos Florensa, Mariano Phielipp, Pieter Abbeel"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": "was proposed,\nit mixed a new policy \u02c6\ud835\udf0b\ud835\udc5b+1with a fixed probability \ud835\udefcas next policy, this method promotes the\ndevelopment of IL and set up the foundation for the later proposed DAgger ",
                    "Citation Text": "St\u00e9phane Ross, Geoffrey Gordon, and Drew Bagnell. 2011. A reduction of imitation learning and structured prediction\nto no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and\nstatistics . JMLR Workshop and Conference Proceedings, 627\u2013635.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1011.0686",
                        "Citation Paper Title": "Title:A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning",
                        "Citation Paper Abstract": "Abstract:Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem.",
                        "Citation Paper Authors": "Authors:Stephane Ross, Geoffrey J. Gordon, J. Andrew Bagnell"
                    }
                },
                {
                    "Sentence ID": 78,
                    "Sentence": ". Other interesting\nresearch fields like meta-learning[ 18,20,27], multi-agent learning ",
                    "Citation Text": "Eric Zhan, Stephan Zheng, Yisong Yue, Long Sha, and Patrick Lucey. 2018. Generating multi-agent trajectories using\nprogrammatic weak supervision. arXiv preprint arXiv:1803.07612 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.07612",
                        "Citation Paper Title": "Title:Generating Multi-Agent Trajectories using Programmatic Weak Supervision",
                        "Citation Paper Abstract": "Abstract:We study the problem of training sequential generative models for capturing coordinated multi-agent trajectory behavior, such as offensive basketball gameplay. When modeling such settings, it is often beneficial to design hierarchical models that can capture long-term coordination using intermediate variables. Furthermore, these intermediate variables should capture interesting high-level behavioral semantics in an interpretable and manipulatable way. We present a hierarchical framework that can effectively learn such sequential generative models. Our approach is inspired by recent work on leveraging programmatically produced weak labels, which we extend to the spatiotemporal regime. In addition to synthetic settings, we show how to instantiate our framework to effectively model complex interactions between basketball players and generate realistic multi-agent trajectories of basketball gameplay over long time periods. We validate our approach using both quantitative and qualitative evaluations, including a user study comparison conducted with professional sports analysts.",
                        "Citation Paper Authors": "Authors:Eric Zhan, Stephan Zheng, Yisong Yue, Long Sha, Patrick Lucey"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "provide more robust and efficient\napproaches for IRL methods; as for the sparse reward function, Hindsight Experience Replay ",
                    "Citation Text": "Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin,\nOpenAI Pieter Abbeel, and Wojciech Zaremba. 2017. Hindsight experience replay. In Advances in neural information\nprocessing systems . 5048\u20135058.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.01495",
                        "Citation Paper Title": "Title:Hindsight Experience Replay",
                        "Citation Paper Abstract": "Abstract:Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum.\nWe demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.",
                        "Citation Paper Authors": "Authors:Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, Wojciech Zaremba"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "which uses small amount of demonstration to significantly accelerate the training process\nby doing pre-training to kick-off and learning from both demonstration and self-generated data.\nLater methods like T-REX ",
                    "Citation Text": "Daniel Brown, Wonjoon Goo, Prabhat Nagarajan, and Scott Niekum. 2019. Extrapolating beyond suboptimal demon-\nstrations via inverse reinforcement learning from observations. In International Conference on Machine Learning . PMLR,\n783\u2013792.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.06387",
                        "Citation Paper Title": "Title:Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations",
                        "Citation Paper Abstract": "Abstract:A critical flaw of existing inverse reinforcement learning (IRL) methods is their inability to significantly outperform the demonstrator. This is because IRL typically seeks a reward function that makes the demonstrator appear near-optimal, rather than inferring the underlying intentions of the demonstrator that may have been poorly executed in practice. In this paper, we introduce a novel reward-learning-from-observation algorithm, Trajectory-ranked Reward EXtrapolation (T-REX), that extrapolates beyond a set of (approximately) ranked demonstrations in order to infer high-quality reward functions from a set of potentially poor demonstrations. When combined with deep reinforcement learning, T-REX outperforms state-of-the-art imitation learning and IRL methods on multiple Atari and MuJoCo benchmark tasks and achieves performance that is often more than twice the performance of the best demonstration. We also demonstrate that T-REX is robust to ranking noise and can accurately extrapolate intention by simply watching a learner noisily improve at a task over time.",
                        "Citation Paper Authors": "Authors:Daniel S. Brown, Wonjoon Goo, Prabhat Nagarajan, Scott Niekum"
                    }
                },
                {
                    "Sentence ID": 65,
                    "Sentence": "and became one of the hot topics in IL. Later research like[ 17,33,\n65,76] were proposed inspired by GAIL and other generative models were gradually adopted\nin IL. Besides GAIL, another important research direction is inspired by Stadie et al. ",
                    "Citation Text": "Bradly C Stadie, Pieter Abbeel, and Ilya Sutskever. 2017. Third-person imitation learning. arXiv preprint arXiv:1703.01703\n(2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.01703",
                        "Citation Paper Title": "Title:Third-Person Imitation Learning",
                        "Citation Paper Abstract": "Abstract:Reinforcement learning (RL) makes it possible to train agents capable of achieving sophisticated goals in complex and uncertain environments. A key difficulty in reinforcement learning is specifying a reward function for the agent to optimize. Traditionally, imitation learning in RL has been used to overcome this problem. Unfortunately, hitherto imitation learning methods tend to require that demonstrations are supplied in the first-person: the agent is provided with a sequence of states and a specification of the actions that it should have taken. While powerful, this kind of imitation learning is limited by the relatively hard problem of collecting first-person demonstrations. Humans address this problem by learning from third-person demonstrations: they observe other humans perform tasks, infer the task, and accomplish the same task themselves.\nIn this paper, we present a method for unsupervised third-person imitation learning. Here third-person refers to training an agent to correctly achieve a simple goal in a simple environment when it is provided a demonstration of a teacher achieving the same goal but from a different viewpoint; and unsupervised refers to the fact that the agent receives only these third-person demonstrations, and is not provided a correspondence between teacher states and student states. Our methods primary insight is that recent advances from domain confusion can be utilized to yield domain agnostic features which are crucial during the training process. To validate our approach, we report successful experiments on learning from third-person demonstrations in a pointmass domain, a reacher domain, and inverted pendulum.",
                        "Citation Paper Authors": "Authors:Bradly C. Stadie, Pieter Abbeel, Ilya Sutskever"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.09779v2": {
            "Paper Title": "Transferring Dexterous Manipulation from GPU Simulation to a Remote\n  Real-World TriFinger",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.03793v3": {
            "Paper Title": "Learning Collision-free and Torque-limited Robot Trajectories based on\n  Alternative Safe Behaviors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.10892v3": {
            "Paper Title": "The Design of Stretch: A Compact, Lightweight Mobile Manipulator for\n  Indoor Human Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.03032v3": {
            "Paper Title": "Learning Multi-Objective Curricula for Robotic Policy Learning",
            "Sentences": [
                {
                    "Sentence ID": 64,
                    "Sentence": "which is a popular policy gradient algorithm that learns a stochastic policy.\n5 Experiments\nWe evaluate and analyze our proposed MOC DRL on the CausalWorld ",
                    "Citation Text": "O. Ahmed, F. Tr \u00a8auble, A. Goyal, A. Neitz, M. Wuthrich, Y . Bengio, B. Sch \u00a8olkopf, and S. Bauer.\nCausalworld: A robotic manipulation benchmark for causal structure and transfer learning. In\nICLR . OpenReview.net, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.04296",
                        "Citation Paper Title": "Title:CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning",
                        "Citation Paper Abstract": "Abstract:Despite recent successes of reinforcement learning (RL), it remains a challenge for agents to transfer learned skills to related environments. To facilitate research addressing this problem, we propose CausalWorld, a benchmark for causal structure and transfer learning in a robotic manipulation environment. The environment is a simulation of an open-source robotic platform, hence offering the possibility of sim-to-real transfer. Tasks consist of constructing 3D shapes from a given set of blocks - inspired by how children learn to build complex structures. The key strength of CausalWorld is that it provides a combinatorial family of such tasks with common causal structure and underlying factors (including, e.g., robot and object masses, colors, sizes). The user (or the agent) may intervene on all causal variables, which allows for fine-grained control over how similar different tasks (or task distributions) are. One can thus easily define training and evaluation distributions of a desired difficulty level, targeting a specific form of generalization (e.g., only changes in appearance or object mass). Further, this common parametrization facilitates defining curricula by interpolating between an initial and a target task. While users may define their own task distributions, we present eight meaningful distributions as concrete benchmarks, ranging from simple to very challenging, all of which require long-horizon planning as well as precise low-level motor control. Finally, we provide baseline results for a subset of these tasks on distinct training curricula and corresponding evaluation protocols, verifying the feasibility of the tasks in this benchmark.",
                        "Citation Paper Authors": "Authors:Ossama Ahmed, Frederik Tr\u00e4uble, Anirudh Goyal, Alexander Neitz, Yoshua Bengio, Bernhard Sch\u00f6lkopf, Manuel W\u00fcthrich, Stefan Bauer"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": ":initial state generator, sub-goal state generator, andreward shaping generator . Our approach\ncan be easily extended to include other forms of curricula ( e.g., selecting environments from a\ndiscrete set ",
                    "Citation Text": "T. Matiisen, A. Oliver, T. Cohen, and J. Schulman. Teacher\u2013student curriculum learning. IEEE\ntransactions on neural networks and learning systems , 31(9):3732\u20133740, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.00183",
                        "Citation Paper Title": "Title:Teacher-Student Curriculum Learning",
                        "Citation Paper Abstract": "Abstract:We propose Teacher-Student Curriculum Learning (TSCL), a framework for automatic curriculum learning, where the Student tries to learn a complex task and the Teacher automatically chooses subtasks from a given set for the Student to train on. We describe a family of Teacher algorithms that rely on the intuition that the Student should practice more those tasks on which it makes the fastest progress, i.e. where the slope of the learning curve is highest. In addition, the Teacher algorithms address the problem of forgetting by also choosing tasks where the Student's performance is getting worse. We demonstrate that TSCL matches or surpasses the results of carefully hand-crafted curricula in two tasks: addition of decimal numbers with LSTM and navigation in Minecraft. Using our automatically generated curriculum enabled to solve a Minecraft maze that could not be solved at all when training directly on solving the maze, and the learning was an order of magnitude faster than uniform sampling of subtasks.",
                        "Citation Paper Authors": "Authors:Tambet Matiisen, Avital Oliver, Taco Cohen, John Schulman"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "can be applied to DRL in a variety of\nways, including adapting initial states [ 6,23], shaping reward functions [ 24,25], or generating goals\n[8,15,26,27]. In a closely related work ",
                    "Citation Text": "R. Portelas, C. Romac, K. Hofmann, and P.-Y . Oudeyer. Meta automatic curriculum learning.\narXiv preprint arXiv:2011.08463 , 2020.\n9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.08463",
                        "Citation Paper Title": "Title:Meta Automatic Curriculum Learning",
                        "Citation Paper Abstract": "Abstract:A major challenge in the Deep RL (DRL) community is to train agents able to generalize their control policy over situations never seen in training. Training on diverse tasks has been identified as a key ingredient for good generalization, which pushed researchers towards using rich procedural task generation systems controlled through complex continuous parameter spaces. In such complex task spaces, it is essential to rely on some form of Automatic Curriculum Learning (ACL) to adapt the task sampling distribution to a given learning agent, instead of randomly sampling tasks, as many could end up being either trivial or unfeasible. Since it is hard to get prior knowledge on such task spaces, many ACL algorithms explore the task space to detect progress niches over time, a costly tabula-rasa process that needs to be performed for each new learning agents, although they might have similarities in their capabilities profiles. To address this limitation, we introduce the concept of Meta-ACL, and formalize it in the context of black-box RL learners, i.e. algorithms seeking to generalize curriculum generation to an (unknown) distribution of learners. In this work, we present AGAIN, a first instantiation of Meta-ACL, and showcase its benefits for curriculum generation over classical ACL in multiple simulated environments including procedurally generated parkour environments with learners of varying morphologies. Videos and code are available at this https URL .",
                        "Citation Paper Authors": "Authors:R\u00e9my Portelas, Cl\u00e9ment Romac, Katja Hofmann, Pierre-Yves Oudeyer"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "Curriculum learning. Automatic curriculum learning (ACL) for deep reinforcement learning\n(DRL) [ 18,5,19,20,21] has recently emerged as a promising tool to learn how to adapt an agent\u2019s\nlearning tasks based on its capacity during training. ACL ",
                    "Citation Text": "A. Graves, M. G. Bellemare, J. Menick, R. Munos, and K. Kavukcuoglu. Automated curriculum\nlearning for neural networks. In international conference on machine learning , pages 1311\u2013\n1320. PMLR, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.03003",
                        "Citation Paper Title": "Title:Automated Curriculum Learning for Neural Networks",
                        "Citation Paper Abstract": "Abstract:We introduce a method for automatically selecting the path, or syllabus, that a neural network follows through a curriculum so as to maximise learning efficiency. A measure of the amount that the network learns from each data sample is provided as a reward signal to a nonstationary multi-armed bandit algorithm, which then determines a stochastic syllabus. We consider a range of signals derived from two distinct indicators of learning progress: rate of increase in prediction accuracy, and rate of increase in network complexity. Experimental results for LSTM networks on three curricula demonstrate that our approach can significantly accelerate learning, in some cases halving the time required to attain a satisfactory performance level.",
                        "Citation Paper Authors": "Authors:Alex Graves, Marc G. Bellemare, Jacob Menick, Remi Munos, Koray Kavukcuoglu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.09047v3": {
            "Paper Title": "Know Thyself: Transferable Visual Control Policies Through\n  Robot-Awareness",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.00060v2": {
            "Paper Title": "Automating Internet of Things Network Traffic Collection with Robotic\n  Arm Interactions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08202v4": {
            "Paper Title": "Manipulation-Oriented Object Perception in Clutter through Affordance\n  Coordinate Frames",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": "in Unreal Engine 4. We select several\nhand-scale objects that support robot grasping and manipu-\nlation for the drink-serving task. The object models include\nthe bottle, mug and bowl categories from the ShapeNet\ndataset ",
                    "Citation Text": "A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li,\nS. Savarese, M. Savva, S. Song, H. Su, J. Xiao, L. Yi, and F. Yu,\n\u201cShapeNet: An Information-Rich 3D Model Repository,\u201d Tech. Rep.\narXiv:1512.03012 [cs.GR], 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.03012",
                        "Citation Paper Title": "Title:ShapeNet: An Information-Rich 3D Model Repository",
                        "Citation Paper Abstract": "Abstract:We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.",
                        "Citation Paper Authors": "Authors:Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, Fisher Yu"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ". In the robotics community,\nworks have focused on the problem of object localization in\nclutter for the purpose of robotic pick and place tasks ",
                    "Citation Text": "Z. Zeng, Z. Zhou, Z. Sui, and O. C. Jenkins, \u201cSemantic robot\nprogramming for goal-directed manipulation in cluttered scenes,\u201d in\nICRA . IEEE, 2018, pp. 7462\u20137469.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.01189",
                        "Citation Paper Title": "Title:Semantic Robot Programming for Goal-Directed Manipulation in Cluttered Scenes",
                        "Citation Paper Abstract": "Abstract:We present the Semantic Robot Programming (SRP) paradigm as a convergence of robot programming by demonstration and semantic mapping. In SRP, a user can directly program a robot manipulator by demonstrating a snapshot of their intended goal scene in workspace. The robot then parses this goal as a scene graph comprised of object poses and inter-object relations, assuming known object geometries. Task and motion planning is then used to realize the user's goal from an arbitrary initial scene configuration. Even when faced with different initial scene configurations, SRP enables the robot to seamlessly adapt to reach the user's demonstrated goal. For scene perception, we propose the Discriminatively-Informed Generative Estimation of Scenes and Transforms (DIGEST) method to infer the initial and goal states of the world from RGBD images. The efficacy of SRP with DIGEST perception is demonstrated for the task of tray-setting with a Michigan Progress Fetch robot. Scene perception and task execution are evaluated with a public household occlusion dataset and our cluttered scene dataset.",
                        "Citation Paper Authors": "Authors:Zhen Zeng, Zheming Zhou, Zhiqiang Sui, Odest Chadwicke Jenkins"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.04729v2": {
            "Paper Title": "Humans' Assessment of Robots as Moral Regulators: Importance of\n  Perceived Fairness and Legitimacy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.09391v2": {
            "Paper Title": "Learning Setup Policies: Reliable Transition Between Locomotion\n  Behaviours",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.10997v6": {
            "Paper Title": "MVGrasp: Real-Time Multi-View 3D Object Grasping in Highly Cluttered\n  Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.03645v2": {
            "Paper Title": "Soft Robots Modeling: a Structured Overview",
            "Sentences": [
                {
                    "Sentence ID": 158,
                    "Sentence": "couples the pseudo-rigid model discretization of Sec. VII-B\nwith a rigid body physics engine of Python to model soft robots.\nChainQueen ",
                    "Citation Text": "Y . Hu, J. Liu, A. Spielberg, J. B. Tenenbaum, W. T. Freeman, J. Wu,\nD. Rus, and W. Matusik, \u201cChainqueen: A real-time differentiable\nphysical simulator for soft robotics,\u201d in IEEE Int. Conf. Robot. Autom. ,\npp. 6265\u20136271, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.01054",
                        "Citation Paper Title": "Title:ChainQueen: A Real-Time Differentiable Physical Simulator for Soft Robotics",
                        "Citation Paper Abstract": "Abstract:Physical simulators have been widely used in robot planning and control. Among them, differentiable simulators are particularly favored, as they can be incorporated into gradient-based optimization algorithms that are efficient in solving inverse problems such as optimal control and motion planning. Simulating deformable objects is, however, more challenging compared to rigid body dynamics. The underlying physical laws of deformable objects are more complex, and the resulting systems have orders of magnitude more degrees of freedom and therefore they are significantly more computationally expensive to simulate. Computing gradients with respect to physical design or controller parameters is typically even more computationally challenging. In this paper, we propose a real-time, differentiable hybrid Lagrangian-Eulerian physical simulator for deformable objects, ChainQueen, based on the Moving Least Squares Material Point Method (MLS-MPM). MLS-MPM can simulate deformable objects including contact and can be seamlessly incorporated into inference, control and co-design systems. We demonstrate that our simulator achieves high precision in both forward simulation and backward gradient computation. We have successfully employed it in a diverse set of control tasks for soft robots, including problems with nearly 3,000 decision variables.",
                        "Citation Paper Authors": "Authors:Yuanming Hu, Jiancheng Liu, Andrew Spielberg, Joshua B. Tenenbaum, William T. Freeman, Jiajun Wu, Daniela Rus, Wojciech Matusik"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.11774v2": {
            "Paper Title": "Learning Stable Vector Fields on Lie Groups",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.14687v2": {
            "Paper Title": "Guaranteed Rejection-free Sampling Method Using Past Behaviours for\n  Motion Planning of Autonomous Systems",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": "combines sampling-based and\noptimisation-based planning, while approximating the con-\n\ufb01guration space. ",
                    "Citation Text": "J. Bialkowski, M. Otte, and E. Frazzoli, \u201cFree-con\ufb01guration biased sam-\npling for motion planning,\u201d in 2013 IEEE/RSJ International Conference\non Intelligent Robots and Systems . IEEE, 2013, pp. 1272\u20131279.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1311.0541",
                        "Citation Paper Title": "Title:Free-configuration Biased Sampling for Motion Planning: Errata",
                        "Citation Paper Abstract": "Abstract:This document contains improved and updated proofs of convergence for the sampling method presented in our paper \"Free-configuration Biased Sampling for Motion Planning\".",
                        "Citation Paper Authors": "Authors:Joshua Bialkowski, Michael Otte, Emilio Frazzoli"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "proposed Neural RRT*,\nwhere a Convolutional Neural Network (CNN) is trained\nwith previously successful paths, and then used to generate\nbiased samples in the neighbourhood of the provided input\ndata. ",
                    "Citation Text": "L. Li, Y . Miao, A. H. Qureshi, and M. C. Yip, \u201cMPC-MPNet: Model-\nPredictive Motion Planning Networks for Fast, Near-Optimal Planning\nunder Kinodynamic Constraints,\u201d IEEE Robotics and Automation Letters ,\nvol. 6, no. 3, pp. 4496\u20134503, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.06798",
                        "Citation Paper Title": "Title:MPC-MPNet: Model-Predictive Motion Planning Networks for Fast, Near-Optimal Planning under Kinodynamic Constraints",
                        "Citation Paper Abstract": "Abstract:Kinodynamic Motion Planning (KMP) is to find a robot motion subject to concurrent kinematics and dynamics constraints. To date, quite a few methods solve KMP problems and those that exist struggle to find near-optimal solutions and exhibit high computational complexity as the planning space dimensionality increases. To address these challenges, we present a scalable, imitation learning-based, Model-Predictive Motion Planning Networks framework that quickly finds near-optimal path solutions with worst-case theoretical guarantees under kinodynamic constraints for practical underactuated systems. Our framework introduces two algorithms built on a neural generator, discriminator, and a parallelizable Model Predictive Controller (MPC). The generator outputs various informed states towards the given target, and the discriminator selects the best possible subset from them for the extension. The MPC locally connects the selected informed states while satisfying the given constraints leading to feasible, near-optimal solutions. We evaluate our algorithms on a range of cluttered, kinodynamically constrained, and underactuated planning problems with results indicating significant improvements in computation times, path qualities, and success rates over existing methods.",
                        "Citation Paper Authors": "Authors:Linjun Li, Yinglong Miao, Ahmed H. Qureshi, Michael C. Yip"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.02545v3": {
            "Paper Title": "New Properties and Invariants of Harmonic Polygons",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": ".\nThe more elementary Brocard porism of triangles is studied in [ 6,7,16,26]. In ",
                    "Citation Text": "Garcia, R., Reznik, D. (2022). Loci of the Brocard points over selected triangle families. Intl.\nJ. of Geom. , 11(2): 35\u201345.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.08561",
                        "Citation Paper Title": "Title:Loci of the Brocard Points over Selected Triangle Families",
                        "Citation Paper Abstract": "Abstract:We study the loci of the Brocard points over two selected families of triangles: (i) 2 vertices fixed on a circumference and a third one which sweeps it, (ii) Poncelet 3-periodics in the homothetic ellipse pair. Loci obtained include circles, ellipses, and teardrop-like curves. We derive expressions for both curves and their areas. We also study the locus of the vertices of Brocard triangles over the homothetic and Brocard-poristic Poncelet families.",
                        "Citation Paper Authors": "Authors:Ronaldo Garcia, Dan Reznik"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.12962v3": {
            "Paper Title": "Bias-reduced Multi-step Hindsight Experience Replay for Efficient\n  Multi-goal Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.11156v2": {
            "Paper Title": "Improved Reinforcement Learning Pushing Policies via Heuristic Rules",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.13129v2": {
            "Paper Title": "Robot Skill Adaptation via Soft Actor-Critic Gaussian Mixture Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.10312v2": {
            "Paper Title": "Example-Driven Model-Based Reinforcement Learning for Solving\n  Long-Horizon Visuomotor Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.03891v3": {
            "Paper Title": "SORNet: Spatial Object-Centric Representations for Sequential\n  Manipulation",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": "sornet(ours)\nValA Accuracy 84.950 97.944 99.006\nValB Accuracy 59.627 98.052 98.222\nTable 1: Zero-shot relation classi\ufb01cation accuracy on CLEVR-CoGenT ",
                    "Citation Text": "J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei, C. L. Zitnick, and R. Girshick. Clevr:\nA diagnostic dataset for compositional language and elementary visual reasoning. In CVPR ,\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.06890",
                        "Citation Paper Title": "Title:CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
                        "Citation Paper Abstract": "Abstract:When building artificial intelligence systems that can reason and answer questions about visual data, we need diagnostic tests to analyze our progress and discover shortcomings. Existing benchmarks for visual question answering can help, but have strong biases that models can exploit to correctly answer questions without reasoning. They also conflate multiple sources of error, making it hard to pinpoint model weaknesses. We present a diagnostic dataset that tests a range of visual reasoning abilities. It contains minimal biases and has detailed annotations describing the kind of reasoning each question requires. We use this dataset to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations.",
                        "Citation Paper Authors": "Authors:Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, Ross Girshick"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "on egocentric videos from a combinary of\ndatasets (e.g. Epic Kitchens ",
                    "Citation Text": "D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, D. Moltisanti,\nJ. Munro, T. Perrett, W. Price, et al. The epic-kitchens dataset: Collection, challenges and\nbaselines. IEEE Transactions on Pattern Analysis and Machine Intelligence , 43(11):4125\u2013\n4141, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.00343",
                        "Citation Paper Title": "Title:The EPIC-KITCHENS Dataset: Collection, Challenges and Baselines",
                        "Citation Paper Abstract": "Abstract:Since its introduction in 2018, EPIC-KITCHENS has attracted attention as the largest egocentric video benchmark, offering a unique viewpoint on people's interaction with objects, their attention, and even intention. In this paper, we detail how this large-scale dataset was captured by 32 participants in their native kitchen environments, and densely annotated with actions and object interactions. Our videos depict nonscripted daily activities, as recording is started every time a participant entered their kitchen. Recording took place in 4 countries by participants belonging to 10 different nationalities, resulting in highly diverse kitchen habits and cooking styles. Our dataset features 55 hours of video consisting of 11.5M frames, which we densely labelled for a total of 39.6K action segments and 454.2K object bounding boxes. Our annotation is unique in that we had the participants narrate their own videos after recording, thus reflecting true intention, and we crowd-sourced ground-truths based on these. We describe our object, action and. anticipation challenges, and evaluate several baselines over two test splits, seen and unseen kitchens. We introduce new baselines that highlight the multimodal nature of the dataset and the importance of explicit temporal modelling to discriminate fine-grained actions e.g. 'closing a tap' from 'opening' it up.",
                        "Citation Paper Authors": "Authors:Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, Michael Wray"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "is also similar, which trains speci\ufb01cally to predict predicates but can use looser supervision.\nVisual Reasoning Recently, several advancements have been made on visual reasoning bench-\nmarks [24, 25, 26] using transformer networks ",
                    "Citation Text": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polo-\nsukhin. Attention is all you need. arXiv preprint arXiv:1706.03762 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "proposed an object-based attention mecha-\nnism and Zhou et al. ",
                    "Citation Text": "H. Zhou, A. Kadav, F. Lai, A. Niculescu-Mizil, M. R. Min, M. Kapadia, and H. P. Graf. Hop-\nper: Multi-hop transformer for spatiotemporal reasoning. arXiv preprint arXiv:2103.10574 ,\n2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.10574",
                        "Citation Paper Title": "Title:Hopper: Multi-hop Transformer for Spatiotemporal Reasoning",
                        "Citation Paper Abstract": "Abstract:This paper considers the problem of spatiotemporal object-centric reasoning in videos. Central to our approach is the notion of object permanence, i.e., the ability to reason about the location of objects as they move through the video while being occluded, contained or carried by other objects. Existing deep learning based approaches often suffer from spatiotemporal biases when applied to video reasoning problems. We propose Hopper, which uses a Multi-hop Transformer for reasoning object permanence in videos. Given a video and a localization query, Hopper reasons over image and object tracks to automatically hop over critical frames in an iterative fashion to predict the final position of the object of interest. We demonstrate the effectiveness of using a contrastive loss to reduce spatiotemporal biases. We evaluate over CATER dataset and find that Hopper achieves 73.2% Top-1 accuracy using just 1 FPS by hopping through just a few critical frames. We also demonstrate Hopper can perform long-term reasoning by building a CATER-h dataset that requires multi-step reasoning to localize objects of interest correctly.",
                        "Citation Paper Authors": "Authors:Honglu Zhou, Asim Kadav, Farley Lai, Alexandru Niculescu-Mizil, Martin Renqiang Min, Mubbasir Kapadia, Hans Peter Graf"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.00916v3": {
            "Paper Title": "2-D Directed Formation Control Based on Bipolar Coordinates",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.05181v5": {
            "Paper Title": "Memory-Augmented Reinforcement Learning for Image-Goal Navigation",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": ".\nWe focus on one such critical problem: image-goal nav-\nigation ",
                    "Citation Text": "Y . Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and\nA. Farhadi, \u201cTarget-driven visual navigation in indoor scenes using\ndeep reinforcement learning,\u201d in ICRA , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.05143",
                        "Citation Paper Title": "Title:Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:Two less addressed issues of deep reinforcement learning are (1) lack of generalization capability to new target goals, and (2) data inefficiency i.e., the model requires several (and often costly) episodes of trial and error to converge, which makes it impractical to be applied to real-world scenarios. In this paper, we address these two issues and apply our model to the task of target-driven visual navigation. To address the first issue, we propose an actor-critic model whose policy is a function of the goal as well as the current state, which allows to better generalize. To address the second issue, we propose AI2-THOR framework, which provides an environment with high-quality 3D scenes and physics engine. Our framework enables agents to take actions and interact with objects. Hence, we can collect a huge number of training samples efficiently.\nWe show that our proposed method (1) converges faster than the state-of-the-art deep reinforcement learning methods, (2) generalizes across targets and across scenes, (3) generalizes to a real robot scenario with a small amount of fine-tuning (although the model is trained in simulation), (4) is end-to-end trainable and does not need feature engineering, feature matching between frames or 3D reconstruction of the environment.\nThe supplementary video can be accessed at the following link: this https URL.",
                        "Citation Paper Authors": "Authors:Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J. Lim, Abhinav Gupta, Li Fei-Fei, Ali Farhadi"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": ".\nI. I NTRODUCTION\nThe challenges of addressing navigation tasks go beyond\nthe classical computer-vision setup of learning from pre-\nde\ufb01ned \ufb01xed datasets. They consist of problems such as\nlow-level control point-goal navigation ",
                    "Citation Text": "E. Wijmans, A. Kadian, A. Morcos, S. Lee, I. Essa, D. Parikh,\nM. Savva, and D. Batra, \u201cDd-ppo: Learning near-perfect pointgoal\nnavigators from 2.5 billion frames,\u201d in ICLR , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.00357",
                        "Citation Paper Title": "Title:DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames",
                        "Citation Paper Abstract": "Abstract:We present Decentralized Distributed Proximal Policy Optimization (DD-PPO), a method for distributed reinforcement learning in resource-intensive simulated environments. DD-PPO is distributed (uses multiple machines), decentralized (lacks a centralized server), and synchronous (no computation is ever stale), making it conceptually simple and easy to implement. In our experiments on training virtual robots to navigate in Habitat-Sim, DD-PPO exhibits near-linear scaling -- achieving a speedup of 107x on 128 GPUs over a serial implementation. We leverage this scaling to train an agent for 2.5 Billion steps of experience (the equivalent of 80 years of human experience) -- over 6 months of GPU-time training in under 3 days of wall-clock time with 64 GPUs.\nThis massive-scale training not only sets the state of art on Habitat Autonomous Navigation Challenge 2019, but essentially solves the task --near-perfect autonomous navigation in an unseen environment without access to a map, directly from an RGB-D camera and a GPS+Compass sensor. Fortuitously, error vs computation exhibits a power-law-like distribution; thus, 90% of peak performance is obtained relatively early (at 100 million steps) and relatively cheaply (under 1 day with 8 GPUs). Finally, we show that the scene understanding and navigation policies learned can be transferred to other navigation tasks -- the analog of ImageNet pre-training + task-specific fine-tuning for embodied AI. Our model outperforms ImageNet pre-trained CNNs on these transfer tasks and can serve as a universal resource (all models and code are publicly available).",
                        "Citation Paper Authors": "Authors:Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, Dhruv Batra"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "implement the lo-\ncalization module with a neural network, while others ",
                    "Citation Text": "S. Gupta, J. Davidson, S. Levine, R. Sukthankar, and J. Malik,\n\u201cCognitive mapping and planning for visual navigation,\u201d in CVPR ,\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.03920",
                        "Citation Paper Title": "Title:Cognitive Mapping and Planning for Visual Navigation",
                        "Citation Paper Abstract": "Abstract:We introduce a neural architecture for navigation in novel environments. Our proposed architecture learns to map from first-person views and plans a sequence of actions towards goals in the environment. The Cognitive Mapper and Planner (CMP) is based on two key ideas: a) a unified joint architecture for mapping and planning, such that the mapping is driven by the needs of the task, and b) a spatial memory with the ability to plan given an incomplete set of observations about the world. CMP constructs a top-down belief map of the world and applies a differentiable neural net planner to produce the next action at each time step. The accumulated belief of the world enables the agent to track visited regions of the environment. We train and test CMP on navigation problems in simulation environments derived from scans of real world buildings. Our experiments demonstrate that CMP outperforms alternate learning-based architectures, as well as, classical mapping and path planning approaches in many cases. Furthermore, it naturally extends to semantically specified goals, such as 'going to a chair'. We also deploy CMP on physical robots in indoor environments, where it achieves reasonable performance, even though it is trained entirely in simulation.",
                        "Citation Paper Authors": "Authors:Saurabh Gupta, Varun Tolani, James Davidson, Sergey Levine, Rahul Sukthankar, Jitendra Malik"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.03180v2": {
            "Paper Title": "Dynamic Semantic Occupancy Mapping using 3D Scene Flow and Closed-Form\n  Bayesian Inference",
            "Sentences": [
                {
                    "Sentence ID": 70,
                    "Sentence": ". Semantic\nlabelsYtfor training are obtained from the Cylinder3D-\nmultiscan model ",
                    "Citation Text": "H. Zhou, X. Zhu, X. Song, Y . Ma, Z. Wang, H. Li, and D. Lin, \u201cCylinder3d:\nAn effective 3d framework for driving-scene lidar semantic segmentation,\u201d\narXiv preprint arXiv:2008.01550 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.01550",
                        "Citation Paper Title": "Title:Cylinder3D: An Effective 3D Framework for Driving-scene LiDAR Semantic Segmentation",
                        "Citation Paper Abstract": "Abstract:State-of-the-art methods for large-scale driving-scene LiDAR semantic segmentation often project and process the point clouds in the 2D space. The projection methods includes spherical projection, bird-eye view projection, etc. Although this process makes the point cloud suitable for the 2D CNN-based networks, it inevitably alters and abandons the 3D topology and geometric relations. A straightforward solution to tackle the issue of 3D-to-2D projection is to keep the 3D representation and process the points in the 3D space. In this work, we first perform an in-depth analysis for different representations and backbones in 2D and 3D spaces, and reveal the effectiveness of 3D representations and networks on LiDAR segmentation. Then, we develop a 3D cylinder partition and a 3D cylinder convolution based framework, termed as Cylinder3D, which exploits the 3D topology relations and structures of driving-scene point clouds. Moreover, a dimension-decomposition based context modeling module is introduced to explore the high-rank context information in point clouds in a progressive manner. We evaluate the proposed model on a large-scale driving-scene dataset, i.e. SematicKITTI. Our method achieves state-of-the-art performance and outperforms existing methods by 6% in terms of mIoU.",
                        "Citation Paper Authors": "Authors:Hui Zhou, Xinge Zhu, Xiao Song, Yuexin Ma, Zhe Wang, Hongsheng Li, Dahua Lin"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": ", that allows one to\nquery maps at arbitrary resolutions. Kernel methods such as\nGaussian Processes (GPs) are well-established for predict-\ning a continuous non-parametric function to represent the\nsemantic map ",
                    "Citation Text": "M. G. Jadidi, L. Gan, S. A. Parkison, J. Li, and R. M. Eustice, \u201cGaussian\nprocesses semantic map representation,\u201d arXiv preprint arXiv:1707.01532 ,\n14 VOLUME X, 2022Aishwarya Unnikrishnan et al. : Dynamic Semantic Occupancy Mapping using 3D Scene Flow and Closed-Form Bayesian Inference\nFIGURE 9: Qualitative results on S-BKI (top left), Kochanov et. al.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.01532",
                        "Citation Paper Title": "Title:Gaussian Processes Semantic Map Representation",
                        "Citation Paper Abstract": "Abstract:In this paper, we develop a high-dimensional map building technique that incorporates raw pixelated semantic measurements into the map representation. The proposed technique uses Gaussian Processes (GPs) multi-class classification for map inference and is the natural extension of GP occupancy maps from binary to multi-class form. The technique exploits the continuous property of GPs and, as a result, the map can be inferred with any resolution. In addition, the proposed GP Semantic Map (GPSM) learns the structural and semantic correlation from measurements rather than resorting to assumptions, and can flexibly learn the spatial correlation as well as any additional non-spatial correlation between map points. We extend the OctoMap to Semantic OctoMap representation and compare with the GPSM mapping performance using NYU Depth V2 dataset. Evaluations of the proposed technique on multiple partially labeled RGBD scans and labels from noisy image segmentation show that the GP semantic map can handle sparse measurements, missing labels in the point cloud, as well as noise corrupted labels.",
                        "Citation Paper Authors": "Authors:Maani Ghaffari Jadidi, Lu Gan, Steven A. Parkison, Jie Li, Ryan M. Eustice"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.13294v4": {
            "Paper Title": "The missing link: Developing a safety case for perception components in\n  automated driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.01348v2": {
            "Paper Title": "Examining average and discounted reward optimality criteria in\n  reinforcement learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.04276v4": {
            "Paper Title": "Offline Meta-Reinforcement Learning for Industrial Insertion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.01289v3": {
            "Paper Title": "A General Relationship between Optimality Criteria and Connectivity\n  Indices for Active Graph-SLAM",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.14663v3": {
            "Paper Title": "MetaGraspNet_v0: A Large-Scale Benchmark Dataset for Vision-driven\n  Robotic Grasping via Physics-based Metaverse Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.02791v7": {
            "Paper Title": "Safety-Critical Learning of Robot Control with Temporal Logic\n  Specifications",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": "by taking deterministic \ufb01nite automatons\n(DFAs) as reward machines to guide the LTL satisfaction.\nAnother work ",
                    "Citation Text": "M. Hasanbeig, Y . Kantaros, A. Abate, D. Kroening, G. J. Pappas, and\nI. Lee, \u201cReinforcement learning for temporal logic control synthesis with\nprobabilistic satisfaction guarantees,\u201d in 2019 IEEE 58th Conference on\nDecision and Control (CDC) . IEEE, 2019, pp. 5338\u20135343.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.05304",
                        "Citation Paper Title": "Title:Reinforcement Learning for Temporal Logic Control Synthesis with Probabilistic Satisfaction Guarantees",
                        "Citation Paper Abstract": "Abstract:Reinforcement Learning (RL) has emerged as an efficient method of choice for solving complex sequential decision making problems in automatic control, computer science, economics, and biology. In this paper we present a model-free RL algorithm to synthesize control policies that maximize the probability of satisfying high-level control objectives given as Linear Temporal Logic (LTL) formulas. Uncertainty is considered in the workspace properties, the structure of the workspace, and the agent actions, giving rise to a Probabilistically-Labeled Markov Decision Process (PL-MDP) with unknown graph structure and stochastic behaviour, which is even more general case than a fully unknown MDP. We first translate the LTL specification into a Limit Deterministic Buchi Automaton (LDBA), which is then used in an on-the-fly product with the PL-MDP. Thereafter, we define a synchronous reward function based on the acceptance condition of the LDBA. Finally, we show that the RL algorithm delivers a policy that maximizes the satisfaction probability asymptotically. We provide experimental results that showcase the efficiency of the proposed method.",
                        "Citation Paper Authors": "Authors:Mohammadhosein Hasanbeig, Yiannis Kantaros, Alessandro Abate, Daniel Kroening, George J. Pappas, Insup Lee"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.05615v4": {
            "Paper Title": "Intelligent Transportation Systems Using External Infrastructure: A\n  Literature Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.06764v4": {
            "Paper Title": "Contact-timing and Trajectory Optimization for 3D Jumping on Quadruped\n  Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.01119v3": {
            "Paper Title": "Cloud-Cluster Architecture for Detection in Intermittently Connected\n  Sensor Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.13169v3": {
            "Paper Title": "GPU Accelerated Voxel Grid Generation for Fast MAV Exploration",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": ". Our method\nis implemented on CPU and GPU and we will discuss the\nperformance difference throughout the paper.\nA. Related Work\nNumerous 3D space representations exist such as signed\ndistance \ufb01elds ",
                    "Citation Text": "H. Oleynikova, Z. Taylor, M. Fehr, R. Siegwart, and J. Nieto,\n\u201cV oxblox: Incremental 3d euclidean signed distance \ufb01elds for on-board\nmav planning,\u201d in 2017 Ieee/rsj International Conference on Intelligent\nRobots and Systems (iros) . IEEE, 2017, pp. 1366\u20131373.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.03631",
                        "Citation Paper Title": "Title:Voxblox: Incremental 3D Euclidean Signed Distance Fields for On-Board MAV Planning",
                        "Citation Paper Abstract": "Abstract:Micro Aerial Vehicles (MAVs) that operate in unstructured, unexplored environments require fast and flexible local planning, which can replan when new parts of the map are explored. Trajectory optimization methods fulfill these needs, but require obstacle distance information, which can be given by Euclidean Signed Distance Fields (ESDFs).\nWe propose a method to incrementally build ESDFs from Truncated Signed Distance Fields (TSDFs), a common implicit surface representation used in computer graphics and vision. TSDFs are fast to build and smooth out sensor noise over many observations, and are designed to produce surface meshes. Meshes allow human operators to get a better assessment of the robot's environment, and set high-level mission goals.\nWe show that we can build TSDFs faster than Octomaps, and that it is more accurate to build ESDFs out of TSDFs than occupancy maps. Our complete system, called voxblox, will be available as open source and runs in real-time on a single CPU core. We validate our approach on-board an MAV, by using our system with a trajectory optimization local planner, entirely on-board and in real-time.",
                        "Citation Paper Authors": "Authors:Helen Oleynikova, Zachary Taylor, Marius Fehr, Juan Nieto, Roland Siegwart"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.11206v2": {
            "Paper Title": "Learning Contact Dynamics using Physically Structured Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.12098v2": {
            "Paper Title": "IDCAIS: Inter-Defender Collision-Aware Interception Strategy against\n  Multiple Attackers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.09605v2": {
            "Paper Title": "Autonomous Reinforcement Learning: Formalism and Benchmarking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.00168v3": {
            "Paper Title": "Learning Skills to Navigate without a Master: A Sequential Multi-Policy\n  Reinforcement Learning Algorithm",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": ". We make use of the Soft Actor-Critic\nalgorithm (SAC) ",
                    "Citation Text": "T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, \u201cSoft actor-critic: Off-\npolicy maximum entropy deep reinforcement learning with a stochastic\nactor,\u201d in International Conference on Machine Learning , 2018, pp.\n1861\u20131870.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.01290",
                        "Citation Paper Title": "Title:Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
                        "Citation Paper Abstract": "Abstract:Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
                        "Citation Paper Authors": "Authors:Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": ".\nThe temperature variable \u000bcan be treated as a trainable\nparameter for better performance of the algorithm ",
                    "Citation Text": "T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan, V . Ku-\nmar, H. Zhu, A. Gupta, P. Abbeel, and S. Levine, \u201cSoft actor-critic\nalgorithms and applications,\u201d in arXiv preprint arXiv:1812.05905 ,\n2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.05905",
                        "Citation Paper Title": "Title:Soft Actor-Critic Algorithms and Applications",
                        "Citation Paper Abstract": "Abstract:Model-free deep reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework. In this framework, the actor aims to simultaneously maximize expected return and entropy. That is, to succeed at the task while acting as randomly as possible. We extend SAC to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate SAC on a range of benchmark tasks, as well as real-world challenging tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that SAC is a promising candidate for learning in real-world robotics tasks.",
                        "Citation Paper Authors": "Authors:Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, Sergey Levine"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": ", but here the curriculum naturally\narises from the necessity of traversing the environment. The\nuse of nested termination functions in our framework is\ninspired by the continual learning architecture in progressive\nneural networks ",
                    "Citation Text": "A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick,\nK. Kavukcuoglu, R. Pascanu, and R. Hadsell, \u201cProgressive neural\nnetworks,\u201d arXiv preprint arXiv:1606.04671 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.04671",
                        "Citation Paper Title": "Title:Progressive Neural Networks",
                        "Citation Paper Abstract": "Abstract:Learning to solve complex sequences of tasks--while both leveraging transfer and avoiding catastrophic forgetting--remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.",
                        "Citation Paper Authors": "Authors:Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, Raia Hadsell"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": ". Hierarchical\nreinforcement learning has also shown remarkable success\nin very complex domains like playing the game of Star-\ncraft ",
                    "Citation Text": "O. Vinyals, T. Ewalds, S. Bartunov, P. Georgiev, A. S. Vezhnevets,\nM. Yeo, A. Makhzani, H. K \u00a8uttler, J. Agapiou, J. Schrittwieser, et al. ,\n\u201cStarcraft ii: A new challenge for reinforcement learning,\u201d arXiv\npreprint arXiv:1708.04782 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.04782",
                        "Citation Paper Title": "Title:StarCraft II: A New Challenge for Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:This paper introduces SC2LE (StarCraft II Learning Environment), a reinforcement learning environment based on the StarCraft II game. This domain poses a new grand challenge for reinforcement learning, representing a more difficult class of problems than considered in most prior work. It is a multi-agent problem with multiple players interacting; there is imperfect information due to a partially observed map; it has a large action space involving the selection and control of hundreds of units; it has a large state space that must be observed solely from raw input feature planes; and it has delayed credit assignment requiring long-term strategies over thousands of steps. We describe the observation, action, and reward specification for the StarCraft II domain and provide an open source Python-based interface for communicating with the game engine. In addition to the main game maps, we provide a suite of mini-games focusing on different elements of StarCraft II gameplay. For the main game maps, we also provide an accompanying dataset of game replay data from human expert players. We give initial baseline results for neural networks trained from this data to predict game outcomes and player actions. Finally, we present initial baseline results for canonical deep reinforcement learning agents applied to the StarCraft II domain. On the mini-games, these agents learn to achieve a level of play that is comparable to a novice player. However, when trained on the main game, these agents are unable to make significant progress. Thus, SC2LE offers a new and challenging environment for exploring deep reinforcement learning algorithms and architectures.",
                        "Citation Paper Authors": "Authors:Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich K\u00fcttler, John Agapiou, Julian Schrittwieser, John Quan, Stephen Gaffney, Stig Petersen, Karen Simonyan, Tom Schaul, Hado van Hasselt, David Silver, Timothy Lillicrap, Kevin Calderone, Paul Keet, Anthony Brunasso, David Lawrence, Anders Ekermo, Jacob Repp, Rodney Tsing"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "has widely been\nadopted for training policies in sparse reward environments\nand has also been recently used in multi-level hierarchi-\ncal reinforcement learning algorithms ",
                    "Citation Text": "A. Levy, G. Konidaris, R. Platt, and K. Saenko, \u201cLearning multi-\nlevel hierarchies with hindsight,\u201d in Proceedings of International\nConference on Learning Representations , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.00948",
                        "Citation Paper Title": "Title:Learning Multi-Level Hierarchies with Hindsight",
                        "Citation Paper Abstract": "Abstract:Hierarchical agents have the potential to solve sequential decision making tasks with greater sample efficiency than their non-hierarchical counterparts because hierarchical agents can break down tasks into sets of subtasks that only require short sequences of decisions. In order to realize this potential of faster learning, hierarchical agents need to be able to learn their multiple levels of policies in parallel so these simpler subproblems can be solved simultaneously. Yet, learning multiple levels of policies in parallel is hard because it is inherently unstable: changes in a policy at one level of the hierarchy may cause changes in the transition and reward functions at higher levels in the hierarchy, making it difficult to jointly learn multiple levels of policies. In this paper, we introduce a new Hierarchical Reinforcement Learning (HRL) framework, Hierarchical Actor-Critic (HAC), that can overcome the instability issues that arise when agents try to jointly learn multiple levels of policies. The main idea behind HAC is to train each level of the hierarchy independently of the lower levels by training each level as if the lower level policies are already optimal. We demonstrate experimentally in both grid world and simulated robotics domains that our approach can significantly accelerate learning relative to other non-hierarchical and hierarchical methods. Indeed, our framework is the first to successfully learn 3-level hierarchies in parallel in tasks with continuous state and action spaces.",
                        "Citation Paper Authors": "Authors:Andrew Levy, George Konidaris, Robert Platt, Kate Saenko"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.02040v3": {
            "Paper Title": "Towards Learning to Play Piano with Dexterous Hands and Touch",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": ". Recent advances also developed robot systems that\ncan control hands to perform challenging tasks such as\nmanipulating cables ",
                    "Citation Text": "Y . She, S. Wang, S. Dong, N. Sunil, A. Rodriguez, and E. Adelson,\n\u201cCable manipulation with a tactile-reactive gripper,\u201d arXiv preprint\narXiv:1910.02860 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.02860",
                        "Citation Paper Title": "Title:Cable Manipulation with a Tactile-Reactive Gripper",
                        "Citation Paper Abstract": "Abstract:Cables are complex, high dimensional, and dynamic objects. Standard approaches to manipulate them often rely on conservative strategies that involve long series of very slow and incremental deformations, or various mechanical fixtures such as clamps, pins or rings. We are interested in manipulating freely moving cables, in real time, with a pair of robotic grippers, and with no added mechanical constraints. The main contribution of this paper is a perception and control framework that moves in that direction, and uses real-time tactile feedback to accomplish the task of following a dangling cable. The approach relies on a vision-based tactile sensor, GelSight, that estimates the pose of the cable in the grip, and the friction forces during cable sliding. We achieve the behavior by combining two tactile-based controllers: 1) Cable grip controller, where a PD controller combined with a leaky integrator regulates the gripping force to maintain the frictional sliding forces close to a suitable value; and 2) Cable pose controller, where an LQR controller based on a learned linear model of the cable sliding dynamics keeps the cable centered and aligned on the fingertips to prevent the cable from falling from the grip. This behavior is possible by a reactive gripper fitted with GelSight-based high-resolution tactile sensors. The robot can follow one meter of cable in random configurations within 2-3 hand regrasps, adapting to cables of different materials and thicknesses. We demonstrate a robot grasping a headphone cable, sliding the fingers to the jack connector, and inserting it. To the best of our knowledge, this is the first implementation of real-time cable following without the aid of mechanical fixtures.",
                        "Citation Paper Authors": "Authors:Yu She, Shaoxiong Wang, Siyuan Dong, Neha Sunil, Alberto Rodriguez, Edward Adelson"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "has been developed\nto provide high-resolution touch signals with high frame rate.\nIn previous works, they demonstrated their potential to im-\nprove the quality for robot hands in sensorimotor tasks ",
                    "Citation Text": "R. Calandra, A. Owens, D. Jayaraman, J. Lin, W. Yuan, J. Malik,\nE. H. Adelson, and S. Levine, \u201cMore than a feeling: Learning to grasp\nand regrasp using vision and touch,\u201d IEEE Robotics and Automation\nLetters , vol. 3, no. 4, pp. 3300\u20133307, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.11085",
                        "Citation Paper Title": "Title:More Than a Feeling: Learning to Grasp and Regrasp using Vision and Touch",
                        "Citation Paper Abstract": "Abstract:For humans, the process of grasping an object relies heavily on rich tactile feedback. Most recent robotic grasping work, however, has been based only on visual input, and thus cannot easily benefit from feedback after initiating contact. In this paper, we investigate how a robot can learn to use tactile information to iteratively and efficiently adjust its grasp. To this end, we propose an end-to-end action-conditional model that learns regrasping policies from raw visuo-tactile data. This model -- a deep, multimodal convolutional network -- predicts the outcome of a candidate grasp adjustment, and then executes a grasp by iteratively selecting the most promising actions. Our approach requires neither calibration of the tactile sensors, nor any analytical modeling of contact forces, thus reducing the engineering effort required to obtain efficient grasping policies. We train our model with data from about 6,450 grasping trials on a two-finger gripper equipped with GelSight high-resolution tactile sensors on each finger. Across extensive experiments, our approach outperforms a variety of baselines at (i) estimating grasp adjustment outcomes, (ii) selecting efficient grasp adjustments for quick grasping, and (iii) reducing the amount of force applied at the fingers, while maintaining competitive performance. Finally, we study the choices made by our model and show that it has successfully acquired useful and interpretable grasping behaviors.",
                        "Citation Paper Authors": "Authors:Roberto Calandra, Andrew Owens, Dinesh Jayaraman, Justin Lin, Wenzhen Yuan, Jitendra Malik, Edward H. Adelson, Sergey Levine"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "also take care of such an aspect.\nIt is also the \ufb01rst time reinforcement learning is used\nto enable a robot hand to play the piano without manually\ndesigned motion planning algorithms. In previous work ",
                    "Citation Text": "A. Rajeswaran, V . Kumar, A. Gupta, G. Vezzani, J. Schulman,\nE. Todorov, and S. Levine, \u201cLearning complex dexterous manipulation\nwith deep reinforcement learning and demonstrations,\u201d arXiv preprint\narXiv:1709.10087 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.10087",
                        "Citation Paper Title": "Title:Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations",
                        "Citation Paper Abstract": "Abstract:Dexterous multi-fingered hands are extremely versatile and provide a generic way to perform a multitude of tasks in human-centric environments. However, effectively controlling them remains challenging due to their high dimensionality and large number of potential contacts. Deep reinforcement learning (DRL) provides a model-agnostic approach to control complex dynamical systems, but has not been shown to scale to high-dimensional dexterous manipulation. Furthermore, deployment of DRL on physical systems remains challenging due to sample inefficiency. Consequently, the success of DRL in robotics has thus far been limited to simpler manipulators and tasks. In this work, we show that model-free DRL can effectively scale up to complex manipulation tasks with a high-dimensional 24-DoF hand, and solve them from scratch in simulated experiments. Furthermore, with the use of a small number of human demonstrations, the sample complexity can be significantly reduced, which enables learning with sample sizes equivalent to a few hours of robot experience. The use of demonstrations result in policies that exhibit very natural movements and, surprisingly, are also substantially more robust.",
                        "Citation Paper Authors": "Authors:Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, Sergey Levine"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.11011v2": {
            "Paper Title": "SOCIALGYM: A Framework for Benchmarking Social Robot Navigation",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": "uses a similar Unity-based approach\nto more general simulators while providing social navigation-\nspeci\ufb01c metrics and scenarios. While SEAN does not provide\nany particular benchmark, SocNavBench ",
                    "Citation Text": "A. Biswas, A. Wang, G. Silvera, A. Steinfeld, and H. Admoni,\n\u201cSocnavbench: A grounded simulation testing framework for evaluating\nsocial navigation,\u201d THRI , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.00047",
                        "Citation Paper Title": "Title:SocNavBench: A Grounded Simulation Testing Framework for Evaluating Social Navigation",
                        "Citation Paper Abstract": "Abstract:The human-robot interaction (HRI) community has developed many methods for robots to navigate safely and socially alongside humans. However, experimental procedures to evaluate these works are usually constructed on a per-method basis. Such disparate evaluations make it difficult to compare the performance of such methods across the literature. To bridge this gap, we introduce SocNavBench, a simulation framework for evaluating social navigation algorithms. SocNavBench comprises a simulator with photo-realistic capabilities and curated social navigation scenarios grounded in real-world pedestrian data. We also provide an implementation of a suite of metrics to quantify the performance of navigation algorithms on these scenarios. Altogether, SocNavBench provides a test framework for evaluating disparate social navigation methods in a consistent and interpretable manner. To illustrate its use, we demonstrate testing three existing social navigation methods and a baseline method on SocNavBench, showing how the suite of metrics helps infer their performance trade-offs. Our code is open-source, allowing the addition of new scenarios and metrics by the community to help evolve SocNavBench to reflect advancements in our understanding of social navigation.",
                        "Citation Paper Authors": "Authors:Abhijat Biswas, Allan Wang, Gustavo Silvera, Aaron Steinfeld, Henny Admoni"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "with autonomous driving in\nmind, some of which can simulate other agents in the\nenvironment, such as AirSim ",
                    "Citation Text": "S. Shah, D. Dey, C. Lovett, and A. Kapoor, \u201cAirsim: High-\n\ufb01delity visual and physical simulation for autonomous vehicles,\u201d\nCoRR , vol. abs/1705.05065, 2017. [Online]. Available: http:\n//arxiv.org/abs/1705.05065",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.05065",
                        "Citation Paper Title": "Title:AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles",
                        "Citation Paper Abstract": "Abstract:Developing and testing algorithms for autonomous vehicles in real world is an expensive and time consuming process. Also, in order to utilize recent advances in machine intelligence and deep learning we need to collect a large amount of annotated training data in a variety of conditions and environments. We present a new simulator built on Unreal Engine that offers physically and visually realistic simulations for both of these goals. Our simulator includes a physics engine that can operate at a high frequency for real-time hardware-in-the-loop (HITL) simulations with support for popular protocols (e.g. MavLink). The simulator is designed from the ground up to be extensible to accommodate new types of vehicles, hardware platforms and software protocols. In addition, the modular design enables various components to be easily usable independently in other projects. We demonstrate the simulator by first implementing a quadrotor as an autonomous vehicle and then experimentally comparing the software components with real-world flights.",
                        "Citation Paper Authors": "Authors:Shital Shah, Debadeepta Dey, Chris Lovett, Ashish Kapoor"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.11547v3": {
            "Paper Title": "Bayesian Active Learning for Sim-to-Real Robotic Perception",
            "Sentences": [
                {
                    "Sentence ID": 33,
                    "Sentence": "which can produce more reliable un-\ncertainty estimates. Along with its theoretic soundness, the\ntask-agnostic characteristic of these approaches can facilitate\nwider applicability for different \ufb01elds. While some only\nexploit the classi\ufb01cation branch for the uncertainty esti-\nmation ",
                    "Citation Text": "D. Miller, L. Nicholson, F. Dayoub, and N. S \u00a8underhauf, \u201cDropout\nsampling for robust object detection in open-set conditions,\u201d in ICRA ,\n2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.06677",
                        "Citation Paper Title": "Title:Dropout Sampling for Robust Object Detection in Open-Set Conditions",
                        "Citation Paper Abstract": "Abstract:Dropout Variational Inference, or Dropout Sampling, has been recently proposed as an approximation technique for Bayesian Deep Learning and evaluated for image classification and regression tasks. This paper investigates the utility of Dropout Sampling for object detection for the first time. We demonstrate how label uncertainty can be extracted from a state-of-the-art object detection system via Dropout Sampling. We evaluate this approach on a large synthetic dataset of 30,000 images, and a real-world dataset captured by a mobile robot in a versatile campus environment. We show that this uncertainty can be utilized to increase object detection performance under the open-set conditions that are typically encountered in robotic vision. A Dropout Sampling network is shown to achieve a 12.3% increase in recall (for the same precision score as a standard network) and a 15.1% increase in precision (for the same recall score as the standard network).",
                        "Citation Paper Authors": "Authors:Dimity Miller, Lachlan Nicholson, Feras Dayoub, Niko S\u00fcnderhauf"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "the margin of the bounding\nbox scores in different layers is used, Kao et al. ",
                    "Citation Text": "C.-C. Kao, T.-Y . Lee, P. Sen, and M.-Y . Liu, \u201cLocalization-aware active\nlearning for object detection,\u201d in ACCV , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.05124",
                        "Citation Paper Title": "Title:Localization-Aware Active Learning for Object Detection",
                        "Citation Paper Abstract": "Abstract:Active learning - a class of algorithms that iteratively searches for the most informative samples to include in a training dataset - has been shown to be effective at annotating data for image classification. However, the use of active learning for object detection is still largely unexplored as determining informativeness of an object-location hypothesis is more difficult. In this paper, we address this issue and present two metrics for measuring the informativeness of an object hypothesis, which allow us to leverage active learning to reduce the amount of annotated data needed to achieve a target object detection performance. Our first metric measures 'localization tightness' of an object hypothesis, which is based on the overlapping ratio between the region proposal and the final prediction. Our second metric measures 'localization stability' of an object hypothesis, which is based on the variation of predicted object locations when input images are corrupted by noise. Our experimental results show that by augmenting a conventional active-learning algorithm designed for classification with the proposed metrics, the amount of labeled training data required can be reduced up to 25%. Moreover, on PASCAL 2007 and 2012 datasets our localization-stability method has an average relative improvement of 96.5% and 81.9% over the baseline method using classification only.",
                        "Citation Paper Authors": "Authors:Chieh-Chi Kao, Teng-Yok Lee, Pradeep Sen, Ming-Yu Liu"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": ". Considering this issue, the paradigm of active\n1https://github.com/DLR-RMlearning is appealing to address the reality gap by utilizing\nannotated real data in an ef\ufb01cient way. In pool-set based\nactive learning ",
                    "Citation Text": "D. A. Cohn, Z. Ghahramani, and M. I. Jordan, \u201cActive learning with\nstatistical models,\u201d Journal of Arti\ufb01cial Intelligence Research , vol. 4,\n1996.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:cs/9603104",
                        "Citation Paper Title": "Title:Active Learning with Statistical Models",
                        "Citation Paper Abstract": "Abstract:  For many types of machine learning algorithms, one can compute the statistically `optimal' way to select training data. In this paper, we review how optimal data selection techniques have been used with feedforward neural networks. We then show how the same principles may be used to select data for two alternative, statistically-based learning architectures: mixtures of Gaussians and locally weighted regression. While the techniques for neural networks are computationally expensive and approximate, the techniques for mixtures of Gaussians and locally weighted regression are both efficient and accurate. Empirically, we observe that the optimality criterion sharply decreases the number of training examples the learner needs in order to achieve good performance.",
                        "Citation Paper Authors": "Authors:D. A. Cohn, Z. Ghahramani, M. I. Jordan"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": ". In contrast, DA focuses on learning\ndomain-invariant representations across the different domains\n(e.g. synthetic and real domain in this context) by sometimes\nincluding data of the target domain ",
                    "Citation Text": "K. Bousmalis, A. Irpan, P. Wohlhart, Y . Bai, M. Kelcey, M. Kalakr-\nishnan, L. Downs, J. Ibarz, P. Pastor, K. Konolige, et al. , \u201cUsing\nsimulation and domain adaptation to improve ef\ufb01ciency of deep\nrobotic grasping,\u201d in ICRA , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.07857",
                        "Citation Paper Title": "Title:Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping",
                        "Citation Paper Abstract": "Abstract:Instrumenting and collecting annotated visual grasping datasets to train modern machine learning algorithms can be extremely time-consuming and expensive. An appealing alternative is to use off-the-shelf simulators to render synthetic data for which ground-truth annotations are generated automatically. Unfortunately, models trained purely on simulated data often fail to generalize to the real world. We study how randomized simulated environments and domain adaptation methods can be extended to train a grasping system to grasp novel objects from raw monocular RGB images. We extensively evaluate our approaches with a total of more than 25,000 physical test grasps, studying a range of simulation conditions and domain adaptation methods, including a novel extension of pixel-level domain adaptation that we term the GraspGAN. We show that, by using synthetic data and domain adaptation, we are able to reduce the number of real-world samples needed to achieve a given level of performance by up to 50 times, using only randomly generated simulated objects. We also show that by using only unlabeled real-world data and our GraspGAN methodology, we obtain real-world grasping performance without any real-world labels that is similar to that achieved with 939,777 labeled real-world samples.",
                        "Citation Paper Authors": "Authors:Konstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai, Matthew Kelcey, Mrinal Kalakrishnan, Laura Downs, Julian Ibarz, Peter Pastor, Kurt Konolige, Sergey Levine, Vincent Vanhoucke"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.12696v2": {
            "Paper Title": "PM-FSM: Policies Modulating Finite State Machine for Robust Quadrupedal\n  Locomotion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.09500v4": {
            "Paper Title": "Disentangled Sequence Clustering for Human Intention Inference",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.00128v4": {
            "Paper Title": "Probabilistic Object Maps for Long-Term Robot Localization",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": "with\na more complex shape model or learned method similar\nto V oteNet ",
                    "Citation Text": "C. R. Qi, O. Litany, K. He, and L. J. Guibas, \u201cDeep hough voting\nfor 3d object detection in point clouds,\u201d in Proceedings of the IEEE\nInternational Conference on Computer Vision , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.09664",
                        "Citation Paper Title": "Title:Deep Hough Voting for 3D Object Detection in Point Clouds",
                        "Citation Paper Abstract": "Abstract:Current 3D object detection methods are heavily influenced by 2D detectors. In order to leverage architectures in 2D detectors, they often convert 3D point clouds to regular grids (i.e., to voxel grids or to bird's eye view images), or rely on detection in 2D images to propose 3D boxes. Few works have attempted to directly detect objects in point clouds. In this work, we return to first principles to construct a 3D detection pipeline for point cloud data and as generic as possible. However, due to the sparse nature of the data -- samples from 2D manifolds in 3D space -- we face a major challenge when directly predicting bounding box parameters from scene points: a 3D object centroid can be far from any surface point thus hard to regress accurately in one step. To address the challenge, we propose VoteNet, an end-to-end 3D object detection network based on a synergy of deep point set networks and Hough voting. Our model achieves state-of-the-art 3D detection on two large datasets of real 3D scans, ScanNet and SUN RGB-D with a simple design, compact model size and high efficiency. Remarkably, VoteNet outperforms previous methods by using purely geometric information without relying on color images.",
                        "Citation Paper Authors": "Authors:Charles R. Qi, Or Litany, Kaiming He, Leonidas J. Guibas"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.04812v3": {
            "Paper Title": "Deep Visual Constraints: Neural Implicit Models for Manipulation\n  Planning from Visual Input",
            "Sentences": [
                {
                    "Sentence ID": 37,
                    "Sentence": ", which\ncould be a better choice depending on the setting, e.g., whether\nreliable depth perception is available ",
                    "Citation Text": "Anthony Simeonov, Yilun Du, Andrea Tagliasac-\nchi, Joshua B. Tenenbaum, Alberto Rodriguez, Pulkit\nAgrawal, and Vincent Sitzmann. Neural descriptor \ufb01elds:\nSe(3)-equivariant object representations for manipula-\ntion. arXiv preprint arXiv:2112.05124 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.05124",
                        "Citation Paper Title": "Title:Neural Descriptor Fields: SE(3)-Equivariant Object Representations for Manipulation",
                        "Citation Paper Abstract": "Abstract:We present Neural Descriptor Fields (NDFs), an object representation that encodes both points and relative poses between an object and a target (such as a robot gripper or a rack used for hanging) via category-level descriptors. We employ this representation for object manipulation, where given a task demonstration, we want to repeat the same task on a new object instance from the same category. We propose to achieve this objective by searching (via optimization) for the pose whose descriptor matches that observed in the demonstration. NDFs are conveniently trained in a self-supervised fashion via a 3D auto-encoding task that does not rely on expert-labeled keypoints. Further, NDFs are SE(3)-equivariant, guaranteeing performance that generalizes across all possible 3D object translations and rotations. We demonstrate learning of manipulation tasks from few (5-10) demonstrations both in simulation and on a real robot. Our performance generalizes across both object instances and 6-DoF object poses, and significantly outperforms a recent baseline that relies on 2D descriptors. Project website: this https URL.",
                        "Citation Paper Authors": "Authors:Anthony Simeonov, Yilun Du, Andrea Tagliasacchi, Joshua B. Tenenbaum, Alberto Rodriguez, Pulkit Agrawal, Vincent Sitzmann"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "formulated\nmanipulation planning problems solely in terms of SDFs as\nrepresentations and proposed to learn manipulation constraints\nas functionals of SDFs. More recently, Driess et al. ",
                    "Citation Text": "Danny Driess, Zhiao Huang, Yunzhu Li, Russ Tedrake,\nand Marc Toussaint. Learning multi-object dynamics\nwith compositional neural radiance \ufb01elds. arXiv preprint\narXiv:2202.11855 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.11855",
                        "Citation Paper Title": "Title:Learning Multi-Object Dynamics with Compositional Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:We present a method to learn compositional multi-object dynamics models from image observations based on implicit object encoders, Neural Radiance Fields (NeRFs), and graph neural networks. NeRFs have become a popular choice for representing scenes due to their strong 3D prior. However, most NeRF approaches are trained on a single scene, representing the whole scene with a global model, making generalization to novel scenes, containing different numbers of objects, challenging. Instead, we present a compositional, object-centric auto-encoder framework that maps multiple views of the scene to a set of latent vectors representing each object separately. The latent vectors parameterize individual NeRFs from which the scene can be reconstructed. Based on those latent vectors, we train a graph neural network dynamics model in the latent space to achieve compositionality for dynamics prediction. A key feature of our approach is that the latent vectors are forced to encode 3D information through the NeRF decoder, which enables us to incorporate structural priors in learning the dynamics models, making long-term predictions more stable compared to several baselines. Simulated and real world experiments show that our method can model and learn the dynamics of compositional scenes including rigid and deformable objects. Video: this https URL",
                        "Citation Paper Authors": "Authors:Danny Driess, Zhiao Huang, Yunzhu Li, Russ Tedrake, Marc Toussaint"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "proposed\nlearning object-centric representations that are used to predict\nthe symbolic predicates of the scene which in turn enables\nsymbolic-level task planning. Driess et al. ",
                    "Citation Text": "Danny Driess, Jung-Su Ha, Marc Toussaint, and Russ\nTedrake. Learning models as functionals of signed-\ndistance \ufb01elds for manipulation planning. In Proc. of\nthe Annual Conf. on Robot Learning (CORL) , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.00792",
                        "Citation Paper Title": "Title:Learning Models as Functionals of Signed-Distance Fields for Manipulation Planning",
                        "Citation Paper Abstract": "Abstract:This work proposes an optimization-based manipulation planning framework where the objectives are learned functionals of signed-distance fields that represent objects in the scene. Most manipulation planning approaches rely on analytical models and carefully chosen abstractions/state-spaces to be effective. A central question is how models can be obtained from data that are not primarily accurate in their predictions, but, more importantly, enable efficient reasoning within a planning framework, while at the same time being closely coupled to perception spaces. We show that representing objects as signed-distance fields not only enables to learn and represent a variety of models with higher accuracy compared to point-cloud and occupancy measure representations, but also that SDF-based models are suitable for optimization-based planning. To demonstrate the versatility of our approach, we learn both kinematic and dynamic models to solve tasks that involve hanging mugs on hooks and pushing objects on a table. We can unify these quite different tasks within one framework, since SDFs are the common object representation. Video: this https URL",
                        "Citation Paper Authors": "Authors:Danny Driess, Jung-Su Ha, Marc Toussaint, Russ Tedrake"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2307.11349v5": {
            "Paper Title": "EV-Planner: Energy-Efficient Robot Navigation via Event-Based\n  Physics-Guided Neuromorphic Planner",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": "proposed a motion compensation scheme\nusing ego-motion filtering to eliminate background events.\nFor object-tracking, ",
                    "Citation Text": "D. Gehrig, H. Rebecq, G. Gallego, and D. Scaramuzza, \u201cAsynchronous, photo-\nmetric feature tracking using events and frames,\u201d in Proceedings of the European\nConference on Computer Vision (ECCV) , 2018, pp. 750\u2013765.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.09713",
                        "Citation Paper Title": "Title:Asynchronous, Photometric Feature Tracking using Events and Frames",
                        "Citation Paper Abstract": "Abstract:We present a method that leverages the complementarity of event cameras and standard cameras to track visual features with low-latency. Event cameras are novel sensors that output pixel-level brightness changes, called \"events\". They offer significant advantages over standard cameras, namely a very high dynamic range, no motion blur, and a latency in the order of microseconds. However, because the same scene pattern can produce different events depending on the motion direction, establishing event correspondences across time is challenging. By contrast, standard cameras provide intensity measurements (frames) that do not depend on motion direction. Our method extracts features on frames and subsequently tracks them asynchronously using events, thereby exploiting the best of both types of data: the frames provide a photometric representation that does not depend on motion direction and the events provide low-latency updates. In contrast to previous works, which are based on heuristics, this is the first principled method that uses raw intensity measurements directly, based on a generative event model within a maximum-likelihood framework. As a result, our method produces feature tracks that are both more accurate (subpixel accuracy) and longer than the state of the art, across a wide variety of scenes.",
                        "Citation Paper Authors": "Authors:Daniel Gehrig, Henri Rebecq, Guillermo Gallego, Davide Scaramuzza"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.13863v2": {
            "Paper Title": "Manipulating Trajectory Prediction with Backdoors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2303.05308v2": {
            "Paper Title": "SpyroPose: SE(3) Pyramids for Object Pose Distribution Estimation",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "is similar, but instead of estimating a latent\nembedding from an image and feeding it to an MLP, they\nlearn a mapping from an image to the weights of an MLP\nwhich maps rotations to unnormalized likelihoods. I2S ",
                    "Citation Text": "David M Klee, Ondrej Biza, Robert Platt, and Robin Walters.\nImage to sphere: Learning equivariant features for efficient\npose prediction. arXiv preprint arXiv:2302.13926 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.13926",
                        "Citation Paper Title": "Title:Image to Sphere: Learning Equivariant Features for Efficient Pose Prediction",
                        "Citation Paper Abstract": "Abstract:Predicting the pose of objects from a single image is an important but difficult computer vision problem. Methods that predict a single point estimate do not predict the pose of objects with symmetries well and cannot represent uncertainty. Alternatively, some works predict a distribution over orientations in $\\mathrm{SO}(3)$. However, training such models can be computation- and sample-inefficient. Instead, we propose a novel mapping of features from the image domain to the 3D rotation manifold. Our method then leverages $\\mathrm{SO}(3)$ equivariant layers, which are more sample efficient, and outputs a distribution over rotations that can be sampled at arbitrary resolution. We demonstrate the effectiveness of our method at object orientation prediction, and achieve state-of-the-art performance on the popular PASCAL3D+ dataset. Moreover, we show that our method can model complex object symmetries, without any modifications to the parameters or loss function. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:David M. Klee, Ondrej Biza, Robert Platt, Robin Walters"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "(2019) -0.43 3.84 0.88 -2.29 -2.29 -2.29 3.70 3.32 4.88 2.90\nDeng et al. ",
                    "Citation Text": "Haowen Deng, Mai Bui, Nassir Navab, Leonidas Guibas,\nSlobodan Ilic, and Tolga Birdal. Deep bingham networks:\nDealing with uncertainty and ambiguity in pose estimation.\nInternational Journal of Computer Vision , pages 1\u201328, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.11002",
                        "Citation Paper Title": "Title:Deep Bingham Networks: Dealing with Uncertainty and Ambiguity in Pose Estimation",
                        "Citation Paper Abstract": "Abstract:In this work, we introduce Deep Bingham Networks (DBN), a generic framework that can naturally handle pose-related uncertainties and ambiguities arising in almost all real life applications concerning 3D data. While existing works strive to find a single solution to the pose estimation problem, we make peace with the ambiguities causing high uncertainty around which solutions to identify as the best. Instead, we report a family of poses which capture the nature of the solution space. DBN extends the state of the art direct pose regression networks by (i) a multi-hypotheses prediction head which can yield different distribution modes; and (ii) novel loss functions that benefit from Bingham distributions on rotations. This way, DBN can work both in unambiguous cases providing uncertainty information, and in ambiguous scenes where an uncertainty per mode is desired. On a technical front, our network regresses continuous Bingham mixture models and is applicable to both 2D data such as images and to 3D data such as point clouds. We proposed new training strategies so as to avoid mode or posterior collapse during training and to improve numerical stability. Our methods are thoroughly tested on two different applications exploiting two different modalities: (i) 6D camera relocalization from images; and (ii) object pose estimation from 3D point clouds, demonstrating decent advantages over the state of the art. For the former we contributed our own dataset composed of five indoor scenes where it is unavoidable to capture images corresponding to views that are hard to uniquely identify. For the latter we achieve the top results especially for symmetric objects of ModelNet dataset.",
                        "Citation Paper Authors": "Authors:Haowen Deng, Mai Bui, Nassir Navab, Leonidas Guibas, Slobodan Ilic, Tolga Birdal"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "extends this idea and estimates dense full 2D-\n3D correspondence distributions. However, the dense corre-\nspondence histograms and -distributions have not yet shown\nto be useful to model pose distributions.There are also pose estimation methods which address\npose uncertainties. In ",
                    "Citation Text": "Guanya Shi, Yifeng Zhu, Jonathan Tremblay, Stan Birch-\nfield, Fabio Ramos, Animashree Anandkumar, and Yuke\nZhu. Fast uncertainty quantification for deep object pose esti-\nmation. In 2021 IEEE International Conference on Robotics\nand Automation (ICRA) , pages 5200\u20135207. IEEE, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.07748",
                        "Citation Paper Title": "Title:Fast Uncertainty Quantification for Deep Object Pose Estimation",
                        "Citation Paper Abstract": "Abstract:Deep learning-based object pose estimators are often unreliable and overconfident especially when the input image is outside the training domain, for instance, with sim2real transfer. Efficient and robust uncertainty quantification (UQ) in pose estimators is critically needed in many robotic tasks. In this work, we propose a simple, efficient, and plug-and-play UQ method for 6-DoF object pose estimation. We ensemble 2-3 pre-trained models with different neural network architectures and/or training data sources, and compute their average pairwise disagreement against one another to obtain the uncertainty quantification. We propose four disagreement metrics, including a learned metric, and show that the average distance (ADD) is the best learning-free metric and it is only slightly worse than the learned metric, which requires labeled target data. Our method has several advantages compared to the prior art: 1) our method does not require any modification of the training process or the model inputs; and 2) it needs only one forward pass for each model. We evaluate the proposed UQ method on three tasks where our uncertainty quantification yields much stronger correlations with pose estimation errors than the baselines. Moreover, in a real robot grasping task, our method increases the grasping success rate from 35% to 90%.",
                        "Citation Paper Authors": "Authors:Guanya Shi, Yifeng Zhu, Jonathan Tremblay, Stan Birchfield, Fabio Ramos, Animashree Anandkumar, Yuke Zhu"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": ", is also cen-\ntered around point estimates and known object symmetries.\nSome methods [11, 8] that provide point estimates do\nhowever handle visual ambiguities in a principled way.\nEPOS ",
                    "Citation Text": "Tomas Hodan, Daniel Barath, and Jiri Matas. Epos: Esti-\nmating 6d pose of objects with symmetries. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition , pages 11703\u201311712, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.00605",
                        "Citation Paper Title": "Title:EPOS: Estimating 6D Pose of Objects with Symmetries",
                        "Citation Paper Abstract": "Abstract:We present a new method for estimating the 6D pose of rigid objects with available 3D models from a single RGB input image. The method is applicable to a broad range of objects, including challenging ones with global or partial symmetries. An object is represented by compact surface fragments which allow handling symmetries in a systematic manner. Correspondences between densely sampled pixels and the fragments are predicted using an encoder-decoder network. At each pixel, the network predicts: (i) the probability of each object's presence, (ii) the probability of the fragments given the object's presence, and (iii) the precise 3D location on each fragment. A data-dependent number of corresponding 3D locations is selected per pixel, and poses of possibly multiple object instances are estimated using a robust and efficient variant of the PnP-RANSAC algorithm. In the BOP Challenge 2019, the method outperforms all RGB and most RGB-D and D methods on the T-LESS and LM-O datasets. On the YCB-V dataset, it is superior to all competitors, with a large margin over the second-best RGB method. Source code is at: this http URL.",
                        "Citation Paper Authors": "Authors:Tomas Hodan, Daniel Barath, Jiri Matas"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2304.02359v2": {
            "Paper Title": "Efficient Optimization-based Cable Force Allocation for Geometric\n  Control of a Multirotor Team Transporting a Payload",
            "Sentences": []
        },
        "http://arxiv.org/abs/2305.18942v2": {
            "Paper Title": "Scaling Planning for Automated Driving using Simplistic Synthetic Data",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "was trained with approximately 40 hours of driving.\nHowever, even if a large dataset is available or different\ndatasets such as ",
                    "Citation Text": "H. Caesar, V . Bankiti, A. H. Lang, S. V ora, V . E. Liong, Q. Xu,\nA. Krishnan, Y . Pan, G. Baldan, and O. Beijbom, \u201cnuscenes: A\nmultimodal dataset for autonomous driving,\u201d in Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition ,\n2020, pp. 11 621\u201311 631.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.11027",
                        "Citation Paper Title": "Title:nuScenes: A multimodal dataset for autonomous driving",
                        "Citation Paper Abstract": "Abstract:Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online.",
                        "Citation Paper Authors": "Authors:Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, Oscar Beijbom"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": ". Both the ego agent\nand surrounding traffic are controlled by the built-in IDM\nmodel ",
                    "Citation Text": "M. Treiber, A. Hennecke, and D. Helbing, \u201cCongested traffic states in\nempirical observations and microscopic simulations,\u201d Physical review\nE, vol. 62, no. 2, p. 1805, 2000.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:cond-mat/0002177",
                        "Citation Paper Title": "Title:Congested Traffic States in Empirical Observations and Microscopic Simulations",
                        "Citation Paper Abstract": "Abstract:  We present data from several German freeways showing different kinds of congested traffic forming near road inhomogeneities, specifically lane closings, intersections, or uphill gradients. The states are localized or extended, homogeneous or oscillating. Combined states are observed as well, like the coexistence of moving localized clusters and clusters pinned at road inhomogeneities, or regions of oscillating congested traffic upstream of nearly homogeneous congested traffic. The experimental findings are consistent with a recently proposed theoretical phase diagram for traffic near on-ramps [D. Helbing, A. Hennecke, and M. Treiber, Phys. Rev. Lett. {\\bf 82}, 4360 (1999)]. We simulate these situations with a novel continuous microscopic single-lane model, the ``intelligent driver model'' (IDM), using the empirical boundary conditions. All observations, including the coexistence of states, are qualitatively reproduced by describing inhomogeneities with local variations of one model parameter.\nWe show that the results of the microscopic model can be understood by formulating the theoretical phase diagram for bottlenecks in a more general way. In particular, a local drop of the road capacity induced by parameter variations has practically the same effect as an on-ramp.",
                        "Citation Paper Authors": "Authors:Martin Treiber, Ansgar Hennecke, Dirk Helbing"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.06365v2": {
            "Paper Title": "A Simulation-based Approach to Kinematics Analysis of a Quadruped Robot\n  and Prototype Leg Testing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2305.01072v2": {
            "Paper Title": "Fast Path Planning Through Large Collections of Safe Boxes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.05019v2": {
            "Paper Title": "Vision-based Learning for Drones: A Survey",
            "Sentences": [
                {
                    "Sentence ID": 186,
                    "Sentence": "Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, \u201cRethinking the\nvalue of network pruning,\u201d in International Conference on Learning\nRepresentations , 2018. ",
                    "Citation Text": "Y . Jiang, S. Wang, V . Valls, B. J. Ko, W.-H. Lee, K. K. Leung, and\nL. Tassiulas, \u201cModel pruning enables efficient federated learning on\nedge devices,\u201d IEEE Transactions on Neural Networks and Learning\nSystems , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.12326",
                        "Citation Paper Title": "Title:Model Pruning Enables Efficient Federated Learning on Edge Devices",
                        "Citation Paper Abstract": "Abstract:Federated learning (FL) allows model training from local data collected by edge/mobile devices while preserving data privacy, which has wide applicability to image and vision applications. A challenge is that client devices in FL usually have much more limited computation and communication resources compared to servers in a datacenter. To overcome this challenge, we propose PruneFL -- a novel FL approach with adaptive and distributed parameter pruning, which adapts the model size during FL to reduce both communication and computation overhead and minimize the overall training time, while maintaining a similar accuracy as the original model. PruneFL includes initial pruning at a selected client and further pruning as part of the FL process. The model size is adapted during this process, which includes maximizing the approximate empirical risk reduction divided by the time of one FL round. Our experiments with various datasets on edge devices (e.g., Raspberry Pi) show that: (i) we significantly reduce the training time compared to conventional FL and various other pruning-based methods; (ii) the pruned model with automatically determined size converges to an accuracy that is very similar to the original model, and it is also a lottery ticket of the original model.",
                        "Citation Paper Authors": "Authors:Yuang Jiang, Shiqiang Wang, Victor Valls, Bong Jun Ko, Wei-Han Lee, Kin K. Leung, Leandros Tassiulas"
                    }
                },
                {
                    "Sentence ID": 185,
                    "Sentence": ",\u201d Journal of Big Data , vol. 10, no. 1, p.\n115, 2023. ",
                    "Citation Text": "Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, \u201cRethinking the\nvalue of network pruning,\u201d in International Conference on Learning\nRepresentations , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.05270",
                        "Citation Paper Title": "Title:Rethinking the Value of Network Pruning",
                        "Citation Paper Abstract": "Abstract:Network pruning is widely used for reducing the heavy inference cost of deep models in low-resource settings. A typical pruning algorithm is a three-stage pipeline, i.e., training (a large model), pruning and fine-tuning. During pruning, according to a certain criterion, redundant weights are pruned and important weights are kept to best preserve the accuracy. In this work, we make several surprising observations which contradict common beliefs. For all state-of-the-art structured pruning algorithms we examined, fine-tuning a pruned model only gives comparable or worse performance than training that model with randomly initialized weights. For pruning algorithms which assume a predefined target network architecture, one can get rid of the full pipeline and directly train the target network from scratch. Our observations are consistent for multiple network architectures, datasets, and tasks, which imply that: 1) training a large, over-parameterized model is often not necessary to obtain an efficient final model, 2) learned \"important\" weights of the large model are typically not useful for the small pruned model, 3) the pruned architecture itself, rather than a set of inherited \"important\" weights, is more crucial to the efficiency in the final model, which suggests that in some cases pruning can be useful as an architecture search paradigm. Our results suggest the need for more careful baseline evaluations in future research on structured pruning methods. We also compare with the \"Lottery Ticket Hypothesis\" (Frankle & Carbin 2019), and find that with optimal learning rate, the \"winning ticket\" initialization as used in Frankle & Carbin (2019) does not bring improvement over random initialization.",
                        "Citation Paper Authors": "Authors:Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, Trevor Darrell"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2310.16542v2": {
            "Paper Title": "ParisLuco3D: A high-quality target dataset for domain generalization of\n  LiDAR perception",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": "sought to increase the speed\nof using these convolutions to enable real-time applications.However, point-based methods also exist. For example,\nPointRCNN ",
                    "Citation Text": "S. Shi, X. Wang, and H. Li, \u201cPointrcnn: 3d object proposal generation\nand detection from point cloud,\u201d in 2019 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) , 2019, pp. 770\u2013779.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.04244",
                        "Citation Paper Title": "Title:PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose PointRCNN for 3D object detection from raw point cloud. The whole framework is composed of two stages: stage-1 for the bottom-up 3D proposal generation and stage-2 for refining proposals in the canonical coordinates to obtain the final detection results. Instead of generating proposals from RGB image or projecting point cloud to bird's view or voxels as previous methods do, our stage-1 sub-network directly generates a small number of high-quality 3D proposals from point cloud in a bottom-up manner via segmenting the point cloud of the whole scene into foreground points and background. The stage-2 sub-network transforms the pooled points of each proposal to canonical coordinates to learn better local spatial features, which is combined with global semantic features of each point learned in stage-1 for accurate box refinement and confidence prediction. Extensive experiments on the 3D detection benchmark of KITTI dataset show that our proposed architecture outperforms state-of-the-art methods with remarkable margins by using only point cloud as input. The code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Shaoshuai Shi, Xiaogang Wang, Hongsheng Li"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "models, which restructure the point cloud into\na regular 3D grid and apply 3D convolution to it ",
                    "Citation Text": "X. Zhu, H. Zhou, T. Wang, F. Hong, Y . Ma, W. Li, H. Li, and\nD. Lin, \u201cCylindrical and asymmetrical 3d convolution networks for\nlidar segmentation,\u201d in 2021 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) , 2021, pp. 9934\u20139943.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.10033",
                        "Citation Paper Title": "Title:Cylindrical and Asymmetrical 3D Convolution Networks for LiDAR Segmentation",
                        "Citation Paper Abstract": "Abstract:State-of-the-art methods for large-scale driving-scene LiDAR segmentation often project the point clouds to 2D space and then process them via 2D convolution. Although this corporation shows the competitiveness in the point cloud, it inevitably alters and abandons the 3D topology and geometric relations. A natural remedy is to utilize the3D voxelization and 3D convolution network. However, we found that in the outdoor point cloud, the improvement obtained in this way is quite limited. An important reason is the property of the outdoor point cloud, namely sparsity and varying density. Motivated by this investigation, we propose a new framework for the outdoor LiDAR segmentation, where cylindrical partition and asymmetrical 3D convolution networks are designed to explore the 3D geometric pat-tern while maintaining these inherent properties. Moreover, a point-wise refinement module is introduced to alleviate the interference of lossy voxel-based label encoding. We evaluate the proposed model on two large-scale datasets, i.e., SemanticKITTI and nuScenes. Our method achieves the 1st place in the leaderboard of SemanticKITTI and outperforms existing methods on nuScenes with a noticeable margin, about 4%. Furthermore, the proposed 3D framework also generalizes well to LiDAR panoptic segmentation and LiDAR 3D detection.",
                        "Citation Paper Authors": "Authors:Xinge Zhu, Hui Zhou, Tai Wang, Fangzhou Hong, Yuexin Ma, Wei Li, Hongsheng Li, Dahua Lin"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": ".\nWhile these methods were extremely fast, their performance\nwas not satisfactory\nIn parallel, point-based architectures were developed by\ntaking the time to redefine the convolution ",
                    "Citation Text": "H. Thomas, C. R. Qi, J.-E. Deschaud, B. Marcotegui, F. Goulette, and\nL. Guibas, \u201cKpconv: Flexible and deformable convolution for point\nclouds,\u201d in 2019 IEEE/CVF International Conference on Computer\nVision (ICCV) , 2019, pp. 6410\u20136419.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.08889",
                        "Citation Paper Title": "Title:KPConv: Flexible and Deformable Convolution for Point Clouds",
                        "Citation Paper Abstract": "Abstract:We present Kernel Point Convolution (KPConv), a new design of point convolution, i.e. that operates on point clouds without any intermediate representation. The convolution weights of KPConv are located in Euclidean space by kernel points, and applied to the input points close to them. Its capacity to use any number of kernel points gives KPConv more flexibility than fixed grid convolutions. Furthermore, these locations are continuous in space and can be learned by the network. Therefore, KPConv can be extended to deformable convolutions that learn to adapt kernel points to local geometry. Thanks to a regular subsampling strategy, KPConv is also efficient and robust to varying densities. Whether they use deformable KPConv for complex tasks, or rigid KPconv for simpler tasks, our networks outperform state-of-the-art classification and segmentation approaches on several datasets. We also offer ablation studies and visualizations to provide understanding of what has been learned by KPConv and to validate the descriptive power of deformable KPConv.",
                        "Citation Paper Authors": "Authors:Hugues Thomas, Charles R. Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, Fran\u00e7ois Goulette, Leonidas J. Guibas"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2304.08842v3": {
            "Paper Title": "UDTIRI: An Online Open-Source Intelligent Road Inspection Benchmark\n  Suite",
            "Sentences": []
        },
        "http://arxiv.org/abs/2307.07607v4": {
            "Paper Title": "SubT-MRS Dataset: Pushing SLAM Towards All-weather Environments",
            "Sentences": [
                {
                    "Sentence ID": 46,
                    "Sentence": "\u2713 \u2713 \u2713 \u2717 \u2713 \u2717 \u2713 \u2717 \u2717 \u2717 \u2713 \u2717 \u2713\nTartanAir ",
                    "Citation Text": "Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu,\nYuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Se-\nbastian Scherer. Tartanair: A dataset to push the limits of\nvisual SLAM. In 2020 IEEE/RSJ International Conference\non Intelligent Robots and Systems (IROS) , 2020. 3, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.14338",
                        "Citation Paper Title": "Title:TartanAir: A Dataset to Push the Limits of Visual SLAM",
                        "Citation Paper Abstract": "Abstract:We present a challenging dataset, the TartanAir, for robot navigation tasks and more. The data is collected in photo-realistic simulation environments with the presence of moving objects, changing light and various weather conditions. By collecting data in simulations, we are able to obtain multi-modal sensor data and precise ground truth labels such as the stereo RGB image, depth image, segmentation, optical flow, camera poses, and LiDAR point cloud. We set up large numbers of environments with various styles and scenes, covering challenging viewpoints and diverse motion patterns that are difficult to achieve by using physical data collection platforms. In order to enable data collection at such a large scale, we develop an automatic pipeline, including mapping, trajectory sampling, data processing, and data verification. We evaluate the impact of various factors on visual SLAM algorithms using our data. The results of state-of-the-art algorithms reveal that the visual SLAM problem is far from solved. Methods that show good performance on established datasets such as KITTI do not perform well in more difficult scenarios. Although we use the simulation, our goal is to push the limits of Visual SLAM algorithms in the real world by providing a challenging benchmark for testing new methods, while also using a large diverse training data for learning-based methods. Our dataset is available at \\url{this http URL}.",
                        "Citation Paper Authors": "Authors:Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, Sebastian Scherer"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "SW Opt Intel i9-12900 0.142 176.44 / 0 9.111 3.037 0.083/0.372 \u2713 \u2713\n4 Li et al ORB-SLAM3 ",
                    "Citation Text": "Carlos Campos, Richard Elvira, Juan J G \u00b4omez Rodr \u00b4\u0131guez,\nJos\u00b4e MM Montiel, and Juan D Tard \u00b4os. Orb-slam3: An accu-\nrate open-source library for visual, visual\u2013inertial, and mul-\ntimap slam. IEEE Transactions on Robotics , 37(6):1874\u2013\n1890, 2021. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.11898",
                        "Citation Paper Title": "Title:ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM",
                        "Citation Paper Abstract": "Abstract:This paper presents ORB-SLAM3, the first system able to perform visual, visual-inertial and multi-map SLAM with monocular, stereo and RGB-D cameras, using pin-hole and fisheye lens models. The first main novelty is a feature-based tightly-integrated visual-inertial SLAM system that fully relies on Maximum-a-Posteriori (MAP) estimation, even during the IMU initialization phase. The result is a system that operates robustly in real-time, in small and large, indoor and outdoor environments, and is 2 to 5 times more accurate than previous approaches. The second main novelty is a multiple map system that relies on a new place recognition method with improved recall. Thanks to it, ORB-SLAM3 is able to survive to long periods of poor visual information: when it gets lost, it starts a new map that will be seamlessly merged with previous maps when revisiting mapped areas. Compared with visual odometry systems that only use information from the last few seconds, ORB-SLAM3 is the first system able to reuse in all the algorithm stages all previous information. This allows to include in bundle adjustment co-visible keyframes, that provide high parallax observations boosting accuracy, even if they are widely separated in time or if they come from a previous mapping session. Our experiments show that, in all sensor configurations, ORB-SLAM3 is as robust as the best systems available in the literature, and significantly more accurate. Notably, our stereo-inertial SLAM achieves an average accuracy of 3.6 cm on the EuRoC drone and 9 mm under quick hand-held motions in the room of TUM-VI dataset, a setting representative of AR/VR scenarios. For the benefit of the community we make public the source code.",
                        "Citation Paper Authors": "Authors:Carlos Campos, Richard Elvira, Juan J. G\u00f3mez Rodr\u00edguez, Jos\u00e9 M. M. Montiel, Juan D. Tard\u00f3s"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "Hybrid Intel i5-9400 0.064 40.35 / 0 4.337 1.093 0.078/0.322 \u2713 \u2713\n3 Thien et al VR-SLAM ",
                    "Citation Text": "Thien Hoang Nguyen, Shenghai Yuan, and Lihua Xie. Vr-\nslam: A visual-range simultaneous localization and mapping\nsystem using monocular camera and ultra-wideband sensors,\n2023. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.10903",
                        "Citation Paper Title": "Title:VR-SLAM: A Visual-Range Simultaneous Localization and Mapping System using Monocular Camera and Ultra-wideband Sensors",
                        "Citation Paper Abstract": "Abstract:In this work, we propose a simultaneous localization and mapping (SLAM) system using a monocular camera and Ultra-wideband (UWB) sensors. Our system, referred to as VRSLAM, is a multi-stage framework that leverages the strengths and compensates for the weaknesses of each sensor. Firstly, we introduce a UWB-aided 7 degree-of-freedom (scale factor, 3D position, and 3D orientation) global alignment module to initialize the visual odometry (VO) system in the world frame defined by the UWB anchors. This module loosely fuses up-to-scale VO and ranging data using either a quadratically constrained quadratic programming (QCQP) or nonlinear least squares (NLS) algorithm based on whether a good initial guess is available. Secondly, we provide an accompanied theoretical analysis that includes the derivation and interpretation of the Fisher Information Matrix (FIM) and its determinant. Thirdly, we present UWBaided bundle adjustment (UBA) and UWB-aided pose graph optimization (UPGO) modules to improve short-term odometry accuracy, reduce long-term drift as well as correct any alignment and scale errors. Extensive simulations and experiments show that our solution outperforms UWB/camera-only and previous approaches, can quickly recover from tracking failure without relying on visual relocalization, and can effortlessly obtain a global map even if there are no loop closures.",
                        "Citation Paper Authors": "Authors:Thien Hoang Nguyen, Shenghai Yuan, Lihua Xie"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "SW Opt AMD Ryzen 9 5900x 0.027 13.289 / 0 1.174 1.209 0.276/0.486 \u2713 \u2713\n1 Peng et al DVI-SLAM ",
                    "Citation Text": "Xiongfeng Peng, Zhihua Liu, Weiming Li, Ping Tan, SoonY-\nong Cho, and Qiang Wang. Dvi-slam: A dual visual inertial\nslam network, 2023. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2309.13814",
                        "Citation Paper Title": "Title:DVI-SLAM: A Dual Visual Inertial SLAM Network",
                        "Citation Paper Abstract": "Abstract:Recent deep learning based visual simultaneous localization and mapping (SLAM) methods have made significant progress. However, how to make full use of visual information as well as better integrate with inertial measurement unit (IMU) in visual SLAM has potential research value. This paper proposes a novel deep SLAM network with dual visual factors. The basic idea is to integrate both photometric factor and re-projection factor into the end-to-end differentiable structure through multi-factor data association module. We show that the proposed network dynamically learns and adjusts the confidence maps of both visual factors and it can be further extended to include the IMU factors as well. Extensive experiments validate that our proposed method significantly outperforms the state-of-the-art methods on several public datasets, including TartanAir, EuRoC and ETH3D-SLAM. Specifically, when dynamically fusing the three factors together, the absolute trajectory error for both monocular and stereo configurations on EuRoC dataset has reduced by 45.3% and 36.2% respectively.",
                        "Citation Paper Authors": "Authors:Xiongfeng Peng, Zhihua Liu, Weiming Li, Ping Tan, SoonYong Cho, Qiang Wang"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "Filter Intel i5-12500 0.268 101.108 / 0 55.64 3.825 0.479/0.615 \u2713 \u2713\n5 Zhong et al DLO ",
                    "Citation Text": "Kenny Chen, Brett T. Lopez, Ali-akbar Agha-mohammadi,\nand Ankur Mehta. Direct lidar odometry: Fast localization\nwith dense point clouds. IEEE Robotics and Automation Let-\nters, 7(2):2000\u20132007, 2022. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.00605",
                        "Citation Paper Title": "Title:Direct LiDAR Odometry: Fast Localization with Dense Point Clouds",
                        "Citation Paper Abstract": "Abstract:Field robotics in perceptually-challenging environments require fast and accurate state estimation, but modern LiDAR sensors quickly overwhelm current odometry algorithms. To this end, this paper presents a lightweight frontend LiDAR odometry solution with consistent and accurate localization for computationally-limited robotic platforms. Our Direct LiDAR Odometry (DLO) method includes several key algorithmic innovations which prioritize computational efficiency and enables the use of dense, minimally-preprocessed point clouds to provide accurate pose estimates in real-time. This is achieved through a novel keyframing system which efficiently manages historical map information, in addition to a custom iterative closest point solver for fast point cloud registration with data structure recycling. Our method is more accurate with lower computational overhead than the current state-of-the-art and has been extensively evaluated in multiple perceptually-challenging environments on aerial and legged robots as part of NASA JPL Team CoSTAR's research and development efforts for the DARPA Subterranean Challenge.",
                        "Citation Paper Authors": "Authors:Kenny Chen, Brett T. Lopez, Ali-akbar Agha-mohammadi, Ankur Mehta"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "Filter Intel Xeon(R)E3-1240v5 0.125 22.63 / 0 4.305 0.663 0.473/0.747 \u2713 \u2713\n4 Kim et al FAST-LIO2 ",
                    "Citation Text": "Wei Xu, Yixi Cai, Dongjiao He, Jiarong Lin, and Fu Zhang.\nFAST-LIO2: fast direct lidar-inertial odometry. IEEE Trans-\nactions on Robotics , abs/2107.06829, 2022. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.06829",
                        "Citation Paper Title": "Title:FAST-LIO2: Fast Direct LiDAR-inertial Odometry",
                        "Citation Paper Abstract": "Abstract:This paper presents FAST-LIO2: a fast, robust, and versatile LiDAR-inertial odometry framework. Building on a highly efficient tightly-coupled iterated Kalman filter, FAST-LIO2 has two key novelties that allow fast, robust, and accurate LiDAR navigation (and mapping). The first one is directly registering raw points to the map (and subsequently update the map, i.e., mapping) without extracting features. This enables the exploitation of subtle features in the environment and hence increases the accuracy. The elimination of a hand-engineered feature extraction module also makes it naturally adaptable to emerging LiDARs of different scanning patterns; The second main novelty is maintaining a map by an incremental k-d tree data structure, ikd-Tree, that enables incremental updates (i.e., point insertion, delete) and dynamic re-balancing. Compared with existing dynamic data structures (octree, R*-tree, nanoflann k-d tree), ikd-Tree achieves superior overall performance while naturally supports downsampling on the tree. We conduct an exhaustive benchmark comparison in 19 sequences from a variety of open LiDAR datasets. FAST-LIO2 achieves consistently higher accuracy at a much lower computation load than other state-of-the-art LiDAR-inertial navigation systems. Various real-world experiments on solid-state LiDARs with small FoV are also conducted. Overall, FAST-LIO2 is computationally-efficient (e.g., up to 100 Hz odometry and mapping in large outdoor environments), robust (e.g., reliable pose estimation in cluttered indoor environments with rotation up to 1000 deg/s), versatile (i.e., applicable to both multi-line spinning and solid-state LiDARs, UAV and handheld platforms, and Intel and ARM-based processors), while still achieving higher accuracy than existing methods. Our implementation of the system FAST-LIO2, and the data structure ikd-Tree are both open-sourced on Github.",
                        "Citation Paper Authors": "Authors:Wei Xu, Yixi Cai, Dongjiao He, Jiarong Lin, Fu Zhang"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "\u2713 \u2713 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2713\nTUM VIO ",
                    "Citation Text": "David Schubert, Thore Goll, Nikolaus Demmel, Vladyslav\nUsenko, J \u00a8org St \u00a8uckler, and Daniel Cremers. The tum vi\nbenchmark for evaluating visual-inertial odometry. In 2018\nIEEE/RSJ International Conference on Intelligent Robots\nand Systems (IROS) , pages 1680\u20131687. IEEE, 2018. 1, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.06120",
                        "Citation Paper Title": "Title:The TUM VI Benchmark for Evaluating Visual-Inertial Odometry",
                        "Citation Paper Abstract": "Abstract:Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024x1024 resolution at 20 Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200 Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120 Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data is publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.",
                        "Citation Paper Authors": "Authors:David Schubert, Thore Goll, Nikolaus Demmel, Vladyslav Usenko, J\u00f6rg St\u00fcckler, Daniel Cremers"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": "\u2713 \u2713 \u2717 \u2713 \u2717 \u2713 \u2717 \u2717 \u2717 \u2713 \u2717 \u2717 \u2717\nCollege Dataset ",
                    "Citation Text": "Lintong Zhang, Marco Camurri, and Maurice Fallon. Multi-\ncamera lidar inertial extension to the newer college dataset.\narXiv preprint arXiv:2112.08854 , 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.08854",
                        "Citation Paper Title": "Title:Multi-Camera LiDAR Inertial Extension to the Newer College Dataset",
                        "Citation Paper Abstract": "Abstract:We present a multi-camera LiDAR inertial dataset of 4.5 km walking distance as an expansion of the Newer College Dataset. The global shutter multi-camera device is hardware synchronized with both the IMU and LiDAR, which is more accurate than the original dataset with software synchronization. This dataset also provides six Degrees of Freedom (DoF) ground truth poses at LiDAR frequency (10 Hz). Three data collections are described and an example use case of multi-camera visual-inertial odometry is demonstrated. This expansion dataset contains small and narrow passages, large scale open spaces, as well as vegetated areas, to test localization and mapping systems. Furthermore, some sequences present challenging situations such as abrupt lighting change, textureless surfaces, and aggressive motion. The dataset is available at: https://ori-drs.github. io/newer-college-dataset/",
                        "Citation Paper Authors": "Authors:Lintong Zhang, Marco Camurri, David Wisth, Maurice Fallon"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "\u2713 \u2713 \u2713 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2713 \u2717 \u2717 \u2717\nVirtual KITTI ",
                    "Citation Text": "Adrien Gaidon, Qiao Wang, Yohann Cabon, and Eleonora\nVig. Virtual worlds as proxy for multi-object tracking anal-\nysis. In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 4340\u20134349, 2016. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.06457",
                        "Citation Paper Title": "Title:Virtual Worlds as Proxy for Multi-Object Tracking Analysis",
                        "Citation Paper Abstract": "Abstract:Modern computer vision algorithms typically require expensive data acquisition and accurate manual labeling. In this work, we instead leverage the recent progress in computer graphics to generate fully labeled, dynamic, and photo-realistic proxy virtual worlds. We propose an efficient real-to-virtual world cloning method, and validate our approach by building and publicly releasing a new video dataset, called Virtual KITTI (see this http URL), automatically labeled with accurate ground truth for object detection, tracking, scene and instance segmentation, depth, and optical flow. We provide quantitative experimental evidence suggesting that (i) modern deep learning algorithms pre-trained on real data behave similarly in real and virtual worlds, and (ii) pre-training on virtual data improves performance. As the gap between real and virtual worlds is small, virtual worlds enable measuring the impact of various weather and imaging conditions on recognition performance, all other things being equal. We show these factors may affect drastically otherwise high-performing deep models for tracking.",
                        "Citation Paper Authors": "Authors:Adrien Gaidon, Qiao Wang, Yohann Cabon, Eleonora Vig"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "incor-\nporates thermal images in outdoor settings but is deficient\nin hardware synchronization, posing challenges for SLAM\nsystem development. The Weichen dataset ",
                    "Citation Text": "Weichen Dai, Yu Zhang, Shenzhou Chen, Donglei Sun, and\nDa Kong. A multi-spectral dataset for evaluating motion es-\ntimation systems. In 2021 IEEE International Conference on\nRobotics and Automation (ICRA) , pages 5560\u20135566. IEEE,\n2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.00622",
                        "Citation Paper Title": "Title:A Multi-spectral Dataset for Evaluating Motion Estimation Systems",
                        "Citation Paper Abstract": "Abstract:Visible images have been widely used for motion estimation. Thermal images, in contrast, are more challenging to be used in motion estimation since they typically have lower resolution, less texture, and more noise. In this paper, a novel dataset for evaluating the performance of multi-spectral motion estimation systems is presented. All the sequences are recorded from a handheld multi-spectral device. It consists of a standard visible-light camera, a long-wave infrared camera, an RGB-D camera, and an inertial measurement unit (IMU). The multi-spectral images, including both color and thermal images in full sensor resolution (640 x 480), are obtained from a standard and a long-wave infrared camera at 32Hz with hardware-synchronization. The depth images are captured by a Microsoft Kinect2 and can have benefits for learning cross-modalities stereo matching. For trajectory evaluation, accurate ground-truth camera poses obtained from a motion capture system are provided. In addition to the sequences with bright illumination, the dataset also contains dim, varying, and complex illumination scenes. The full dataset, including raw data and calibration data with detailed data format specifications, is publicly available.",
                        "Citation Paper Authors": "Authors:Weichen Dai, Yu Zhang, Shenzhou Chen, Donglei Sun, Da Kong"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": ", although covering most modali-\nties, is geared towards on-road scenarios and lacks thermal\ncameras, limiting its applicability to simple, controlled en-\nvironments. Conversely, the ViViD++ dataset ",
                    "Citation Text": "Alex Junho Lee, Younggun Cho, Young-sik Shin, Ayoung\nKim, and Hyun Myung. Vivid++: Vision for visibility\ndataset. IEEE Robotics and Automation Letters , 7(3):6282\u2013\n6289, 2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.06183",
                        "Citation Paper Title": "Title:ViViD++: Vision for Visibility Dataset",
                        "Citation Paper Abstract": "Abstract:In this paper, we present a dataset capturing diverse visual data formats that target varying luminance conditions. While RGB cameras provide nourishing and intuitive information, changes in lighting conditions potentially result in catastrophic failure for robotic applications based on vision sensors. Approaches overcoming illumination problems have included developing more robust algorithms or other types of visual sensors, such as thermal and event cameras. Despite the alternative sensors' potential, there still are few datasets with alternative vision sensors. Thus, we provided a dataset recorded from alternative vision sensors, by handheld or mounted on a car, repeatedly in the same space but in different conditions. We aim to acquire visible information from co-aligned alternative vision sensors. Our sensor system collects data more independently from visible light intensity by measuring the amount of infrared dissipation, depth by structured reflection, and instantaneous temporal changes in luminance. We provide these measurements along with inertial sensors and ground-truth for developing robust visual SLAM under poor illumination. The full dataset is available at: this https URL",
                        "Citation Paper Authors": "Authors:Alex Junho Lee, Younggun Cho, Young-sik Shin, Ayoung Kim, Hyun Myung"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2401.00524v1": {
            "Paper Title": "Effect of Optimizer, Initializer, and Architecture of Hypernetworks on\n  Continual Learning from Demonstration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2401.00445v1": {
            "Paper Title": "Energy-Efficient Power Control for Multiple-Task Split Inference in\n  UAVs: A Tiny Learning-Based Approach",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": ". It is capable of providing long-term computational\nservices for various scenarios. In ",
                    "Citation Text": "F. Zhou, Y . Wu, R. Q. Hu, and Y . Qian, \u201cComputation rate ma ximiza-\ntion in uav-enabled wireless-powered mobile-edge computi ng systems,\u201d\nIEEE Journal on Selected Areas in Communications , vol. 36, no. 9, pp.\n1927\u20131941, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.04589",
                        "Citation Paper Title": "Title:Computation Rate Maximization in UAV-Enabled Wireless Powered Mobile-Edge Computing Systems",
                        "Citation Paper Abstract": "Abstract:Mobile edge computing (MEC) and wireless power transfer (WPT) are two promising techniques to enhance the computation capability and to prolong the operational time of low-power wireless devices that are ubiquitous in Internet of Things. However, the computation performance and the harvested energy are significantly impacted by the severe propagation loss. In order to address this issue, an unmanned aerial vehicle (UAV)-enabled MEC wireless powered system is studied in this paper. The computation rate maximization problems in a UAV-enabled MEC wireless powered system are investigated under both partial and binary computation offloading modes, subject to the energy harvesting causal constraint and the UAV's speed constraint. These problems are non-convex and challenging to solve. A two-stage algorithm and a three-stage alternative algorithm are respectively proposed for solving the formulated problems. The closed-form expressions for the optimal central processing unit frequencies, user offloading time, and user transmit power are derived. The optimal selection scheme on whether users choose to locally compute or offload computation tasks is proposed for the binary computation offloading mode. Simulation results show that our proposed resource allocation schemes outperforms other benchmark schemes. The results also demonstrate that the proposed schemes converge fast and have low computational complexity.",
                        "Citation Paper Authors": "Authors:Fuhui Zhou, Yongpeng Wu, Rose Qingyang Hu, Yi Qian"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.11451v2": {
            "Paper Title": "Language-Assisted 3D Scene Understanding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2401.00391v1": {
            "Paper Title": "Controllable Safety-Critical Closed-loop Traffic Simulation via Guided\n  Diffusion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2401.00315v1": {
            "Paper Title": "Bidirectional Temporal Plan Graph: Enabling Switchable Passing Orders\n  for More Efficient Multi-Agent Path Finding Plan Execution",
            "Sentences": []
        },
        "http://arxiv.org/abs/2401.00242v1": {
            "Paper Title": "Laboratory Experiments of Model-based Reinforcement Learning for\n  Adaptive Optics Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2401.00167v1": {
            "Paper Title": "Leveraging Partial Symmetry for Multi-Agent Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2308.07931v2": {
            "Paper Title": "Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2306.08144v3": {
            "Paper Title": "Correct-by-Construction Design of Contextual Robotic Missions Using\n  Contracts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17731v1": {
            "Paper Title": "MURP: Multi-Agent Ultra-Wideband Relative Pose Estimation with\n  Constrained Communications in 3D Environments",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": "Many (2 or 4) NLLS UWB\nXu et al.\n(2022) ",
                    "Citation Text": "Hao Xu, Yichen Zhang, Boyu Zhou, Luqi Wang, Xinjie Yao, Guotao\nMeng, and Shaojie Shen. Omni-swarm: A decentralized omnidirec-\ntional visual-inertial-uwb state estimation system for aerial swarm.\nCoRR , abs/2103.04131, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.04131",
                        "Citation Paper Title": "Title:Omni-swarm: A Decentralized Omnidirectional Visual-Inertial-UWB State Estimation System for Aerial Swarms",
                        "Citation Paper Abstract": "Abstract:Decentralized state estimation is one of the most fundamental components of autonomous aerial swarm systems in GPS-denied areas yet it still remains a highly challenging research topic. Omni-swarm, a decentralized omnidirectional visual-inertial-UWB state estimation system for aerial swarms, is proposed in this paper to address this research niche. To solve the issues of observability, complicated initialization, insufficient accuracy, and lack of global consistency, we introduce an omnidirectional perception front-end in Omni-swarm. It consists of stereo wide-FoV cameras and ultra-wideband sensors, visual-inertial odometry, multi-drone map-based localization, and visual drone tracking algorithms. The measurements from the front-end are fused with graph-based optimization in the back-end. The proposed method achieves centimeter-level relative state estimation accuracy while guaranteeing global consistency in the aerial swarm, as evidenced by the experimental results. Moreover, supported by Omni-swarm, inter-drone collision avoidance can be accomplished without any external devices, demonstrating the potential of Omni-swarm as the foundation of autonomous aerial swarms.",
                        "Citation Paper Authors": "Authors:Hao Xu, Yichen Zhang, Boyu Zhou, Luqi Wang, Xinjie Yao, Guotao Meng, Shaojie Shen"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "SingleZM Gaussian +\nOcclusion RejectionUWB + VIO + Visual\nTracks\nXun et al.\n(2023) ",
                    "Citation Text": "Zhiren Xun, Jian Huang, Zhehan Li, Zhenjun Ying, Yingjian Wang,\nChao Xu, Fei Gao, and Yanjun Cao. CREPES: Cooperative RElative\nPose Estimation System, March 2023. arXiv:2302.01036 [cs].",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.01036",
                        "Citation Paper Title": "Title:CREPES: Cooperative RElative Pose Estimation System",
                        "Citation Paper Abstract": "Abstract:Mutual localization plays a crucial role in multi-robot cooperation. CREPES, a novel system that focuses on six degrees of freedom (DOF) relative pose estimation for multi-robot systems, is proposed in this paper. CREPES has a compact hardware design using active infrared (IR) LEDs, an IR fish-eye camera, an ultra-wideband (UWB) module and an inertial measurement unit (IMU). By leveraging IR light communication, the system solves data association between visual detection and UWB ranging. Ranging measurements from the UWB and directional information from the camera offer relative 3-DOF position estimation. Combining the mutual relative position with neighbors and the gravity constraints provided by IMUs, we can estimate the 6-DOF relative pose from a single frame of sensor measurements. In addition, we design an estimator based on the error-state Kalman filter (ESKF) to enhance system accuracy and robustness. When multiple neighbors are available, a Pose Graph Optimization (PGO) algorithm is applied to further improve system accuracy. We conduct enormous experiments to demonstrate CREPES' accuracy between robot pairs and a team of robots, as well as performance under challenging conditions.",
                        "Citation Paper Authors": "Authors:Zhiren Xun, Jian Huang, Zhehan Li, Zhenjun Ying, Yingjian Wang, Chao Xu, Fei Gao, Yanjun Cao"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2311.07745v3": {
            "Paper Title": "Simplifying Complex Observation Models in Continuous POMDP Planning with\n  Probabilistic Guarantees and Practice",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17668v1": {
            "Paper Title": "Vocalics in Human-Drone Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2309.02609v2": {
            "Paper Title": "Directionality-Aware Mixture Model Parallel Sampling for Efficient\n  Linear Parameter Varying Dynamical System Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17634v1": {
            "Paper Title": "Developing Flying Explorer for Autonomous Digital Modelling in Wild\n  Unknowns",
            "Sentences": []
        },
        "http://arxiv.org/abs/2308.11234v4": {
            "Paper Title": "Traffic Flow Optimisation for Lifelong Multi-Agent Path Finding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17605v1": {
            "Paper Title": "Unified Task and Motion Planning using Object-centric Abstractions of\n  Motion Constraints",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": "Over the last year, several approaches combining task\nand motion planning for the robotic execution of tasks\nin unstructured scenarios have been proposed ",
                    "Citation Text": "C. R. Garrett, R. Chitnis, R. Holladay, B. Kim, T. Silver, L. P. Kael-\nbling, and T. Lozano-P \u00b4erez, \u201cIntegrated task and motion planning,\u201d\nAnnual review of control, robotics, and autonomous systems , vol. 4,\npp. 265\u2013293, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.01083",
                        "Citation Paper Title": "Title:Integrated Task and Motion Planning",
                        "Citation Paper Abstract": "Abstract:The problem of planning for a robot that operates in environments containing a large number of objects, taking actions to move itself through the world as well as to change the state of the objects, is known as task and motion planning (TAMP). TAMP problems contain elements of discrete task planning, discrete-continuous mathematical programming, and continuous motion planning, and thus cannot be effectively addressed by any of these fields directly. In this paper, we define a class of TAMP problems and survey algorithms for solving them, characterizing the solution methods in terms of their strategies for solving the continuous-space subproblems and their techniques for integrating the discrete and continuous components of the search.",
                        "Citation Paper Authors": "Authors:Caelan Reed Garrett, Rohan Chitnis, Rachel Holladay, Beomjoon Kim, Tom Silver, Leslie Pack Kaelbling, Tom\u00e1s Lozano-P\u00e9rez"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.17552v1": {
            "Paper Title": "Exploring Deep Reinforcement Learning for Robust Target Tracking using\n  Micro Aerial Vehicles",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "propose robustification strategies\nbased on domain randomization and max-min optimization,\nwhereas ",
                    "Citation Text": "M. Turchetta, A. Krause, and S. Trimpe, \u201cRobust model-free reinforce-\nment learning with multi-objective bayesian optimization,\u201d in 2020\nIEEE International Conference on Robotics and Automation , 2020,\npp. 10 702\u201310 708.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.13399",
                        "Citation Paper Title": "Title:Robust Model-free Reinforcement Learning with Multi-objective Bayesian Optimization",
                        "Citation Paper Abstract": "Abstract:In reinforcement learning (RL), an autonomous agent learns to perform complex tasks by maximizing an exogenous reward signal while interacting with its environment. In real-world applications, test conditions may differ substantially from the training scenario and, therefore, focusing on pure reward maximization during training may lead to poor results at test time. In these cases, it is important to trade-off between performance and robustness while learning a policy. While several results exist for robust, model-based RL, the model-free case has not been widely investigated. In this paper, we cast the robust, model-free RL problem as a multi-objective optimization problem. To quantify the robustness of a policy, we use delay margin and gain margin, two robustness indicators that are common in control theory. We show how these metrics can be estimated from data in the model-free setting. We use multi-objective Bayesian optimization (MOBO) to solve efficiently this expensive-to-evaluate, multi-objective optimization problem. We show the benefits of our robust formulation both in sim-to-real and pure hardware experiments to balance a Furuta pendulum.",
                        "Citation Paper Authors": "Authors:Matteo Turchetta, Andreas Krause, Sebastian Trimpe"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": ". These methods do not typically provide an easy way\nto optimize performance and to handle trajectory constraints.\nTo address this issue, ",
                    "Citation Text": "A. Saviolo, G. Li, and G. Loianno, \u201cPhysics-inspired temporal learn-\ning of quadrotor dynamics for accurate model predictive trajectory\ntracking,\u201d IEEE Robotics and Automation Letters , vol. 7, no. 4, pp.\n10 256\u201310 263, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.03305",
                        "Citation Paper Title": "Title:Physics-Inspired Temporal Learning of Quadrotor Dynamics for Accurate Model Predictive Trajectory Tracking",
                        "Citation Paper Abstract": "Abstract:Accurately modeling quadrotor's system dynamics is critical for guaranteeing agile, safe, and stable navigation. The model needs to capture the system behavior in multiple flight regimes and operating conditions, including those producing highly nonlinear effects such as aerodynamic forces and torques, rotor interactions, or possible system configuration modifications. Classical approaches rely on handcrafted models and struggle to generalize and scale to capture these effects. In this paper, we present a novel Physics-Inspired Temporal Convolutional Network (PI-TCN) approach to learning quadrotor's system dynamics purely from robot experience. Our approach combines the expressive power of sparse temporal convolutions and dense feed-forward connections to make accurate system predictions. In addition, physics constraints are embedded in the training process to facilitate the network's generalization capabilities to data outside the training distribution. Finally, we design a model predictive control approach that incorporates the learned dynamics for accurate closed-loop trajectory tracking fully exploiting the learned model predictions in a receding horizon fashion. Experimental results demonstrate that our approach accurately extracts the structure of the quadrotor's dynamics from data, capturing effects that would remain hidden to classical approaches. To the best of our knowledge, this is the first time physics-inspired deep learning is successfully applied to temporal convolutional networks and to the system identification task, while concurrently enabling predictive control.",
                        "Citation Paper Authors": "Authors:Alessandro Saviolo, Guanrui Li, Giuseppe Loianno"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.02008v2": {
            "Paper Title": "Multi-Agent Behavior Retrieval: Retrieval-Augmented Policy Training for\n  Cooperative Manipulation by Mobile Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.16391v2": {
            "Paper Title": "Toward Spatial Temporal Consistency of Joint Visual Tactile Perception\n  in VR Applications",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "P. K and S. Chaudhuri, \u201cEnhancing haptic distinguishability of surface\nmaterials with boosting technique,\u201d in 2022 IEEE Haptics Symposium\n(HAPTICS) , 2022, pp. 1\u20136. ",
                    "Citation Text": "Y . Gao, L. A. Hendricks, K. J. Kuchenbecker, and T. Darrell, \u201cDeep\nlearning for tactile understanding from visual and haptic data,\u201d in\nICRA , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06065",
                        "Citation Paper Title": "Title:Deep Learning for Tactile Understanding From Visual and Haptic Data",
                        "Citation Paper Abstract": "Abstract:Robots which interact with the physical world will benefit from a fine-grained tactile understanding of objects and surfaces. Additionally, for certain tasks, robots may need to know the haptic properties of an object before touching it. To enable better tactile understanding for robots, we propose a method of classifying surfaces with haptic adjectives (e.g., compressible or smooth) from both visual and physical interaction data. Humans typically combine visual predictions and feedback from physical interactions to accurately predict haptic properties and interact with the world. Inspired by this cognitive pattern, we propose and explore a purely visual haptic prediction model. Purely visual models enable a robot to \"feel\" without physical interaction. Furthermore, we demonstrate that using both visual and physical interaction signals together yields more accurate haptic classification. Our models take advantage of recent advances in deep neural networks by employing a unified approach to learning features for physical interaction and visual observations. Even though we employ little domain specific knowledge, our model still achieves better results than methods based on hand-designed features.",
                        "Citation Paper Authors": "Authors:Yang Gao, Lisa Anne Hendricks, Katherine J. Kuchenbecker, Trevor Darrell"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.16818v2": {
            "Paper Title": "Difficulties in Dynamic Analysis of Drone Firmware and Its Solutions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17420v1": {
            "Paper Title": "Exact Consistency Tests for Gaussian Mixture Filters using Normalized\n  Deviation Squared Statistics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2401.00025v1": {
            "Paper Title": "Any-point Trajectory Modeling for Policy Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17215v1": {
            "Paper Title": "Control Barrier Function Based UAV Safety Controller in Autonomous\n  Airborne Tracking and Following Systems",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": "shows a CBF\nbased approach for a leader-follower formation control with\ncollision avoidance. CBFs are also useful in exploration\nin cluttered environments, as shown in ",
                    "Citation Text": "C. Lerch, D. Dong, and I. Abraham, \u201cSafety-critical ergodic explo-\nration in cluttered environments via control barrier functions,\u201d in 2023\nIEEE International Conference on Robotics and Automation (ICRA) .\nIEEE, 2023, pp. 10 205\u201310 211.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2211.04310",
                        "Citation Paper Title": "Title:Safety-Critical Ergodic Exploration in Cluttered Environments via Control Barrier Functions",
                        "Citation Paper Abstract": "Abstract:In this paper, we address the problem of safe trajectory planning for autonomous search and exploration in constrained, cluttered environments. Guaranteeing safe (collision-free) trajectories is a challenging problem that has garnered significant due to its importance in the successful utilization of robots in search and exploration tasks. This work contributes a method that generates guaranteed safety-critical search trajectories in a cluttered environment. Our approach integrates safety-critical constraints using discrete control barrier functions (DCBFs) with ergodic trajectory optimization to enable safe exploration. Ergodic trajectory optimization plans continuous exploratory trajectories that guarantee complete coverage of a space. We demonstrate through simulated and experimental results on a drone that our approach is able to generate trajectories that enable safe and effective exploration. Furthermore, we show the efficacy of our approach for safe exploration using real-world single- and multi- drone platforms.",
                        "Citation Paper Authors": "Authors:Cameron Lerch, Dayi Dong, Ian Abraham"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.17135v1": {
            "Paper Title": "InsActor: Instruction-driven Physics-based Characters",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": "and input to a control policy that directly predict actions. It is also a commonly used\nlearning paradigm in conditional imitation learning ",
                    "Citation Text": "Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic\nmanipulation. In Proceedings of the 5th Conference on Robot Learning (CoRL) , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.12098",
                        "Citation Paper Title": "Title:CLIPort: What and Where Pathways for Robotic Manipulation",
                        "Citation Paper Abstract": "Abstract:How can we imbue robots with the ability to manipulate objects precisely but also to reason about them in terms of abstract concepts? Recent works in manipulation have shown that end-to-end networks can learn dexterous skills that require precise spatial reasoning, but these methods often fail to generalize to new goals or quickly learn transferable concepts across tasks. In parallel, there has been great progress in learning generalizable semantic representations for vision and language by training on large-scale internet data, however these representations lack the spatial understanding necessary for fine-grained manipulation. To this end, we propose a framework that combines the best of both worlds: a two-stream architecture with semantic and spatial pathways for vision-based manipulation. Specifically, we present CLIPort, a language-conditioned imitation-learning agent that combines the broad semantic understanding (what) of CLIP [1] with the spatial precision (where) of Transporter [2]. Our end-to-end framework is capable of solving a variety of language-specified tabletop tasks from packing unseen objects to folding cloths, all without any explicit representations of object poses, instance segmentations, memory, symbolic states, or syntactic structures. Experiments in simulated and real-world settings show that our approach is data efficient in few-shot settings and generalizes effectively to seen and unseen semantic concepts. We even learn one multi-task policy for 10 simulated and 9 real-world tasks that is better or comparable to single-task policies.",
                        "Citation Paper Authors": "Authors:Mohit Shridhar, Lucas Manuelli, Dieter Fox"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": ", where language instructions are encoded by a pretrained cross-modal text\nencoder ",
                    "Citation Text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\nSutskever. Learning transferable visual models from natural language supervision. CoRR ,\nabs/2103.00020, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.00020",
                        "Citation Paper Title": "Title:Learning Transferable Visual Models From Natural Language Supervision",
                        "Citation Paper Abstract": "Abstract:State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",
                        "Citation Paper Authors": "Authors:Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": ". We use the diffuser as a kinematic\ncontroller and train a target-state tracking policy. The baseline can also be viewed as a Decision\nDiffuser ",
                    "Citation Text": "Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit\nAgrawal. Is conditional generative modeling all you need for decision-making? arXiv preprint\narXiv:2211.15657 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2211.15657",
                        "Citation Paper Title": "Title:Is Conditional Generative Modeling all you need for Decision-Making?",
                        "Citation Paper Abstract": "Abstract:Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decision-making. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a return-conditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. Our results illustrate that conditional generative modeling is a powerful tool for decision-making.",
                        "Citation Paper Authors": "Authors:Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, Pulkit Agrawal"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": ".\nNonetheless, current methods are unable to accurately control a humanoid character, as evidenced by\nour experiments. Recent work utilizing diffusion model to generate high-level pedestrian trajectories\nand ground the trajectories with a low level controller ",
                    "Citation Text": "Davis Rempe, Zhengyi Luo, Xue Bin Peng, Ye Yuan, Kris Kitani, Karsten Kreis, Sanja Fidler,\nand Or Litany. Trace and pace: Controllable pedestrian animation via guided trajectory diffusion.\nInConference on Computer Vision and Pattern Recognition (CVPR) , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.01893",
                        "Citation Paper Title": "Title:Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion",
                        "Citation Paper Abstract": "Abstract:We introduce a method for generating realistic pedestrian trajectories and full-body animations that can be controlled to meet user-defined goals. We draw on recent advances in guided diffusion modeling to achieve test-time controllability of trajectories, which is normally only associated with rule-based systems. Our guided diffusion model allows users to constrain trajectories through target waypoints, speed, and specified social groups while accounting for the surrounding environment context. This trajectory diffusion model is integrated with a novel physics-based humanoid controller to form a closed-loop, full-body pedestrian animation system capable of placing large crowds in a simulated environment with varying terrains. We further propose utilizing the value function learned during RL training of the animation controller to guide diffusion to produce trajectories better suited for particular scenarios such as collision avoidance and traversing uneven terrain. Video results are available on the project page at this https URL .",
                        "Citation Paper Authors": "Authors:Davis Rempe, Zhengyi Luo, Xue Bin Peng, Ye Yuan, Kris Kitani, Karsten Kreis, Sanja Fidler, Or Litany"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.17116v1": {
            "Paper Title": "Generalizable Visual Reinforcement Learning with Segment Anything Model",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": "stabilize Q-\nfunction learning with a two-stream architecture. Yuan et al. ",
                    "Citation Text": "Zhecheng Yuan, Zhengrong Xue, Bo Yuan, Xueqian Wang,\nYi Wu, Yang Gao, and Huazhe Xu. Pre-trained im-\nage encoder for generalizable visual reinforcement learning.\nNeurIPS , 2022. 1, 2, 5, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.08860",
                        "Citation Paper Title": "Title:Pre-Trained Image Encoder for Generalizable Visual Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:Learning generalizable policies that can adapt to unseen environments remains challenging in visual Reinforcement Learning (RL). Existing approaches try to acquire a robust representation via diversifying the appearances of in-domain observations for better generalization. Limited by the specific observations of the environment, these methods ignore the possibility of exploring diverse real-world image datasets. In this paper, we investigate how a visual RL agent would benefit from the off-the-shelf visual representations. Surprisingly, we find that the early layers in an ImageNet pre-trained ResNet model could provide rather generalizable representations for visual RL. Hence, we propose Pre-trained Image Encoder for Generalizable visual reinforcement learning (PIE-G), a simple yet effective framework that can generalize to the unseen visual scenarios in a zero-shot manner. Extensive experiments are conducted on DMControl Generalization Benchmark, DMControl Manipulation Tasks, Drawer World, and CARLA to verify the effectiveness of PIE-G. Empirical evidence suggests PIE-G improves sample efficiency and significantly outperforms previous state-of-the-art methods in terms of generalization performance. In particular, PIE-G boasts a 55% generalization performance gain on average in the challenging video background setting. Project Page: this https URL.",
                        "Citation Paper Authors": "Authors:Zhecheng Yuan, Zhengrong Xue, Bo Yuan, Xueqian Wang, Yi Wu, Yang Gao, Huazhe Xu"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "pioneer the use of 3D pre-training for visual represen-\ntations and facilitate a robust sim-to-real transfer. Bertoin\net al. ",
                    "Citation Text": "David Bertoin, Adil Zouitine, Mehdi Zouitine, and Em-\nmanuel Rachelson. Look where you look! saliency-guided\nq-networks for generalization in visual reinforcement learn-\ning. NeurIPS , 2022. 1, 2, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2209.09203",
                        "Citation Paper Title": "Title:Look where you look! Saliency-guided Q-networks for generalization in visual Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:Deep reinforcement learning policies, despite their outstanding efficiency in simulated visual control tasks, have shown disappointing ability to generalize across disturbances in the input training images. Changes in image statistics or distracting background elements are pitfalls that prevent generalization and real-world applicability of such control policies. We elaborate on the intuition that a good visual policy should be able to identify which pixels are important for its decision, and preserve this identification of important sources of information across images. This implies that training of a policy with small generalization gap should focus on such important pixels and ignore the others. This leads to the introduction of saliency-guided Q-networks (SGQN), a generic method for visual reinforcement learning, that is compatible with any value function learning method. SGQN vastly improves the generalization capability of Soft Actor-Critic agents and outperforms existing stateof-the-art methods on the Deepmind Control Generalization benchmark, setting a new reference in terms of training efficiency, generalization gap, and policy interpretability.",
                        "Citation Paper Authors": "Authors:David Bertoin, Adil Zouitine, Mehdi Zouitine, Emmanuel Rachelson"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "propose to\ndecouple data augmentation from the policy learning pro-\ncess, thereby reducing the instability that augmentation may\nintroduce during training. Hansen et al. ",
                    "Citation Text": "Nicklas Hansen, Hao Su, and Xiaolong Wang. Stabilizing\ndeep q-learning with convnets and vision transformers under\ndata augmentation. NeurIPS , 2021. 1, 2, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.00644",
                        "Citation Paper Title": "Title:Stabilizing Deep Q-Learning with ConvNets and Vision Transformers under Data Augmentation",
                        "Citation Paper Abstract": "Abstract:While agents trained by Reinforcement Learning (RL) can solve increasingly challenging tasks directly from visual observations, generalizing learned skills to novel environments remains very challenging. Extensive use of data augmentation is a promising technique for improving generalization in RL, but it is often found to decrease sample efficiency and can even lead to divergence. In this paper, we investigate causes of instability when using data augmentation in common off-policy RL algorithms. We identify two problems, both rooted in high-variance Q-targets. Based on our findings, we propose a simple yet effective technique for stabilizing this class of algorithms under augmentation. We perform extensive empirical evaluation of image-based RL using both ConvNets and Vision Transformers (ViT) on a family of benchmarks based on DeepMind Control Suite, as well as in robotic manipulation tasks. Our method greatly improves stability and sample efficiency of ConvNets under augmentation, and achieves generalization results competitive with state-of-the-art methods for image-based RL in environments with unseen visuals. We further show that our method scales to RL with ViT-based architectures, and that data augmentation may be especially important in this setting.",
                        "Citation Paper Authors": "Authors:Nicklas Hansen, Hao Su, Xiaolong Wang"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "learn to recon-\nstruct foreground and background separately; Yuan et al. ",
                    "Citation Text": "Zhecheng Yuan, Guozheng Ma, Yao Mu, Bo Xia, Bo Yuan,\nXueqian Wang, Ping Luo, and Huazhe Xu. Don\u2019t touch what\nmatters: Task-aware lipschitz data augmentation for visual\nreinforcement learning. arXiv , 2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.09982",
                        "Citation Paper Title": "Title:Don't Touch What Matters: Task-Aware Lipschitz Data Augmentation for Visual Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:One of the key challenges in visual Reinforcement Learning (RL) is to learn policies that can generalize to unseen environments. Recently, data augmentation techniques aiming at enhancing data diversity have demonstrated proven performance in improving the generalization ability of learned policies. However, due to the sensitivity of RL training, naively applying data augmentation, which transforms each pixel in a task-agnostic manner, may suffer from instability and damage the sample efficiency, thus further exacerbating the generalization performance. At the heart of this phenomenon is the diverged action distribution and high-variance value estimation in the face of augmented images. To alleviate this issue, we propose Task-aware Lipschitz Data Augmentation (TLDA) for visual RL, which explicitly identifies the task-correlated pixels with large Lipschitz constants, and only augments the task-irrelevant pixels. To verify the effectiveness of TLDA, we conduct extensive experiments on DeepMind Control suite, CARLA and DeepMind Manipulation tasks, showing that TLDA improves both sample efficiency in training time and generalization in test time. It outperforms previous state-of-the-art methods across the 3 different visual control benchmarks.",
                        "Citation Paper Authors": "Authors:Zhecheng Yuan, Guozheng Ma, Yao Mu, Bo Xia, Bo Yuan, Xueqian Wang, Ping Luo, Huazhe Xu"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "utilize the keypoint detection and visual\nattention for segmentation; Fu et al. ",
                    "Citation Text": "Xiang Fu, Ge Yang, Pulkit Agrawal, and Tommi Jaakkola.\nLearning task informed abstractions. In ICML , 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.15612",
                        "Citation Paper Title": "Title:Learning Task Informed Abstractions",
                        "Citation Paper Abstract": "Abstract:Current model-based reinforcement learning methods struggle when operating from complex visual scenes due to their inability to prioritize task-relevant features. To mitigate this problem, we propose learning Task Informed Abstractions (TIA) that explicitly separates reward-correlated visual features from distractors. For learning TIA, we introduce the formalism of Task Informed MDP (TiMDP) that is realized by training two models that learn visual features via cooperative reconstruction, but one model is adversarially dissociated from the reward signal. Empirical evaluation shows that TIA leads to significant performance gains over state-of-the-art methods on many visual control tasks where natural and unconstrained visual distractions pose a formidable challenge.",
                        "Citation Paper Authors": "Authors:Xiang Fu, Ge Yang, Pulkit Agrawal, Tommi Jaakkola"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "employ saliency maps derived from Q-functions to\nguide agents in learning a mask. Yang et al. ",
                    "Citation Text": "Sizhe Yang, Yanjie Ze, and Huazhe Xu. Movie: Vi-\nsual model-based policy adaptation for view generalization.\narXiv , 2023. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2307.00972",
                        "Citation Paper Title": "Title:MoVie: Visual Model-Based Policy Adaptation for View Generalization",
                        "Citation Paper Abstract": "Abstract:Visual Reinforcement Learning (RL) agents trained on limited views face significant challenges in generalizing their learned abilities to unseen views. This inherent difficulty is known as the problem of $\\textit{view generalization}$. In this work, we systematically categorize this fundamental problem into four distinct and highly challenging scenarios that closely resemble real-world situations. Subsequently, we propose a straightforward yet effective approach to enable successful adaptation of visual $\\textbf{Mo}$del-based policies for $\\textbf{Vie}$w generalization ($\\textbf{MoVie}$) during test time, without any need for explicit reward signals and any modification during training time. Our method demonstrates substantial advancements across all four scenarios encompassing a total of $\\textbf{18}$ tasks sourced from DMControl, xArm, and Adroit, with a relative improvement of $\\mathbf{33}$%, $\\mathbf{86}$%, and $\\mathbf{152}$% respectively. The superior results highlight the immense potential of our approach for real-world robotics applications. Videos are available at this https URL .",
                        "Citation Paper Authors": "Authors:Sizhe Yang, Yanjie Ze, Huazhe Xu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.17110v1": {
            "Paper Title": "Toward Semantic Scene Understanding for Fine-Grained 3D Modeling of\n  Plants",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.16741v1": {
            "Paper Title": "Bin-picking of novel objects through category-agnostic-segmentation: RGB\n  matters",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": ". It has the potential\nfor complex tasks, such as bin-picking ",
                    "Citation Text": "M. Danielczuk, M. Matl, S. Gupta, A. Li, A. Lee, J. Mahler, and\nK. Goldberg, \u201cSegmenting unknown 3d objects from real depth images\nusing mask r-cnn trained on synthetic data,\u201d in 2019 International\nConference on Robotics and Automation (ICRA) . IEEE, 2019, pp.\n7283\u20137290.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.05825",
                        "Citation Paper Title": "Title:Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data",
                        "Citation Paper Abstract": "Abstract:The ability to segment unknown objects in depth images has potential to enhance robot skills in grasping and object tracking. Recent computer vision research has demonstrated that Mask R-CNN can be trained to segment specific categories of objects in RGB images when massive hand-labeled datasets are available. As generating these datasets is time consuming, we instead train with synthetic depth images. Many robots now use depth sensors, and recent results suggest training on synthetic depth data can transfer successfully to the real world. We present a method for automated dataset generation and rapidly generate a synthetic training dataset of 50,000 depth images and 320,000 object masks using simulated heaps of 3D CAD models. We train a variant of Mask R-CNN with domain randomization on the generated dataset to perform category-agnostic instance segmentation without any hand-labeled data and we evaluate the trained network, which we refer to as Synthetic Depth (SD) Mask R-CNN, on a set of real, high-resolution depth images of challenging, densely-cluttered bins containing objects with highly-varied geometry. SD Mask R-CNN outperforms point cloud clustering baselines by an absolute 15% in Average Precision and 20% in Average Recall on COCO benchmarks, and achieves performance levels similar to a Mask R-CNN trained on a massive, hand-labeled RGB dataset and fine-tuned on real images from the experimental setup. We deploy the model in an instance-specific grasping pipeline to demonstrate its usefulness in a robotics application. Code, the synthetic training dataset, and supplementary material are available at this https URL.",
                        "Citation Paper Authors": "Authors:Michael Danielczuk, Matthew Matl, Saurabh Gupta, Andrew Li, Andrew Lee, Jeffrey Mahler, Ken Goldberg"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": ". One type of solution for this category of bin-picking\nis designed to be gripper-specific ",
                    "Citation Text": "J. Mahler, J. Liang, S. Niyaz, M. Laskey, R. Doan, X. Liu, J. A. Ojea,\nand K. Goldberg, \u201cDex-net 2.0: Deep learning to plan robust grasps with\nsynthetic point clouds and analytic grasp metrics,\u201d in in Proceedings of\nRobotics: Science and Systems (RSS) . IEEE, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.09312",
                        "Citation Paper Title": "Title:Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics",
                        "Citation Paper Abstract": "Abstract:To reduce data collection time for deep learning of robust robotic grasp plans, we explore training from a synthetic dataset of 6.7 million point clouds, grasps, and analytic grasp metrics generated from thousands of 3D models from Dex-Net 1.0 in randomized poses on a table. We use the resulting dataset, Dex-Net 2.0, to train a Grasp Quality Convolutional Neural Network (GQ-CNN) model that rapidly predicts the probability of success of grasps from depth images, where grasps are specified as the planar position, angle, and depth of a gripper relative to an RGB-D sensor. Experiments with over 1,000 trials on an ABB YuMi comparing grasp planning methods on singulated objects suggest that a GQ-CNN trained with only synthetic data from Dex-Net 2.0 can be used to plan grasps in 0.8sec with a success rate of 93% on eight known objects with adversarial geometry and is 3x faster than registering point clouds to a precomputed dataset of objects and indexing grasps. The Dex-Net 2.0 grasp planner also has the highest success rate on a dataset of 10 novel rigid objects and achieves 99% precision (one false positive out of 69 grasps classified as robust) on a dataset of 40 novel household objects, some of which are articulated or deformable. Code, datasets, videos, and supplementary material are available at this http URL .",
                        "Citation Paper Authors": "Authors:Jeffrey Mahler, Jacky Liang, Sherdil Niyaz, Michael Laskey, Richard Doan, Xinyu Liu, Juan Aparicio Ojea, Ken Goldberg"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": ". Another important area of research\nin instance segmentation is panoptic segmentation, which\ncombines instance segmentation with semantic segmentation\nto provide a unified view of the scene ",
                    "Citation Text": "A. Kirillov, R. Girshick, K. He, and P. Doll\u2019ar, \u201cPanoptic segmentation,\u201d\ninProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.00868",
                        "Citation Paper Title": "Title:Panoptic Segmentation",
                        "Citation Paper Abstract": "Abstract:We propose and study a task we name panoptic segmentation (PS). Panoptic segmentation unifies the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance). The proposed task requires generating a coherent scene segmentation that is rich and complete, an important step toward real-world vision systems. While early work in computer vision addressed related image/scene parsing tasks, these are not currently popular, possibly due to lack of appropriate metrics or associated recognition challenges. To address this, we propose a novel panoptic quality (PQ) metric that captures performance for all classes (stuff and things) in an interpretable and unified manner. Using the proposed metric, we perform a rigorous study of both human and machine performance for PS on three existing datasets, revealing interesting insights about the task. The aim of our work is to revive the interest of the community in a more unified view of image segmentation.",
                        "Citation Paper Authors": "Authors:Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, Piotr Doll\u00e1r"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": ", one of the most widelyused instance segmentation methods, extends the popular\nobject detection framework, Faster R-CNN ",
                    "Citation Text": "S. Ren, K. He, R. Girshick, and J. Sun, \u201cFaster r-cnn: Towards real-time\nobject detection with region proposal networks,\u201d in Proceedings of the\nConference on Neural Information Processing Systems (NIPS) , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.01497",
                        "Citation Paper Title": "Title:Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
                        "Citation Paper Abstract": "Abstract:State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.",
                        "Citation Paper Authors": "Authors:Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2310.01824v2": {
            "Paper Title": "Mini-BEHAVIOR: A Procedurally Generated Benchmark for Long-horizon\n  Decision-Making in Embodied AI",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.08820v3": {
            "Paper Title": "How to Raise a Robot -- A Case for Neuro-Symbolic AI in Constrained Task\n  Planning for Humanoid Assistive Robots",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": ". By directly incorporating\naccess control into task planning, robots are unable to even \u201cthink\nabout\u201d forbidden behavior. This tech report is an extended version\nof our poster abstract ",
                    "Citation Text": "Niklas Hemken, Florian Jacob, Fabian Peller-Konrad, Rainer Kartmann, Tamim\nAsfour, and Hannes Hartenstein. 2023. Poster: How to Raise a Robot - Beyond\nAccess Control Constraints in Assistive Humanoid Robots. In Proceedings of\nthe 28th ACM Symposium on Access Control Models and Technologies (Trento,\nItaly) (SACMAT \u201923) . Association for Computing Machinery, New York, NY, USA,\n55\u201357. https://doi.org/10.1145/3589608.3595078",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2312.08820",
                        "Citation Paper Title": "Title:How to Raise a Robot -- A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots",
                        "Citation Paper Abstract": "Abstract:Humanoid robots will be able to assist humans in their daily life, in particular due to their versatile action capabilities. However, while these robots need a certain degree of autonomy to learn and explore, they also should respect various constraints, for access control and beyond. We explore the novel field of incorporating privacy, security, and access control constraints with robot task planning approaches. We report preliminary results on the classical symbolic approach, deep-learned neural networks, and modern ideas using large language models as knowledge base. From analyzing their trade-offs, we conclude that a hybrid approach is necessary, and thereby present a new use case for the emerging field of neuro-symbolic artificial intelligence.",
                        "Citation Paper Authors": "Authors:Niklas Hemken, Florian Jacob, Fabian Peller-Konrad, Rainer Kartmann, Tamim Asfour, Hannes Hartenstein"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.16648v1": {
            "Paper Title": "LIP-Loc: LiDAR Image Pretraining for Cross-Modal Localization",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": "was for ImageNet, but all these factors could\nmake it equivalent to ImageNetV2 ",
                    "Citation Text": "Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and\nVaishaal Shankar. Do imagenet classifiers generalize to im-\nagenet?, 2019. 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.10811",
                        "Citation Paper Title": "Title:Do ImageNet Classifiers Generalize to ImageNet?",
                        "Citation Paper Abstract": "Abstract:We build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively re-used test sets. By closely following the original dataset creation processes, we test to what extent current classification models generalize to new data. We evaluate a broad range of models and find accuracy drops of 3% - 15% on CIFAR-10 and 11% - 14% on ImageNet. However, accuracy gains on the original test sets translate to larger gains on the new test sets. Our results suggest that the accuracy drops are not caused by adaptivity, but by the models' inability to generalize to slightly \"harder\" images than those found in the original test sets.",
                        "Citation Paper Authors": "Authors:Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, Vaishaal Shankar"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": ", which are designed to extract local features\nfrom the image. However, with the advent of deep learning,\nrecent years have seen a shift towards the use of convolu-\ntional neural networks (CNNs) such as ResNet ",
                    "Citation Text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition, 2015. 2, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.03385",
                        "Citation Paper Title": "Title:Deep Residual Learning for Image Recognition",
                        "Citation Paper Abstract": "Abstract:Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
                        "Citation Paper Authors": "Authors:Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "presents a visual-LiDAR descriptor fusion in a weighted\nFigure 1. Batched Contrastive Learning Architecture\nway using a pairwise margin-based loss. A similar approach\ni3dLoc ",
                    "Citation Text": "Peng Yin, Xu Lingyun, Ji Zhang, Howie Choset, and Se-\nbastian Scherer. i3dloc: Image-to-range cross-domain local-\nization robust to inconsistent environmental conditions. 07\n2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.12883",
                        "Citation Paper Title": "Title:i3dLoc: Image-to-range Cross-domain Localization Robust to Inconsistent Environmental Conditions",
                        "Citation Paper Abstract": "Abstract:We present a method for localizing a single camera with respect to a point cloud map in indoor and outdoor scenes. The problem is challenging because correspondences of local invariant features are inconsistent across the domains between image and 3D. The problem is even more challenging as the method must handle various environmental conditions such as illumination, weather, and seasonal changes. Our method can match equirectangular images to the 3D range projections by extracting cross-domain symmetric place descriptors. Our key insight is to retain condition-invariant 3D geometry features from limited data samples while eliminating the condition-related features by a designed Generative Adversarial Network. Based on such features, we further design a spherical convolution network to learn viewpoint-invariant symmetric place descriptors. We evaluate our method on extensive self-collected datasets, which involve \\textit{Long-term} (variant appearance conditions), \\textit{Large-scale} (up to $2km$ structure/unstructured environment), and \\textit{Multistory} (four-floor confined space). Our method surpasses other current state-of-the-arts by achieving around $3$ times higher place retrievals to inconsistent environments, and above $3$ times accuracy on online localization. To highlight our method's generalization capabilities, we also evaluate the recognition across different datasets. With a single trained model, i3dLoc can demonstrate reliable visual localization in random conditions.",
                        "Citation Paper Authors": "Authors:Peng Yin, Lingyun Xu, Ji Zhang, Howie Choset, Sebastian Scherer"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "proposed a teacher-\nstudent training approach on Oxford Robotcar using triplet\nloss and jointly trains a 2D network for the images and 3D\nnetwork for the point clouds. Similarly, AdaFusion ",
                    "Citation Text": "Haowen Lai, Peng Yin, and Sebastian Scherer. Adafusion:\nVisual-lidar fusion with adaptive weights for place recogni-\ntion. IEEE Robotics and Automation Letters , PP:1\u20138, 10\n2022. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.11739",
                        "Citation Paper Title": "Title:AdaFusion: Visual-LiDAR Fusion with Adaptive Weights for Place Recognition",
                        "Citation Paper Abstract": "Abstract:Recent years have witnessed the increasing application of place recognition in various environments, such as city roads, large buildings, and a mix of indoor and outdoor places. This task, however, still remains challenging due to the limitations of different sensors and the changing appearance of environments. Current works only consider the use of individual sensors, or simply combine different sensors, ignoring the fact that the importance of different sensors varies as the environment changes. In this paper, an adaptive weighting visual-LiDAR fusion method, named AdaFusion, is proposed to learn the weights for both images and point cloud features. Features of these two modalities are thus contributed differently according to the current environmental situation. The learning of weights is achieved by the attention branch of the network, which is then fused with the multi-modality feature extraction branch. Furthermore, to better utilize the potential relationship between images and point clouds, we design a twostage fusion approach to combine the 2D and 3D attention. Our work is tested on two public datasets, and experiments show that the adaptive weights help improve recognition accuracy and system robustness to varying environments.",
                        "Citation Paper Authors": "Authors:Haowen Lai, Peng Yin, Sebastian Scherer"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "proposes a deep network to jointly learn the\n2D image and 3D point cloud keypoint descriptors.\nMore recently, Cattaneo et al . ",
                    "Citation Text": "D. Cattaneo, M. Vaghi, S. Fontana, A. L. Ballardini, and\nD. G. Sorrenti. Global visual localization in lidar-maps\nthrough shared 2d-3d embedding space. 2020 IEEE Interna-\ntional Conference on Robotics and Automation (ICRA) , May\n2020. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.04871",
                        "Citation Paper Title": "Title:Global visual localization in LiDAR-maps through shared 2D-3D embedding space",
                        "Citation Paper Abstract": "Abstract:Global localization is an important and widely studied problem for many robotic applications. Place recognition approaches can be exploited to solve this task, e.g., in the autonomous driving field. While most vision-based approaches match an image w.r.t. an image database, global visual localization within LiDAR-maps remains fairly unexplored, even though the path toward high definition 3D maps, produced mainly from LiDARs, is clear. In this work we leverage Deep Neural Network (DNN) approaches to create a shared embedding space between images and LiDAR-maps, allowing for image to 3D-LiDAR place recognition. We trained a 2D and a 3D DNN that create embeddings, respectively from images and from point clouds, that are close to each other whether they refer to the same place. An extensive experimental activity is presented to assess the effectiveness of the approach w.r.t. different learning paradigms, network architectures, and loss functions. All the evaluations have been performed using the Oxford Robotcar Dataset, which encompasses a wide range of weather and light conditions.",
                        "Citation Paper Authors": "Authors:Daniele Cattaneo, Matteo Vaghi, Simone Fontana, Augusto Luis Ballardini, Domenico Giorgio Sorrenti"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.16502v1": {
            "Paper Title": "Bezier-based Regression Feature Descriptor for Deformable Linear Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.16465v1": {
            "Paper Title": "Multi-Contact Whole Body Force Control for Position-Controlled Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.04316v3": {
            "Paper Title": "Towards Knowledge-driven Autonomous Driving",
            "Sentences": [
                {
                    "Sentence ID": 262,
                    "Sentence": ". LLM-based systems demonstrate the capa-\nbility to identify objects on roads and comprehend traffic signs ",
                    "Citation Text": "A. Elhafsi, R. Sinha, C. Agia, E. Schmerling, I. A. Nesnas, and\nM. Pavone, \u201cSemantic anomaly detection with large language models,\u201d\nAutonomous Robots , pp. 1\u201321, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.11307",
                        "Citation Paper Title": "Title:Semantic Anomaly Detection with Large Language Models",
                        "Citation Paper Abstract": "Abstract:As robots acquire increasingly sophisticated skills and see increasingly complex and varied environments, the threat of an edge case or anomalous failure is ever present. For example, Tesla cars have seen interesting failure modes ranging from autopilot disengagements due to inactive traffic lights carried by trucks to phantom braking caused by images of stop signs on roadside billboards. These system-level failures are not due to failures of any individual component of the autonomy stack but rather system-level deficiencies in semantic reasoning. Such edge cases, which we call semantic anomalies, are simple for a human to disentangle yet require insightful reasoning. To this end, we study the application of large language models (LLMs), endowed with broad contextual understanding and reasoning capabilities, to recognize such edge cases and introduce a monitoring framework for semantic anomaly detection in vision-based policies. Our experiments apply this framework to a finite state machine policy for autonomous driving and a learned policy for object manipulation. These experiments demonstrate that the LLM-based monitor can effectively identify semantic anomalies in a manner that shows agreement with human reasoning. Finally, we provide an extended discussion on the strengths and weaknesses of this approach and motivate a research outlook on how we can further use foundation models for semantic anomaly detection.",
                        "Citation Paper Authors": "Authors:Amine Elhafsi, Rohan Sinha, Christopher Agia, Edward Schmerling, Issa Nesnas, Marco Pavone"
                    }
                },
                {
                    "Sentence ID": 254,
                    "Sentence": ". Despite the advantages of few-shot\nprompts, challenges exist, especially in complex tasks where the\nnumber of prompts may be insufficient. Building powerful au-\ntonomous driving systems involves fine-tuning generalized models\nfor specific driving scenarios ",
                    "Citation Text": "B. Peng, C. Li, P. He, M. Galley, and J. Gao, \u201cInstruction tuning with\nGPT-4,\u201d arXiv preprint arXiv:2304.03277 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.03277",
                        "Citation Paper Title": "Title:Instruction Tuning with GPT-4",
                        "Citation Paper Abstract": "Abstract:Prior work has shown that finetuning large language models (LLMs) using machine-generated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and no human-written instructions are needed. In this paper, we present the first attempt to use GPT-4 to generate instruction-following data for LLM finetuning. Our early experiments on instruction-tuned LLaMA models show that the 52K English and Chinese instruction-following data generated by GPT-4 leads to superior zero-shot performance on new tasks to the instruction-following data generated by previous state-of-the-art models. We also collect feedback and comparison data from GPT-4 to enable a comprehensive evaluation and reward model training. We make our data generated using GPT-4 as well as our codebase publicly available.",
                        "Citation Paper Authors": "Authors:Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, Jianfeng Gao"
                    }
                },
                {
                    "Sentence ID": 61,
                    "Sentence": ". Some works seek to emulate and even fully\nreplace drivers ",
                    "Citation Text": "Z. Xu, Y . Zhang, E. Xie, Z. Zhao, Y . Guo, K. K. Wong, Z. Li, and\nH. Zhao, \u201cDriveGPT4: Interpretable end-to-end autonomous driving via\nlarge language model,\u201d arXiv preprint arXiv:2310.01412 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2310.01412",
                        "Citation Paper Title": "Title:DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model",
                        "Citation Paper Abstract": "Abstract:In the past decade, autonomous driving has experienced rapid development in both academia and industry. However, its limited interpretability remains a significant unsolved problem, severely hindering autonomous vehicle commercialization and further development. Previous approaches utilizing small language models have failed to address this issue due to their lack of flexibility, generalization ability, and robustness. Recently, multimodal large language models (LLMs) have gained considerable attention from the research community for their capability to process and reason non-text data (e.g., images and videos) by text. In this paper, we present DriveGPT4, an interpretable end-to-end autonomous driving system utilizing LLMs. DriveGPT4 is capable of interpreting vehicle actions and providing corresponding reasoning, as well as answering diverse questions posed by human users for enhanced interaction. Additionally, DriveGPT4 predicts vehicle low-level control signals in an end-to-end fashion. These capabilities stem from a customized visual instruction tuning dataset specifically designed for autonomous driving. To the best of our knowledge, DriveGPT4 is the first work focusing on interpretable end-to-end autonomous driving. When evaluated on multiple tasks alongside conventional methods and video understanding LLMs, DriveGPT4 demonstrates superior qualitative and quantitative performance. Additionally, DriveGPT4 can be generalized in a zero-shot fashion to accommodate more unseen scenarios. The project page is available at this https URL .",
                        "Citation Paper Authors": "Authors:Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, Kwan-Yee. K. Wong, Zhenguo Li, Hengshuang Zhao"
                    }
                },
                {
                    "Sentence ID": 80,
                    "Sentence": "is a fully end-to-end generative13\nmodel that utilizes video, text, and action inputs to generate real\ndriving scenarios, and also enables prediction of future tokenized\nsequences. Differing from the aforementioned approaches, Zhang\net al. ",
                    "Citation Text": "L. Zhang, Y . Xiong, Z. Yang, S. Casas, R. Hu, and R. Urtasun, \u201cLearn-\ning unsupervised world models for autonomous driving via discrete\ndiffusion,\u201d arXiv preprint arXiv:2311.01017 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2311.01017",
                        "Citation Paper Title": "Title:Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion",
                        "Citation Paper Abstract": "Abstract:Learning world models can teach an agent how the world works in an unsupervised manner. Even though it can be viewed as a special case of sequence modeling, progress for scaling world models on robotic applications such as autonomous driving has been somewhat less rapid than scaling language models with Generative Pre-trained Transformers (GPT). We identify two reasons as major bottlenecks: dealing with complex and unstructured observation space, and having a scalable generative model. Consequently, we propose a novel world modeling approach that first tokenizes sensor observations with VQVAE, then predicts the future via discrete diffusion. To efficiently decode and denoise tokens in parallel, we recast Masked Generative Image Transformer into the discrete diffusion framework with a few simple changes, resulting in notable improvement. When applied to learning world models on point cloud observations, our model reduces prior SOTA Chamfer distance by more than 65% for 1s prediction, and more than 50% for 3s prediction, across NuScenes, KITTI Odometry, and Argoverse2 datasets. Our results demonstrate that discrete diffusion on tokenized agent experience can unlock the power of GPT-like unsupervised learning for robotic agents.",
                        "Citation Paper Authors": "Authors:Lunjun Zhang, Yuwen Xiong, Ze Yang, Sergio Casas, Rui Hu, Raquel Urtasun"
                    }
                },
                {
                    "Sentence ID": 223,
                    "Sentence": "propose an unsupervised world model on sensor data\nderived from point clouds, it tokenizes point clouds using a\nvector quantized variational autoencoder (VQV AE) ",
                    "Citation Text": "A. Van Den Oord, O. Vinyals et al. , \u201cNeural discrete representation\nlearning,\u201d Advances in Neural Information Processing Systems , vol. 30,\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.00937",
                        "Citation Paper Title": "Title:Neural Discrete Representation Learning",
                        "Citation Paper Abstract": "Abstract:Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of \"posterior collapse\" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.",
                        "Citation Paper Authors": "Authors:Aaron van den Oord, Oriol Vinyals, Koray Kavukcuoglu"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "\u2717 \u2717 \u2713 5.5 97.0\n\u2020means that the generation quality are evaluated on the nuScenes ",
                    "Citation Text": "H. Caesar, V . Bankiti, A. H. Lang, S. V ora, V . E. Liong, Q. Xu,\nA. Krishnan, Y . Pan, G. Baldan, and O. Beijbom, \u201cnuScenes: A\nmultimodal dataset for autonomous driving,\u201d in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2020, pp. 11 621\u201311 631.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.11027",
                        "Citation Paper Title": "Title:nuScenes: A multimodal dataset for autonomous driving",
                        "Citation Paper Abstract": "Abstract:Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online.",
                        "Citation Paper Authors": "Authors:Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, Oscar Beijbom"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.16170v1": {
            "Paper Title": "EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards\n  Embodied AI",
            "Sentences": [
                {
                    "Sentence ID": 61,
                    "Sentence": ". We use cross-entropy loss and scene-\nclass affinity loss ",
                    "Citation Text": "Yi Wei, Linqing Zhao, Wenzhao Zheng, Zheng Zhu, Jie\nZhou, and Jiwen Lu. Surroundocc: Multi-camera 3d occu-\npancy prediction for autonomous driving. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, 2023. 3, 6, 7, 8, 9, 11",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.09551",
                        "Citation Paper Title": "Title:SurroundOcc: Multi-Camera 3D Occupancy Prediction for Autonomous Driving",
                        "Citation Paper Abstract": "Abstract:3D scene understanding plays a vital role in vision-based autonomous driving. While most existing methods focus on 3D object detection, they have difficulty describing real-world objects of arbitrary shapes and infinite classes. Towards a more comprehensive perception of a 3D scene, in this paper, we propose a SurroundOcc method to predict the 3D occupancy with multi-camera images. We first extract multi-scale features for each image and adopt spatial 2D-3D attention to lift them to the 3D volume space. Then we apply 3D convolutions to progressively upsample the volume features and impose supervision on multiple levels. To obtain dense occupancy prediction, we design a pipeline to generate dense occupancy ground truth without expansive occupancy annotations. Specifically, we fuse multi-frame LiDAR scans of dynamic objects and static scenes separately. Then we adopt Poisson Reconstruction to fill the holes and voxelize the mesh to get dense occupancy labels. Extensive experiments on nuScenes and SemanticKITTI datasets demonstrate the superiority of our method. Code and dataset are available at this https URL",
                        "Citation Paper Authors": "Authors:Yi Wei, Linqing Zhao, Wenzhao Zheng, Zheng Zhu, Jie Zhou, Jiwen Lu"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "for supervising\nthe multi-scale output. During inference, we only take the\nhigh-resolution output as the final prediction.\nFor the visual grounding decoder, we adopt several trans-\nformer layers to fuse the 3D sparse feature and text feature.\nSimilar to GroupFree3D ",
                    "Citation Text": "Ze Liu, Zheng Zhang, Yue Cao, Han Hu, and Xin Tong.\nGroup-free 3d object detection via transformers. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision , 2021. 3, 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.00678",
                        "Citation Paper Title": "Title:Group-Free 3D Object Detection via Transformers",
                        "Citation Paper Abstract": "Abstract:Recently, directly detecting 3D objects from 3D point clouds has received increasing attention. To extract object representation from an irregular point cloud, existing methods usually take a point grouping step to assign the points to an object candidate so that a PointNet-like network could be used to derive object features from the grouped points. However, the inaccurate point assignments caused by the hand-crafted grouping scheme decrease the performance of 3D object detection.\nIn this paper, we present a simple yet effective method for directly detecting 3D objects from the 3D point cloud. Instead of grouping local points to each object candidate, our method computes the feature of an object from all the points in the point cloud with the help of an attention mechanism in the Transformers \\cite{vaswani2017attention}, where the contribution of each point is automatically learned in the network training. With an improved attention stacking scheme, our method fuses object features in different stages and generates more accurate object detection results. With few bells and whistles, the proposed method achieves state-of-the-art 3D object detection performance on two widely used benchmarks, ScanNet V2 and SUN RGB-D. The code and models are publicly available at \\url{this https URL}",
                        "Citation Paper Authors": "Authors:Ze Liu, Zheng Zhang, Yue Cao, Han Hu, Xin Tong"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "Depth 3.20 6.11 0.38 1.22 6.31 12.26 1.81 3.34 1.00 1.83\nFCAF3D ",
                    "Citation Text": "Danila Rukhovich, Anna V orontsova, and Anton Konushin.\nFcaf3d: Fully convolutional anchor-free 3d object detection.\nInEuropean Conference on Computer Vision , 2022. 2, 3, 6,\n7, 8, 9, 15",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.00322",
                        "Citation Paper Title": "Title:FCAF3D: Fully Convolutional Anchor-Free 3D Object Detection",
                        "Citation Paper Abstract": "Abstract:Recently, promising applications in robotics and augmented reality have attracted considerable attention to 3D object detection from point clouds. In this paper, we present FCAF3D - a first-in-class fully convolutional anchor-free indoor 3D object detection method. It is a simple yet effective method that uses a voxel representation of a point cloud and processes voxels with sparse convolutions. FCAF3D can handle large-scale scenes with minimal runtime through a single fully convolutional feed-forward pass. Existing 3D object detection methods make prior assumptions on the geometry of objects, and we argue that it limits their generalization ability. To get rid of any prior assumptions, we propose a novel parametrization of oriented bounding boxes that allows obtaining better results in a purely data-driven way. The proposed method achieves state-of-the-art 3D object detection results in terms of mAP@0.5 on ScanNet V2 (+4.5), SUN RGB-D (+3.5), and S3DIS (+20.5) datasets. The code and models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Danila Rukhovich, Anna Vorontsova, Anton Konushin"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "Depth 14.30 31.44 1.68 5.14 54.00 2.41 19.53 14.72 21.80 45.58 13.49 68.16\nImV oteNet ",
                    "Citation Text": "Charles R Qi, Xinlei Chen, Or Litany, and Leonidas J\nGuibas. Imvotenet: Boosting 3d object detection in point\nclouds with image votes. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition ,\n2020. 3, 7, 15",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2001.10692",
                        "Citation Paper Title": "Title:ImVoteNet: Boosting 3D Object Detection in Point Clouds with Image Votes",
                        "Citation Paper Abstract": "Abstract:3D object detection has seen quick progress thanks to advances in deep learning on point clouds. A few recent works have even shown state-of-the-art performance with just point clouds input (e.g. VoteNet). However, point cloud data have inherent limitations. They are sparse, lack color information and often suffer from sensor noise. Images, on the other hand, have high resolution and rich texture. Thus they can complement the 3D geometry provided by point clouds. Yet how to effectively use image information to assist point cloud based detection is still an open question. In this work, we build on top of VoteNet and propose a 3D detection architecture called ImVoteNet specialized for RGB-D scenes. ImVoteNet is based on fusing 2D votes in images and 3D votes in point clouds. Compared to prior work on multi-modal detection, we explicitly extract both geometric and semantic features from the 2D images. We leverage camera parameters to lift these features to 3D. To improve the synergy of 2D-3D feature fusion, we also propose a multi-tower training scheme. We validate our model on the challenging SUN RGB-D dataset, advancing state-of-the-art results by 5.7 mAP. We also provide rich ablation studies to analyze the contribution of each design choice.",
                        "Citation Paper Authors": "Authors:Charles R. Qi, Xinlei Chen, Or Litany, Leonidas J. Guibas"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.16168v1": {
            "Paper Title": "Social-Transmotion: Promptable Human Trajectory Prediction",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": ". In\naddition, we compare with recent state-of-the-art models EqMotion ",
                    "Citation Text": "Chenxin Xu, Robby T Tan, Yuhong Tan, Siheng Chen, Yu Guang Wang, Xinchao Wang, and Yanfeng Wang. Eqmotion:\nEquivariant multi-agent motion prediction with invariant interaction reasoning. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 1410\u20131420, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.10876",
                        "Citation Paper Title": "Title:EqMotion: Equivariant Multi-agent Motion Prediction with Invariant Interaction Reasoning",
                        "Citation Paper Abstract": "Abstract:Learning to predict agent motions with relationship reasoning is important for many applications. In motion prediction tasks, maintaining motion equivariance under Euclidean geometric transformations and invariance of agent interaction is a critical and fundamental principle. However, such equivariance and invariance properties are overlooked by most existing methods. To fill this gap, we propose EqMotion, an efficient equivariant motion prediction model with invariant interaction reasoning. To achieve motion equivariance, we propose an equivariant geometric feature learning module to learn a Euclidean transformable feature through dedicated designs of equivariant operations. To reason agent's interactions, we propose an invariant interaction reasoning module to achieve a more stable interaction modeling. To further promote more comprehensive motion features, we propose an invariant pattern feature learning module to learn an invariant pattern feature, which cooperates with the equivariant geometric feature to enhance network expressiveness. We conduct experiments for the proposed model on four distinct scenarios: particle dynamics, molecule dynamics, human skeleton motion prediction and pedestrian trajectory prediction. Experimental results show that our method is not only generally applicable, but also achieves state-of-the-art prediction performances on all the four tasks, improving by 24.0/30.1/8.6/9.2%. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Chenxin Xu, Robby T. Tan, Yuhong Tan, Siheng Chen, Yu Guang Wang, Xinchao Wang, Yanfeng Wang"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": ", which highlighted the utility of an individual pedestrian\u2019s\n3d body pose for predicting their trajectory. However, our research incorporates social interactions among poses, a\nfeature overlooked in their study. Also, unlike ",
                    "Citation Text": "Irtiza Hasan, Francesco Setti, Theodore Tsesmelis, Vasileios Belagiannis, Sikandar Amin, Alessio Del Bue, Marco Cristani,\nand Fabio Galasso. Forecasting people trajectories and head poses by jointly reasoning on tracklets and vislets. IEEE\ntransactions on pattern analysis and machine intelligence , 43(4):1267\u20131278, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.02000",
                        "Citation Paper Title": "Title:Forecasting People Trajectories and Head Poses by Jointly Reasoning on Tracklets and Vislets",
                        "Citation Paper Abstract": "Abstract:In this work, we explore the correlation between people trajectories and their head orientations. We argue that people trajectory and head pose forecasting can be modelled as a joint problem. Recent approaches on trajectory forecasting leverage short-term trajectories (aka tracklets) of pedestrians to predict their future paths. In addition, sociological cues, such as expected destination or pedestrian interaction, are often combined with tracklets. In this paper, we propose MiXing-LSTM (MX-LSTM) to capture the interplay between positions and head orientations (vislets) thanks to a joint unconstrained optimization of full covariance matrices during the LSTM backpropagation. We additionally exploit the head orientations as a proxy for the visual attention, when modeling social interactions. MX-LSTM predicts future pedestrians location and head pose, increasing the standard capabilities of the current approaches on long-term trajectory forecasting. Compared to the state-of-the-art, our approach shows better performances on an extensive set of public benchmarks. MX-LSTM is particularly effective when people move slowly, i.e. the most challenging scenario for all other models. The proposed approach also allows for accurate predictions on a longer time horizon.",
                        "Citation Paper Authors": "Authors:Irtiza Hasan, Francesco Setti, Theodore Tsesmelis, Vasileios Belagiannis, Sikandar Amin, Alessio Del Bue, Marco Cristani, Fabio Galasso"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.16106v1": {
            "Paper Title": "Clique Analysis and Bypassing in Continuous-Time Conflict-Based Search",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": ". This is followed by a description of the new symme-\ntry breaking enhancements. Finally, we present a comprehensive\nablation study on all of the enhancements.\n2 PROBLEM DEFINITION\nMAPF was originally defined for a \u201cclassic\" setting ",
                    "Citation Text": "Roni Stern, Nathan R. Sturtevant, Ariel Felner, Sven Koenig, Hang Ma, Thayne T.\nWalker, Jiaoyang Li, Dor Atzmon, T. K. Satish Kumar, Eli Boyarski, and Roman\nBart\u00e1k. 2019. Multi-Agent Pathfinding: Definitions, Variants, and Benchmarks.\nInInternational Symposium on Combinatorial Search . 151\u2013159.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.08291",
                        "Citation Paper Title": "Title:Multi-Agent Pathfinding: Definitions, Variants, and Benchmarks",
                        "Citation Paper Abstract": "Abstract:The MAPF problem is the fundamental problem of planning paths for multiple agents, where the key constraint is that the agents will be able to follow these paths concurrently without colliding with each other. Applications of MAPF include automated warehouses and autonomous vehicles. Research on MAPF has been flourishing in the past couple of years. Different MAPF research papers make different assumptions, e.g., whether agents can traverse the same road at the same time, and have different objective functions, e.g., minimize makespan or sum of agents' actions costs. These assumptions and objectives are sometimes implicitly assumed or described informally. This makes it difficult to establish appropriate baselines for comparison in research papers, as well as making it difficult for practitioners to find the papers relevant to their concrete application. This paper aims to fill this gap and support researchers and practitioners by providing a unifying terminology for describing common MAPF assumptions and objectives. In addition, we also provide pointers to two MAPF benchmarks. In particular, we introduce a new grid-based benchmark for MAPF, and demonstrate experimentally that it poses a challenge to contemporary MAPF algorithms.",
                        "Citation Paper Authors": "Authors:Roni Stern, Nathan Sturtevant, Ariel Felner, Sven Koenig, Hang Ma, Thayne Walker, Jiaoyang Li, Dor Atzmon, Liron Cohen, T. K. Satish Kumar, Eli Boyarski, Roman Bartak"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": ", hence,\noptimization of the MAPF Rproblem is also NP-hard.\n3 BACKGROUND\nWe now describe CCBS and other prior work.\n3.1 Conflict-Based Search\nContinuous-time Conflict-Based Search (CCBS) ",
                    "Citation Text": "Anton Andreychuk, Konstantin Yakovlev, Pavel Surynek, Dor Atzmon, and Roni\nStern. 2022. Multi-agent pathfinding with continuous time. Artificial Intelligence\n305 (2022), 103662.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.05506",
                        "Citation Paper Title": "Title:Multi-Agent Pathfinding with Continuous Time",
                        "Citation Paper Abstract": "Abstract:Multi-Agent Pathfinding (MAPF) is the problem of finding paths for multiple agents such that every agent reaches its goal and the agents do not collide. Most prior work on MAPF was on grids, assumed agents' actions have uniform duration, and that time is discretized into timesteps. We propose a MAPF algorithm that does not rely on these assumptions, is complete, and provides provably optimal solutions. This algorithm is based on a novel adaptation of Safe interval path planning (SIPP), a continuous time single-agent planning algorithm, and a modified version of Conflict-based search (CBS), a state of the art multi-agent pathfinding algorithm. We analyze this algorithm, discuss its pros and cons, and evaluate it experimentally on several standard benchmarks.",
                        "Citation Paper Authors": "Authors:Anton Andreychuk, Konstantin Yakovlev, Dor Atzmon, Roni Stern"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.16016v1": {
            "Paper Title": "V-STRONG: Visual Self-Supervised Traversability Learning for Off-road\n  Navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2309.14970v4": {
            "Paper Title": "Recurrent Hypernetworks are Surprisingly Strong in Meta-RL",
            "Sentences": []
        },
        "http://arxiv.org/abs/2307.07763v3": {
            "Paper Title": "Tightly-Coupled LiDAR-Visual SLAM Based on Geometric Features for Mobile\n  Agents",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": "use inertial measurements for\nstate propagation and update the visual data to improve\naccuracy.\nIn the field of high-cost and high-complexity visual-\nLiDAR-inertial SLAM systems, the optimization-based algo-\nrithm LVI-SAM ",
                    "Citation Text": "T. Shan, B. Englot, C. Ratti, and D. Rus, \u201cLVI-SAM: Tightly-coupled\nlidar-visual-inertial odometry via smoothing and mapping,\u201d in ICRA ,\n2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.10831",
                        "Citation Paper Title": "Title:LVI-SAM: Tightly-coupled Lidar-Visual-Inertial Odometry via Smoothing and Mapping",
                        "Citation Paper Abstract": "Abstract:We propose a framework for tightly-coupled lidar-visual-inertial odometry via smoothing and mapping, LVI-SAM, that achieves real-time state estimation and map-building with high accuracy and robustness. LVI-SAM is built atop a factor graph and is composed of two sub-systems: a visual-inertial system (VIS) and a lidar-inertial system (LIS). The two sub-systems are designed in a tightly-coupled manner, in which the VIS leverages LIS estimation to facilitate initialization. The accuracy of the VIS is improved by extracting depth information for visual features using lidar measurements. In turn, the LIS utilizes VIS estimation for initial guesses to support scan-matching. Loop closures are first identified by the VIS and further refined by the LIS. LVI-SAM can also function when one of the two sub-systems fails, which increases its robustness in both texture-less and feature-less environments. LVI-SAM is extensively evaluated on datasets gathered from several platforms over a variety of scales and environments. Our implementation is available at this https URL",
                        "Citation Paper Authors": "Authors:Tixiao Shan, Brendan Englot, Carlo Ratti, Daniela Rus"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "suggests reconstruct-\ning the line segments with multiple views instead of just\ntwo. Furthermore, several algorithms ",
                    "Citation Text": "H. Lim, Y . Kim, K. Jung, S. Hu, and H. Myung, \u201cAvoiding degeneracy\nfor monocular visual SLAM with point and line features,\u201d in ICRA ,\n2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.01501",
                        "Citation Paper Title": "Title:Avoiding Degeneracy for Monocular Visual SLAM with Point and Line Features",
                        "Citation Paper Abstract": "Abstract:In this paper, a degeneracy avoidance method for a point and line based visual SLAM algorithm is proposed. Visual SLAM predominantly uses point features. However, point features lack robustness in low texture and illuminance variant environments. Therefore, line features are used to compensate the weaknesses of point features. In addition, point features are poor in representing discernable features for the naked eye, meaning mapped point features cannot be recognized. To overcome the limitations above, line features were actively employed in previous studies. However, since degeneracy arises in the process of using line features, this paper attempts to solve this problem. First, a simple method to identify degenerate lines is presented. In addition, a novel structural constraint is proposed to avoid the degeneracy problem. At last, a point and line based monocular SLAM system using a robust optical-flow based lien tracking method is implemented. The results are verified using experiments with the EuRoC dataset and compared with other state-of-the-art algorithms. It is proven that our method yields more accurate localization as well as mapping results.",
                        "Citation Paper Authors": "Authors:Hyunjun Lim, Yeeun Kim, Kwangik Jung, Sumin Hu, Hyun Myung"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "and a Bag-of-Words model.\nHowever, the extracted point-only depth as prior fac-\ntors has a flaw: The multi-sensor aligned error caused by\nmechanical changes cannot be mitigated without an auto-\nmatic extrinsic calibration procedure. LIMO ",
                    "Citation Text": "J. Graeter, A. Wilczynski, and M. Lauer, \u201cLIMO: Lidar-monocular\nvisual odometry,\u201d in IROS , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.07524",
                        "Citation Paper Title": "Title:LIMO: Lidar-Monocular Visual Odometry",
                        "Citation Paper Abstract": "Abstract:Higher level functionality in autonomous driving depends strongly on a precise motion estimate of the vehicle. Powerful algorithms have been developed. However, their great majority focuses on either binocular imagery or pure LIDAR measurements. The promising combination of camera and LIDAR for visual localization has mostly been unattended. In this work we fill this gap, by proposing a depth extraction algorithm from LIDAR measurements for camera feature tracks and estimating motion by robustified keyframe based Bundle Adjustment. Semantic labeling is used for outlier rejection and weighting of vegetation landmarks. The capability of this sensor combination is demonstrated on the competitive KITTI dataset, achieving a placement among the top 15. The code is released to the community.",
                        "Citation Paper Authors": "Authors:Johannes Graeter, Alexander Wilczynski, Martin Lauer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.15863v1": {
            "Paper Title": "PDiT: Interleaving Perception and Decision-making Transformers for Deep\n  Reinforcement Learning",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": ". Here, CQL-MLP numbers are reported from the original paper; BEAR, BRAC-v and AWR are\nreported from the D4RL paper ",
                    "Citation Text": "Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine.\n2020. D4RL: Datasets for Deep Data-Driven Reinforcement Learning.\narXiv:2004.07219 [cs.LG]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.07219",
                        "Citation Paper Title": "Title:D4RL: Datasets for Deep Data-Driven Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:The offline reinforcement learning (RL) setting (also known as full batch RL), where a policy is learned from a static dataset, is compelling as progress enables RL methods to take advantage of large, previously-collected datasets, much like how the rise of large datasets has fueled results in supervised learning. However, existing online RL benchmarks are not tailored towards the offline setting and existing offline RL benchmarks are restricted to data generated by partially-trained agents, making progress in offline RL difficult to measure. In this work, we introduce benchmarks specifically designed for the offline setting, guided by key properties of datasets relevant to real-world applications of offline RL. With a focus on dataset collection, examples of such properties include: datasets generated via hand-designed controllers and human demonstrators, multitask datasets where an agent performs different tasks in the same environment, and datasets collected with mixtures of policies. By moving beyond simple benchmark tasks and data collected by partially-trained RL agents, we reveal important and unappreciated deficiencies of existing algorithms. To facilitate research, we have released our benchmark tasks and datasets with a comprehensive evaluation of existing algorithms, an evaluation protocol, and open-source examples. This serves as a common starting point for the community to identify shortcomings in existing offline RL methods and a collaborative route for progress in this emerging area.",
                        "Citation Paper Authors": "Authors:Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, Sergey Levine"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": ". In computer vision, some methods\napply two Transformers to handle tasks like image classification\nand person search, e.g., TNT ",
                    "Citation Text": "Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang.\n2021. Transformer in transformer. Advances in Neural Information Processing\nSystems 34 (2021), 15908\u201315919.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.00112",
                        "Citation Paper Title": "Title:Transformer in Transformer",
                        "Citation Paper Abstract": "Abstract:Transformer is a new kind of neural architecture which encodes the input data as powerful features via the attention mechanism. Basically, the visual transformers first divide the input images into several local patches and then calculate both representations and their relationship. Since natural images are of high complexity with abundant detail and color information, the granularity of the patch dividing is not fine enough for excavating features of objects in different scales and locations. In this paper, we point out that the attention inside these local patches are also essential for building visual transformers with high performance and we explore a new architecture, namely, Transformer iN Transformer (TNT). Specifically, we regard the local patches (e.g., 16$\\times$16) as \"visual sentences\" and present to further divide them into smaller patches (e.g., 4$\\times$4) as \"visual words\". The attention of each word will be calculated with other words in the given visual sentence with negligible computational costs. Features of both words and sentences will be aggregated to enhance the representation ability. Experiments on several benchmarks demonstrate the effectiveness of the proposed TNT architecture, e.g., we achieve an 81.5% top-1 accuracy on the ImageNet, which is about 1.7% higher than that of the state-of-the-art visual transformer with similar computational cost. The PyTorch code is available at this https URL, and the MindSpore code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, Yunhe Wang"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": ", are purely based on the Transformer,\nwhich fuses the role of environmental perception and decision-\nmaking simultaneously in one Transformer. These models signifi-\ncantly simplified the model structure and improved the scalability,PerceptionandDecision: arespecializedby two modelsPerceptionandDecision:are fusedin one modelMGDT ",
                    "Citation Text": "Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiao-\nhua Wang, Yu Qiao, Xiaogang Wang, Wenhai Wang, et al .2023. Uni-perceiver\nv2: A generalist model for large-scale vision and vision-language tasks. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .\n2691\u20132700.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2211.09808",
                        "Citation Paper Title": "Title:Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks",
                        "Citation Paper Abstract": "Abstract:Despite the remarkable success of foundation models, their task-specific fine-tuning paradigm makes them inconsistent with the goal of general perception modeling. The key to eliminating this inconsistency is to use generalist models for general task modeling. However, existing attempts at generalist models are inadequate in both versatility and performance. In this paper, we propose Uni-Perceiver v2, which is the first generalist model capable of handling major large-scale vision and vision-language tasks with competitive performance. Specifically, images are encoded as general region proposals, while texts are encoded via a Transformer-based language model. The encoded representations are transformed by a task-agnostic decoder. Different tasks are formulated as a unified maximum likelihood estimation problem. We further propose an improved optimizer to ensure stable multi-task learning with an unmixed sampling strategy, which is helpful for tasks requiring large batch-size training. After being jointly trained on various tasks, Uni-Perceiver v2 is capable of directly handling downstream tasks without any task-specific adaptation. Results show that Uni-Perceiver v2 outperforms all existing generalist models in both versatility and performance. Meanwhile, compared with the commonly-recognized strong baselines that require tasks-specific fine-tuning, Uni-Perceiver v2 achieves competitive performance on a broad range of vision and vision-language tasks.",
                        "Citation Paper Authors": "Authors:Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang, Wenhai Wang, Jifeng Dai"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.17266v1": {
            "Paper Title": "Automatic laminectomy cutting plane planning based on artificial\n  intelligence in robot assisted laminectomy surgery",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": ". SPU-Net also includes encoder and decoder. The specific modules in the network are described in Figure 2. Due to the excellent performance of Patch Merging and Patch Expanding in Swin-UNet ",
                    "Citation Text": "H. Cao, Y. Wang, J. Chen, et al., \u201cSwin-Unet: Unet-like Pure Transformer for Medical Image Segmentation,\u201d 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.05537",
                        "Citation Paper Title": "Title:Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation",
                        "Citation Paper Abstract": "Abstract:In the past few years, convolutional neural networks (CNNs) have achieved milestones in medical image analysis. Especially, the deep neural networks based on U-shaped architecture and skip-connections have been widely applied in a variety of medical image tasks. However, although CNN has achieved excellent performance, it cannot learn global and long-range semantic information interaction well due to the locality of the convolution operation. In this paper, we propose Swin-Unet, which is an Unet-like pure Transformer for medical image segmentation. The tokenized image patches are fed into the Transformer-based U-shaped Encoder-Decoder architecture with skip-connections for local-global semantic feature learning. Specifically, we use hierarchical Swin Transformer with shifted windows as the encoder to extract context features. And a symmetric Swin Transformer-based decoder with patch expanding layer is designed to perform the up-sampling operation to restore the spatial resolution of the feature maps. Under the direct down-sampling and up-sampling of the inputs and outputs by 4x, experiments on multi-organ and cardiac segmentation tasks demonstrate that the pure Transformer-based U-shaped Encoder-Decoder network outperforms those methods with full-convolution or the combination of transformer and convolution. The codes and trained models will be publicly available at this https URL.",
                        "Citation Paper Authors": "Authors:Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, Manning Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2307.08668v2": {
            "Paper Title": "A Study in Zucker: Insights on Interactions Between Humans and Small\n  Service Robots",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": ". Finally, our\ndataset can inform the development of more accurate crowd\nsimulation models that can facilitate the training and evalua-\ntion of robot navigation methods. In existing robot simulators\nand benchmarking tools ",
                    "Citation Text": "M. Everett, Y . F. Chen, and J. P. How, \u201cCollision avoidance in pedestrian-\nrich environments with deep reinforcement learning,\u201d IEEE Access ,\nvol. 9, pp. 10 357\u201310 377, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.11689",
                        "Citation Paper Title": "Title:Collision Avoidance in Pedestrian-Rich Environments with Deep Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:Collision avoidance algorithms are essential for safe and efficient robot operation among pedestrians. This work proposes using deep reinforcement (RL) learning as a framework to model the complex interactions and cooperation with nearby, decision-making agents, such as pedestrians and other robots. Existing RL-based works assume homogeneity of agent properties, use specific motion models over short timescales, or lack a principled method to handle a large, possibly varying number of agents. Therefore, this work develops an algorithm that learns collision avoidance among a variety of heterogeneous, non-communicating, dynamic agents without assuming they follow any particular behavior rules. It extends our previous work by introducing a strategy using Long Short-Term Memory (LSTM) that enables the algorithm to use observations of an arbitrary number of other agents, instead of a small, fixed number of neighbors. The proposed algorithm is shown to outperform a classical collision avoidance algorithm, another deep RL-based algorithm, and scales with the number of agents better (fewer collisions, shorter time to goal) than our previously published learning-based approach. Analysis of the LSTM provides insights into how observations of nearby agents affect the hidden state and quantifies the performance impact of various agent ordering heuristics. The learned policy generalizes to several applications beyond the training scenarios: formation control (arrangement into letters), demonstrations on a fleet of four multirotors and on a fully autonomous robotic vehicle capable of traveling at human walking speed among pedestrians.",
                        "Citation Paper Authors": "Authors:Michael Everett, Yu Fan Chen, Jonathan P. How"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": ". As such, we use CVM as a baseline\nand compare its performance to two SOTA approaches for\nmodel-free trajectory prediction, Trajectron++ ",
                    "Citation Text": "T. Salzmann, B. Ivanovic, P. Chakravarty, and M. Pavone, \u201cTrajectron++:\nDynamically-feasible trajectory forecasting with heterogeneous data,\u201d in\nEuropean Conf. Comput. Vis. , 2020, pp. 683\u2013700.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2001.03093",
                        "Citation Paper Title": "Title:Trajectron++: Dynamically-Feasible Trajectory Forecasting With Heterogeneous Data",
                        "Citation Paper Abstract": "Abstract:Reasoning about human motion is an important prerequisite to safe and socially-aware robotic navigation. As a result, multi-agent behavior prediction has become a core component of modern human-robot interactive systems, such as self-driving cars. While there exist many methods for trajectory forecasting, most do not enforce dynamic constraints and do not account for environmental information (e.g., maps). Towards this end, we present Trajectron++, a modular, graph-structured recurrent model that forecasts the trajectories of a general number of diverse agents while incorporating agent dynamics and heterogeneous data (e.g., semantic maps). Trajectron++ is designed to be tightly integrated with robotic planning and control frameworks; for example, it can produce predictions that are optionally conditioned on ego-agent motion plans. We demonstrate its performance on several challenging real-world trajectory forecasting datasets, outperforming a wide array of state-of-the-art deterministic and generative methods.",
                        "Citation Paper Authors": "Authors:Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, Marco Pavone"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.15817v1": {
            "Paper Title": "Contrastive Learning-Based Framework for Sim-to-Real Mapping of Lidar\n  Point Clouds in Autonomous Driving Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2306.01704v3": {
            "Paper Title": "Temporal-controlled Frame Swap for Generating High-Fidelity Stereo\n  Driving Data for Autonomy Analysis",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": ", offering\nrich data modalities such as stereo RGB images, depth maps, semantic segmentation masks,\nand camera poses across diverse environments. Similarly, CARLA ",
                    "Citation Text": "Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen\nKoltun. Carla: An open urban driving simulator. In Conference on robot learning ,\npages 1\u201316. PMLR, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.03938",
                        "Citation Paper Title": "Title:CARLA: An Open Urban Driving Simulator",
                        "Citation Paper Abstract": "Abstract:We introduce CARLA, an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use CARLA to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by CARLA, illustrating the platform's utility for autonomous driving research. The supplementary video can be viewed at this https URL",
                        "Citation Paper Authors": "Authors:Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, Vladlen Koltun"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ",\na high-fidelity simulator that has been used to create the TartanAir dataset ",
                    "Citation Text": "Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang,\nYafei Hu, Ashish Kapoor, and Sebastian Scherer. Tartanair: A dataset to push the limits\nof visual slam. In 2020 IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS) , pages 4909\u20134916. IEEE, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.14338",
                        "Citation Paper Title": "Title:TartanAir: A Dataset to Push the Limits of Visual SLAM",
                        "Citation Paper Abstract": "Abstract:We present a challenging dataset, the TartanAir, for robot navigation tasks and more. The data is collected in photo-realistic simulation environments with the presence of moving objects, changing light and various weather conditions. By collecting data in simulations, we are able to obtain multi-modal sensor data and precise ground truth labels such as the stereo RGB image, depth image, segmentation, optical flow, camera poses, and LiDAR point cloud. We set up large numbers of environments with various styles and scenes, covering challenging viewpoints and diverse motion patterns that are difficult to achieve by using physical data collection platforms. In order to enable data collection at such a large scale, we develop an automatic pipeline, including mapping, trajectory sampling, data processing, and data verification. We evaluate the impact of various factors on visual SLAM algorithms using our data. The results of state-of-the-art algorithms reveal that the visual SLAM problem is far from solved. Methods that show good performance on established datasets such as KITTI do not perform well in more difficult scenarios. Although we use the simulation, our goal is to push the limits of Visual SLAM algorithms in the real world by providing a challenging benchmark for testing new methods, while also using a large diverse training data for learning-based methods. Our dataset is available at \\url{this http URL}.",
                        "Citation Paper Authors": "Authors:Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, Sebastian Scherer"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": ", while in \"Play For Bench-\nmark\", they provided over 250,000 high-resolution video frames annotated with ground-truth\ndata for benchmarking multi-level vision tasks, such as tracking and perception. ",
                    "Citation Text": "Stephan R. Richter, Zeeshan Hayder, and Vladlen Koltun. Playing for benchmarks. In\nIEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October\n22-29, 2017 , pages 2232\u20132241, 2017. doi: 10.1109/ICCV .2017.243. URL https:\n//doi.org/10.1109/ICCV.2017.243 .12 LUO ET AL.,: STEREO SYNTHESIS",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.07322",
                        "Citation Paper Title": "Title:Playing for Benchmarks",
                        "Citation Paper Abstract": "Abstract:We present a benchmark suite for visual perception. The benchmark is based on more than 250K high-resolution video frames, all annotated with ground-truth data for both low-level and high-level vision tasks, including optical flow, semantic instance segmentation, object detection and tracking, object-level 3D scene layout, and visual odometry. Ground-truth data for all tasks is available for every frame. The data was collected while driving, riding, and walking a total of 184 kilometers in diverse ambient conditions in a realistic virtual world. To create the benchmark, we have developed a new approach to collecting ground-truth data from simulated worlds without access to their source code or content. We conduct statistical analyses that show that the composition of the scenes in the benchmark closely matches the composition of corresponding physical environments. The realism of the collected data is further validated via perceptual experiments. We analyze the performance of state-of-the-art methods for multiple tasks, providing reference baselines and highlighting challenges for future research. The supplementary video can be viewed at this https URL",
                        "Citation Paper Authors": "Authors:Stephan R. Richter, Zeeshan Hayder, Vladlen Koltun"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "Numerous synthetic datasets have been created from the commercial game GTA V due to its\nhigh-quality graphics and complex environment. Authors in \"Play For Data\" offered 25,000\nvideo frames for training semantic segmentation systems ",
                    "Citation Text": "Stephan R. Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. Playing for data:\nGround truth from computer games. In European Conference on Computer Vision\n(ECCV) , pages 102\u2013118. Springer, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1608.02192",
                        "Citation Paper Title": "Title:Playing for Data: Ground Truth from Computer Games",
                        "Citation Paper Abstract": "Abstract:Recent progress in computer vision has been driven by high-capacity models trained on large datasets. Unfortunately, creating large datasets with pixel-level labels has been extremely costly due to the amount of human effort required. In this paper, we present an approach to rapidly creating pixel-accurate semantic label maps for images extracted from modern computer games. Although the source code and the internal operation of commercial games are inaccessible, we show that associations between image patches can be reconstructed from the communication between the game and the graphics hardware. This enables rapid propagation of semantic labels within and across images synthesized by the game, with no access to the source code or the content. We validate the presented approach by producing dense pixel-level semantic annotations for 25 thousand images synthesized by a photorealistic open-world computer game. Experiments on semantic segmentation datasets show that using the acquired data to supplement real-world images significantly increases accuracy and that the acquired data enables reducing the amount of hand-labeled real-world data: models trained with game data and just 1/3 of the CamVid training set outperform models trained on the complete CamVid training set.",
                        "Citation Paper Authors": "Authors:Stephan R. Richter, Vibhav Vineet, Stefan Roth, Vladlen Koltun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2304.00910v2": {
            "Paper Title": "Integrating One-Shot View Planning with a Single Next-Best View via\n  Long-Tail Multiview Sampling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15738v1": {
            "Paper Title": "Enhanced Robot Motion Block of A-star Algorithm for Robotic Path\n  Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15679v1": {
            "Paper Title": "BDIS-SLAM: A lightweight CPU-based dense stereo SLAM for surgery",
            "Sentences": []
        },
        "http://arxiv.org/abs/2309.16426v3": {
            "Paper Title": "QwenGrasp: A Usage of Large Vision-Language Model for Target-Oriented\n  Grasping",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": "will generate the bounding box of the target object, and the REGNet ",
                    "Citation Text": "B. Zhao, H. Zhang, X. Lan, H. Wang, Z. Tian, and\nN. Zheng, \u201cRegnet: Region-based grasp network for end-\nto-end grasp detection in point clouds,\u201d in 2021 IEEE\nInternational Conference on Robotics and Automation\n(ICRA) . IEEE, Conference Proceedings, pp. 13 474\u2013\n13 480.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.12647",
                        "Citation Paper Title": "Title:REGNet: REgion-based Grasp Network for End-to-end Grasp Detection in Point Clouds",
                        "Citation Paper Abstract": "Abstract:Reliable robotic grasping in unstructured environments is a crucial but challenging task. The main problem is to generate the optimal grasp of novel objects from partial noisy observations. This paper presents an end-to-end grasp detection network taking one single-view point cloud as input to tackle the problem. Our network includes three stages: Score Network (SN), Grasp Region Network (GRN), and Refine Network (RN). Specifically, SN regresses point grasp confidence and selects positive points with high confidence. Then GRN conducts grasp proposal prediction on the selected positive points. RN generates more accurate grasps by refining proposals predicted by GRN. To further improve the performance, we propose a grasp anchor mechanism, in which grasp anchors with assigned gripper orientations are introduced to generate grasp proposals. Experiments demonstrate that REGNet achieves a success rate of 79.34% and a completion rate of 96% in real-world clutter, which significantly outperforms several state-of-the-art point-cloud based methods, including GPD, PointNetGPD, and S4G. The code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Binglei Zhao, Hanbo Zhang, Xuguang Lan, Haoyu Wang, Zhiqiang Tian, Nanning Zheng"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "is a large model which is composed of vision\ntransformer and the large language model PaLM ",
                    "Citation Text": "A. Chowdhery, S. Narang, J. Devlin, M. Bosma,\nG. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sut-\nton, and S. Gehrmann, \u201cPalm: Scaling language modeling\nwith pathways,\u201d arXiv preprint arXiv:2204.02311 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.02311",
                        "Citation Paper Title": "Title:PaLM: Scaling Language Modeling with Pathways",
                        "Citation Paper Abstract": "Abstract:Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",
                        "Citation Paper Authors": "Authors:Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2310.07937v2": {
            "Paper Title": "Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation using\n  Large Language Models",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": "to predict all the categories. The map size is 24\u00d724m,\nwith a resolution of 0.05 m. For our global planner, we\nleveraged the widely-adopted GPT3.5-turbo ",
                    "Citation Text": "L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin,\nC. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton,\nF. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano,\nJ. Leike, and R. Lowe, \u201cTraining language models to follow instruc-\ntions with human feedback,\u201d mar 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.02155",
                        "Citation Paper Title": "Title:Training language models to follow instructions with human feedback",
                        "Citation Paper Abstract": "Abstract:Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
                        "Citation Paper Authors": "Authors:Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": ". For each robot,\nthe observation space contains 480\u00d7640RGBD images, a\nbase odometry sensor, and a goal object category represented\nas an integer. We utilized the finetuned RedNet model ",
                    "Citation Text": "J. Jiang, L. Zheng, F. Luo, and Z. Zhang, \u201cRedNet: Residual Encoder-\nDecoder Network for indoor RGB-D Semantic Segmentation,\u201d arXiv ,\njun 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.01054",
                        "Citation Paper Title": "Title:RedNet: Residual Encoder-Decoder Network for indoor RGB-D Semantic Segmentation",
                        "Citation Paper Abstract": "Abstract:Indoor semantic segmentation has always been a difficult task in computer vision. In this paper, we propose an RGB-D residual encoder-decoder architecture, named RedNet, for indoor RGB-D semantic segmentation. In RedNet, the residual module is applied to both the encoder and decoder as the basic building block, and the skip-connection is used to bypass the spatial feature between the encoder and decoder. In order to incorporate the depth information of the scene, a fusion structure is constructed, which makes inference on RGB image and depth image separately, and fuses their features over several layers. In order to efficiently optimize the network's parameters, we propose a `pyramid supervision' training scheme, which applies supervised learning over different layers in the decoder, to cope with the problem of gradients vanishing. Experiment results show that the proposed RedNet(ResNet-50) achieves a state-of-the-art mIoU accuracy of 47.8% on the SUN RGB-D benchmark dataset.",
                        "Citation Paper Authors": "Authors:Jindong Jiang, Lunan Zheng, Fei Luo, Zhijun Zhang"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": ": chair, sofa, plant,\nbed, toilet, and tv.\n2) Experiment Details: We evaluate our methods in the\n3D indoor simulator Habitat platform ",
                    "Citation Text": "M. Savva, A. Kadian, O. Maksymets, Y . Zhao, E. Wijmans, B. Jain,\nJ. Straub, J. Liu, V . Koltun, J. Malik, D. Parikh, and D. Batra,\n\u201cHabitat: A Platform for Embodied AI Research,\u201d in 2019 IEEE/CVF\nInternational Conference on Computer Vision (ICCV) , vol. 2019-\nOctob, pp. 9338\u20139346, IEEE, oct 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.01201",
                        "Citation Paper Title": "Title:Habitat: A Platform for Embodied AI Research",
                        "Citation Paper Abstract": "Abstract:We present Habitat, a platform for research in embodied artificial intelligence (AI). Habitat enables training embodied agents (virtual robots) in highly efficient photorealistic 3D simulation. Specifically, Habitat consists of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is fast -- when rendering a scene from Matterport3D, it achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU. (ii) Habitat-API: a modular high-level library for end-to-end development of embodied AI algorithms -- defining tasks (e.g., navigation, instruction following, question answering), configuring, training, and benchmarking embodied agents.\nThese large-scale engineering contributions enable us to answer scientific questions requiring experiments that were till now impracticable or 'merely' impractical. Specifically, in the context of point-goal navigation: (1) we revisit the comparison between learning and SLAM approaches from two recent works and find evidence for the opposite conclusion -- that learning outperforms SLAM if scaled to an order of magnitude more experience than previous investigations, and (2) we conduct the first cross-dataset generalization experiments {train, test} x {Matterport3D, Gibson} for multiple sensors {blind, RGB, RGBD, D} and find that only agents with depth (D) sensors generalize across datasets. We hope that our open-source platform and these findings will advance research in embodied AI.",
                        "Citation Paper Authors": "Authors:Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, Dhruv Batra"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "employs language models to generate\nscene layouts, manifesting the visual planning capabilities\nof LLMs. ",
                    "Citation Text": "S. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor, \u201cChatGPT for\nRobotics : Design Principles and Model Abilities,\u201d 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2306.17582",
                        "Citation Paper Title": "Title:ChatGPT for Robotics: Design Principles and Model Abilities",
                        "Citation Paper Abstract": "Abstract:This paper presents an experimental study regarding the use of OpenAI's ChatGPT for robotics applications. We outline a strategy that combines design principles for prompt engineering and the creation of a high-level function library which allows ChatGPT to adapt to different robotics tasks, simulators, and form factors. We focus our evaluations on the effectiveness of different prompt engineering techniques and dialog strategies towards the execution of various types of robotics tasks. We explore ChatGPT's ability to use free-form dialog, parse XML tags, and to synthesize code, in addition to the use of task-specific prompting functions and closed-loop reasoning through dialogues. Our study encompasses a range of tasks within the robotics domain, from basic logical, geometrical, and mathematical reasoning all the way to complex domains such as aerial navigation, manipulation, and embodied agents. We show that ChatGPT can be effective at solving several of such tasks, while allowing users to interact with it primarily via natural language instructions. In addition to these studies, we introduce an open-sourced research tool called PromptCraft, which contains a platform where researchers can collaboratively upload and vote on examples of good prompting schemes for robotics applications, as well as a sample robotics simulator with ChatGPT integration, making it easier for users to get started with using ChatGPT for robotics.",
                        "Citation Paper Authors": "Authors:Sai Vemprala, Rogerio Bonatti, Arthur Bucker, Ashish Kapoor"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": ", have been employed to train high-\nlevel policies. Techniques like zero-shot ",
                    "Citation Text": "A. Majumdar, G. Aggarwal, B. Devnani, J. Hoffman, and D. Batra,\n\u201cZSON: Zero-Shot Object-Goal Navigation using Multimodal Goal\nEmbeddings,\u201d Advances in Neural Information Processing Systems ,\nvol. 35, pp. 32340\u201332352, jun 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.12403",
                        "Citation Paper Title": "Title:ZSON: Zero-Shot Object-Goal Navigation using Multimodal Goal Embeddings",
                        "Citation Paper Abstract": "Abstract:We present a scalable approach for learning open-world object-goal navigation (ObjectNav) -- the task of asking a virtual robot (agent) to find any instance of an object in an unexplored environment (e.g., \"find a sink\"). Our approach is entirely zero-shot -- i.e., it does not require ObjectNav rewards or demonstrations of any kind. Instead, we train on the image-goal navigation (ImageNav) task, in which agents find the location where a picture (i.e., goal image) was captured. Specifically, we encode goal images into a multimodal, semantic embedding space to enable training semantic-goal navigation (SemanticNav) agents at scale in unannotated 3D environments (e.g., HM3D). After training, SemanticNav agents can be instructed to find objects described in free-form natural language (e.g., \"sink\", \"bathroom sink\", etc.) by projecting language goals into the same multimodal, semantic embedding space. As a result, our approach enables open-world ObjectNav. We extensively evaluate our agents on three ObjectNav datasets (Gibson, HM3D, and MP3D) and observe absolute improvements in success of 4.2% - 20.0% over existing zero-shot methods. For reference, these gains are similar or better than the 5% improvement in success between the Habitat 2020 and 2021 ObjectNav challenge winners. In an open-world setting, we discover that our agents can generalize to compound instructions with a room explicitly mentioned (e.g., \"Find a kitchen sink\") and when the target room can be inferred (e.g., \"Find a sink and a stove\").",
                        "Citation Paper Authors": "Authors:Arjun Majumdar, Gunjan Aggarwal, Bhavika Devnani, Judy Hoffman, Dhruv Batra"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.15630v1": {
            "Paper Title": "Mesh-LOAM: Real-time Mesh-Based LiDAR Odometry and Mapping",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": "present a volumetric depth fusion\nfor large-scale mapping, they require multiple passes over the\nsame scene for compelling reconstruction results.\nLearning-based methods ",
                    "Citation Text": "X. Zhong, Y . Pan, J. Behley, and C. Stachniss, \u201cShine-mapping: Large-\nscale 3d mapping using sparse hierarchical implicit neural representa-\ntions,\u201d in Proceedings of the IEEE International Conference on Robotics\nand Automation , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.02299",
                        "Citation Paper Title": "Title:SHINE-Mapping: Large-Scale 3D Mapping Using Sparse Hierarchical Implicit Neural Representations",
                        "Citation Paper Abstract": "Abstract:Accurate mapping of large-scale environments is an essential building block of most outdoor autonomous systems. Challenges of traditional mapping methods include the balance between memory consumption and mapping accuracy. This paper addresses the problem of achieving large-scale 3D reconstruction using implicit representations built from 3D LiDAR measurements. We learn and store implicit features through an octree-based, hierarchical structure, which is sparse and extensible. The implicit features can be turned into signed distance values through a shallow neural network. We leverage binary cross entropy loss to optimize the local features with the 3D measurements as supervision. Based on our implicit representation, we design an incremental mapping system with regularization to tackle the issue of forgetting in continual learning. Our experiments show that our 3D reconstructions are more accurate, complete, and memory-efficient than current state-of-the-art 3D mapping methods.",
                        "Citation Paper Authors": "Authors:Xingguang Zhong, Yue Pan, Jens Behley, Cyrill Stachniss"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "employ the Gaussian\nprocess reconstruction to reconstruct meshes by adopting\ndomain decomposition and local regression techniques ",
                    "Citation Text": "J. Ruan, B. Li, Y . Wang, and Z. Fang, \u201cGp-slam+: real-time 3d lidar slam\nbased on improved regionalized gaussian process map reconstruction,\u201d\nin2020 IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS) . IEEE, 2020, pp. 5171\u20135178.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.00644",
                        "Citation Paper Title": "Title:GP-SLAM+: real-time 3D lidar SLAM based on improved regionalized Gaussian process map reconstruction",
                        "Citation Paper Abstract": "Abstract:This paper presents a 3D lidar SLAM system based on improved regionalized Gaussian process (GP) map reconstruction to provide both low-drift state estimation and mapping in real-time for robotics applications. We utilize spatial GP regression to model the environment. This tool enables us to recover surfaces including those in sparsely scanned areas and obtain uniform samples with uncertainty. Those properties facilitate robust data association and map updating in our scan-to-map registration scheme, especially when working with sparse range data. Compared with previous GP-SLAM, this work overcomes the prohibitive computational complexity of GP and redesigns the registration strategy to meet the accuracy requirements in 3D scenarios. For large-scale tasks, a two-thread framework is employed to suppress the drift further. Aerial and ground-based experiments demonstrate that our method allows robust odometry and precise mapping in real-time. It also outperforms the state-of-the-art lidar SLAM systems in our tests with light-weight sensors.",
                        "Citation Paper Authors": "Authors:Jianyuan Ruan, Bo Li, Yinqiang Wang, Zhou Fang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.15471v1": {
            "Paper Title": "Residual Learning for Image Point Descriptors",
            "Sentences": [
                {
                    "Sentence ID": 39,
                    "Sentence": ". Nonetheless, the question of perspective changes from different viewpoints still remain\nvery recently tackled by ",
                    "Citation Text": "Dominik Muhle, Lukas Koestler, Krishna Murthy Jatavallabhula, and Daniel Cremers. Learning correspon-\ndence uncertainty via differentiable nonlinear least squares. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 13102\u201313112, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.09527",
                        "Citation Paper Title": "Title:Learning Correspondence Uncertainty via Differentiable Nonlinear Least Squares",
                        "Citation Paper Abstract": "Abstract:We propose a differentiable nonlinear least squares framework to account for uncertainty in relative pose estimation from feature correspondences. Specifically, we introduce a symmetric version of the probabilistic normal epipolar constraint, and an approach to estimate the covariance of feature positions by differentiating through the camera pose estimation procedure. We evaluate our approach on synthetic, as well as the KITTI and EuRoC real-world datasets. On the synthetic dataset, we confirm that our learned covariances accurately approximate the true noise distribution. In real world experiments, we find that our approach consistently outperforms state-of-the-art non-probabilistic and probabilistic approaches, regardless of the feature extraction algorithm of choice.",
                        "Citation Paper Authors": "Authors:Dominik Muhle, Lukas Koestler, Krishna Murthy Jatavallabhula, Daniel Cremers"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.16217v1": {
            "Paper Title": "ManipLLM: Embodied Multimodal Large Language Model for Object-Centric\n  Robotic Manipulation",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "also explores a vision-based\nmethod for perceiving and manipulating 3D articulated ob-\njects through predicting point-wise motion flow. Further-\nmore, V oxPoser ",
                    "Citation Text": "Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li,\nJiajun Wu, and Li Fei-Fei. V oxposer: Composable 3d value\nmaps for robotic manipulation with language models. arXiv\npreprint arXiv:2307.05973 , 2023. 2, 3, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2307.05973",
                        "Citation Paper Title": "Title:VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models",
                        "Citation Paper Abstract": "Abstract:Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a vision-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop robot trajectories with robustness to dynamic perturbations. We further demonstrate how the proposed framework can benefit from online experiences by efficiently learning a dynamics model for scenes that involve contact-rich interactions. We present a large-scale study of the proposed method in both simulated and real-robot environments, showcasing the ability to perform a large variety of everyday manipulation tasks specified in free-form natural language. Videos and code at this https URL",
                        "Citation Paper Authors": "Authors:Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, Li Fei-Fei"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": ". However, when\nit comes to more complex settings, vision-based observa-\ntion [6, 10, 14, 16, 23, 27, 28, 30, 32\u201334, 40] becomes nec-\nessary to perceive the environment and understand the com-\nplex scene and objects [5, 18]. Where2Act ",
                    "Citation Text": "Kaichun Mo, Shilin Zhu, Angel X. Chang, Li Yi, Subarna\nTripathi, Leonidas J. Guibas, and Hao Su. PartNet: A large-\nscale benchmark for fine-grained and hierarchical part-level\n3D object understanding. In The IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR) , 2019. 1, 2, 6,\n7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.02713",
                        "Citation Paper Title": "Title:PartNet: A Large-scale Benchmark for Fine-grained and Hierarchical Part-level 3D Object Understanding",
                        "Citation Paper Abstract": "Abstract:We present PartNet: a consistent, large-scale dataset of 3D objects annotated with fine-grained, instance-level, and hierarchical 3D part information. Our dataset consists of 573,585 part instances over 26,671 3D models covering 24 object categories. This dataset enables and serves as a catalyst for many tasks such as shape analysis, dynamic 3D scene modeling and simulation, affordance analysis, and others. Using our dataset, we establish three benchmarking tasks for evaluating 3D part recognition: fine-grained semantic segmentation, hierarchical semantic segmentation, and instance segmentation. We benchmark four state-of-the-art 3D deep learning algorithms for fine-grained semantic segmentation and three baseline methods for hierarchical semantic segmentation. We also propose a novel method for part instance segmentation and demonstrate its superior performance over existing methods.",
                        "Citation Paper Authors": "Authors:Kaichun Mo, Shilin Zhu, Angel X. Chang, Li Yi, Subarna Tripathi, Leonidas J. Guibas, Hao Su"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": ", as our back-\nbone and follow its training strategy. Given an RGB image\nI\u2208RH\u00d7W\u00d73, we adopt the visual encoder of CLIP ",
                    "Citation Text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning , pages\n8748\u20138763. PMLR, 2021. 3, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.00020",
                        "Citation Paper Title": "Title:Learning Transferable Visual Models From Natural Language Supervision",
                        "Citation Paper Abstract": "Abstract:State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",
                        "Citation Paper Authors": "Authors:Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": ". After aligning visual and text feature repre-\nsentation with the multi-modal projection module, LLaMa\nis required to conduct multi-modal understanding and give\ncorrect answers. During training, we only fine-tune the in-\njected adapters ",
                    "Citation Text": "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685 , 2021. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.09685",
                        "Citation Paper Title": "Title:LoRA: Low-Rank Adaptation of Large Language Models",
                        "Citation Paper Abstract": "Abstract:An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at this https URL.",
                        "Citation Paper Authors": "Authors:Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2308.10169v2": {
            "Paper Title": "Efficient Real-time Path Planning with Self-evolving Particle Swarm\n  Optimization in Dynamic Scenarios",
            "Sentences": []
        },
        "http://arxiv.org/abs/2311.17842v2": {
            "Paper Title": "Look Before You Leap: Unveiling the Power of GPT-4V in Robotic\n  Vision-Language Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2308.12110v2": {
            "Paper Title": "Constrained Stein Variational Trajectory Optimization",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": ", and\nsimilar ideas to those developed in the gradient flows for\nconstrained optimization literature were recently explored in\nO-SVGD ",
                    "Citation Text": "R. Zhang, Q. Liu, and X. Tong, \u201cSampling in constrained domains with\northogonal-space variational gradient descent,\u201d Proc. Int. Conf. Neural\nInformation Processing Systems , vol. 35, pp. 37 108\u201337 120, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.06447",
                        "Citation Paper Title": "Title:Sampling in Constrained Domains with Orthogonal-Space Variational Gradient Descent",
                        "Citation Paper Abstract": "Abstract:Sampling methods, as important inference and learning techniques, are typically designed for unconstrained domains. However, constraints are ubiquitous in machine learning problems, such as those on safety, fairness, robustness, and many other properties that must be satisfied to apply sampling results in real-life applications. Enforcing these constraints often leads to implicitly-defined manifolds, making efficient sampling with constraints very challenging. In this paper, we propose a new variational framework with a designed orthogonal-space gradient flow (O-Gradient) for sampling on a manifold $\\mathcal{G}_0$ defined by general equality constraints. O-Gradient decomposes the gradient into two parts: one decreases the distance to $\\mathcal{G}_0$ and the other decreases the KL divergence in the orthogonal space. While most existing manifold sampling methods require initialization on $\\mathcal{G}_0$, O-Gradient does not require such prior knowledge. We prove that O-Gradient converges to the target constrained distribution with rate $\\widetilde{O}(1/\\text{the number of iterations})$ under mild conditions. Our proof relies on a new Stein characterization of conditional measure which could be of independent interest. We implement O-Gradient through both Langevin dynamics and Stein variational gradient descent and demonstrate its effectiveness in various experiments, including Bayesian deep neural networks.",
                        "Citation Paper Authors": "Authors:Ruqi Zhang, Qiang Liu, Xin T. Tong"
                    }
                },
                {
                    "Sentence ID": 61,
                    "Sentence": ". The\nhyperparameters we use in all experiments are shown in Table\nI. For all experiments, the costs and constraint functions are\nwritten using PyTorch ",
                    "Citation Text": "A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. K \u00a8opf,\nE. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,\nL. Fang, J. Bai, and S. Chintala, \u201cPytorch: An imperative style, high-\nperformance deep learning library,\u201d in Proc. Int. Conf. Neural Information\nProcessing Systems , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.01703",
                        "Citation Paper Title": "Title:PyTorch: An Imperative Style, High-Performance Deep Learning Library",
                        "Citation Paper Abstract": "Abstract:Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.\nIn this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.\nWe demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.",
                        "Citation Paper Authors": "Authors:Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ". This framing results in estimating\na distribution over low-cost trajectories, rather than a single\noptimal trajectory. By using this framing we can leverage\napproximate inference tools for trajectory optimization, in\nparticular, Variational Inference ",
                    "Citation Text": "D. M. Blei, A. Kucukelbir, and J. D. McAuliffe, \u201cVariational inference:\nA review for statisticians,\u201d J. Am. Stat. Assoc. , vol. 112, no. 518, pp.\n859\u2013877, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1601.00670",
                        "Citation Paper Title": "Title:Variational Inference: A Review for Statisticians",
                        "Citation Paper Abstract": "Abstract:One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.",
                        "Citation Paper Authors": "Authors:David M. Blei, Alp Kucukelbir, Jon D. McAuliffe"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.15364v1": {
            "Paper Title": "WildScenes: A Benchmark for 2D and 3D Semantic Segmentation in\n  Large-scale Natural Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15339v1": {
            "Paper Title": "MaDi: Learning to Mask Distractions for Generalization in Visual Deep\n  Reinforcement Learning",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": "apply more targeted masking similar to\nMaDi. InfoGating has only experimented on offline RL, and it uses\na large U-Net ",
                    "Citation Text": "Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-Net: Convolutional\nNetworks for Biomedical Image Segmentation. In Medical Image Computing\nand Computer-Assisted Intervention\u2013MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III 18 . Springer, 234\u2013241.\nURL: https://arxiv.org/abs/1505.04597. (Cited in Section 2)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1505.04597",
                        "Citation Paper Title": "Title:U-Net: Convolutional Networks for Biomedical Image Segmentation",
                        "Citation Paper Abstract": "Abstract:There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at this http URL .",
                        "Citation Paper Authors": "Authors:Olaf Ronneberger, Philipp Fischer, Thomas Brox"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": "( overlay augmentation).\nMasking in Visual RL. There exist a few works that aim to im-\nprove the generalization ability of RL agents by masking parts of\nthe input. Yu et al. ",
                    "Citation Text": "Tao Yu, Zhizheng Zhang, Cuiling Lan, Yan Lu, and Zhibo Chen. 2022. Mask-\nbased Latent Reconstruction for Reinforcement Learning. Advances in Neural\nInformation Processing Systems 35 (2022), 25117\u201325131. URL: https://arxiv.org/\nabs/2201.12096. (Cited in Section 2)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.12096",
                        "Citation Paper Title": "Title:Mask-based Latent Reconstruction for Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:For deep reinforcement learning (RL) from pixels, learning effective state representations is crucial for achieving high performance. However, in practice, limited experience and high-dimensional inputs prevent effective representation learning. To address this, motivated by the success of mask-based modeling in other research fields, we introduce mask-based reconstruction to promote state representation learning in RL. Specifically, we propose a simple yet effective self-supervised method, Mask-based Latent Reconstruction (MLR), to predict complete state representations in the latent space from the observations with spatially and temporally masked pixels. MLR enables better use of context information when learning state representations to make them more informative, which facilitates the training of RL agents. Extensive experiments show that our MLR significantly improves the sample efficiency in RL and outperforms the state-of-the-art sample-efficient RL methods on multiple continuous and discrete control benchmarks. Our code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Tao Yu, Zhizheng Zhang, Cuiling Lan, Yan Lu, Zhibo Chen"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": ", domain randomization\n[2,42], meta-learning [ 9,46], contrastive learning [ 1,29], imitation\nlearning ",
                    "Citation Text": "Linxi Fan, Guanzhi Wang, De-An Huang, Zhiding Yu, Li Fei-Fei, Yuke Zhu,\nand Anima Anandkumar. 2021. SECANT: Self-Expert Cloning for Zero-Shot\nGeneralization of Visual Policies. International Conference on Machine Learning\n(2021). URL: https://arxiv.org/abs/2106.09678. (Cited in Section 2)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.09678",
                        "Citation Paper Title": "Title:SECANT: Self-Expert Cloning for Zero-Shot Generalization of Visual Policies",
                        "Citation Paper Abstract": "Abstract:Generalization has been a long-standing challenge for reinforcement learning (RL). Visual RL, in particular, can be easily distracted by irrelevant factors in high-dimensional observation space. In this work, we consider robust policy learning which targets zero-shot generalization to unseen visual environments with large distributional shift. We propose SECANT, a novel self-expert cloning technique that leverages image augmentation in two stages to decouple robust representation learning from policy optimization. Specifically, an expert policy is first trained by RL from scratch with weak augmentations. A student network then learns to mimic the expert policy by supervised learning with strong augmentations, making its representation more robust against visual variations compared to the expert. Extensive experiments demonstrate that SECANT significantly advances the state of the art in zero-shot generalization across 4 challenging domains. Our average reward improvements over prior SOTAs are: DeepMind Control (+26.5%), robotic manipulation (+337.8%), vision-based autonomous driving (+47.7%), and indoor object navigation (+15.8%). Code release and video are available at this https URL.",
                        "Citation Paper Authors": "Authors:Linxi Fan, Guanzhi Wang, De-An Huang, Zhiding Yu, Li Fei-Fei, Yuke Zhu, Anima Anandkumar"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2304.00670v3": {
            "Paper Title": "CRN: Camera Radar Net for Accurate, Robust, Efficient 3D Perception",
            "Sentences": [
                {
                    "Sentence ID": 88,
                    "Sentence": "C R50 256\u00d7704 47.5 35.1 0.639 0.267 0.479 0.428 0.198 11.6\nRCBEV4d\u2020 ",
                    "Citation Text": "Taohua Zhou, Junjie Chen, Yining Shi, Kun Jiang, Meng-\nmeng Yang, and Diange Yang. Bridging the view disparity\nbetween radar and camera features for multi-modal fusion 3d\nobject detection. IEEE Transactions on Intelligent Vehicles\n(IEEE Trans. Intell. Veh.) , 2023. 3, 6, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2208.12079",
                        "Citation Paper Title": "Title:Bridging the View Disparity Between Radar and Camera Features for Multi-modal Fusion 3D Object Detection",
                        "Citation Paper Abstract": "Abstract:Environmental perception with the multi-modal fusion of radar and camera is crucial in autonomous driving to increase accuracy, completeness, and robustness. This paper focuses on utilizing millimeter-wave (MMW) radar and camera sensor fusion for 3D object detection. A novel method that realizes the feature-level fusion under the bird's-eye view (BEV) for a better feature representation is proposed. Firstly, radar points are augmented with temporal accumulation and sent to a spatial-temporal encoder for radar feature extraction. Meanwhile, multi-scale image 2D features which adapt to various spatial scales are obtained by image backbone and neck model. Then, image features are transformed to BEV with the designed view transformer. In addition, this work fuses the multi-modal features with a two-stage fusion model called point-fusion and ROI-fusion, respectively. Finally, a detection head regresses objects category and 3D locations. Experimental results demonstrate that the proposed method realizes the state-of-the-art (SOTA) performance under the most crucial detection metrics-mean average precision (mAP) and nuScenes detection score (NDS) on the challenging nuScenes dataset.",
                        "Citation Paper Authors": "Authors:Taohua Zhou, Yining Shi, Junjie Chen, Kun Jiang, Mengmeng Yang, Diange Yang"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "improves detection performance by pre-training depth es-\ntimation task on depth dataset ",
                    "Citation Text": "Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raven-\ntos, and Adrien Gaidon. 3D Packing for Self-Supervised\nMonocular Depth Estimation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) , pages 2485\u20132494, 2020. 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.02693",
                        "Citation Paper Title": "Title:3D Packing for Self-Supervised Monocular Depth Estimation",
                        "Citation Paper Abstract": "Abstract:Although cameras are ubiquitous, robotic platforms typically rely on active sensors like LiDAR for direct 3D perception. In this work, we propose a novel self-supervised monocular depth estimation method combining geometry with a new deep network, PackNet, learned only from unlabeled monocular videos. Our architecture leverages novel symmetrical packing and unpacking blocks to jointly learn to compress and decompress detail-preserving representations using 3D convolutions. Although self-supervised, our method outperforms other self, semi, and fully supervised methods on the KITTI benchmark. The 3D inductive bias in PackNet enables it to scale with input resolution and number of parameters without overfitting, generalizing better on out-of-domain data such as the NuScenes dataset. Furthermore, it does not require large-scale supervised pretraining on ImageNet and can run in real-time. Finally, we release DDAD (Dense Depth for Automated Driving), a new urban driving dataset with more challenging and accurate depth evaluation, thanks to longer-range and denser ground-truth depth generated from high-density LiDARs mounted on a fleet of self-driving cars operating world-wide.",
                        "Citation Paper Authors": "Authors:Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raventos, Adrien Gaidon"
                    }
                },
                {
                    "Sentence ID": 82,
                    "Sentence": "C R101 900\u00d71600 44.2 37.0 0.711 0.267 0.383 0.865 0.201 1.7\nMVFusion\u2020 ",
                    "Citation Text": "Zizhang Wu, Guilian Chen, Yuanzhu Gan, Lei Wang, and\nJian Pu. Mvfusion: Multi-view 3d object detection with\nsemantic-aligned radar and camera fusion. In Proceedings\nof the IEEE International Conference on Robotics and Au-\ntomation (ICRA) , 2023. 3, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.10511",
                        "Citation Paper Title": "Title:MVFusion: Multi-View 3D Object Detection with Semantic-aligned Radar and Camera Fusion",
                        "Citation Paper Abstract": "Abstract:Multi-view radar-camera fused 3D object detection provides a farther detection range and more helpful features for autonomous driving, especially under adverse weather. The current radar-camera fusion methods deliver kinds of designs to fuse radar information with camera data. However, these fusion approaches usually adopt the straightforward concatenation operation between multi-modal features, which ignores the semantic alignment with radar features and sufficient correlations across modals. In this paper, we present MVFusion, a novel Multi-View radar-camera Fusion method to achieve semantic-aligned radar features and enhance the cross-modal information interaction. To achieve so, we inject the semantic alignment into the radar features via the semantic-aligned radar encoder (SARE) to produce image-guided radar features. Then, we propose the radar-guided fusion transformer (RGFT) to fuse our radar and image features to strengthen the two modals' correlation from the global scope via the cross-attention mechanism. Extensive experiments show that MVFusion achieves state-of-the-art performance (51.7% NDS and 45.3% mAP) on the nuScenes dataset. We shall release our code and trained networks upon publication.",
                        "Citation Paper Authors": "Authors:Zizhang Wu, Guilian Chen, Yuanzhu Gan, Lei Wang, Jian Pu"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "C R50 256\u00d7704 53.4 42.7 0.567 0.274 0.411 0.252 0.188 11.4\nCRN C+R R18 256\u00d7704 54.3 44.8 0.518 0.283 0.552 0.279 0.180 27.9\nCRN C+R R50 256\u00d7704 56.0 49.0 0.487 0.277 0.542 0.344 0.197 20.4\nPETR\u2020 ",
                    "Citation Text": "Yingfei Liu, Tiancai Wang, Xiangyu Zhang, and Jian Sun.\nPetr: Position embedding transformation for multi-view 3d\nobject detection. In Proceedings of the European Conference\non Computer Vision (ECCV) , pages 531\u2013\u2013548, 2022. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.05625",
                        "Citation Paper Title": "Title:PETR: Position Embedding Transformation for Multi-View 3D Object Detection",
                        "Citation Paper Abstract": "Abstract:In this paper, we develop position embedding transformation (PETR) for multi-view 3D object detection. PETR encodes the position information of 3D coordinates into image features, producing the 3D position-aware features. Object query can perceive the 3D position-aware features and perform end-to-end object detection. PETR achieves state-of-the-art performance (50.4% NDS and 44.1% mAP) on standard nuScenes dataset and ranks 1st place on the benchmark. It can serve as a simple yet strong baseline for future research. Code is available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Yingfei Liu, Tiancai Wang, Xiangyu Zhang, Jian Sun"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "L V oxel - 65.3 56.9 0.285 0.253 0.323 0.272 0.186 -\nBEVDet\u2020 ",
                    "Citation Text": "Junjie Huang, Guan Huang, Zheng Zhu, and Dalong Du.\nBevdet: High-performance multi-camera 3d object detection\nin bird-eye-view. In arXiv preprint arXiv:2112.11790 , 2021.\n1, 6, 13, 14",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.11790",
                        "Citation Paper Title": "Title:BEVDet: High-performance Multi-camera 3D Object Detection in Bird-Eye-View",
                        "Citation Paper Abstract": "Abstract:Autonomous driving perceives its surroundings for decision making, which is one of the most complex scenarios in visual perception. The success of paradigm innovation in solving the 2D object detection task inspires us to seek an elegant, feasible, and scalable paradigm for fundamentally pushing the performance boundary in this area. To this end, we contribute the BEVDet paradigm in this paper. BEVDet performs 3D object detection in Bird-Eye-View (BEV), where most target values are defined and route planning can be handily performed. We merely reuse existing modules to build its framework but substantially develop its performance by constructing an exclusive data augmentation strategy and upgrading the Non-Maximum Suppression strategy. In the experiment, BEVDet offers an excellent trade-off between accuracy and time-efficiency. As a fast version, BEVDet-Tiny scores 31.2% mAP and 39.2% NDS on the nuScenes val set. It is comparable with FCOS3D, but requires just 11% computational budget of 215.3 GFLOPs and runs 9.2 times faster at 15.6 FPS. Another high-precision version dubbed BEVDet-Base scores 39.3% mAP and 47.2% NDS, significantly exceeding all published results. With a comparable inference speed, it surpasses FCOS3D by a large margin of +9.8% mAP and +10.0% NDS. The source code is publicly available for further research at this https URL .",
                        "Citation Paper Authors": "Authors:Junjie Huang, Guan Huang, Zheng Zhu, Yun Ye, Dalong Du"
                    }
                },
                {
                    "Sentence ID": 64,
                    "Sentence": "feature. Some approaches further utilize point and voxel\nfeatures together ",
                    "Citation Text": "Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping\nShi, Xiaogang Wang, and Hongsheng Li. PV-RCNN: Point-\nV oxel Feature Set Abstraction for 3D Object Detection. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR) , pages 10529\u201310538,\n2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.13192",
                        "Citation Paper Title": "Title:PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection",
                        "Citation Paper Abstract": "Abstract:We present a novel and high-performance 3D object detection framework, named PointVoxel-RCNN (PV-RCNN), for accurate 3D object detection from point clouds. Our proposed method deeply integrates both 3D voxel Convolutional Neural Network (CNN) and PointNet-based set abstraction to learn more discriminative point cloud features. It takes advantages of efficient learning and high-quality proposals of the 3D voxel CNN and the flexible receptive fields of the PointNet-based networks. Specifically, the proposed framework summarizes the 3D scene with a 3D voxel CNN into a small set of keypoints via a novel voxel set abstraction module to save follow-up computations and also to encode representative scene features. Given the high-quality 3D proposals generated by the voxel CNN, the RoI-grid pooling is proposed to abstract proposal-specific features from the keypoints to the RoI-grid points via keypoint set abstraction with multiple receptive fields. Compared with conventional pooling operations, the RoI-grid feature points encode much richer context information for accurately estimating object confidences and locations. Extensive experiments on both the KITTI dataset and the Waymo Open dataset show that our proposed PV-RCNN surpasses state-of-the-art 3D detection methods with remarkable margins by using only point clouds. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, Hongsheng Li"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.15275v1": {
            "Paper Title": "MARS: Multi-Scale Adaptive Robotics Vision for Underwater Object\n  Detection and Domain Generalization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2303.00952v3": {
            "Paper Title": "MuscleMap: Towards Video-based Activated Muscle Group Estimation in the\n  Wild",
            "Sentences": [
                {
                    "Sentence ID": 83,
                    "Sentence": ", obviously outperform theTABLE V: Ablation for the MCTF on the MuscleMap bench-\nmark.\nMethodknown\nvalnew\nvalmean\nvalknown\ntestnew\ntestmean\ntest\nSum ",
                    "Citation Text": "A. Roitberg, K. Peng, Z. Marinov, C. Seibold, D. Schneider, and\nR. Stiefelhagen, \u201cA comparative analysis of decision-level fusion for\nmultimodal driver behaviour understanding,\u201d in Proc. IV , 2022, pp.\n1438\u20131444.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.04734",
                        "Citation Paper Title": "Title:A Comparative Analysis of Decision-Level Fusion for Multimodal Driver Behaviour Understanding",
                        "Citation Paper Abstract": "Abstract:Visual recognition inside the vehicle cabin leads to safer driving and more intuitive human-vehicle interaction but such systems face substantial obstacles as they need to capture different granularities of driver behaviour while dealing with highly limited body visibility and changing illumination. Multimodal recognition mitigates a number of such issues: prediction outcomes of different sensors complement each other due to different modality-specific strengths and weaknesses. While several late fusion methods have been considered in previously published frameworks, they constantly feature different architecture backbones and building blocks making it very hard to isolate the role of the chosen late fusion strategy itself. This paper presents an empirical evaluation of different paradigms for decision-level late fusion in video-based driver observation. We compare seven different mechanisms for joining the results of single-modal classifiers which have been both popular, (e.g. score averaging) and not yet considered (e.g. rank-level fusion) in the context of driver observation evaluating them based on different criteria and benchmark settings. This is the first systematic study of strategies for fusing outcomes of multimodal predictors inside the vehicles, conducted with the goal to provide guidance for fusion scheme selection.",
                        "Citation Paper Authors": "Authors:Alina Roitberg, Kunyu Peng, Zdravko Marinov, Constantin Seibold, David Schneider, Rainer Stiefelhagen"
                    }
                },
                {
                    "Sentence ID": 69,
                    "Sentence": ", methods that make use of\nknowledge about label relations ",
                    "Citation Text": "Z.-M. Chen, X.-S. Wei, P. Wang, and Y . Guo, \u201cMulti-label image\nrecognition with graph convolutional networks,\u201d in Proc. CVPR , 2019,\npp. 5177\u20135186.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.03582",
                        "Citation Paper Title": "Title:Multi-Label Image Recognition with Graph Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:The task of multi-label image recognition is to predict a set of object labels that present in an image. As objects normally co-occur in an image, it is desirable to model the label dependencies to improve the recognition performance. To capture and explore such important dependencies, we propose a multi-label classification model based on Graph Convolutional Network (GCN). The model builds a directed graph over the object labels, where each node (label) is represented by word embeddings of a label, and GCN is learned to map this label graph into a set of inter-dependent object classifiers. These classifiers are applied to the image descriptors extracted by another sub-net, enabling the whole network to be end-to-end trainable. Furthermore, we propose a novel re-weighted scheme to create an effective label correlation matrix to guide information propagation among the nodes in GCN. Experiments on two multi-label image recognition datasets show that our approach obviously outperforms other existing state-of-the-art methods. In addition, visualization analyses reveal that the classifiers learned by our model maintain meaningful semantic topology.",
                        "Citation Paper Authors": "Authors:Zhao-Min Chen, Xiu-Shen Wei, Peng Wang, Yanwen Guo"
                    }
                },
                {
                    "Sentence ID": 62,
                    "Sentence": ", or based on knowledge about the\nrelations of data samples or features (relation-based) ",
                    "Citation Text": "P. Chen, S. Liu, H. Zhao, and J. Jia, \u201cDistilling knowledge via knowledge\nreview,\u201d in CVPR , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.09044",
                        "Citation Paper Title": "Title:Distilling Knowledge via Knowledge Review",
                        "Citation Paper Abstract": "Abstract:Knowledge distillation transfers knowledge from the teacher network to the student one, with the goal of greatly improving the performance of the student network. Previous methods mostly focus on proposing feature transformation and loss functions between the same level's features to improve the effectiveness. We differently study the factor of connection path cross levels between teacher and student networks, and reveal its great importance. For the first time in knowledge distillation, cross-stage connection paths are proposed. Our new review mechanism is effective and structurally simple. Our finally designed nested and compact framework requires negligible computation overhead, and outperforms other methods on a variety of tasks. We apply our method to classification, object detection, and instance segmentation tasks. All of them witness significant student network performance improvement. Code is available at this https URL",
                        "Citation Paper Authors": "Authors:Pengguang Chen, Shu Liu, Hengshuang Zhao, Jiaya Jia"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": "became a common tech-\nnique to reduce the size of a neural network while maintaining\nperformance. In review ",
                    "Citation Text": "J. Gou, B. Yu, S. J. Maybank, and D. Tao, \u201cKnowledge distillation: A\nsurvey,\u201d International Journal of Computer Vision , vol. 129, no. 6, pp.\n1789\u20131819, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.05525",
                        "Citation Paper Title": "Title:Knowledge Distillation: A Survey",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher-student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded.",
                        "Citation Paper Authors": "Authors:Jianping Gou, Baosheng Yu, Stephen John Maybank, Dacheng Tao"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "which was propelled\nby the advent of Convolutional Neural Networks (CNNs) with\n2D-CNNs ",
                    "Citation Text": "C. Feichtenhofer, A. Pinz, and A. Zisserman, \u201cConvolutional two-stream\nnetwork fusion for video action recognition,\u201d in Proc. CVPR , 2016, pp.\n1933\u20131941.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1604.06573",
                        "Citation Paper Title": "Title:Convolutional Two-Stream Network Fusion for Video Action Recognition",
                        "Citation Paper Abstract": "Abstract:Recent applications of Convolutional Neural Networks (ConvNets) for human action recognition in videos have proposed different solutions for incorporating the appearance and motion information. We study a number of ways of fusing ConvNet towers both spatially and temporally in order to best take advantage of this spatio-temporal information. We make the following findings: (i) that rather than fusing at the softmax layer, a spatial and temporal network can be fused at a convolution layer without loss of performance, but with a substantial saving in parameters; (ii) that it is better to fuse such networks spatially at the last convolutional layer than earlier, and that additionally fusing at the class prediction layer can boost accuracy; finally (iii) that pooling of abstract convolutional features over spatiotemporal neighbourhoods further boosts performance. Based on these studies we propose a new ConvNet architecture for spatiotemporal fusion of video snippets, and evaluate its performance on standard benchmarks where this architecture achieves state-of-the-art results.",
                        "Citation Paper Authors": "Authors:Christoph Feichtenhofer, Axel Pinz, Andrew Zisserman"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2301.04576v2": {
            "Paper Title": "Collision-free Source Seeking and Flocking Control of Multi-agents with\n  Connectivity Preservation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15204v1": {
            "Paper Title": "DexDLO: Learning Goal-Conditioned Dexterous Policy for Dynamic\n  Manipulation of Deformable Linear Objects",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": ", 5)\nwhipping, by fixing one end of the DLO and fixing the\nother end to the robot arm ",
                    "Citation Text": "C. Chi, B. Burchfiel, E. Cousineau, S. Feng, and S. Song, \u201cItera-\ntive residual policy: for goal-conditioned dynamic manipulation of\ndeformable objects,\u201d arXiv preprint arXiv:2203.00663 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.00663",
                        "Citation Paper Title": "Title:Iterative Residual Policy: for Goal-Conditioned Dynamic Manipulation of Deformable Objects",
                        "Citation Paper Abstract": "Abstract:This paper tackles the task of goal-conditioned dynamic manipulation of deformable objects. This task is highly challenging due to its complex dynamics (introduced by object deformation and high-speed action) and strict task requirements (defined by a precise goal specification). To address these challenges, we present Iterative Residual Policy (IRP), a general learning framework applicable to repeatable tasks with complex dynamics. IRP learns an implicit policy via delta dynamics -- instead of modeling the entire dynamical system and inferring actions from that model, IRP learns delta dynamics that predict the effects of delta action on the previously-observed trajectory. When combined with adaptive action sampling, the system can quickly optimize its actions online to reach a specified goal. We demonstrate the effectiveness of IRP on two tasks: whipping a rope to hit a target point and swinging a cloth to reach a target pose. Despite being trained only in simulation on a fixed robot setup, IRP is able to efficiently generalize to noisy real-world dynamics, new objects with unseen physical properties, and even different robot hardware embodiments, demonstrating its excellent generalization capability relative to alternative approaches. Video is available at this https URL",
                        "Citation Paper Authors": "Authors:Cheng Chi, Benjamin Burchfiel, Eric Cousineau, Siyuan Feng, Shuran Song"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2311.12893v2": {
            "Paper Title": "A Safer Vision-based Autonomous Planning System for Quadrotor UAVs with\n  Dynamic Obstacle Trajectory Prediction and Its Application with LLMs",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": ". And given the functions, GPT\ncan generate controls for long-step tasks ",
                    "Citation Text": "Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun\nTakamatsu, and Katsushi Ikeuchi. Chatgpt empowered long-\nstep robot control in various environments: A case applica-\ntion. arXiv preprint arXiv:2304.03893 , 2023. 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.03893",
                        "Citation Paper Title": "Title:ChatGPT Empowered Long-Step Robot Control in Various Environments: A Case Application",
                        "Citation Paper Abstract": "Abstract:This paper demonstrates how OpenAI's ChatGPT can be used in a few-shot setting to convert natural language instructions into a sequence of executable robot actions. The paper proposes easy-to-customize input prompts for ChatGPT that meet common requirements in practical applications, such as easy integration with robot execution systems and applicability to various environments while minimizing the impact of ChatGPT's token limit. The prompts encourage ChatGPT to output a sequence of predefined robot actions, represent the operating environment in a formalized style, and infer the updated state of the operating environment. Experiments confirmed that the proposed prompts enable ChatGPT to act according to requirements in various environments, and users can adjust ChatGPT's output with natural language feedback for safe and robust operation. The proposed prompts and source code are open-source and publicly available at this https URL",
                        "Citation Paper Authors": "Authors:Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, Katsushi Ikeuchi"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "utilizes the voxel map to identify dynamic vox-\nels and estimate their velocities. ",
                    "Citation Text": "Gang Chen, Peng Peng, Peihan Zhang, and Wei Dong. Risk-\naware trajectory sampling for quadrotor obstacle avoidance\nin dynamic environments. IEEE Transactions on Industrial\nElectronics , 2023. 1, 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.06645",
                        "Citation Paper Title": "Title:Risk-aware Trajectory Sampling for Quadrotor Obstacle Avoidance in Dynamic Environments",
                        "Citation Paper Abstract": "Abstract:Obstacle avoidance of quadrotors in dynamic environments is still a very open problem. Current works commonly leverage traditional static maps to represent static obstacles and the detection and tracking of moving objects (DATMO) method to model dynamic obstacles separately. The detection module requires pre-training, and the dynamic obstacles can only be modeled with certain shapes, such as cylinders or ellipsoids. This work utilizes the dual-structure particle-based (DSP) dynamic occupancy map to represent the arbitrary-shaped static obstacles and dynamic obstacles simultaneously, and proposes an efficient risk-aware sampling-based local trajectory planner to realize safe flights in this map. The trajectory is planned by sampling motion primitives generated in the state space. Each motion primitive is divided into two phases: a short-term phase with a strict risk limitation and a relatively long-term phase designed to avoid high-risk regions. The risk is evaluated with the predicted particle-form future occupancy status, considering the time dimension. With an approach to split from and merge to an arbitrary global trajectory, the planner can also be used in the tasks with preplanned global trajectories. Comparison experiments show that the obstacle avoidance system composed of the DSP map and our planner performs the best in dynamic environments. In real-world tests, our quadrotor reaches a speed of 6 m/s with the motion capture system and 2.5 m/s with everything running on a low-price single-board computer.",
                        "Citation Paper Authors": "Authors:Gang Chen, Peng Peng, Peihan Zhang, Wei Dong"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.15122v1": {
            "Paper Title": "Scaling Is All You Need: Training Strong Policies for Autonomous Driving\n  with JAX-Accelerated Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.15096v1": {
            "Paper Title": "Optimal In-Place Compaction of Sliding Cubes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2311.01248v2": {
            "Paper Title": "Multimodal and Force-Matched Imitation Learning with a See-Through\n  Visuotactile Sensor",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": ", where the expert\u2019s reward function is\ninferred from the demonstration set. Both behaviour cloning ",
                    "Citation Text": "T. Ablett, Y . Zhai, and J. Kelly, \u201cSeeing All the Angles: Learning\nMultiview Manipulation Policies for Contact-Rich Tasks from Demon-\nstrations,\u201d in Proceedings of the IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS\u201921) , Prague, Czech Republic, Sept.\n2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.13907",
                        "Citation Paper Title": "Title:Seeing All the Angles: Learning Multiview Manipulation Policies for Contact-Rich Tasks from Demonstrations",
                        "Citation Paper Abstract": "Abstract:Learned visuomotor policies have shown considerable success as an alternative to traditional, hand-crafted frameworks for robotic manipulation. Surprisingly, an extension of these methods to the multiview domain is relatively unexplored. A successful multiview policy could be deployed on a mobile manipulation platform, allowing the robot to complete a task regardless of its view of the scene. In this work, we demonstrate that a multiview policy can be found through imitation learning by collecting data from a variety of viewpoints. We illustrate the general applicability of the method by learning to complete several challenging multi-stage and contact-rich tasks, from numerous viewpoints, both in a simulated environment and on a real mobile manipulation platform. Furthermore, we analyze our policies to determine the benefits of learning from multiview data compared to learning with data collected from a fixed perspective. We show that learning from multiview data results in little, if any, penalty to performance for a fixed-view task compared to learning with an equivalent amount of fixed-view data. Finally, we examine the visual features learned by the multiview and fixed-view policies. Our results indicate that multiview policies implicitly learn to identify spatially correlated features.",
                        "Citation Paper Authors": "Authors:Trevor Ablett, Yifan Zhai, Jonathan Kelly"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2308.07948v2": {
            "Paper Title": "Leveraging Symmetries in Pick and Place",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.14781v1": {
            "Paper Title": "ROS package search for robot software development: a knowledge\n  graph-based approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2303.09565v3": {
            "Paper Title": "SPSysML: A meta-model for quantitative evaluation of Simulation-Physical\n  Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.14730v1": {
            "Paper Title": "To Fuse or Not to Fuse: Measuring Consistency in Multi-Sensor Fusion for\n  Aerial Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.14717v1": {
            "Paper Title": "Kinematic Characterization of Micro-Mobility Vehicles During Evasive\n  Maneuvers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.14706v1": {
            "Paper Title": "BonnBeetClouds3D: A Dataset Towards Point Cloud-based Organ-level\n  Phenotyping of Sugar Beet Plants under Field Conditions",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": "K. Stein-Bachinger, F. Gottwald, A. Haub, and E. Schmidt. To what\nextent does organic farming promote species richness and abundance\nin temperate climates? a review. Organic Agriculture , 11, 03 2021. ",
                    "Citation Text": "P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V . Patnaik, P. Tsui,\nJ. Guo, Y . Zhou, Y . Chai, B. Caine, et al. Scalability in perception for\nautonomous driving: Waymo open dataset. In Proc. of the IEEE/CVF\nConf. on Computer Vision and Pattern Recognition (CVPR) , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.04838",
                        "Citation Paper Title": "Title:Scalability in Perception for Autonomous Driving: Waymo Open Dataset",
                        "Citation Paper Abstract": "Abstract:The research community has increasing interest in autonomous driving research, despite the resource intensity of obtaining representative real world data. Existing self-driving datasets are limited in the scale and variation of the environments they capture, even though generalization within and between operating regions is crucial to the overall viability of the technology. In an effort to help align the research community's contributions with real-world self-driving problems, we introduce a new large scale, high quality, diverse dataset. Our new dataset consists of 1150 scenes that each span 20 seconds, consisting of well synchronized and calibrated high quality LiDAR and camera data captured across a range of urban and suburban geographies. It is 15x more diverse than the largest camera+LiDAR dataset available based on our proposed diversity metric. We exhaustively annotated this data with 2D (camera image) and 3D (LiDAR) bounding boxes, with consistent identifiers across frames. Finally, we provide strong baselines for 2D as well as 3D detection and tracking tasks. We further study the effects of dataset size and generalization across geographies on 3D detection methods. Find data, code and more up-to-date information at this http URL.",
                        "Citation Paper Authors": "Authors:Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Sheng Zhao, Shuyang Cheng, Yu Zhang, Jonathon Shlens, Zhifeng Chen, Dragomir Anguelov"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2304.11241v2": {
            "Paper Title": "AutoNeRF: Training Implicit Scene Representations with Autonomous Agents",
            "Sentences": []
        },
        "http://arxiv.org/abs/2306.11706v2": {
            "Paper Title": "RoboCat: A Self-Improving Generalist Agent for Robotic Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2311.02389v2": {
            "Paper Title": "Multiplayer Homicidal Chauffeur Reach-Avoid Games: A Pursuit Enclosure\n  Function Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.17260v1": {
            "Paper Title": "TimePillars: Temporally-Recurrent 3D LiDAR Object Detection",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": "employed sparse-convolutions\nand achieved a much shorter inference time and improved\nperformance. A substantial change in voxelization philoso-\nphy was brought by PointPillars ",
                    "Citation Text": "Alex H. Lang, Sourabh V ora, Holger Caesar, Lubing Zhou,\nJiong Yang, and Oscar Beijbom. PointPillars: Fast Encoders\nfor Object Detection from Point Clouds. Proceedings of the\nIEEE Computer Society Conference on Computer Vision and\nPattern Recognition , 2019-June:12689\u201312697, 12 2018. 2,\n3, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.05784",
                        "Citation Paper Title": "Title:PointPillars: Fast Encoders for Object Detection from Point Clouds",
                        "Citation Paper Abstract": "Abstract:Object detection in point clouds is an important aspect of many robotics applications such as autonomous driving. In this paper we consider the problem of encoding a point cloud into a format appropriate for a downstream detection pipeline. Recent literature suggests two types of encoders; fixed encoders tend to be fast but sacrifice accuracy, while encoders that are learned from data are more accurate, but slower. In this work we propose PointPillars, a novel encoder which utilizes PointNets to learn a representation of point clouds organized in vertical columns (pillars). While the encoded features can be used with any standard 2D convolutional detection architecture, we further propose a lean downstream network. Extensive experimentation shows that PointPillars outperforms previous encoders with respect to both speed and accuracy by a large margin. Despite only using lidar, our full detection pipeline significantly outperforms the state of the art, even among fusion methods, with respect to both the 3D and bird's eye view KITTI benchmarks. This detection performance is achieved while running at 62 Hz: a 2 - 4 fold runtime improvement. A faster version of our method matches the state of the art at 105 Hz. These benchmarks suggest that PointPillars is an appropriate encoding for object detection in point clouds.",
                        "Citation Paper Authors": "Authors:Alex H. Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, Oscar Beijbom"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "attempted\nto add a convLSTM in the PointPillars architecture after the\nbackbone and before the detection head. Nevertheless, the\nresults obtained were not improving the overall former ar-\nchitecture if fine-grained pillars were utilized. An alterna-\ntive, proposed in ",
                    "Citation Text": "Rui Huang, Wanyue Zhang, Abhijit Kundu, Caroline Panto-\nfaru, David A Ross, Thomas Funkhouser, and Alireza Fathi.\nAn LSTM Approach to Temporal 3D Object Detection in Li-\nDAR Point Clouds. 7 2020. 1, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.12392",
                        "Citation Paper Title": "Title:An LSTM Approach to Temporal 3D Object Detection in LiDAR Point Clouds",
                        "Citation Paper Abstract": "Abstract:Detecting objects in 3D LiDAR data is a core technology for autonomous driving and other robotics applications. Although LiDAR data is acquired over time, most of the 3D object detection algorithms propose object bounding boxes independently for each frame and neglect the useful information available in the temporal domain. To address this problem, in this paper we propose a sparse LSTM-based multi-frame 3d object detection algorithm. We use a U-Net style 3D sparse convolution network to extract features for each frame's LiDAR point-cloud. These features are fed to the LSTM module together with the hidden and memory features from last frame to predict the 3d objects in the current frame as well as hidden and memory features that are passed to the next frame. Experiments on the Waymo Open Dataset show that our algorithm outperforms the traditional frame by frame approach by 7.5% mAP@0.7 and other multi-frame approaches by 1.2% while using less memory and computation per frame. To the best of our knowledge, this is the first work to use an LSTM for 3D object detection in sparse point clouds.",
                        "Citation Paper Authors": "Authors:Rui Huang, Wanyue Zhang, Abhijit Kundu, Caroline Pantofaru, David A Ross, Thomas Funkhouser, Alireza Fathi"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "replaced the frame fusion model, which owned 3D convo-\nlutions, by a more efficient module based on 2D convolu-\ntions. Most recently, ",
                    "Citation Text": "Lue Fan, Yuxue Yang, Feng Wang, Naiyan Wang, and\nZhaoxiang Zhang. Super Sparse 3D Object Detection. 1\n2023. 1, 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2301.02562",
                        "Citation Paper Title": "Title:Super Sparse 3D Object Detection",
                        "Citation Paper Abstract": "Abstract:As the perception range of LiDAR expands, LiDAR-based 3D object detection contributes ever-increasingly to the long-range perception in autonomous driving. Mainstream 3D object detectors often build dense feature maps, where the cost is quadratic to the perception range, making them hardly scale up to the long-range settings. To enable efficient long-range detection, we first propose a fully sparse object detector termed FSD. FSD is built upon the general sparse voxel encoder and a novel sparse instance recognition (SIR) module. SIR groups the points into instances and applies highly-efficient instance-wise feature extraction. The instance-wise grouping sidesteps the issue of the center feature missing, which hinders the design of the fully sparse architecture. To further enjoy the benefit of fully sparse characteristic, we leverage temporal information to remove data redundancy and propose a super sparse detector named FSD++. FSD++ first generates residual points, which indicate the point changes between consecutive frames. The residual points, along with a few previous foreground points, form the super sparse input data, greatly reducing data redundancy and computational overhead. We comprehensively analyze our method on the large-scale Waymo Open Dataset, and state-of-the-art performance is reported. To showcase the superiority of our method in long-range detection, we also conduct experiments on Argoverse 2 Dataset, where the perception range ($200m$) is much larger than Waymo Open Dataset ($75m$). Code is open-sourced at this https URL.",
                        "Citation Paper Authors": "Authors:Lue Fan, Yuxue Yang, Feng Wang, Naiyan Wang, Zhaoxiang Zhang"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": ", which instead of us-\ning regular voxels, point clouds were discretised in vertical\ncolumns, defining pillars.\n2.2. Aggregation of point clouds\nFaF ",
                    "Citation Text": "Wenjie Luo, Bin Yang, and Raquel Urtasun. Fast and Fu-\nrious: Real Time End-to-End 3D Detection, Tracking and\nMotion Forecasting with a Single Convolutional Net. Pro-\nceedings of the IEEE Computer Society Conference on Com-\nputer Vision and Pattern Recognition , pages 3569\u20133577, 12\n2018. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.12395",
                        "Citation Paper Title": "Title:Fast and Furious: Real Time End-to-End 3D Detection, Tracking and Motion Forecasting with a Single Convolutional Net",
                        "Citation Paper Abstract": "Abstract:In this paper we propose a novel deep neural network that is able to jointly reason about 3D detection, tracking and motion forecasting given data captured by a 3D sensor. By jointly reasoning about these tasks, our holistic approach is more robust to occlusion as well as sparse data at range. Our approach performs 3D convolutions across space and time over a bird's eye view representation of the 3D world, which is very efficient in terms of both memory and computation. Our experiments on a new very large scale dataset captured in several north american cities, show that we can outperform the state-of-the-art by a large margin. Importantly, by sharing computation we can perform all tasks in as little as 30 ms.",
                        "Citation Paper Authors": "Authors:Wenjie Luo, Bin Yang, Raquel Urtasun"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "Despite the success of 2D object detection methods\n[14,21,22,26], the need for robust perception has promoted\nthe active exploration of 3D alternatives. As the first big\nstep in deep learning on raw point clouds, Qi et al. ",
                    "Citation Text": "Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas.\nPointNet: Deep Learning on Point Sets for 3D Classifica-\ntion and Segmentation. Proceedings - 30th IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2017 ,\n2017-January:77\u201385, 12 2016. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.00593",
                        "Citation Paper Title": "Title:PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation",
                        "Citation Paper Abstract": "Abstract:Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds and well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.",
                        "Citation Paper Authors": "Authors:Charles R. Qi, Hao Su, Kaichun Mo, Leonidas J. Guibas"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.14481v1": {
            "Paper Title": "Part to Whole: Collaborative Prompting for Surgical Instrument\n  Segmentation",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": ". However, this approach\nrequires complete fine-tuning of the CLIP image encoder,leading to high computational cost. By contrast, Surgi-\ncalSAM offers an efficient-tuning approach based on SAM ",
                    "Citation Text": "Wenxi Yue, Jing Zhang, Kun Hu, Yong Xia, Jiebo Luo,\nand Zhiyong Wang. Surgicalsam: Efficient class prompt-\nable surgical instrument segmentation. arXiv preprint\narXiv:2308.08746 , 2023. 2, 3, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2308.08746",
                        "Citation Paper Title": "Title:SurgicalSAM: Efficient Class Promptable Surgical Instrument Segmentation",
                        "Citation Paper Abstract": "Abstract:The Segment Anything Model (SAM) is a powerful foundation model that has revolutionised image segmentation. To apply SAM to surgical instrument segmentation, a common approach is to locate precise points or boxes of instruments and then use them as prompts for SAM in a zero-shot manner. However, we observe two problems with this naive pipeline: (1) the domain gap between natural objects and surgical instruments leads to inferior generalisation of SAM; and (2) SAM relies on precise point or box locations for accurate segmentation, requiring either extensive manual guidance or a well-performing specialist detector for prompt preparation, which leads to a complex multi-stage pipeline. To address these problems, we introduce SurgicalSAM, a novel end-to-end efficient-tuning approach for SAM to effectively integrate surgical-specific information with SAM's pre-trained knowledge for improved generalisation. Specifically, we propose a lightweight prototype-based class prompt encoder for tuning, which directly generates prompt embeddings from class prototypes and eliminates the use of explicit prompts for improved robustness and a simpler pipeline. In addition, to address the low inter-class variance among surgical instrument categories, we propose contrastive prototype learning, further enhancing the discrimination of the class prototypes for more accurate class prompting. The results of extensive experiments on both EndoVis2018 and EndoVis2017 datasets demonstrate that SurgicalSAM achieves state-of-the-art performance while only requiring a small number of tunable parameters. The source code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Wenxi Yue, Jing Zhang, Kun Hu, Yong Xia, Jiebo Luo, Zhiyong Wang"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "65.72 60.88 38.60 72.90 31.07 64.73 10.24 12.28 61.05 17.93 -\nPerSAM ",
                    "Citation Text": "Renrui Zhang, Zhengkai Jiang, Ziyu Guo, Shilin Yan, Junt-\ning Pan, Hao Dong, Peng Gao, and Hongsheng Li. Person-\nalize segment anything model with one shot. arXiv preprint\narXiv:2305.03048 , 2023. 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.03048",
                        "Citation Paper Title": "Title:Personalize Segment Anything Model with One Shot",
                        "Citation Paper Abstract": "Abstract:Driven by large-data pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful and promptable framework, revolutionizing the segmentation models. Despite the generality, customizing SAM for specific visual concepts without man-powered prompting is under explored, e.g., automatically segmenting your pet dog in different images. In this paper, we propose a training-free Personalization approach for SAM, termed as PerSAM. Given only a single image with a reference mask, PerSAM first localizes the target concept by a location prior, and segments it within other images or videos via three techniques: target-guided attention, target-semantic prompting, and cascaded post-refinement. In this way, we effectively adapt SAM for private use without any training. To further alleviate the mask ambiguity, we present an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the entire SAM, we introduce two learnable weights for multi-scale masks, only training 2 parameters within 10 seconds for improved performance. To demonstrate our efficacy, we construct a new segmentation dataset, PerSeg, for personalized evaluation, and test our methods on video object segmentation with competitive performance. Besides, our approach can also enhance DreamBooth to personalize Stable Diffusion for text-to-image generation, which discards the background disturbance for better target appearance learning. Code is released at this https URL",
                        "Citation Paper Authors": "Authors:Renrui Zhang, Zhengkai Jiang, Ziyu Guo, Shilin Yan, Junting Pan, Xianzheng Ma, Hao Dong, Peng Gao, Hongsheng Li"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "+ SAM 78.72 78.72 52.50 85.95 82.31 44.08 0.00 49.80 92.17 13.18 68.72M\nTrackAnything (1 Point) ",
                    "Citation Text": "Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing\nWang, and Feng Zheng. Track anything: Segment anything\nmeets videos. arXiv preprint arXiv:2304.11968 , 2023. 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.11968",
                        "Citation Paper Title": "Title:Track Anything: Segment Anything Meets Videos",
                        "Citation Paper Abstract": "Abstract:Recently, the Segment Anything Model (SAM) gains lots of attention rapidly due to its impressive segmentation performance on images. Regarding its strong ability on image segmentation and high interactivity with different prompts, we found that it performs poorly on consistent segmentation in videos. Therefore, in this report, we propose Track Anything Model (TAM), which achieves high-performance interactive tracking and segmentation in videos. To be detailed, given a video sequence, only with very little human participation, i.e., several clicks, people can track anything they are interested in, and get satisfactory results in one-pass inference. Without additional training, such an interactive design performs impressively on video object tracking and segmentation. All resources are available on {this https URL}. We hope this work can facilitate related research.",
                        "Citation Paper Authors": "Authors:Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, Feng Zheng"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "+ SAM 78.49 78.49 56.07 79.83 74.86 43.12 62.88 16.74 91.62 23.45 57.67M\nMask2Former ",
                    "Citation Text": "Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexan-\nder Kirillov, and Rohit Girdhar. Masked-attention mask\ntransformer for universal image segmentation. In CVPR ,\npages 1290\u20131299, 2022. 2, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.01527",
                        "Citation Paper Title": "Title:Masked-attention Mask Transformer for Universal Image Segmentation",
                        "Citation Paper Abstract": "Abstract:Image segmentation is about grouping pixels with different semantics, e.g., category or instance membership, where each choice of semantics defines a task. While only the semantics of each task differ, current research focuses on designing specialized architectures for each task. We present Masked-attention Mask Transformer (Mask2Former), a new architecture capable of addressing any image segmentation task (panoptic, instance or semantic). Its key components include masked attention, which extracts localized features by constraining cross-attention within predicted mask regions. In addition to reducing the research effort by at least three times, it outperforms the best specialized architectures by a significant margin on four popular datasets. Most notably, Mask2Former sets a new state-of-the-art for panoptic segmentation (57.8 PQ on COCO), instance segmentation (50.1 AP on COCO) and semantic segmentation (57.7 mIoU on ADE20K).",
                        "Citation Paper Authors": "Authors:Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, Rohit Girdhar"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "and flow-based temporal priors\n[20, 46]. An alternative strategy to semantic segmentation is\ninstance segmentation. ISINet adopts Mask-RCNN [15, 16]\nfor this task, which is later enhanced by Baby et al. ",
                    "Citation Text": "Britty Baby, Daksh Thapar, Mustafa Chasmai, Tamajit\nBanerjee, Kunal Dargan, Ashish Suri, Subhashis Banerjee,\nand Chetan Arora. From forks to forceps: A new framework\nfor instance segmentation of surgical instruments. In WACV ,\npages 6180\u20136190. IEEE, 2023. 2, 5, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2211.16200",
                        "Citation Paper Title": "Title:From Forks to Forceps: A New Framework for Instance Segmentation of Surgical Instruments",
                        "Citation Paper Abstract": "Abstract:Minimally invasive surgeries and related applications demand surgical tool classification and segmentation at the instance level. Surgical tools are similar in appearance and are long, thin, and handled at an angle. The fine-tuning of state-of-the-art (SOTA) instance segmentation models trained on natural images for instrument segmentation has difficulty discriminating instrument classes. Our research demonstrates that while the bounding box and segmentation mask are often accurate, the classification head mis-classifies the class label of the surgical instrument. We present a new neural network framework that adds a classification module as a new stage to existing instance segmentation models. This module specializes in improving the classification of instrument masks generated by the existing model. The module comprises multi-scale mask attention, which attends to the instrument region and masks the distracting background features. We propose training our classifier module using metric learning with arc loss to handle low inter-class variance of surgical instruments. We conduct exhaustive experiments on the benchmark datasets EndoVis2017 and EndoVis2018. We demonstrate that our method outperforms all (more than 18) SOTA methods compared with, and improves the SOTA performance by at least 12 points (20%) on the EndoVis2017 benchmark challenge and generalizes effectively across the datasets.",
                        "Citation Paper Authors": "Authors:Britty Baby, Daksh Thapar, Mustafa Chasmai, Tamajit Banerjee, Kunal Dargan, Ashish Suri, Subhashis Banerjee, Chetan Arora"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "with\na specialised classification module. In addition, TraSeTR\nutilises a track-to-segment transformer with tracking cues ",
                    "Citation Text": "Zixu Zhao, Yueming Jin, and Pheng-Ann Heng. TraSeTr:\ntrack-to-segment transformer with contrastive query for\ninstance-level instrument segmentation in robotic surgery. In\nICRA , pages 11186\u201311193. IEEE, 2022. 2, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.08453",
                        "Citation Paper Title": "Title:TraSeTR: Track-to-Segment Transformer with Contrastive Query for Instance-level Instrument Segmentation in Robotic Surgery",
                        "Citation Paper Abstract": "Abstract:Surgical instrument segmentation -- in general a pixel classification task -- is fundamentally crucial for promoting cognitive intelligence in robot-assisted surgery (RAS). However, previous methods are struggling with discriminating instrument types and instances. To address the above issues, we explore a mask classification paradigm that produces per-segment predictions. We propose TraSeTR, a novel Track-to-Segment Transformer that wisely exploits tracking cues to assist surgical instrument segmentation. TraSeTR jointly reasons about the instrument type, location, and identity with instance-level predictions i.e., a set of class-bbox-mask pairs, by decoding query embeddings. Specifically, we introduce the prior query that encoded with previous temporal knowledge, to transfer tracking signals to current instances via identity matching. A contrastive query learning strategy is further applied to reshape the query feature space, which greatly alleviates the tracking difficulty caused by large temporal variations. The effectiveness of our method is demonstrated with state-of-the-art instrument type segmentation results on three public datasets, including two RAS benchmarks from EndoVis Challenges and one cataract surgery dataset CaDIS.",
                        "Citation Paper Authors": "Authors:Zixu Zhao, Yueming Jin, Pheng-Ann Heng"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "67.87 39.14 24.68 69.23 6.10 11.68 14.00 0.91 70.24 0.57 37.73M\nDual-MF ",
                    "Citation Text": "Zixu Zhao, Yueming Jin, Xiaojie Gao, Qi Dou, and Pheng-\nAnn Heng. Learning motion flows for semi-supervised in-\nstrument segmentation from robotic surgical video. In MIC-\nCAI, pages 679\u2013689. Springer, 2020. 2, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.02501",
                        "Citation Paper Title": "Title:Learning Motion Flows for Semi-supervised Instrument Segmentation from Robotic Surgical Video",
                        "Citation Paper Abstract": "Abstract:Performing low hertz labeling for surgical videos at intervals can greatly releases the burden of surgeons. In this paper, we study the semi-supervised instrument segmentation from robotic surgical videos with sparse annotations. Unlike most previous methods using unlabeled frames individually, we propose a dual motion based method to wisely learn motion flows for segmentation enhancement by leveraging temporal dynamics. We firstly design a flow predictor to derive the motion for jointly propagating the frame-label pairs given the current labeled frame. Considering the fast instrument motion, we further introduce a flow compensator to estimate intermediate motion within continuous frames, with a novel cycle learning strategy. By exploiting generated data pairs, our framework can recover and even enhance temporal consistency of training sequences to benefit segmentation. We validate our framework with binary, part, and type tasks on 2017 MICCAI EndoVis Robotic Instrument Segmentation Challenge dataset. Results show that our method outperforms the state-of-the-art semi-supervised methods by a large margin, and even exceeds fully supervised training on two tasks.",
                        "Citation Paper Authors": "Authors:Zixu Zhao, Yueming Jin, Xiaojie Gao, Qi Dou, Pheng-Ann Heng"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "46.22 39.87 14.19 44.20 4.67 0.00 0.00 0.00 50.44 0.00 32.20M\nMF-TAPNet ",
                    "Citation Text": "Yueming Jin, Keyun Cheng, Qi Dou, and Pheng-Ann Heng.\nIncorporating temporal prior from motion flow for instru-\nment segmentation in minimally invasive surgery video. In\nMICCAI , pages 440\u2013448. Springer, 2019. 2, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.07899",
                        "Citation Paper Title": "Title:Incorporating Temporal Prior from Motion Flow for Instrument Segmentation in Minimally Invasive Surgery Video",
                        "Citation Paper Abstract": "Abstract:Automatic instrument segmentation in video is an essentially fundamental yet challenging problem for robot-assisted minimally invasive surgery. In this paper, we propose a novel framework to leverage instrument motion information, by incorporating a derived temporal prior to an attention pyramid network for accurate segmentation. Our inferred prior can provide reliable indication of the instrument location and shape, which is propagated from the previous frame to the current frame according to inter-frame motion flow. This prior is injected to the middle of an encoder-decoder segmentation network as an initialization of a pyramid of attention modules, to explicitly guide segmentation output from coarse to fine. In this way, the temporal dynamics and the attention network can effectively complement and benefit each other. As additional usage, our temporal prior enables semi-supervised learning with periodically unlabeled video frames, simply by reverse execution. We extensively validate our method on the public 2017 MICCAI EndoVis Robotic Instrument Segmentation Challenge dataset with three different tasks. Our method consistently exceeds the state-of-the-art results across all three tasks by a large margin. Our semi-supervised variant also demonstrates a promising potential for reducing annotation cost in the clinical practice.",
                        "Citation Paper Authors": "Authors:Yueming Jin, Keyun Cheng, Qi Dou, Pheng-Ann Heng"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "2.1. Surgical Instrument Segmentation\nMost surgical instrument segmentation methods focus on\ndeveloping vision-based specialist models. Early research\nadopts a semantic segmentation pipeline with the pioneer-\ning work TernausNet introducing a U-Net based encoder-\ndecoder model ",
                    "Citation Text": "Alexey A Shvets, Alexander Rakhlin, Alexandr A Kalinin,\nand Vladimir I Iglovikov. Automatic instrument segmenta-\ntion in robot-assisted surgery using deep learning. In ICMLA ,\npages 624\u2013628. IEEE, 2018. 2, 5, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.01207",
                        "Citation Paper Title": "Title:Automatic Instrument Segmentation in Robot-Assisted Surgery Using Deep Learning",
                        "Citation Paper Abstract": "Abstract:Semantic segmentation of robotic instruments is an important problem for the robot-assisted surgery. One of the main challenges is to correctly detect an instrument's position for the tracking and pose estimation in the vicinity of surgical scenes. Accurate pixel-wise instrument segmentation is needed to address this challenge. In this paper we describe our winning solution for MICCAI 2017 Endoscopic Vision SubChallenge: Robotic Instrument Segmentation. Our approach demonstrates an improvement over the state-of-the-art results using several novel deep neural network architectures. It addressed the binary segmentation problem, where every pixel in an image is labeled as an instrument or background from the surgery video feed. In addition, we solve a multi-class segmentation problem, where we distinguish different instruments or different parts of an instrument from the background. In this setting, our approach outperforms other methods in every task subcategory for automatic instrument segmentation thereby providing state-of-the-art solution for this problem. The source code for our solution is made publicly available at this https URL",
                        "Citation Paper Authors": "Authors:Alexey Shvets, Alexander Rakhlin, Alexandr A. Kalinin, Vladimir Iglovikov"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "84.92 83.61 65.44 84.28 73.18 78.88 92.20 23.73 66.67 39.12 -\nSAM-based ModelMaskTrack-RCNN ",
                    "Citation Text": "Linjie Yang, Yuchen Fan, and Ning Xu. Video instance seg-\nmentation. In ICCV , pages 5188\u20135197, 2019. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.04804",
                        "Citation Paper Title": "Title:Video Instance Segmentation",
                        "Citation Paper Abstract": "Abstract:In this paper we present a new computer vision task, named video instance segmentation. The goal of this new task is simultaneous detection, segmentation and tracking of instances in videos. In words, it is the first time that the image instance segmentation problem is extended to the video domain. To facilitate research on this new task, we propose a large-scale benchmark called YouTube-VIS, which consists of 2883 high-resolution YouTube videos, a 40-category label set and 131k high-quality instance masks. In addition, we propose a novel algorithm called MaskTrack R-CNN for this task. Our new method introduces a new tracking branch to Mask R-CNN to jointly perform the detection, segmentation and tracking tasks simultaneously. Finally, we evaluate the proposed method and several strong baselines on our new dataset. Experimental results clearly demonstrate the advantages of the proposed algorithm and reveal insight for future improvement. We believe the video instance segmentation task will motivate the community along the line of research for video understanding.",
                        "Citation Paper Authors": "Authors:Linjie Yang, Yuchen Fan, Ning Xu"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "introduces\na text promptable framework exploiting the pre-trained\nvision-language model CLIP ",
                    "Citation Text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML , pages 8748\u20138763. PMLR, 2021. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.00020",
                        "Citation Paper Title": "Title:Learning Transferable Visual Models From Natural Language Supervision",
                        "Citation Paper Abstract": "Abstract:State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",
                        "Citation Paper Authors": "Authors:Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.14474v1": {
            "Paper Title": "MonoLSS: Learnable Sample Selection For Monocular 3D Detection",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": "CVPR2020 None 13.04 9.99 8.65 19.28 14.83 12.89 60\nMonoDLE ",
                    "Citation Text": "Xinzhu Ma, Yinmin Zhang, Dan Xu, Dongzhan Zhou, Shuai\nYi, Haojie Li, and Wanli Ouyang. Delving into localization\nerrors for monocular 3d object detection. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) , pages 4721\u20134730, 2021. 1, 5, 6, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.16237",
                        "Citation Paper Title": "Title:Delving into Localization Errors for Monocular 3D Object Detection",
                        "Citation Paper Abstract": "Abstract:Estimating 3D bounding boxes from monocular images is an essential component in autonomous driving, while accurate 3D object detection from this kind of data is very challenging. In this work, by intensive diagnosis experiments, we quantify the impact introduced by each sub-task and found the `localization error' is the vital factor in restricting monocular 3D detection. Besides, we also investigate the underlying reasons behind localization errors, analyze the issues they might bring, and propose three strategies. First, we revisit the misalignment between the center of the 2D bounding box and the projected center of the 3D object, which is a vital factor leading to low localization accuracy. Second, we observe that accurately localizing distant objects with existing technologies is almost impossible, while those samples will mislead the learned network. To this end, we propose to remove such samples from the training set for improving the overall performance of the detector. Lastly, we also propose a novel 3D IoU oriented loss for the size estimation of the object, which is not affected by `localization error'. We conduct extensive experiments on the KITTI dataset, where the proposed method achieves real-time detection and outperforms previous methods by a large margin. The code will be made available at: this https URL.",
                        "Citation Paper Authors": "Authors:Xinzhu Ma, Yinmin Zhang, Dan Xu, Dongzhan Zhou, Shuai Yi, Haojie Li, Wanli Ouyang"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "ICCV 2021 None 20.11 14.20 11.77 - - - 30\nDEVIANT ",
                    "Citation Text": "Abhinav Kumar, Garrick Brazil, Enrique Corona, Armin Par-\nchami, and Xiaoming Liu. Deviant: Depth equivariant net-\nwork for monocular 3d object detection. In European Con-\nference on Computer Vision , pages 664\u2013683. Springer, 2022.\n6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2207.10758",
                        "Citation Paper Title": "Title:DEVIANT: Depth EquiVarIAnt NeTwork for Monocular 3D Object Detection",
                        "Citation Paper Abstract": "Abstract:Modern neural networks use building blocks such as convolutions that are equivariant to arbitrary 2D translations. However, these vanilla blocks are not equivariant to arbitrary 3D translations in the projective manifold. Even then, all monocular 3D detectors use vanilla blocks to obtain the 3D coordinates, a task for which the vanilla blocks are not designed for. This paper takes the first step towards convolutions equivariant to arbitrary 3D translations in the projective manifold. Since the depth is the hardest to estimate for monocular detection, this paper proposes Depth EquiVarIAnt NeTwork (DEVIANT) built with existing scale equivariant steerable blocks. As a result, DEVIANT is equivariant to the depth translations in the projective manifold whereas vanilla networks are not. The additional depth equivariance forces the DEVIANT to learn consistent depth estimates, and therefore, DEVIANT achieves state-of-the-art monocular 3D detection results on KITTI and Waymo datasets in the image-only category and performs competitively to methods using extra information. Moreover, DEVIANT works better than vanilla networks in cross-dataset evaluation. Code and models at this https URL",
                        "Citation Paper Authors": "Authors:Abhinav Kumar, Garrick Brazil, Enrique Corona, Armin Parchami, Xiaoming Liu"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "CVPR 2019 LIDAR 10.76 7.25 5.85 18.33 12.58 9.91 200\nPatchNet ",
                    "Citation Text": "Xinzhu Ma, Shinan Liu, Zhiyi Xia, Hongwen Zhang, Xingyu\nZeng, and Wanli Ouyang. Rethinking pseudo-lidar represen-\ntation. In Proceedings of the European Conference on Com-\nputer Vision (ECCV) , 2020. 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.04582",
                        "Citation Paper Title": "Title:Rethinking Pseudo-LiDAR Representation",
                        "Citation Paper Abstract": "Abstract:The recently proposed pseudo-LiDAR based 3D detectors greatly improve the benchmark of monocular/stereo 3D detection task. However, the underlying mechanism remains obscure to the research community. In this paper, we perform an in-depth investigation and observe that the efficacy of pseudo-LiDAR representation comes from the coordinate transformation, instead of data representation itself. Based on this observation, we design an image based CNN detector named Patch-Net, which is more generalized and can be instantiated as pseudo-LiDAR based 3D detectors. Moreover, the pseudo-LiDAR data in our PatchNet is organized as the image representation, which means existing 2D CNN designs can be easily utilized for extracting deep features from input data and boosting 3D detection performance. We conduct extensive experiments on the challenging KITTI dataset, where the proposed PatchNet outperforms all existing pseudo-LiDAR based counterparts. Code has been made available at: this https URL.",
                        "Citation Paper Authors": "Authors:Xinzhu Ma, Shinan Liu, Zhiyi Xia, Hongwen Zhang, Xingyu Zeng, Wanli Ouyang"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "uses\nLIDAR points to generate depth maps and estimates depth\nby an additional monocular network, then converts the fea-\nture to BEV perspective for prediction. CMKD ",
                    "Citation Text": "Yu Hong, Hang Dai, and Yong Ding. Cross-modality knowl-\nedge distillation network for monocular 3d object detection.\nInECCV . Springer, 2022. 3, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2211.07171",
                        "Citation Paper Title": "Title:Cross-Modality Knowledge Distillation Network for Monocular 3D Object Detection",
                        "Citation Paper Abstract": "Abstract:Leveraging LiDAR-based detectors or real LiDAR point data to guide monocular 3D detection has brought significant improvement, e.g., Pseudo-LiDAR methods. However, the existing methods usually apply non-end-to-end training strategies and insufficiently leverage the LiDAR information, where the rich potential of the LiDAR data has not been well exploited. In this paper, we propose the Cross-Modality Knowledge Distillation (CMKD) network for monocular 3D detection to efficiently and directly transfer the knowledge from LiDAR modality to image modality on both features and responses. Moreover, we further extend CMKD as a semi-supervised training framework by distilling knowledge from large-scale unlabeled data and significantly boost the performance. Until submission, CMKD ranks $1^{st}$ among the monocular 3D detectors with publications on both KITTI $test$ set and Waymo $val$ set with significant performance gains compared to previous state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Yu Hong, Hang Dai, Yong Ding"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "learns auxiliary monocular contexts pro-\njected from the 3D bounding boxes in training and discards\nthem for better inference efficiency in inference. MonoDDE ",
                    "Citation Text": "Zhuoling Li, Zhan Qu, Yang Zhou, Jianzhuang Liu, Haoqian\nWang, and Lihui Jiang. Diversity matters: Fully exploit-\ning depth clues for reliable monocular 3d object detection.\nInProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) , pages 2791\u20132800,\n2022. 1, 2, 3, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.09373",
                        "Citation Paper Title": "Title:Diversity Matters: Fully Exploiting Depth Clues for Reliable Monocular 3D Object Detection",
                        "Citation Paper Abstract": "Abstract:As an inherently ill-posed problem, depth estimation from single images is the most challenging part of monocular 3D object detection (M3OD). Many existing methods rely on preconceived assumptions to bridge the missing spatial information in monocular images, and predict a sole depth value for every object of interest. However, these assumptions do not always hold in practical applications. To tackle this problem, we propose a depth solving system that fully explores the visual clues from the subtasks in M3OD and generates multiple estimations for the depth of each target. Since the depth estimations rely on different assumptions in essence, they present diverse distributions. Even if some assumptions collapse, the estimations established on the remaining assumptions are still reliable. In addition, we develop a depth selection and combination strategy. This strategy is able to remove abnormal estimations caused by collapsed assumptions, and adaptively combine the remaining estimations into a single one. In this way, our depth solving system becomes more precise and robust. Exploiting the clues from multiple subtasks of M3OD and without introducing any extra information, our method surpasses the current best method by more than 20% relatively on the Moderate level of test split in the KITTI 3D object detection benchmark, while still maintaining real-time efficiency.",
                        "Citation Paper Authors": "Authors:Zhuoling Li, Zhan Qu, Yang Zhou, Jianzhuang Liu, Haoqian Wang, Lihui Jiang"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "proposes\ndepth-guided convolution in which the receptive field is de-\ntermined adaptively by the predicted depth. DID-M3D ",
                    "Citation Text": "Liang Peng, Xiaopei Wu, Zheng Yang, Haifeng Liu, and\nDeng Cai. Did-m3d: Decoupling instance depth for monoc-\nular 3d object detection. In European Conference on Com-\nputer Vision , 2022. 1, 2, 3, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2207.08531",
                        "Citation Paper Title": "Title:DID-M3D: Decoupling Instance Depth for Monocular 3D Object Detection",
                        "Citation Paper Abstract": "Abstract:Monocular 3D detection has drawn much attention from the community due to its low cost and setup simplicity. It takes an RGB image as input and predicts 3D boxes in the 3D space. The most challenging sub-task lies in the instance depth estimation. Previous works usually use a direct estimation method. However, in this paper we point out that the instance depth on the RGB image is non-intuitive. It is coupled by visual depth clues and instance attribute clues, making it hard to be directly learned in the network. Therefore, we propose to reformulate the instance depth to the combination of the instance visual surface depth (visual depth) and the instance attribute depth (attribute depth). The visual depth is related to objects' appearances and positions on the image. By contrast, the attribute depth relies on objects' inherent attributes, which are invariant to the object affine transformation on the image. Correspondingly, we decouple the 3D location uncertainty into visual depth uncertainty and attribute depth uncertainty. By combining different types of depths and associated uncertainties, we can obtain the final instance depth. Furthermore, data augmentation in monocular 3D detection is usually limited due to the physical nature, hindering the boost of performance. Based on the proposed instance depth disentanglement strategy, we can alleviate this problem. Evaluated on KITTI, our method achieves new state-of-the-art results, and extensive ablation studies validate the effectiveness of each component in our method. The codes are released at this https URL.",
                        "Citation Paper Authors": "Authors:Liang Peng, Xiaopei Wu, Zheng Yang, Haifeng Liu, Deng Cai"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "to divide the\n7481 training images into a training set (3712) and vali-dation set (3769) for ablation study. Following the offi-\ncial protocol ",
                    "Citation Text": "Andrea Simonelli, Samuel Rota Bulo, Lorenzo Porzi,\nManuel Lopez-Antequera, and Peter Kontschieder. Disen-\ntangling monocular 3d object detection. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision\n(ICCV) , 2019. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.12365",
                        "Citation Paper Title": "Title:Disentangling Monocular 3D Object Detection",
                        "Citation Paper Abstract": "Abstract:In this paper we propose an approach for monocular 3D object detection from a single RGB image, which leverages a novel disentangling transformation for 2D and 3D detection losses and a novel, self-supervised confidence score for 3D bounding boxes. Our proposed loss disentanglement has the twofold advantage of simplifying the training dynamics in the presence of losses with complex interactions of parameters, and sidestepping the issue of balancing independent regression terms. Our solution overcomes these issues by isolating the contribution made by groups of parameters to a given loss, without changing its nature. We further apply loss disentanglement to another novel, signed Intersection-over-Union criterion-driven loss for improving 2D detection results. Besides our methodological innovations, we critically review the AP metric used in KITTI3D, which emerged as the most important dataset for comparing 3D detection results. We identify and resolve a flaw in the 11-point interpolated AP metric, affecting all previously published detection results and particularly biases the results of monocular 3D detection. We provide extensive experimental evaluations and ablation studies on the KITTI3D and nuScenes datasets, setting new state-of-the-art results on object category car by large margins.",
                        "Citation Paper Authors": "Authors:Andrea Simonelli, Samuel Rota Rota Bul\u00f2, Lorenzo Porzi, Manuel L\u00f3pez-Antequera, Peter Kontschieder"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "ECCV 2022 Depth 24.40 16.29 13.75 32.95 22.76 19.83 40\nDD3D ",
                    "Citation Text": "Dennis Park, Rares Ambrus, Vitor Guizilini, Jie Li, and\nAdrien Gaidon. Is pseudo-lidar needed for monocular 3d\nobject detection? In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV) , pages 3142\u2013\n3152, 2021. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.06417",
                        "Citation Paper Title": "Title:Is Pseudo-Lidar needed for Monocular 3D Object detection?",
                        "Citation Paper Abstract": "Abstract:Recent progress in 3D object detection from single images leverages monocular depth estimation as a way to produce 3D pointclouds, turning cameras into pseudo-lidar sensors. These two-stage detectors improve with the accuracy of the intermediate depth estimation network, which can itself be improved without manual labels via large-scale self-supervised learning. However, they tend to suffer from overfitting more than end-to-end methods, are more complex, and the gap with similar lidar-based detectors remains significant. In this work, we propose an end-to-end, single stage, monocular 3D object detector, DD3D, that can benefit from depth pre-training like pseudo-lidar methods, but without their limitations. Our architecture is designed for effective information transfer between depth estimation and 3D detection, allowing us to scale with the amount of unlabeled pre-training data. Our method achieves state-of-the-art results on two challenging benchmarks, with 16.34% and 9.28% AP for Cars and Pedestrians (respectively) on the KITTI-3D benchmark, and 41.5% mAP on NuScenes.",
                        "Citation Paper Authors": "Authors:Dennis Park, Rares Ambrus, Vitor Guizilini, Jie Li, Adrien Gaidon"
                    }
                },
                {
                    "Sentence ID": 60,
                    "Sentence": "predicts 3D bounding boxes by combining a single\nkeypoint estimation module. Furthermore, MonoFlex ",
                    "Citation Text": "Yunpeng Zhang, Jiwen Lu, and Jie Zhou. Objects are differ-\nent: Flexible monocular 3d object detection. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR) , pages 3289\u20133298, 2021. 2, 3, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.02323",
                        "Citation Paper Title": "Title:Objects are Different: Flexible Monocular 3D Object Detection",
                        "Citation Paper Abstract": "Abstract:The precise localization of 3D objects from a single image without depth information is a highly challenging problem. Most existing methods adopt the same approach for all objects regardless of their diverse distributions, leading to limited performance for truncated objects. In this paper, we propose a flexible framework for monocular 3D object detection which explicitly decouples the truncated objects and adaptively combines multiple approaches for object depth estimation. Specifically, we decouple the edge of the feature map for predicting long-tail truncated objects so that the optimization of normal objects is not influenced. Furthermore, we formulate the object depth estimation as an uncertainty-guided ensemble of directly regressed object depth and solved depths from different groups of keypoints. Experiments demonstrate that our method outperforms the state-of-the-art method by relatively 27\\% for the moderate level and 30\\% for the hard level in the test set of KITTI benchmark while maintaining real-time efficiency. Code will be available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Yunpeng Zhang, Jiwen Lu, Jie Zhou"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "optimizes the truncated obstacles prediction method with an\nedge heatmap and edge fusion module. MonoPair ",
                    "Citation Text": "Yongjian Chen, Lei Tai, Kai Sun, and Mingyang Li.\nMonopair: Monocular 3d object detection using pairwise\nspatial relationships. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) , 2020. 2, 3, 5, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.00504",
                        "Citation Paper Title": "Title:MonoPair: Monocular 3D Object Detection Using Pairwise Spatial Relationships",
                        "Citation Paper Abstract": "Abstract:Monocular 3D object detection is an essential component in autonomous driving while challenging to solve, especially for those occluded samples which are only partially visible. Most detectors consider each 3D object as an independent training target, inevitably resulting in a lack of useful information for occluded samples. To this end, we propose a novel method to improve the monocular 3D object detection by considering the relationship of paired samples. This allows us to encode spatial constraints for partially-occluded objects from their adjacent neighbors. Specifically, the proposed detector computes uncertainty-aware predictions for object locations and 3D distances for the adjacent object pairs, which are subsequently jointly optimized by nonlinear least squares. Finally, the one-stage uncertainty-aware prediction structure and the post-optimization module are dedicatedly integrated for ensuring the run-time efficiency. Experiments demonstrate that our method yields the best performance on KITTI 3D detection benchmark, by outperforming state-of-the-art competitors by wide margins, especially for the hard samples.",
                        "Citation Paper Authors": "Authors:Yongjian Chen, Lei Tai, Kai Sun, Mingyang Li"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "de-\nvelops a cross-modality method to transfer the knowledge\nfrom LiDAR modality to image. Besides depth maps and\nLIDAR, methods such as AutoShape ",
                    "Citation Text": "Zongdai Liu, Dingfu Zhou, Feixiang Lu, Jin Fang, and\nLiangjun Zhang. Autoshape: Real-time shape-aware monoc-\nular 3d object detection. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV) , pages\n15641\u201315650, 2021. 3, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.11127",
                        "Citation Paper Title": "Title:AutoShape: Real-Time Shape-Aware Monocular 3D Object Detection",
                        "Citation Paper Abstract": "Abstract:Existing deep learning-based approaches for monocular 3D object detection in autonomous driving often model the object as a rotated 3D cuboid while the object's geometric shape has been ignored. In this work, we propose an approach for incorporating the shape-aware 2D/3D constraints into the 3D detection framework. Specifically, we employ the deep neural network to learn distinguished 2D keypoints in the 2D image domain and regress their corresponding 3D coordinates in the local 3D object coordinate first. Then the 2D/3D geometric constraints are built by these correspondences for each object to boost the detection performance. For generating the ground truth of 2D/3D keypoints, an automatic model-fitting approach has been proposed by fitting the deformed 3D object model and the object mask in the 2D image. The proposed framework has been verified on the public KITTI dataset and the experimental results demonstrate that by using additional geometrical constraints the detection performance has been significantly improved as compared to the baseline method. More importantly, the proposed framework achieves state-of-the-art performance with real time. Data and code will be available at this https URL",
                        "Citation Paper Authors": "Authors:Zongdai Liu, Dingfu Zhou, Feixiang Lu, Jin Fang, Liangjun Zhang"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "ECCV 2020 Depth 15.68 11.12 10.17 22.97 16.86 14.97 400\nMonoRUn ",
                    "Citation Text": "Hansheng Chen, Yuyao Huang, Wei Tian, Zhong Gao, and\nLu Xiong. Monorun: Monocular 3d object detection by re-\nconstruction and uncertainty propagation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) , pages 10379\u201310388, 2021. 3, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.12605",
                        "Citation Paper Title": "Title:MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation",
                        "Citation Paper Abstract": "Abstract:Object localization in 3D space is a challenging aspect in monocular 3D object detection. Recent advances in 6DoF pose estimation have shown that predicting dense 2D-3D correspondence maps between image and object 3D model and then estimating object pose via Perspective-n-Point (PnP) algorithm can achieve remarkable localization accuracy. Yet these methods rely on training with ground truth of object geometry, which is difficult to acquire in real outdoor scenes. To address this issue, we propose MonoRUn, a novel detection framework that learns dense correspondences and geometry in a self-supervised manner, with simple 3D bounding box annotations. To regress the pixel-related 3D object coordinates, we employ a regional reconstruction network with uncertainty awareness. For self-supervised training, the predicted 3D coordinates are projected back to the image plane. A Robust KL loss is proposed to minimize the uncertainty-weighted reprojection error. During testing phase, we exploit the network uncertainty by propagating it through all downstream modules. More specifically, the uncertainty-driven PnP algorithm is leveraged to estimate object pose and its covariance. Extensive experiments demonstrate that our proposed approach outperforms current state-of-the-art methods on KITTI benchmark.",
                        "Citation Paper Authors": "Authors:Hansheng Chen, Yuyao Huang, Wei Tian, Zhong Gao, Lu Xiong"
                    }
                },
                {
                    "Sentence ID": 61,
                    "Sentence": "adopts a standalone 3D region proposal net-\nwork and proposes a depth-wise convolution to predict ob-\njects. Based on a CenterNet-style ",
                    "Citation Text": "Xingyi Zhou, Dequan Wang, and Philipp Kr \u00a8ahenb \u00a8uhl. Ob-\njects as points. In arXiv preprint arXiv:1904.07850 , 2019. 1,\n2, 4, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.07850",
                        "Citation Paper Title": "Title:Objects as Points",
                        "Citation Paper Abstract": "Abstract:Detection identifies objects as axis-aligned boxes in an image. Most successful object detectors enumerate a nearly exhaustive list of potential object locations and classify each. This is wasteful, inefficient, and requires additional post-processing. In this paper, we take a different approach. We model an object as a single point --- the center point of its bounding box. Our detector uses keypoint estimation to find center points and regresses to all other object properties, such as size, 3D location, orientation, and even pose. Our center point based approach, CenterNet, is end-to-end differentiable, simpler, faster, and more accurate than corresponding bounding box based detectors. CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1% AP at 142 FPS, 37.4% AP at 52 FPS, and 45.1% AP with multi-scale testing at 1.4 FPS. We use the same approach to estimate 3D bounding box in the KITTI benchmark and human pose on the COCO keypoint dataset. Our method performs competitively with sophisticated multi-stage methods and runs in real-time.",
                        "Citation Paper Authors": "Authors:Xingyi Zhou, Dequan Wang, Philipp Kr\u00e4henb\u00fchl"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "achieves probability sampling with-\nout random choices. Based on this work, Gumbel-Softmax ",
                    "Citation Text": "Eric Jang, Shixiang Gu, and Ben Poole. Categorical\nreparametrization with gumble-softmax. In InternationalConference on Learning Representations (ICLR 2017) .\nOpenReview. net, 2017. 2, 4, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.01144",
                        "Citation Paper Title": "Title:Categorical Reparameterization with Gumbel-Softmax",
                        "Citation Paper Abstract": "Abstract:Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.",
                        "Citation Paper Authors": "Authors:Eric Jang, Shixiang Gu, Ben Poole"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "uses the softmax function as the continuous, differen-\ntiable approximation to argmax , and achieves overall dif-\nferentiability with the help of reparameterization. Gumbel-\nTop-k ",
                    "Citation Text": "Wouter Kool, Herke Van Hoof, and Max Welling. Stochastic\nbeams and where to find them: The gumbel-top-k trick for\nsampling sequences without replacement. In International\nConference on Machine Learning , pages 3499\u20133508. PMLR,\n2019. 2, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.06059",
                        "Citation Paper Title": "Title:Stochastic Beams and Where to Find Them: The Gumbel-Top-k Trick for Sampling Sequences Without Replacement",
                        "Citation Paper Abstract": "Abstract:The well-known Gumbel-Max trick for sampling from a categorical distribution can be extended to sample $k$ elements without replacement. We show how to implicitly apply this 'Gumbel-Top-$k$' trick on a factorized distribution over sequences, allowing to draw exact samples without replacement using a Stochastic Beam Search. Even for exponentially large domains, the number of model evaluations grows only linear in $k$ and the maximum sampled sequence length. The algorithm creates a theoretical connection between sampling and (deterministic) beam search and can be used as a principled intermediate alternative. In a translation task, the proposed method compares favourably against alternatives to obtain diverse yet good quality translations. We show that sequences sampled without replacement can be used to construct low-variance estimators for expected sentence-level BLEU score and model entropy.",
                        "Citation Paper Authors": "Authors:Wouter Kool, Herke van Hoof, Max Welling"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": ". It takes\nan image I\u2208RH\u00d7W\u00d73as input and adopts DLA34 ",
                    "Citation Text": "Fisher Yu, Dequan Wang, Evan Shelhamer, and Trevor Dar-\nrell. Deep layer aggregation. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR) , 2018. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.06484",
                        "Citation Paper Title": "Title:Deep Layer Aggregation",
                        "Citation Paper Abstract": "Abstract:Visual recognition requires rich representations that span levels from low to high, scales from small to large, and resolutions from fine to coarse. Even with the depth of features in a convolutional network, a layer in isolation is not enough: compounding and aggregating these representations improves inference of what and where. Architectural efforts are exploring many dimensions for network backbones, designing deeper or wider architectures, but how to best aggregate layers and blocks across a network deserves further attention. Although skip connections have been incorporated to combine layers, these connections have been \"shallow\" themselves, and only fuse by simple, one-step operations. We augment standard architectures with deeper aggregation to better fuse information across layers. Our deep layer aggregation structures iteratively and hierarchically merge the feature hierarchy to make networks with better accuracy and fewer parameters. Experiments across architectures and tasks show that deep layer aggregation improves recognition and resolution compared to existing branching and merging schemes. The code is at this https URL.",
                        "Citation Paper Authors": "Authors:Fisher Yu, Dequan Wang, Evan Shelhamer, Trevor Darrell"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "allocates samples by set-\nting adaptive IOU thresholds based on statistical character-\nistics of the object. MTL ",
                    "Citation Text": "Wei Ke, Tianliang Zhang, Zeyi Huang, Qixiang Ye,\nJianzhuang Liu, and Dong Huang. Multiple anchor learn-\ning for visual object detection. In IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) , 2020. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.02252",
                        "Citation Paper Title": "Title:Multiple Anchor Learning for Visual Object Detection",
                        "Citation Paper Abstract": "Abstract:Classification and localization are two pillars of visual object detectors. However, in CNN-based detectors, these two modules are usually optimized under a fixed set of candidate (or anchor) bounding boxes. This configuration significantly limits the possibility to jointly optimize classification and localization. In this paper, we propose a Multiple Instance Learning (MIL) approach that selects anchors and jointly optimizes the two modules of a CNN-based object detector. Our approach, referred to as Multiple Anchor Learning (MAL), constructs anchor bags and selects the most representative anchors from each bag. Such an iterative selection process is potentially NP-hard to optimize. To address this issue, we solve MAL by repetitively depressing the confidence of selected anchors by perturbing their corresponding features. In an adversarial selection-depression manner, MAL not only pursues optimal solutions but also fully leverages multiple anchors/features to learn a detection model. Experiments show that MAL improves the baseline RetinaNet with significant margins on the commonly used MS-COCO object detection benchmark and achieves new state-of-the-art detection performance compared with recent methods.",
                        "Citation Paper Authors": "Authors:Wei Ke, Tianliang Zhang, Zeyi Huang, Qixiang Ye, Jianzhuang Liu, Dong Huang"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "combines the deep feature maps and estimates dense depth\nmaps to regress 3D bounding boxes. D4LCN ",
                    "Citation Text": "Mingyu Ding, Yuqi Huo, Hongwei Yi, Zhe Wang, Jianping\nShi, Zhiwu Lu, and Ping Luo. Learning depth-guided con-\nvolutions for monocular 3d object detection. In IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR) , 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.04799",
                        "Citation Paper Title": "Title:Learning Depth-Guided Convolutions for Monocular 3D Object Detection",
                        "Citation Paper Abstract": "Abstract:3D object detection from a single image without LiDAR is a challenging task due to the lack of accurate depth information. Conventional 2D convolutions are unsuitable for this task because they fail to capture local object and its scale information, which are vital for 3D object detection. To better represent 3D structure, prior arts typically transform depth maps estimated from 2D images into a pseudo-LiDAR representation, and then apply existing 3D point-cloud based object detectors. However, their results depend heavily on the accuracy of the estimated depth maps, resulting in suboptimal performance. In this work, instead of using pseudo-LiDAR representation, we improve the fundamental 2D fully convolutions by proposing a new local convolutional network (LCN), termed Depth-guided Dynamic-Depthwise-Dilated LCN (D$^4$LCN), where the filters and their receptive fields can be automatically learned from image-based depth maps, making different pixels of different images have different filters. D$^4$LCN overcomes the limitation of conventional 2D convolutions and narrows the gap between image representation and 3D point cloud representation. Extensive experiments show that D$^4$LCN outperforms existing works by large margins. For example, the relative improvement of D$^4$LCN against the state-of-the-art on KITTI is 9.1\\% in the moderate setting. The code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Mingyu Ding, Yuqi Huo, Hongwei Yi, Zhe Wang, Jianping Shi, Zhiwu Lu, Ping Luo"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2310.13230v3": {
            "Paper Title": "Absolute Policy Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.14466v1": {
            "Paper Title": "Towards Assessing Compliant Robotic Grasping from First-Object\n  Perspective via Instrumented Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.14457v1": {
            "Paper Title": "QUAR-VLA: Vision-Language-Action Model for Quadruped Robots",
            "Sentences": [
                {
                    "Sentence ID": 49,
                    "Sentence": "to associate tokens from the model\u2019s\nexisting tokenization with the discrete action bins. It is\nworth noting that training VLMs to override existing to-\nkens with action tokens is a form of symbol tuning ",
                    "Citation Text": "Jerry Wei, Le Hou, Andrew Lampinen, Xiangning Chen,\nDa Huang, Yi Tay, Xinyun Chen, Yifeng Lu, Denny Zhou,\nTengyu Ma, et al. Symbol tuning improves in-context learn-\ning in language models. arXiv preprint arXiv:2305.08298 ,\n2023. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.08298",
                        "Citation Paper Title": "Title:Symbol tuning improves in-context learning in language models",
                        "Citation Paper Abstract": "Abstract:We present symbol tuning - finetuning language models on in-context input-label pairs where natural language labels (e.g., \"positive/negative sentiment\") are replaced with arbitrary symbols (e.g., \"foo/bar\"). Symbol tuning leverages the intuition that when a model cannot use instructions or natural language labels to figure out a task, it must instead do so by learning the input-label mappings.\nWe experiment with symbol tuning across Flan-PaLM models up to 540B parameters and observe benefits across various settings. First, symbol tuning boosts performance on unseen in-context learning tasks and is much more robust to underspecified prompts, such as those without instructions or without natural language labels. Second, symbol-tuned models are much stronger at algorithmic reasoning tasks, with up to 18.2% better performance on the List Functions benchmark and up to 15.3% better performance on the Simple Turing Concepts benchmark. Finally, symbol-tuned models show large improvements in following flipped-labels presented in-context, meaning that they are more capable of using in-context information to override prior semantic knowledge.",
                        "Citation Paper Authors": "Authors:Jerry Wei, Le Hou, Andrew Lampinen, Xiangning Chen, Da Huang, Yi Tay, Xinyun Chen, Yifeng Lu, Denny Zhou, Tengyu Ma, Quoc V. Le"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "\u03c41(t|zv)to\ncompute a compact set of tokens t, and finally a Trans-\nformer decoder ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, AidanN. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Neural Information\nProcessing Systems,Neural Information Processing Systems ,\nJun 2017. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "2.1. Legged Robot Learning\nPrevious research has delved into single-model policies.\nOn one hand, locomotion tasks utilize vision perception to\npredict a privileged terrain map, enabling the robot to per-\nform specific locomotion based on the terrain characteris-\ntics ",
                    "Citation Text": "Simar Kareer, Naoki Yokoyama, Dhruv Batra, Sehoon Ha,\nand Joanne Truong. Vinl: Visual navigation and locomo-\ntion over obstacles. 2023 IEEE International Conference on\nRobotics and Automation (ICRA) , pages 2018\u20132024, 2022.\n2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.14791",
                        "Citation Paper Title": "Title:ViNL: Visual Navigation and Locomotion Over Obstacles",
                        "Citation Paper Abstract": "Abstract:We present Visual Navigation and Locomotion over obstacles (ViNL), which enables a quadrupedal robot to navigate unseen apartments while stepping over small obstacles that lie in its path (e.g., shoes, toys, cables), similar to how humans and pets lift their feet over objects as they walk. ViNL consists of: (1) a visual navigation policy that outputs linear and angular velocity commands that guides the robot to a goal coordinate in unfamiliar indoor environments; and (2) a visual locomotion policy that controls the robot's joints to avoid stepping on obstacles while following provided velocity commands. Both the policies are entirely \"model-free\", i.e. sensors-to-actions neural networks trained end-to-end. The two are trained independently in two entirely different simulators and then seamlessly co-deployed by feeding the velocity commands from the navigator to the locomotor, entirely \"zero-shot\" (without any co-training). While prior works have developed learning methods for visual navigation or visual locomotion, to the best of our knowledge, this is the first fully learned approach that leverages vision to accomplish both (1) intelligent navigation in new environments, and (2) intelligent visual locomotion that aims to traverse cluttered environments without disrupting obstacles. On the task of navigation to distant goals in unknown environments, ViNL using just egocentric vision significantly outperforms prior work on robust locomotion using privileged terrain maps (+32.8% success and -4.42 collisions per meter). Additionally, we ablate our locomotion policy to show that each aspect of our approach helps reduce obstacle collisions. Videos and code at this http URL",
                        "Citation Paper Authors": "Authors:Simar Kareer, Naoki Yokoyama, Dhruv Batra, Sehoon Ha, Joanne Truong"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.14436v1": {
            "Paper Title": "REBEL: A Regularization-Based Solution for Reward Overoptimization in\n  Reinforcement Learning from Human Feedback",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": ", where the problem was reformulated as a con-\nstraint optimization framework by maximizing the disparity\nbetween the empirical feature expectations derived from\nunsuccessful examples and the feature expectation learned\nfrom the data. ",
                    "Citation Text": "J. Ho, J. K. Gupta, and S. Ermon, \u201cModel-free imitation learning with\npolicy optimization,\u201d 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.08478",
                        "Citation Paper Title": "Title:Model-Free Imitation Learning with Policy Optimization",
                        "Citation Paper Abstract": "Abstract:In imitation learning, an agent learns how to behave in an environment with an unknown cost function by mimicking expert demonstrations. Existing imitation learning algorithms typically involve solving a sequence of planning or reinforcement learning problems. Such algorithms are therefore not directly applicable to large, high-dimensional environments, and their performance can significantly degrade if the planning problems are not solved to optimality. Under the apprenticeship learning formalism, we develop alternative model-free algorithms for finding a parameterized stochastic policy that performs at least as well as an expert policy on an unknown cost function, based on sample trajectories from the expert. Our approach, based on policy gradients, scales to large continuous environments with guaranteed convergence to local minima.",
                        "Citation Paper Authors": "Authors:Jonathan Ho, Jayesh K. Gupta, Stefano Ermon"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.14360v1": {
            "Paper Title": "Designing a Skilled Soccer Team for RoboCup: Exploring\n  Skill-Set-Primitives through Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.14358v1": {
            "Paper Title": "A utility belt for an agricultural robot: reflection-in-action for\n  applied design research",
            "Sentences": []
        },
        "http://arxiv.org/abs/2303.12966v2": {
            "Paper Title": "Rate-Tunable Control Barrier Functions: Methods and Algorithms for\n  Online Adaptation",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "does offline analysis to find the domain over which CBF\ncondition can be satisfied in the presence of input bounds.arXiv:2303.12966v2  [math.OC]  21 Dec 2023 ",
                    "Citation Text": "A. Robey, H. Hu, L. Lindemann, H. Zhang, D. V . Dimarogonas, S. Tu,\nand N. Matni, \u201cLearning control barrier functions from expert demon-\nstrations,\u201d in 2020 59th IEEE Conference on Decision and Control\n(CDC) . IEEE, 2020, pp. 3717\u20133724.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.03315",
                        "Citation Paper Title": "Title:Learning Control Barrier Functions from Expert Demonstrations",
                        "Citation Paper Abstract": "Abstract:Inspired by the success of imitation and inverse reinforcement learning in replicating expert behavior through optimal control, we propose a learning based approach to safe controller synthesis based on control barrier functions (CBFs). We consider the setting of a known nonlinear control affine dynamical system and assume that we have access to safe trajectories generated by an expert - a practical example of such a setting would be a kinematic model of a self-driving vehicle with safe trajectories (e.g., trajectories that avoid collisions with obstacles in the environment) generated by a human driver. We then propose and analyze an optimization-based approach to learning a CBF that enjoys provable safety guarantees under suitable Lipschitz smoothness assumptions on the underlying dynamical system. A strength of our approach is that it is agnostic to the parameterization used to represent the CBF, assuming only that the Lipschitz constant of such functions can be efficiently bounded. Furthermore, if the CBF parameterization is convex, then under mild assumptions, so is our learning process. We end with extensive numerical evaluations of our results on both planar and realistic examples, using both random feature and deep neural network parameterizations of the CBF. To the best of our knowledge, these are the first results that learn provably safe control barrier functions from data.",
                        "Citation Paper Authors": "Authors:Alexander Robey, Haimin Hu, Lars Lindemann, Hanwen Zhang, Dimos V. Dimarogonas, Stephen Tu, Nikolai Matni"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "generalizes Exponential CBFs to generic\nnonlinear class- Kfunctions in the form of Higher-Order CBFs\n(HOCBF). ",
                    "Citation Text": "A. J. Taylor, P. Ong, T. G. Molnar, and A. D. Ames, \u201cSafe backstepping\nwith control barrier functions,\u201d arXiv preprint arXiv:2204.00653 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.00653",
                        "Citation Paper Title": "Title:Safe Backstepping with Control Barrier Functions",
                        "Citation Paper Abstract": "Abstract:Complex control systems are often described in a layered fashion, represented as higher-order systems where the inputs appear after a chain of integrators. While Control Barrier Functions (CBFs) have proven to be powerful tools for safety-critical controller design of nonlinear systems, their application to higher-order systems adds complexity to the controller synthesis process -- it necessitates dynamically extending the CBF to include higher order terms, which consequently modifies the safe set in complex ways. We propose an alternative approach for addressing safety of higher-order systems through Control Barrier Function Backstepping. Drawing inspiration from the method of Lyapunov backstepping, we provide a constructive framework for synthesizing safety-critical controllers and CBFs for higher-order systems from a top-level dynamics safety specification and controller design. Furthermore, we integrate the proposed method with Lyapunov backstepping, allowing the tasks of stability and safety to be expressed individually but achieved jointly. We demonstrate the efficacy of this approach in simulation.",
                        "Citation Paper Authors": "Authors:Andrew J. Taylor, Pio Ong, Tamas G. Molnar, Aaron D. Ames"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.14134v1": {
            "Paper Title": "Diffusion Reward: Learning Rewards via Conditional Video Diffusion",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": ") and its stan-\ndardized variant (VIPER-std.) that use VideoGPT ",
                    "Citation Text": "Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind\nSrinivas. Videogpt: Video generation using vq-vae and trans-\nformers. ArXiv , abs/2104.10157, 2021. 1, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.10157",
                        "Citation Paper Title": "Title:VideoGPT: Video Generation using VQ-VAE and Transformers",
                        "Citation Paper Abstract": "Abstract:We present VideoGPT: a conceptually simple architecture for scaling likelihood based generative modeling to natural videos. VideoGPT uses VQ-VAE that learns downsampled discrete latent representations of a raw video by employing 3D convolutions and axial self-attention. A simple GPT-like architecture is then used to autoregressively model the discrete latents using spatio-temporal position encodings. Despite the simplicity in formulation and ease of training, our architecture is able to generate samples competitive with state-of-the-art GAN models for video generation on the BAIR Robot dataset, and generate high fidelity natural videos from UCF-101 and Tumbler GIF Dataset (TGIF). We hope our proposed architecture serves as a reproducible reference for a minimalistic implementation of transformer based video generation models. Samples and code are available at this https URL",
                        "Citation Paper Authors": "Authors:Wilson Yan, Yunzhi Zhang, Pieter Abbeel, Aravind Srinivas"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": ", and 0/1 environmental sparse reward.\nFor pertaining reward models, we collect 20 expert videos\nfor each Metaword task via the scripted policy provided by\nthe official repository, and 50 for Adroit via the policies\ntrained with performant RL method ",
                    "Citation Text": "Che Wang, Xufang Luo, Keith W. Ross, and Dongsheng Li.\nVrl3: A data-driven framework for visual deep reinforce-\nment learning. In Advances in Neural Information Process-\ning Systems (NeurIPS) , 2022. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.10324",
                        "Citation Paper Title": "Title:VRL3: A Data-Driven Framework for Visual Deep Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:We propose VRL3, a powerful data-driven framework with a simple design for solving challenging visual deep reinforcement learning (DRL) tasks. We analyze a number of major obstacles in taking a data-driven approach, and present a suite of design principles, novel findings, and critical insights about data-driven visual DRL. Our framework has three stages: in stage 1, we leverage non-RL datasets (e.g. ImageNet) to learn task-agnostic visual representations; in stage 2, we use offline RL data (e.g. a limited number of expert demonstrations) to convert the task-agnostic representations into more powerful task-specific representations; in stage 3, we fine-tune the agent with online RL. On a set of challenging hand manipulation tasks with sparse reward and realistic visual inputs, compared to the previous SOTA, VRL3 achieves an average of 780% better sample efficiency. And on the hardest task, VRL3 is 1220% more sample efficient (2440% when using a wider encoder) and solves the task with only 10% of the computation. These significant results clearly demonstrate the great potential of data-driven deep reinforcement learning.",
                        "Citation Paper Authors": "Authors:Che Wang, Xufang Luo, Keith Ross, Dongsheng Li"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "to train the encoder, associated with 8\u00d78\nsize of codebook across all domains, with an additional per-\nceptual loss ",
                    "Citation Text": "Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR) , 2018. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.03924",
                        "Citation Paper Title": "Title:The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
                        "Citation Paper Abstract": "Abstract:While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called \"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",
                        "Citation Paper Authors": "Authors:Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "are probabilistic models that aim to\nmodel the data distribution by gradually denoising a normal\ndistribution through a reverse diffusion process ",
                    "Citation Text": "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. In Advances in Neural Informa-\ntion Processing Systems (NeurIPS) , 2020. 3, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.11239",
                        "Citation Paper Title": "Title:Denoising Diffusion Probabilistic Models",
                        "Citation Paper Abstract": "Abstract:We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at this https URL",
                        "Citation Paper Authors": "Authors:Jonathan Ho, Ajay Jain, Pieter Abbeel"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2304.05483v4": {
            "Paper Title": "Contingency Games for Multi-Agent Interaction",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": "con-\nstruct a flow-based generative model and a likelihood-based\ngenerative model, respectively. Meanwhile, Rhinehart et al. ",
                    "Citation Text": "N. Rhinehart, J. He, C. Packer, M. A. Wright, R. McAllister,\nJ. E. Gonzalez, and S. Levine, \u201cContingencies from observa-\ntions: Tractable contingency planning with learned behavior\nmodels,\u201d Proc. of the IEEE Intl. Conf. on Robotics & Automa-\ntion (ICRA) , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.10558",
                        "Citation Paper Title": "Title:Contingencies from Observations: Tractable Contingency Planning with Learned Behavior Models",
                        "Citation Paper Abstract": "Abstract:Humans have a remarkable ability to make decisions by accurately reasoning about future events, including the future behaviors and states of mind of other agents. Consider driving a car through a busy intersection: it is necessary to reason about the physics of the vehicle, the intentions of other drivers, and their beliefs about your own intentions. If you signal a turn, another driver might yield to you, or if you enter the passing lane, another driver might decelerate to give you room to merge in front. Competent drivers must plan how they can safely react to a variety of potential future behaviors of other agents before they make their next move. This requires contingency planning: explicitly planning a set of conditional actions that depend on the stochastic outcome of future events. In this work, we develop a general-purpose contingency planner that is learned end-to-end using high-dimensional scene observations and low-dimensional behavioral observations. We use a conditional autoregressive flow model to create a compact contingency planning space, and show how this model can tractably learn contingencies from behavioral observations. We developed a closed-loop control benchmark of realistic multi-agent scenarios in a driving simulator (CARLA), on which we compare our method to various noncontingent methods that reason about multi-agent future behavior, including several state-of-the-art deep learning-based planning approaches. We illustrate that these noncontingent planning methods fundamentally fail on this benchmark, and find that our deep contingency planning method achieves significantly superior performance. Code to run our benchmark and reproduce our results is available at this https URL",
                        "Citation Paper Authors": "Authors:Nicholas Rhinehart, Jeff He, Charles Packer, Matthew A. Wright, Rowan McAllister, Joseph E. Gonzalez, Sergey Levine"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.14115v1": {
            "Paper Title": "LingoQA: Video Question Answering for Autonomous Driving",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": "on autonomous driving scenarios show that LLMs/VLMs\ndemonstrate superior performance in scene understanding\nand causal reasoning compared to existing autonomous sys-\ntems. Works such as ADAPT ",
                    "Citation Text": "Bu Jin, Xinyu Liu, Yupeng Zheng, Pengfei Li, Hao Zhao,\nTong Zhang, Yuhang Zheng, Guyue Zhou, and Jingjing Liu.\nAdapt: Action-aware driving caption transformer, 2023. 2,\n3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.00673",
                        "Citation Paper Title": "Title:ADAPT: Action-aware Driving Caption Transformer",
                        "Citation Paper Abstract": "Abstract:End-to-end autonomous driving has great potential in the transportation industry. However, the lack of transparency and interpretability of the automatic decision-making process hinders its industrial adoption in practice. There have been some early attempts to use attention maps or cost volume for better model explainability which is difficult for ordinary passengers to understand. To bridge the gap, we propose an end-to-end transformer-based architecture, ADAPT (Action-aware Driving cAPtion Transformer), which provides user-friendly natural language narrations and reasoning for each decision making step of autonomous vehicular control and action. ADAPT jointly trains both the driving caption task and the vehicular control prediction task, through a shared video representation. Experiments on BDD-X (Berkeley DeepDrive eXplanation) dataset demonstrate state-of-the-art performance of the ADAPT framework on both automatic metrics and human evaluation. To illustrate the feasibility of the proposed framework in real-world applications, we build a novel deployable system that takes raw car videos as input and outputs the action narrations and reasoning in real time. The code, models and data are available at this https URL.",
                        "Citation Paper Authors": "Authors:Bu Jin, Xinyu Liu, Yupeng Zheng, Pengfei Li, Hao Zhao, Tong Zhang, Yuhang Zheng, Guyue Zhou, Jingjing Liu"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": ". The\nearly explorations of GPT3.5 [39, 46] and GPT4-V ",
                    "Citation Text": "Licheng Wen, Xuemeng Yang, Daocheng Fu, Xiaofeng\nWang, Pinlong Cai, Xin Li, Tao Ma, Yingxuan Li, Linran\nXu, Dengke Shang, Zheng Zhu, Shaoyan Sun, Yeqi Bai,\nXinyu Cai, Min Dou, Shuanglu Hu, and Botian Shi. On\nthe road with gpt-4v(ision): Early explorations of visual-\nlanguage model on autonomous driving, 2023. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2311.05332",
                        "Citation Paper Title": "Title:On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving",
                        "Citation Paper Abstract": "Abstract:The pursuit of autonomous driving technology hinges on the sophisticated integration of perception, decision-making, and control systems. Traditional approaches, both data-driven and rule-based, have been hindered by their inability to grasp the nuance of complex driving environments and the intentions of other road users. This has been a significant bottleneck, particularly in the development of common sense reasoning and nuanced scene understanding necessary for safe and reliable autonomous driving. The advent of Visual Language Models (VLM) represents a novel frontier in realizing fully autonomous vehicle driving. This report provides an exhaustive evaluation of the latest state-of-the-art VLM, GPT-4V(ision), and its application in autonomous driving scenarios. We explore the model's abilities to understand and reason about driving scenes, make decisions, and ultimately act in the capacity of a driver. Our comprehensive tests span from basic scene recognition to complex causal reasoning and real-time decision-making under varying conditions. Our findings reveal that GPT-4V demonstrates superior performance in scene understanding and causal reasoning compared to existing autonomous systems. It showcases the potential to handle out-of-distribution scenarios, recognize intentions, and make informed decisions in real driving contexts. However, challenges remain, particularly in direction discernment, traffic light recognition, vision grounding, and spatial reasoning tasks. These limitations underscore the need for further research and development. Project is now available on GitHub for interested parties to access and utilize: \\url{this https URL}",
                        "Citation Paper Authors": "Authors:Licheng Wen, Xuemeng Yang, Daocheng Fu, Xiaofeng Wang, Pinlong Cai, Xin Li, Tao Ma, Yingxuan Li, Linran Xu, Dengke Shang, Zheng Zhu, Shaoyan Sun, Yeqi Bai, Xinyu Cai, Min Dou, Shuanglu Hu, Botian Shi, Yu Qiao"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.13910v1": {
            "Paper Title": "Multi-Agent Probabilistic Ensembles with Trajectory Sampling for\n  Connected Autonomous Vehicles",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": ". Specifically,\nwe run our experiments using the CA V simulation platform\nSMARTS ",
                    "Citation Text": "M. Zhou, J. Luo, J. Villella , et al. , \u201cSMARTS: An open-source scalable\nmulti-agent rl training school for autonomous driving,\u201d in Proc. Mach.\nLearn. Res. (PMLR) , Cambridge, MA, USA, Nov 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.09776",
                        "Citation Paper Title": "Title:SMARTS: Scalable Multi-Agent Reinforcement Learning Training School for Autonomous Driving",
                        "Citation Paper Abstract": "Abstract:Multi-agent interaction is a fundamental aspect of autonomous driving in the real world. Despite more than a decade of research and development, the problem of how to competently interact with diverse road users in diverse scenarios remains largely unsolved. Learning methods have much to offer towards solving this problem. But they require a realistic multi-agent simulator that generates diverse and competent driving interactions. To meet this need, we develop a dedicated simulation platform called SMARTS (Scalable Multi-Agent RL Training School). SMARTS supports the training, accumulation, and use of diverse behavior models of road users. These are in turn used to create increasingly more realistic and diverse interactions that enable deeper and broader research on multi-agent interaction. In this paper, we describe the design goals of SMARTS, explain its basic architecture and its key features, and illustrate its use through concrete multi-agent experiments on interactive scenarios. We open-source the SMARTS platform and the associated benchmark tasks and evaluation metrics to encourage and empower research on multi-agent learning for autonomous driving. Our code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Ming Zhou, Jun Luo, Julian Villella, Yaodong Yang, David Rusu, Jiayu Miao, Weinan Zhang, Montgomery Alban, Iman Fadakar, Zheng Chen, Aurora Chongxi Huang, Ying Wen, Kimia Hassanzadeh, Daniel Graves, Dong Chen, Zhengbang Zhu, Nhat Nguyen, Mohamed Elsayed, Kun Shao, Sanjeevan Ahilan, Baokuan Zhang, Jiannan Wu, Zhengang Fu, Kasra Rezaee, Peyman Yadmellat, Mohsen Rohani, Nicolas Perez Nieves, Yihan Ni, Seyedershad Banijamali, Alexander Cowen Rivers, Zheng Tian, Daniel Palenicek, Haitham bou Ammar, Hongbo Zhang, Wulong Liu, Jianye Hao, Jun Wang"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "proposes a UCCRL algorithm that derivessub-linear regret bounds \u02dcO\u0010\nT2+\u03b1\n2+2\u03b1\u0011\nwith a parameter \u03b1for\nun-discounted RL in continuous state space. By using more\nefficient posterior sampling for episodic RL, ",
                    "Citation Text": "I. Osband, D. Russo, and B. Van Roy, \u201c(More) Efficient reinforcement\nlearning via posterior sampling,\u201d in Proc. Adv. Neural Inf. Proces. Syst.\n(NIPS) , Lake Tahoe, Nevada, USA, Dec. 2013.14",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1306.0940",
                        "Citation Paper Title": "Title:(More) Efficient Reinforcement Learning via Posterior Sampling",
                        "Citation Paper Abstract": "Abstract:Most provably-efficient learning algorithms introduce optimism about poorly-understood states and actions to encourage exploration. We study an alternative approach for efficient exploration, posterior sampling for reinforcement learning (PSRL). This algorithm proceeds in repeated episodes of known duration. At the start of each episode, PSRL updates a prior distribution over Markov decision processes and takes one sample from this posterior. PSRL then follows the policy that is optimal for this sample during the episode. The algorithm is conceptually simple, computationally efficient and allows an agent to encode prior knowledge in a natural way. We establish an $\\tilde{O}(\\tau S \\sqrt{AT})$ bound on the expected regret, where $T$ is time, $\\tau$ is the episode length and $S$ and $A$ are the cardinalities of the state and action spaces. This bound is one of the first for an algorithm not based on optimism, and close to the state of the art for any reinforcement learning algorithm. We show through simulation that PSRL significantly outperforms existing algorithms with similar regret bounds.",
                        "Citation Paper Authors": "Authors:Ian Osband, Daniel Russo, Benjamin Van Roy"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "discusses\nthe performance guarantees of a learned policy with poly-\nnomial scaling in the size of the state and action spaces. ",
                    "Citation Text": "P. Auer and R. Ortner, \u201cLogarithmic online regret bounds for undis-\ncounted reinforcement learning,\u201d in Proc. Adv. Neural Inf. Proces. Syst.\n(NIPS) , Vancouver, BC, Canada, Dec. 2006.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1302.2550",
                        "Citation Paper Title": "Title:Online Regret Bounds for Undiscounted Continuous Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:We derive sublinear regret bounds for undiscounted reinforcement learning in continuous state space. The proposed algorithm combines state aggregation with the use of upper confidence bounds for implementing optimism in the face of uncertainty. Beside the existence of an optimal policy which satisfies the Poisson equation, the only assumptions made are Holder continuity of rewards and transition probabilities.",
                        "Citation Paper Authors": "Authors:Ronald Ortner, Daniil Ryabko"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "takes advantage of\nModel Predictive Control (MPC) to optimize the RL agent\u2019s\nbehavior policy by predicting and planning within the modeled\nvirtual environment at each training step. Until recently, ",
                    "Citation Text": "J. Wu, Z. Huang, and C. Lv, \u201cUncertainty-aware model-based reinforce-\nment learning: Methodology and application in autonomous driving,\u201d\nIEEE Trans. Intell. Vehicl , vol. 8, no. 1, pp. 194\u2013203, Jan. 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.12194",
                        "Citation Paper Title": "Title:Uncertainty-Aware Model-Based Reinforcement Learning with Application to Autonomous Driving",
                        "Citation Paper Abstract": "Abstract:To further improve the learning efficiency and performance of reinforcement learning (RL), in this paper we propose a novel uncertainty-aware model-based RL (UA-MBRL) framework, and then implement and validate it in autonomous driving under various task scenarios. First, an action-conditioned ensemble model with the ability of uncertainty assessment is established as the virtual environment model. Then, a novel uncertainty-aware model-based RL framework is developed based on the adaptive truncation approach, providing virtual interactions between the agent and environment model, and improving RL's training efficiency and performance. The developed algorithms are then implemented in end-to-end autonomous vehicle control tasks, validated and compared with state-of-the-art methods under various driving scenarios. The validation results suggest that the proposed UA-MBRL method surpasses the existing model-based and model-free RL approaches, in terms of learning efficiency and achieved performance. The results also demonstrate the good ability of the proposed method with respect to the adaptiveness and robustness, under various autonomous driving scenarios.",
                        "Citation Paper Authors": "Authors:Jingda Wu, Zhiyu Huang, Chen Lv"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": ", where additional data to\nimprove the efficiency of the RL is generated from interactions\nbetween the RL and the virtual environment. ",
                    "Citation Text": "Y . Guan, Y . Ren, S. E. Li , et al. , \u201cCentralized cooperation for connected\nand automated vehicles at intersections by proximal policy optimiza-\ntion,\u201d IEEE Trans. Veh. Technol. , vol. 69, no. 11, pp. 12 597\u201312 608,\nSep. 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.08410",
                        "Citation Paper Title": "Title:Centralized Cooperation for Connected and Automated Vehicles at Intersections by Proximal Policy Optimization",
                        "Citation Paper Abstract": "Abstract:Connected vehicles will change the modes of future transportation management and organization, especially at an intersection without traffic light. Centralized coordination methods globally coordinate vehicles approaching the intersection from all sections by considering their states altogether. However, they need substantial computation resources since they own a centralized controller to optimize the trajectories for all approaching vehicles in real-time. In this paper, we propose a centralized coordination scheme of automated vehicles at an intersection without traffic signals using reinforcement learning (RL) to address low computation efficiency suffered by current centralized coordination methods. We first propose an RL training algorithm, model accelerated proximal policy optimization (MA-PPO), which incorporates a prior model into proximal policy optimization (PPO) algorithm to accelerate the learning process in terms of sample efficiency. Then we present the design of state, action and reward to formulate centralized coordination as an RL problem. Finally, we train a coordinate policy in a simulation setting and compare computing time and traffic efficiency with a coordination scheme based on model predictive control (MPC) method. Results show that our method spends only 1/400 of the computing time of MPC and increase the efficiency of the intersection by 4.5 times.",
                        "Citation Paper Authors": "Authors:Yang Guan, Yangang Ren, Shengbo Eben Li, Qi Sun, Laiquan Luo, Keqiang Li"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.13906v1": {
            "Paper Title": "EfficientPPS: Part-aware Panoptic Segmentation of Transparent Objects\n  for Robotic Manipulation",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ", particularly in conjunc-\ntion with part -aware perception methods ",
                    "Citation Text": "S. Thermos, P. Daras, and G. Potamianos, \u2018A Deep \nLearning Approach  to Object Affordance Segmenta-\ntion\u2019, in ICASSP 2020 - 2020 IEEE International \nConference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP) , May 2020, pp. 2358 \u20132362. doi: \n10.1109/ICASSP40776.2020.9054167.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.08644",
                        "Citation Paper Title": "Title:A Deep Learning Approach to Object Affordance Segmentation",
                        "Citation Paper Abstract": "Abstract:Learning to understand and infer object functionalities is an important step towards robust visual intelligence. Significant research efforts have recently focused on segmenting the object parts that enable specific types of human-object interaction, the so-called \"object affordances\". However, most works treat it as a static semantic segmentation problem, focusing solely on object appearance and relying on strong supervision and object detection. In this paper, we propose a novel approach that exploits the spatio-temporal nature of human-object interaction for affordance segmentation. In particular, we design an autoencoder that is trained using ground-truth labels of only the last frame of the sequence, and is able to infer pixel-wise affordance labels in both videos and static images. Our model surpasses the need for object labels and bounding boxes by using a soft-attention mechanism that enables the implicit localization of the interaction hotspot. For evaluation purposes, we introduce the SOR3D-AFF corpus, which consists of human-object interaction sequences and supports 9 types of affordances in terms of pixel-wise annotation, covering typical manipulations of tool-like objects. We show that our model achieves competitive results compared to strongly supervised methods on SOR3D-AFF, while being able to predict affordances for similar unseen objects in two affordance image-only datasets.",
                        "Citation Paper Authors": "Authors:Spyridon Thermos, Petros Daras, Gerasimos Potamianos"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": ". \nIn robotics, part affordances commonly guide the genera-\ntion of task -dependent grasps ",
                    "Citation Text": "W. Liu, A. Daruna, and S.  Chernova, \u2018CAGE: Con-\ntext-Aware Grasping Engine\u2019, in 2020 IEEE Interna-\ntional Conference on Robotics and Automation \n(ICRA) , May 2020, pp. 2550 \u20132556. doi: \n10.1109/ICRA40945.2020.9197289.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.11142",
                        "Citation Paper Title": "Title:CAGE: Context-Aware Grasping Engine",
                        "Citation Paper Abstract": "Abstract:Semantic grasping is the problem of selecting stable grasps that are functionally suitable for specific object manipulation tasks. In order for robots to effectively perform object manipulation, a broad sense of contexts, including object and task constraints, needs to be accounted for. We introduce the Context-Aware Grasping Engine, which combines a novel semantic representation of grasp contexts with a neural network structure based on the Wide & Deep model, capable of capturing complex reasoning patterns. We quantitatively validate our approach against three prior methods on a novel dataset consisting of 14,000 semantic grasps for 44 objects, 7 tasks, and 6 different object states. Our approach outperformed all baselines by statistically significant margins, producing new insights into the importance of balancing memorization and generalization of contexts for semantic grasping. We further demonstrate the effectiveness of our approach on robot experiments in which the presented model successfully achieved 31 of 32 suitable grasps. The code and data are available at: this https URL",
                        "Citation Paper Authors": "Authors:Weiyu Liu, Angel Daruna, Sonia Chernova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.13891v1": {
            "Paper Title": "A Summarized History-based Dialogue System for Amnesia-Free Prompt\n  Updates",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13816v1": {
            "Paper Title": "Team Flow at DRC2023: Building Common Ground and Text-based Turn-taking\n  in a Travel Agent Spoken Dialogue System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13802v1": {
            "Paper Title": "A Dense Subframe-based SLAM Framework with Side-scan Sonar",
            "Sentences": []
        },
        "http://arxiv.org/abs/2308.08746v2": {
            "Paper Title": "SurgicalSAM: Efficient Class Promptable Surgical Instrument Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.06039v2": {
            "Paper Title": "Singularly Perturbed Layered Control of Deformable Bodies",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13655v1": {
            "Paper Title": "Compositional Zero-Shot Learning for Attribute-Based Object Reference in\n  Human-Robot Interaction",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": ". Some recent\nworks use graph structure to leverage information transfer between seen to unseen pairs using graph\nconvolutional networks ",
                    "Citation Text": "M. Mancini, M. F. Naeem, Y . Xian, and Z. Akata. Learning graph embeddings for open\nworld compositional zero-shot learning. IEEE Transactions on Pattern Analysis and Machine\nIntelligence , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.01017",
                        "Citation Paper Title": "Title:Learning Graph Embeddings for Open World Compositional Zero-Shot Learning",
                        "Citation Paper Abstract": "Abstract:Compositional Zero-Shot learning (CZSL) aims to recognize unseen compositions of state and object visual primitives seen during training. A problem with standard CZSL is the assumption of knowing which unseen compositions will be available at test time. In this work, we overcome this assumption operating on the open world setting, where no limit is imposed on the compositional space at test time, and the search space contains a large number of unseen compositions. To address this problem, we propose a new approach, Compositional Cosine Graph Embeddings (Co-CGE), based on two principles. First, Co-CGE models the dependency between states, objects and their compositions through a graph convolutional neural network. The graph propagates information from seen to unseen concepts, improving their representations. Second, since not all unseen compositions are equally feasible, and less feasible ones may damage the learned representations, Co-CGE estimates a feasibility score for each unseen composition, using the scores as margins in a cosine similarity-based loss and as weights in the adjacency matrix of the graphs. Experiments show that our approach achieves state-of-the-art performances in standard CZSL while outperforming previous methods in the open world scenario.",
                        "Citation Paper Authors": "Authors:Massimiliano Mancini, Muhammad Ferjad Naeem, Yongqin Xian, Zeynep Akata"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "suggests encoding objects as vectors and states as linear operators that transform\nthese vectors. Similarly, a recent work ",
                    "Citation Text": "Y .-L. Li, Y . Xu, X. Mao, and C. Lu. Symmetry and group in attribute-object compositions.\n2020 ieee. In The IEEE Conference on Computer Vision and Pattern Recognition , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.00587",
                        "Citation Paper Title": "Title:Symmetry and Group in Attribute-Object Compositions",
                        "Citation Paper Abstract": "Abstract:Attributes and objects can compose diverse compositions. To model the compositional nature of these general concepts, it is a good choice to learn them through transformations, such as coupling and decoupling. However, complex transformations need to satisfy specific principles to guarantee the rationality. In this paper, we first propose a previously ignored principle of attribute-object transformation: Symmetry. For example, coupling peeled-apple with attribute peeled should result in peeled-apple, and decoupling peeled from apple should still output apple. Incorporating the symmetry principle, a transformation framework inspired by group theory is built, i.e. SymNet. SymNet consists of two modules, Coupling Network and Decoupling Network. With the group axioms and symmetry property as objectives, we adopt Deep Neural Networks to implement SymNet and train it in an end-to-end paradigm. Moreover, we propose a Relative Moving Distance (RMD) based recognition method to utilize the attribute change instead of the attribute pattern itself to classify attributes. Our symmetry learning can be utilized for the Compositional Zero-Shot Learning task and outperforms the state-of-the-art on widely-used benchmarks. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Yong-Lu Li, Yue Xu, Xiaohan Mao, Cewu Lu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.13623v1": {
            "Paper Title": "Learning Rhythmic Trajectories with Geometric Constraints for\n  Laser-Based Skincare Procedures",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11868v2": {
            "Paper Title": "Dynamic Loco-manipulation on HECTOR: Humanoid for Enhanced ConTrol and\n  Open-source Research",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.07488v2": {
            "Paper Title": "LMDrive: Closed-Loop End-to-End Driving with Large Language Models",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": "lever-\naged a LLM to reason the complex scenarios and output\nhigh-level driving decisions. Then the method tunes a\nparameter matrix to convert the decision into the low-level\ncontrol signals. LLM-Driver ",
                    "Citation Text": "Long Chen, Oleg Sinavski, Jan H \u00a8unermann, Alice Karnsund,\nAndrew James Willmott, Danny Birch, Daniel Maund, and\nJamie Shotton. Driving with llms: Fusing object-level vector\nmodality for explainable autonomous driving. arXiv preprint\narXiv:2310.01957 , 2023. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2310.01957",
                        "Citation Paper Title": "Title:Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving",
                        "Citation Paper Abstract": "Abstract:Large Language Models (LLMs) have shown promise in the autonomous driving sector, particularly in generalization and interpretability. We introduce a unique object-level multimodal LLM architecture that merges vectorized numeric modalities with a pre-trained LLM to improve context understanding in driving situations. We also present a new dataset of 160k QA pairs derived from 10k driving scenarios, paired with high quality control commands collected with RL agent and question answer pairs generated by teacher LLM (GPT-3.5). A distinct pretraining strategy is devised to align numeric vector modalities with static LLM representations using vector captioning language data. We also introduce an evaluation metric for Driving QA and demonstrate our LLM-driver's proficiency in interpreting driving scenarios, answering questions, and decision-making. Our findings highlight the potential of LLM-based driving action generation in comparison to traditional behavioral cloning. We make our benchmark, datasets, and model available for further exploration.",
                        "Citation Paper Authors": "Authors:Long Chen, Oleg Sinavski, Jan H\u00fcnermann, Alice Karnsund, Andrew James Willmott, Danny Birch, Daniel Maund, Jamie Shotton"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "reformulated motion planning as the task\nof natural language modeling by converting heterogeneous\nscene input to language tokens. LanguageMPC ",
                    "Citation Text": "Hao Sha, Yao Mu, Yuxuan Jiang, Li Chen, Chenfeng Xu,\nPing Luo, Shengbo Eben Li, Masayoshi Tomizuka, Wei\nZhan, and Mingyu Ding. Languagempc: Large language\nmodels as decision makers for autonomous driving. arXiv\npreprint arXiv:2310.03026 , 2023. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2310.03026",
                        "Citation Paper Title": "Title:LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving",
                        "Citation Paper Abstract": "Abstract:Existing learning-based autonomous driving (AD) systems face challenges in comprehending high-level information, generalizing to rare events, and providing interpretability. To address these problems, this work employs Large Language Models (LLMs) as a decision-making component for complex AD scenarios that require human commonsense understanding. We devise cognitive pathways to enable comprehensive reasoning with LLMs, and develop algorithms for translating LLM decisions into actionable driving commands. Through this approach, LLM decisions are seamlessly integrated with low-level controllers by guided parameter matrix adaptation. Extensive experiments demonstrate that our proposed method not only consistently surpasses baseline approaches in single-vehicle tasks, but also helps handle complex driving behaviors even multi-vehicle coordination, thanks to the commonsense reasoning capabilities of LLMs. This paper presents an initial step toward leveraging LLMs as effective decision-makers for intricate AD scenarios in terms of safety, efficiency, generalizability, and interoperability. We aspire for it to serve as inspiration for future research in this field. Project page: this https URL",
                        "Citation Paper Authors": "Authors:Hao Sha, Yao Mu, Yuxuan Jiang, Li Chen, Chenfeng Xu, Ping Luo, Shengbo Eben Li, Masayoshi Tomizuka, Wei Zhan, Mingyu Ding"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "employed a reinforcement learning agent with\nprivileged access to environmental information and distills\na model as the final agent. ASAPRL ",
                    "Citation Text": "Letian Wang, Jie Liu, Hao Shao, Wenshuo Wang, Ruobing\nChen, Yu Liu, and Steven L Waslander. Efficient reinforce-\nment learning for autonomous driving with parameterized\nskills and priors. arXiv preprint arXiv:2305.04412 , 2023.\n3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.04412",
                        "Citation Paper Title": "Title:Efficient Reinforcement Learning for Autonomous Driving with Parameterized Skills and Priors",
                        "Citation Paper Abstract": "Abstract:When autonomous vehicles are deployed on public roads, they will encounter countless and diverse driving situations. Many manually designed driving policies are difficult to scale to the real world. Fortunately, reinforcement learning has shown great success in many tasks by automatic trial and error. However, when it comes to autonomous driving in interactive dense traffic, RL agents either fail to learn reasonable performance or necessitate a large amount of data. Our insight is that when humans learn to drive, they will 1) make decisions over the high-level skill space instead of the low-level control space and 2) leverage expert prior knowledge rather than learning from scratch. Inspired by this, we propose ASAP-RL, an efficient reinforcement learning algorithm for autonomous driving that simultaneously leverages motion skills and expert priors. We first parameterized motion skills, which are diverse enough to cover various complex driving scenarios and situations. A skill parameter inverse recovery method is proposed to convert expert demonstrations from control space to skill space. A simple but effective double initialization technique is proposed to leverage expert priors while bypassing the issue of expert suboptimality and early performance degradation. We validate our proposed method on interactive dense-traffic driving tasks given simple and sparse rewards. Experimental results show that our method can lead to higher learning efficiency and better driving performance relative to previous methods that exploit skills and priors differently. Code is open-sourced to facilitate further research.",
                        "Citation Paper Authors": "Authors:Letian Wang, Jie Liu, Hao Shao, Wenshuo Wang, Ruobing Chen, Yu Liu, Steven L. Waslander"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": "trained in a supervised way to get a latent\nrepresentation of the environment observation and conducts\nreinforcement learning using the representation as the input.\nRoach ",
                    "Citation Text": "Zhejun Zhang, Alexander Liniger, Dengxin Dai, Fisher Yu,\nand Luc Van Gool. End-to-end urban driving by imitat-\ning a reinforcement learning coach. In Proceedings of\nthe IEEE/CVF international conference on computer vision ,\npages 15222\u201315232, 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.08265",
                        "Citation Paper Title": "Title:End-to-End Urban Driving by Imitating a Reinforcement Learning Coach",
                        "Citation Paper Abstract": "Abstract:End-to-end approaches to autonomous driving commonly rely on expert demonstrations. Although humans are good drivers, they are not good coaches for end-to-end algorithms that demand dense on-policy supervision. On the contrary, automated experts that leverage privileged information can efficiently generate large scale on-policy and off-policy demonstrations. However, existing automated experts for urban driving make heavy use of hand-crafted rules and perform suboptimally even on driving simulators, where ground-truth information is available. To address these issues, we train a reinforcement learning expert that maps bird's-eye view images to continuous low-level actions. While setting a new performance upper-bound on CARLA, our expert is also a better coach that provides informative supervision signals for imitation learning agents to learn from. Supervised by our reinforcement learning coach, a baseline end-to-end agent with monocular camera-input achieves expert-level performance. Our end-to-end agent achieves a 78% success rate while generalizing to a new town and new weather on the NoCrash-dense benchmark and state-of-the-art performance on the challenging public routes of the CARLA LeaderBoard.",
                        "Citation Paper Authors": "Authors:Zhejun Zhang, Alexander Liniger, Dengxin Dai, Fisher Yu, Luc Van Gool"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "proposed a transformer-\nbased framework to fully fuse and process information from\n2multi-modal multi-view sensors for comprehensive scene\nunderstanding. TCP ",
                    "Citation Text": "Penghao Wu, Xiaosong Jia, Li Chen, Junchi Yan, Hongyang\nLi, and Yu Qiao. Trajectory-guided control prediction for\nend-to-end autonomous driving: A simple yet strong base-\nline. Advances in Neural Information Processing Systems ,\n35:6119\u20136132, 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.08129",
                        "Citation Paper Title": "Title:Trajectory-guided Control Prediction for End-to-end Autonomous Driving: A Simple yet Strong Baseline",
                        "Citation Paper Abstract": "Abstract:Current end-to-end autonomous driving methods either run a controller based on a planned trajectory or perform control prediction directly, which have spanned two separately studied lines of research. Seeing their potential mutual benefits to each other, this paper takes the initiative to explore the combination of these two well-developed worlds. Specifically, our integrated approach has two branches for trajectory planning and direct control, respectively. The trajectory branch predicts the future trajectory, while the control branch involves a novel multi-step prediction scheme such that the relationship between current actions and future states can be reasoned. The two branches are connected so that the control branch receives corresponding guidance from the trajectory branch at each time step. The outputs from two branches are then fused to achieve complementary advantages. Our results are evaluated in the closed-loop urban driving setting with challenging scenarios using the CARLA simulator. Even with a monocular camera input, the proposed approach ranks first on the official CARLA Leaderboard, outperforming other complex candidates with multiple sensors or fusion mechanisms by a large margin. The source code is publicly available at this https URL",
                        "Citation Paper Authors": "Authors:Penghao Wu, Xiaosong Jia, Li Chen, Junchi Yan, Hongyang Li, Yu Qiao"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "exploited temporal and global information of the driving\nscene to improve perception performance and benefit oc-\nclusion detection. InterFuser ",
                    "Citation Text": "Hao Shao, Letian Wang, Ruobing Chen, Hongsheng Li, and\nYu Liu. Safety-enhanced autonomous driving using inter-\npretable sensor fusion transformer. In Conference on Robot\nLearning , pages 726\u2013737. PMLR, 2023. 2, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2207.14024",
                        "Citation Paper Title": "Title:Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer",
                        "Citation Paper Abstract": "Abstract:Large-scale deployment of autonomous vehicles has been continually delayed due to safety concerns. On the one hand, comprehensive scene understanding is indispensable, a lack of which would result in vulnerability to rare but complex traffic situations, such as the sudden emergence of unknown objects. However, reasoning from a global context requires access to sensors of multiple types and adequate fusion of multi-modal sensor signals, which is difficult to achieve. On the other hand, the lack of interpretability in learning models also hampers the safety with unverifiable failure causes. In this paper, we propose a safety-enhanced autonomous driving framework, named Interpretable Sensor Fusion Transformer(InterFuser), to fully process and fuse information from multi-modal multi-view sensors for achieving comprehensive scene understanding and adversarial event detection. Besides, intermediate interpretable features are generated from our framework, which provide more semantics and are exploited to better constrain actions to be within the safe sets. We conducted extensive experiments on CARLA benchmarks, where our model outperforms prior methods, ranking the first on the public CARLA Leaderboard. Our code will be made available at this https URL",
                        "Citation Paper Authors": "Authors:Hao Shao, Letian Wang, RuoBing Chen, Hongsheng Li, Yu Liu"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "designed a Look Mod-\nule to retrieve information from critical regions and utilize\nthe features to refine the coarse prediction. ReasonNet ",
                    "Citation Text": "Hao Shao, Letian Wang, Ruobing Chen, Steven L Waslan-\nder, Hongsheng Li, and Yu Liu. Reasonnet: End-to-end driv-\ning with temporal and global reasoning. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 13723\u201313733, 2023. 1, 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.10507",
                        "Citation Paper Title": "Title:ReasonNet: End-to-End Driving with Temporal and Global Reasoning",
                        "Citation Paper Abstract": "Abstract:The large-scale deployment of autonomous vehicles is yet to come, and one of the major remaining challenges lies in urban dense traffic scenarios. In such cases, it remains challenging to predict the future evolution of the scene and future behaviors of objects, and to deal with rare adverse events such as the sudden appearance of occluded objects. In this paper, we present ReasonNet, a novel end-to-end driving framework that extensively exploits both temporal and global information of the driving scene. By reasoning on the temporal behavior of objects, our method can effectively process the interactions and relationships among features in different frames. Reasoning about the global information of the scene can also improve overall perception performance and benefit the detection of adverse events, especially the anticipation of potential danger from occluded objects. For comprehensive evaluation on occlusion events, we also release publicly a driving simulation benchmark DriveOcclusionSim consisting of diverse occlusion events. We conduct extensive experiments on multiple CARLA benchmarks, where our model outperforms all prior methods, ranking first on the sensor track of the public CARLA Leaderboard.",
                        "Citation Paper Authors": "Authors:Hao Shao, Letian Wang, Ruobing Chen, Steven L. Waslander, Hongsheng Li, Yu Liu"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "devised\na framework that incorporates full-stack driving tasks and\nutilizes query-unified interfaces to communicate between\ndifferent tasks. ThinkTwice ",
                    "Citation Text": "Xiaosong Jia, Penghao Wu, Li Chen, Jiangwei Xie, Con-\nghui He, Junchi Yan, and Hongyang Li. Think twice be-\nfore driving: Towards scalable decoders for end-to-end au-\ntonomous driving. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , pages\n21983\u201321994, 2023. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.06242",
                        "Citation Paper Title": "Title:Think Twice before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving",
                        "Citation Paper Abstract": "Abstract:End-to-end autonomous driving has made impressive progress in recent years. Existing methods usually adopt the decoupled encoder-decoder paradigm, where the encoder extracts hidden features from raw sensor data, and the decoder outputs the ego-vehicle's future trajectories or actions. Under such a paradigm, the encoder does not have access to the intended behavior of the ego agent, leaving the burden of finding out safety-critical regions from the massive receptive field and inferring about future situations to the decoder. Even worse, the decoder is usually composed of several simple multi-layer perceptrons (MLP) or GRUs while the encoder is delicately designed (e.g., a combination of heavy ResNets or Transformer). Such an imbalanced resource-task division hampers the learning process.\nIn this work, we aim to alleviate the aforementioned problem by two principles: (1) fully utilizing the capacity of the encoder; (2) increasing the capacity of the decoder. Concretely, we first predict a coarse-grained future position and action based on the encoder features. Then, conditioned on the position and action, the future scene is imagined to check the ramification if we drive accordingly. We also retrieve the encoder features around the predicted coordinate to obtain fine-grained information about the safety-critical region. Finally, based on the predicted future and the retrieved salient feature, we refine the coarse-grained position and action by predicting its offset from ground-truth. The above refinement module could be stacked in a cascaded fashion, which extends the capacity of the decoder with spatial-temporal prior knowledge about the conditioned future. We conduct experiments on the CARLA simulator and achieve state-of-the-art performance in closed-loop benchmarks. Extensive ablation studies demonstrate the effectiveness of each proposed module.",
                        "Citation Paper Authors": "Authors:Xiaosong Jia, Penghao Wu, Li Chen, Jiangwei Xie, Conghui He, Junchi Yan, Hongyang Li"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "2.1. End-to-End Autonomous Driving\nMuch progress [7, 8] has been achieved recently in the field\nof end-to-end autonomous driving. UniAD ",
                    "Citation Text": "Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima,\nXizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai\nWang, et al. Planning-oriented autonomous driving. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 17853\u201317862, 2023. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.10156",
                        "Citation Paper Title": "Title:Planning-oriented Autonomous Driving",
                        "Citation Paper Abstract": "Abstract:Modern autonomous driving system is characterized as modular tasks in sequential order, i.e., perception, prediction, and planning. In order to perform a wide diversity of tasks and achieve advanced-level intelligence, contemporary approaches either deploy standalone models for individual tasks, or design a multi-task paradigm with separate heads. However, they might suffer from accumulative errors or deficient task coordination. Instead, we argue that a favorable framework should be devised and optimized in pursuit of the ultimate goal, i.e., planning of the self-driving car. Oriented at this, we revisit the key components within perception and prediction, and prioritize the tasks such that all these tasks contribute to planning. We introduce Unified Autonomous Driving (UniAD), a comprehensive framework up-to-date that incorporates full-stack driving tasks in one network. It is exquisitely devised to leverage advantages of each module, and provide complementary feature abstractions for agent interaction from a global perspective. Tasks are communicated with unified query interfaces to facilitate each other toward planning. We instantiate UniAD on the challenging nuScenes benchmark. With extensive ablations, the effectiveness of using such a philosophy is proven by substantially outperforming previous state-of-the-arts in all aspects. Code and models are public.",
                        "Citation Paper Authors": "Authors:Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu Qiao, Hongyang Li"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.13139v2": {
            "Paper Title": "Unleashing Large-Scale Video Generative Pre-training for Visual Robot\n  Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13469v1": {
            "Paper Title": "Neural feels with neural fields: Visuo-tactile perception for in-hand\n  manipulation",
            "Sentences": [
                {
                    "Sentence ID": 72,
                    "Sentence": "and is lightweight (21.7M parameters) compared to its fully-convolution\ncounterparts ",
                    "Citation Text": "Sudharshan Suresh, Zilin Si, Stuart Anderson, Michael Kaess, and Mustafa Mukadam.\nMidastouch: Monte-carlo inference over distributions across sliding touch. In 6th Annual\nConference on Robot Learning , 2022. 2, 9, 20",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.14210",
                        "Citation Paper Title": "Title:MidasTouch: Monte-Carlo inference over distributions across sliding touch",
                        "Citation Paper Abstract": "Abstract:We present MidasTouch, a tactile perception system for online global localization of a vision-based touch sensor sliding on an object surface. This framework takes in posed tactile images over time, and outputs an evolving distribution of sensor pose on the object's surface, without the need for visual priors. Our key insight is to estimate local surface geometry with tactile sensing, learn a compact representation for it, and disambiguate these signals over a long time horizon. The backbone of MidasTouch is a Monte-Carlo particle filter, with a measurement model based on a tactile code network learned from tactile simulation. This network, inspired by LIDAR place recognition, compactly summarizes local surface geometries. These generated codes are efficiently compared against a precomputed tactile codebook per-object, to update the pose distribution. We further release the YCB-Slide dataset of real-world and simulated forceful sliding interactions between a vision-based tactile sensor and standard YCB objects. While single-touch localization can be inherently ambiguous, we can quickly localize our sensor by traversing salient surface geometries. Project page: this https URL",
                        "Citation Paper Authors": "Authors:Sudharshan Suresh, Zilin Si, Stuart Anderson, Michael Kaess, Mustafa Mukadam"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "is ambiguous. Robust segmentation of the image stream Ic\ntvia prompts has successfully been\ndemonstrated by image foundation models, like the Segment Anything Model (SAM) ",
                    "Citation Text": "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura\nGustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Seg-\nment anything. arXiv preprint arXiv:2304.02643 , 2023. 18, 19",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.02643",
                        "Citation Paper Title": "Title:Segment Anything",
                        "Citation Paper Abstract": "Abstract:We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at this https URL to foster research into foundation models for computer vision.",
                        "Citation Paper Authors": "Authors:Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00e1r, Ross Girshick"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.13410v1": {
            "Paper Title": "Shared Affordance-awareness via Augmented Reality for Proactive\n  Assistance in Human-robot Collaboration",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": "scales, we will acquire participant opinions on our sys-\ntem\u2019s ease of use, helpfulness, perceived workload, as well\nas overall user experience. By equipping the AR headset\nwith a Pupil Labs eye tracker add-on ",
                    "Citation Text": "Moritz Kassner, William Patera, and Andreas Bulling. Pupil:\nAn Open Source Platform for Pervasive Eye Tracking and\nMobile Gaze-based Interaction. In ACM International Joint\nConference on Pervasive and Ubiquitous Computing: Ad-\njunct Publication , pages 1151\u20131160, 2014. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1405.0006",
                        "Citation Paper Title": "Title:Pupil: An Open Source Platform for Pervasive Eye Tracking and Mobile Gaze-based Interaction",
                        "Citation Paper Abstract": "Abstract:Commercial head-mounted eye trackers provide useful features to customers in industry and research but are expensive and rely on closed source hardware and software. This limits the application areas and use of mobile eye tracking to expert users and inhibits user-driven development, customisation, and extension. In this paper we present Pupil -- an accessible, affordable, and extensible open source platform for mobile eye tracking and gaze-based interaction. Pupil comprises 1) a light-weight headset with high-resolution cameras, 2) an open source software framework for mobile eye tracking, as well as 3) a graphical user interface (GUI) to playback and visualize video and gaze data. Pupil features high-resolution scene and eye cameras for monocular and binocular gaze estimation. The software and GUI are platform-independent and include state-of-the-art algorithms for real-time pupil detection and tracking, calibration, and accurate gaze estimation. Results of a performance evaluation show that Pupil can provide an average gaze estimation accuracy of 0.6 degree of visual angle (0.08 degree precision) with a latency of the processing pipeline of only 0.045 seconds.",
                        "Citation Paper Authors": "Authors:Moritz Kassner, William Patera, Andreas Bulling"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": ".\nFurthermore, a human can act around the robot\u2019s planned\nbehavior by relying on the AR interface\u2019s feedback.\n3.2. Spatial Computing\nSpatial computing is the ability for devices to track key\nfeatures in an environment and represent them digitally ",
                    "Citation Text": "Jeffrey Delmerico, Roi Poranne, Federica Bogo, Helen\nOleynikova, Eric V ollenweider, Stelian Coros, Juan Nieto,\nand Marc Pollefeys. Spatial Computing and Intuitive Inter-\naction: Bringing Mixed Reality and Robotics Together. IEEE\nRobotics & Automation Magazine , 29(1):45\u201357, Mar. 2022.\n2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.01493",
                        "Citation Paper Title": "Title:Spatial Computing and Intuitive Interaction: Bringing Mixed Reality and Robotics Together",
                        "Citation Paper Abstract": "Abstract:Spatial computing -- the ability of devices to be aware of their surroundings and to represent this digitally -- offers novel capabilities in human-robot interaction. In particular, the combination of spatial computing and egocentric sensing on mixed reality devices enables them to capture and understand human actions and translate these to actions with spatial meaning, which offers exciting new possibilities for collaboration between humans and robots. This paper presents several human-robot systems that utilize these capabilities to enable novel robot use cases: mission planning for inspection, gesture-based control, and immersive teleoperation. These works demonstrate the power of mixed reality as a tool for human-robot interaction, and the potential of spatial computing and mixed reality to drive the future of human-robot interaction.",
                        "Citation Paper Authors": "Authors:Jeffrey Delmerico, Roi Poranne, Federica Bogo, Helen Oleynikova, Eric Vollenweider, Stelian Coros, Juan Nieto, Marc Pollefeys"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.13385v1": {
            "Paper Title": "ORBSLAM3-Enhanced Autonomous Toy Drones: Pioneering Indoor Exploration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13219v1": {
            "Paper Title": "Interactive Visual Task Learning for Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.13076v1": {
            "Paper Title": "How to Integrate Digital Twin and Virtual Reality in Robotics Systems?\n  Design and Implementation for Providing Robotics Maintenance Services in Data\n  Centers",
            "Sentences": [
                {
                    "Sentence ID": 48,
                    "Sentence": "; a 3D gaming platform is developed to enable\nreal-time monitoring and display visual analysis of the operating condition in\na DC ( ",
                    "Citation Text": "Hubbell, M., Moran, A., Arcand, W., Bestor, D., Bergeron, B., Byun,\nC., Gadepally, V., Michaleas, P., Mullen, J., Prout, A., et al.: Big data\nstrategies for data center infrastructure management using a 3D gam-\ning platform. In: 2015 IEEE High Performance Extreme Computing\nConference (HPEC), pp. 1\u20136 (2015). https://doi.org/10.1109/hpec.2015.\n7322471. IEEE",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.08505",
                        "Citation Paper Title": "Title:Big Data Strategies for Data Center Infrastructure Management Using a 3D Gaming Platform",
                        "Citation Paper Abstract": "Abstract:High Performance Computing (HPC) is intrinsically linked to effective Data Center Infrastructure Management (DCIM). Cloud services and HPC have become key components in Department of Defense and corporate Information Technology competitive strategies in the global and commercial spaces. As a result, the reliance on consistent, reliable Data Center space is more critical than ever. The costs and complexity of providing quality DCIM are constantly being tested and evaluated by the United States Government and companies such as Google, Microsoft and Facebook. This paper will demonstrate a system where Big Data strategies and 3D gaming technology is leveraged to successfully monitor and analyze multiple HPC systems and a lights-out modular HP EcoPOD 240a Data Center on a singular platform. Big Data technology and a 3D gaming platform enables the relative real time monitoring of 5000 environmental sensors, more than 3500 IT data points and display visual analytics of the overall operating condition of the Data Center from a command center over 100 miles away. In addition, the Big Data model allows for in depth analysis of historical trends and conditions to optimize operations achieving even greater efficiencies and reliability.",
                        "Citation Paper Authors": "Authors:Matthew Hubbell, Andrew Moran, William Arcand, David Bestor, Bill Bergeron, Chansup Byun, Vijay Gadepally, Peter Michaleas, Julie Mullen, Andrew Prout, Albert Reuther, Antonio Rosa, Charles Yee, Jeremy Kepner"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2310.09688v2": {
            "Paper Title": "Recursively-Constrained Partially Observable Markov Decision Processes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12861v1": {
            "Paper Title": "Safe Multi-Agent Reinforcement Learning for Formation Control without\n  Individual Reference Targets",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": "used a trained\ncentralized neural network as a safety layer, along with MAD-\nDPG, and showed a reduction in the number of collisions in a\ncooperative navigation scenario. Moreover, in ",
                    "Citation Text": "I. ElSayed-Aly, S. Bharadwaj, C. Amato, R. Ehlers, U. Topcu, and\nL. Feng, \u201cSafe multi-agent reinforcement learning via shielding,\u201d arXiv\npreprint arXiv:2101.11196 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.11196",
                        "Citation Paper Title": "Title:Safe Multi-Agent Reinforcement Learning via Shielding",
                        "Citation Paper Abstract": "Abstract:Multi-agent reinforcement learning (MARL) has been increasingly used in a wide range of safety-critical applications, which require guaranteed safety (e.g., no unsafe states are ever visited) during the learning process.Unfortunately, current MARL methods do not have safety guarantees. Therefore, we present two shielding approaches for safe MARL. In centralized shielding, we synthesize a single shield to monitor all agents' joint actions and correct any unsafe action if necessary. In factored shielding, we synthesize multiple shields based on a factorization of the joint state space observed by all agents; the set of shields monitors agents concurrently and each shield is only responsible for a subset of agents at each step.Experimental results show that both approaches can guarantee the safety of agents during learning without compromising the quality of learned policies; moreover, factored shielding is more scalable in the number of agents than centralized shielding.",
                        "Citation Paper Authors": "Authors:Ingy Elsayed-Aly, Suda Bharadwaj, Christopher Amato, R\u00fcdiger Ehlers, Ufuk Topcu, Lu Feng"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "the au-\nthors used imitation learning along with RL to formulate a\ndistributed formation controller based on a leader-follower\nscheme. In ",
                    "Citation Text": "A. Khan, E. Tolstaya, A. Ribeiro, and V . Kumar, \u201cGraph policy gradients\nfor large scale robot control,\u201d in Conference on robot learning . PMLR,\n2020, pp. 823\u2013834.Speed Limit\nvmin in Simulation 0m/s\nvmax in Simulation 0.75m/s\nwmin in Simulation \u22121.0rad/s\nwmax in Simulation 1.0m/s\nvmin in Real-World 0. m/s\nvmax in Real-World 0.3m/s\nwmin in Real-World \u22121.0rad/s\nwmax in Real-World 1.0rad/s\nTABLE XII: Speed limits for robots.\nReward Value\nrgoal 300\nrcollision\ni-2000\nrformation\ni-2.0\nrobs\ni-50\nrgoal\ncentroid-4.0\nrMPC\nRL-5.0\nTABLE XIII: Rewards for MARL. rMPC\nRL represents the penalty\ngiven to the MARL for deviating from the MPC safe actions.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.03822",
                        "Citation Paper Title": "Title:Graph Policy Gradients for Large Scale Robot Control",
                        "Citation Paper Abstract": "Abstract:In this paper, we consider the problem of learning policies to control a large number of homogeneous robots. To this end, we propose a new algorithm we call Graph Policy Gradients (GPG) that exploits the underlying graph symmetry among the robots. The curse of dimensionality one encounters when working with a large number of robots is mitigated by employing a graph convolutional neural (GCN) network to parametrize policies for the robots. The GCN reduces the dimensionality of the problem by learning filters that aggregate information among robots locally, similar to how a convolutional neural network is able to learn local features in an image. Through experiments on formation flying, we show that our proposed method is able to scale better than existing reinforcement methods that employ fully connected networks. More importantly, we show that by using our locally learned filters we are able to zero-shot transfer policies trained on just three robots to over hundred robots.",
                        "Citation Paper Authors": "Authors:Arbaaz Khan, Ekaterina Tolstaya, Alejandro Ribeiro, Vijay Kumar"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2311.15803v2": {
            "Paper Title": "SOAC: Spatio-Temporal Overlap-Aware Multi-Sensor Calibration using\n  Neural Radiance Fields",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": ") to\nregularize relative poses between successive images. GN-\neRF ",
                    "Citation Text": "Quan Meng, Anpei Chen, Haimin Luo, Minye Wu, Hao Su,\nLan Xu, Xuming He, and Jingyi Yu. Gnerf: Gan-based neu-\nral radiance field without posed camera. In ICCV , pages\n6351\u20136361, 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.15606",
                        "Citation Paper Title": "Title:GNeRF: GAN-based Neural Radiance Field without Posed Camera",
                        "Citation Paper Abstract": "Abstract:We introduce GNeRF, a framework to marry Generative Adversarial Networks (GAN) with Neural Radiance Field (NeRF) reconstruction for the complex scenarios with unknown and even randomly initialized camera poses. Recent NeRF-based advances have gained popularity for remarkable realistic novel view synthesis. However, most of them heavily rely on accurate camera poses estimation, while few recent methods can only optimize the unknown camera poses in roughly forward-facing scenes with relatively short camera trajectories and require rough camera poses initialization. Differently, our GNeRF only utilizes randomly initialized poses for complex outside-in scenarios. We propose a novel two-phases end-to-end framework. The first phase takes the use of GANs into the new realm for optimizing coarse camera poses and radiance fields jointly, while the second phase refines them with additional photometric loss. We overcome local minima using a hybrid and iterative optimization scheme. Extensive experiments on a variety of synthetic and natural scenes demonstrate the effectiveness of GNeRF. More impressively, our approach outperforms the baselines favorably in those scenes with repeated patterns or even low textures that are regarded as extremely challenging before.",
                        "Citation Paper Authors": "Authors:Quan Meng, Anpei Chen, Haimin Luo, Minye Wu, Hao Su, Lan Xu, Xuming He, Jingyi Yu"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "achieves pose optimization with sparse\ninput views by relying on pixel matching and depth consis-\ntency.\nWhile the aforementioned methods need an initial esti-\nmate of the camera poses, some recent methods completely\nremove the need for prior poses. NoPe-NeRF ",
                    "Citation Text": "Wenjing Bian, Zirui Wang, Kejie Li, Jia-Wang Bian, and\nVictor Adrian Prisacariu. Nope-nerf: Optimising neural ra-\ndiance field with no pose prior. In CVPR , pages 4160\u20134169,\n2023. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.07388",
                        "Citation Paper Title": "Title:NoPe-NeRF: Optimising Neural Radiance Field with No Pose Prior",
                        "Citation Paper Abstract": "Abstract:Training a Neural Radiance Field (NeRF) without pre-computed camera poses is challenging. Recent advances in this direction demonstrate the possibility of jointly optimising a NeRF and camera poses in forward-facing scenes. However, these methods still face difficulties during dramatic camera movement. We tackle this challenging problem by incorporating undistorted monocular depth priors. These priors are generated by correcting scale and shift parameters during training, with which we are then able to constrain the relative poses between consecutive frames. This constraint is achieved using our proposed novel loss functions. Experiments on real-world indoor and outdoor scenes show that our method can handle challenging camera trajectories and outperforms existing methods in terms of novel view rendering quality and pose estimation accuracy. Our project page is https://nope-nerf.active.vision.",
                        "Citation Paper Authors": "Authors:Wenjing Bian, Zirui Wang, Kejie Li, Jia-Wang Bian, Victor Adrian Prisacariu"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "adds camera distortion estimation and uses a\ndifferent 6-vector rotation formulation in the optimization,\nwhile SPARF ",
                    "Citation Text": "Prune Truong, Marie-Julie Rakotosaona, Fabian Manhardt,\nand Federico Tombari. Sparf: Neural radiance fields from\nsparse and noisy poses. In CVPR , pages 4190\u20134200, 2023. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2211.11738",
                        "Citation Paper Title": "Title:SPARF: Neural Radiance Fields from Sparse and Noisy Poses",
                        "Citation Paper Abstract": "Abstract:Neural Radiance Field (NeRF) has recently emerged as a powerful representation to synthesize photorealistic novel views. While showing impressive performance, it relies on the availability of dense input views with highly accurate camera poses, thus limiting its application in real-world scenarios. In this work, we introduce Sparse Pose Adjusting Radiance Field (SPARF), to address the challenge of novel-view synthesis given only few wide-baseline input images (as low as 3) with noisy camera poses. Our approach exploits multi-view geometry constraints in order to jointly learn the NeRF and refine the camera poses. By relying on pixel matches extracted between the input views, our multi-view correspondence objective enforces the optimized scene and camera poses to converge to a global and geometrically accurate solution. Our depth consistency loss further encourages the reconstructed scene to be consistent from any viewpoint. Our approach sets a new state of the art in the sparse-view regime on multiple challenging datasets.",
                        "Citation Paper Authors": "Authors:Prune Truong, Marie-Julie Rakotosaona, Fabian Manhardt, Federico Tombari"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "with the use of a pre-\ntrained NeRF as a map, to build a real-time global local-\nization method. CROSSFIRE ",
                    "Citation Text": "Arthur Moreau, Nathan Piasco, Moussab Bennehar, Dzmitry\nTsishkou, Bogdan Stanciulescu, and Arnaud de La Fortelle.\nCROSSFIRE: Camera Relocalization On Self-Supervised\nFeatures from an Implicit Representation. ICCV , 2023. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.04869",
                        "Citation Paper Title": "Title:CROSSFIRE: Camera Relocalization On Self-Supervised Features from an Implicit Representation",
                        "Citation Paper Abstract": "Abstract:Beyond novel view synthesis, Neural Radiance Fields are useful for applications that interact with the real world. In this paper, we use them as an implicit map of a given scene and propose a camera relocalization algorithm tailored for this representation. The proposed method enables to compute in real-time the precise position of a device using a single RGB camera, during its navigation. In contrast with previous work, we do not rely on pose regression or photometric alignment but rather use dense local features obtained through volumetric rendering which are specialized on the scene with a self-supervised objective. As a result, our algorithm is more accurate than competitors, able to operate in dynamic outdoor environments with changing lightning conditions and can be readily integrated in any volumetric neural renderer.",
                        "Citation Paper Authors": "Authors:Arthur Moreau, Nathan Piasco, Moussab Bennehar, Dzmitry Tsishkou, Bogdan Stanciulescu, Arnaud de La Fortelle"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.12036v2": {
            "Paper Title": "LHManip: A Dataset for Long-Horizon Language-Grounded Manipulation Tasks\n  in Cluttered Tabletop Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12791v1": {
            "Paper Title": "Model-Based Control with Sparse Neural Dynamics",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": ", while the efficiency of these mixed-integer models has been thoroughly studied in ",
                    "Citation Text": "Ross Anderson, Joey Huchette, Will Ma, Christian Tjandraatmadja, and Juan Pablo Vielma.\nStrong mixed-integer programming formulations for trained neural networks. Mathematical\nProgramming , pages 1\u201337, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.08359",
                        "Citation Paper Title": "Title:Strong mixed-integer programming formulations for trained neural networks",
                        "Citation Paper Abstract": "Abstract:We present an ideal mixed-integer programming (MIP) formulation for a rectified linear unit (ReLU) appearing in a trained neural network. Our formulation requires a single binary variable and no additional continuous variables beyond the input and output variables of the ReLU. We contrast it with an ideal \"extended\" formulation with a linear number of additional continuous variables, derived through standard techniques. An apparent drawback of our formulation is that it requires an exponential number of inequality constraints, but we provide a routine to separate the inequalities in linear time. We also prove that these exponentially-many constraints are facet-defining under mild conditions. Finally, we study network verification problems and observe that dynamically separating from the exponential inequalities 1) is much more computationally efficient and scalable than the extended formulation, 2) decreases the solve time of a state-of-the-art MIP solver by a factor of 7 on smaller instances, and 3) nearly matches the dual bounds of a state-of-the-art MIP solver on harder instances, after just a few rounds of separation and in orders of magnitude less time.",
                        "Citation Paper Authors": "Authors:Ross Anderson, Joey Huchette, Christian Tjandraatmadja, Juan Pablo Vielma"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.12680v1": {
            "Paper Title": "Trajectory Approximation of Video Based on Phase Correlation for Forward\n  Facing Camera",
            "Sentences": []
        },
        "http://arxiv.org/abs/2311.08206v2": {
            "Paper Title": "Human-Centric Autonomous Systems With LLMs for User Command Reasoning",
            "Sentences": [
                {
                    "Sentence ID": 46,
                    "Sentence": ". The GPT models are accessed on-\nline, whereas Llama models offer an on-board solution,\nwith both their code and pretrained weights open-sourced.\nCodeLlama-34b-Instruct is an enhanced version of the orig-\ninal Llama ",
                    "Citation Text": "Hugo Touvron, Thibaut Lavril, Gautier Izacard,\nXavier Martinet, Marie-Anne Lachaux, Timoth\u00b4 ee\nLacroix, Baptiste Rozi` ere, Naman Goyal, Eric Ham-\nbro, Faisal Azhar, et al. Llama: Open and ef\ufb01-\ncient foundation language models. arXiv preprint\narXiv:2302.13971 , 2023. 3\n7A. Qualitative Results of GPT4\nThere are three examples of the GPT4 real results with explan ations for different user commands in Tab. 5, Tab. 6, Tab. 7.\nSome user command examples\na) LLM Conditioning [Set as in Tab. 1.]\nb) User Assistant [Provide few-shots for LLM to learn shown in Tab. 1.]\n[Test on real commands. ]\nc) User Share the vehicle\u2019s location to my dad.\nGPT-4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.13971",
                        "Citation Paper Title": "Title:LLaMA: Open and Efficient Foundation Language Models",
                        "Citation Paper Abstract": "Abstract:We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",
                        "Citation Paper Authors": "Authors:Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": ". However, the\nsuccess of adapting language models to new tasks largely\ndepends on the prompting strategy and the quality of the\nprompts ",
                    "Citation Text": "Brian Lester, Rami Al-Rfou, and Noah Constant. The powe r\nof scale for parameter-ef\ufb01cient prompt tuning. arXiv preprint\narXiv:2104.08691 , 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.08691",
                        "Citation Paper Title": "Title:The Power of Scale for Parameter-Efficient Prompt Tuning",
                        "Citation Paper Abstract": "Abstract:In this work, we explore \"prompt tuning\", a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's \"few-shot\" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.",
                        "Citation Paper Authors": "Authors:Brian Lester, Rami Al-Rfou, Noah Constant"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.12639v1": {
            "Paper Title": "Collective Anomaly Perception During Multi-Robot Patrol: Constrained\n  Interactions Can Promote Accurate Consensus",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12637v1": {
            "Paper Title": "Domain-Independent Disperse and Pick method for Robotic Grasping",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12634v1": {
            "Paper Title": "MotionScript: Natural Language Descriptions for Expressive 3D Human\n  Motions",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": "proposed a neuro-\nsymbolic, program-like representation that described mo-\ntions as a composition of high-level parametric primitives\ni.e. circular, linear, or stationary and further extended by\nadding general spline primitives ",
                    "Citation Text": "Sumith Kulal, Jiayuan Mao, Alex Aiken, and Jiajun Wu. Pro-\ngrammatic concept learning for human motion description\nand synthesis. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 13843\u2013\n13852, 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.13502",
                        "Citation Paper Title": "Title:Programmatic Concept Learning for Human Motion Description and Synthesis",
                        "Citation Paper Abstract": "Abstract:We introduce Programmatic Motion Concepts, a hierarchical motion representation for human actions that captures both low-level motion and high-level description as motion concepts. This representation enables human motion description, interactive editing, and controlled synthesis of novel video sequences within a single framework. We present an architecture that learns this concept representation from paired video and action sequences in a semi-supervised manner. The compactness of our representation also allows us to present a low-resource training recipe for data-efficient learning. By outperforming established baselines, especially in the small data regime, we demonstrate the efficiency and effectiveness of our framework for multiple applications.",
                        "Citation Paper Authors": "Authors:Sumith Kulal, Jiayuan Mao, Alex Aiken, Jiajun Wu"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "proposed\na rule-based Gesture Description Language (GDL) to repre-\nsent human body skeleton data with synthetic descriptions.\nHierarchical Motion Understanding ",
                    "Citation Text": "Sumith Kulal, Jiayuan Mao, Alex Aiken, and Jiajun Wu. Hi-\nerarchical motion understanding via motion programs. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition , pages 6568\u20136576, 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.11216",
                        "Citation Paper Title": "Title:Hierarchical Motion Understanding via Motion Programs",
                        "Citation Paper Abstract": "Abstract:Current approaches to video analysis of human motion focus on raw pixels or keypoints as the basic units of reasoning. We posit that adding higher-level motion primitives, which can capture natural coarser units of motion such as backswing or follow-through, can be used to improve downstream analysis tasks. This higher level of abstraction can also capture key features, such as loops of repeated primitives, that are currently inaccessible at lower levels of representation. We therefore introduce Motion Programs, a neuro-symbolic, program-like representation that expresses motions as a composition of high-level primitives. We also present a system for automatically inducing motion programs from videos of human motion and for leveraging motion programs in video synthesis. Experiments show that motion programs can accurately describe a diverse set of human motions and the inferred programs contain semantically meaningful motion primitives, such as arm swings and jumping jacks. Our representation also benefits downstream tasks such as video interpolation and video prediction and outperforms off-the-shelf models. We further demonstrate how these programs can detect diverse kinds of repetitive motion and facilitate interactive video editing.",
                        "Citation Paper Authors": "Authors:Sumith Kulal, Jiayuan Mao, Alex Aiken, Jiajun Wu"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "introduced bi-\nnary captions describing articulation angle or relative po-\nsition of joints while ",
                    "Citation Text": "Georgios Pavlakos, Xiaowei Zhou, and Kostas Daniilidis.\nOrdinal depth supervision for 3d human pose estimation. In\nProceedings of the IEEE conference on computer vision and\npattern recognition , pages 7307\u20137316, 2018. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.04095",
                        "Citation Paper Title": "Title:Ordinal Depth Supervision for 3D Human Pose Estimation",
                        "Citation Paper Abstract": "Abstract:Our ability to train end-to-end systems for 3D human pose estimation from single images is currently constrained by the limited availability of 3D annotations for natural images. Most datasets are captured using Motion Capture (MoCap) systems in a studio setting and it is difficult to reach the variability of 2D human pose datasets, like MPII or LSP. To alleviate the need for accurate 3D ground truth, we propose to use a weaker supervision signal provided by the ordinal depths of human joints. This information can be acquired by human annotators for a wide range of images and poses. We showcase the effectiveness and flexibility of training Convolutional Networks (ConvNets) with these ordinal relations in different settings, always achieving competitive performance with ConvNets trained with accurate 3D joint coordinates. Additionally, to demonstrate the potential of the approach, we augment the popular LSP and MPII datasets with ordinal depth annotations. This extension allows us to present quantitative and qualitative evaluation in non-studio conditions. Simultaneously, these ordinal annotations can be easily incorporated in the training procedure of typical ConvNets for 3D human pose. Through this inclusion we achieve new state-of-the-art performance for the relevant benchmarks and validate the effectiveness of ordinal depth supervision for 3D human pose.",
                        "Citation Paper Authors": "Authors:Georgios Pavlakos, Xiaowei Zhou, Kostas Daniilidis"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "proposed a method to generate fine-grained human\nbody motions by analyzing the linguistics-structure in the\nmotion caption. To elevate data for compositional actions,\nSINC ",
                    "Citation Text": "Nikos Athanasiou, Mathis Petrovich, Michael J Black, and\nG\u00a8ul Varol. Sinc: Spatial composition of 3d human motions\nfor simultaneous action generation supplementary material.\n2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.10417",
                        "Citation Paper Title": "Title:SINC: Spatial Composition of 3D Human Motions for Simultaneous Action Generation",
                        "Citation Paper Abstract": "Abstract:Our goal is to synthesize 3D human motions given textual inputs describing simultaneous actions, for example 'waving hand' while 'walking' at the same time. We refer to generating such simultaneous movements as performing 'spatial compositions'. In contrast to temporal compositions that seek to transition from one action to another, spatial compositing requires understanding which body parts are involved in which action, to be able to move them simultaneously. Motivated by the observation that the correspondence between actions and body parts is encoded in powerful language models, we extract this knowledge by prompting GPT-3 with text such as \"what are the body parts involved in the action <action name>?\", while also providing the parts list and few-shot examples. Given this action-part mapping, we combine body parts from two motions together and establish the first automated method to spatially compose two actions. However, training data with compositional actions is always limited by the combinatorics. Hence, we further create synthetic data with this approach, and use it to train a new state-of-the-art text-to-motion generation model, called SINC (\"SImultaneous actioN Compositions for 3D human motions\"). In our experiments, that training with such GPT-guided synthetic data improves spatial composition generation over baselines. Our code is publicly available at this https URL.",
                        "Citation Paper Authors": "Authors:Nikos Athanasiou, Mathis Petrovich, Michael J. Black, G\u00fcl Varol"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "gen-\nerates arbitrarily long dances, by enforcing temporal conti-\nnuity between batches of multiple sequences. Make-An-\nAnimation (MAA) ",
                    "Citation Text": "Samaneh Azadi, Akbar Shah, Thomas Hayes, Devi Parikh,\nand Sonal Gupta. Make-an-animation: Large-scale text-\nconditional 3d human motion generation. arXiv preprint\narXiv:2305.09662 , 2023. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.09662",
                        "Citation Paper Title": "Title:Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation",
                        "Citation Paper Abstract": "Abstract:Text-guided human motion generation has drawn significant interest because of its impactful applications spanning animation and robotics. Recently, application of diffusion models for motion generation has enabled improvements in the quality of generated motions. However, existing approaches are limited by their reliance on relatively small-scale motion capture data, leading to poor performance on more diverse, in-the-wild prompts. In this paper, we introduce Make-An-Animation, a text-conditioned human motion generation model which learns more diverse poses and prompts from large-scale image-text datasets, enabling significant improvement in performance over prior works. Make-An-Animation is trained in two stages. First, we train on a curated large-scale dataset of (text, static pseudo-pose) pairs extracted from image-text datasets. Second, we fine-tune on motion capture data, adding additional layers to model the temporal dimension. Unlike prior diffusion models for motion generation, Make-An-Animation uses a U-Net architecture similar to recent text-to-video generation models. Human evaluation of motion realism and alignment with input text shows that our model reaches state-of-the-art performance on text-to-motion generation.",
                        "Citation Paper Authors": "Authors:Samaneh Azadi, Akbar Shah, Thomas Hayes, Devi Parikh, Sonal Gupta"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": "proposed a method to generate motions conditioned on\nprevious time step and current text description in an autore-\ngressive fashion to manage the scarcity of human motion-\ncaptured data for long prompts. Likewise, EDGE ",
                    "Citation Text": "Jonathan Tseng, Rodrigo Castellon, and Karen Liu. Edge:\nEditable dance generation from music. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 448\u2013458, 2023. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2211.10658",
                        "Citation Paper Title": "Title:EDGE: Editable Dance Generation From Music",
                        "Citation Paper Abstract": "Abstract:Dance is an important human art form, but creating new dances can be difficult and time-consuming. In this work, we introduce Editable Dance GEneration (EDGE), a state-of-the-art method for editable dance generation that is capable of creating realistic, physically-plausible dances while remaining faithful to the input music. EDGE uses a transformer-based diffusion model paired with Jukebox, a strong music feature extractor, and confers powerful editing capabilities well-suited to dance, including joint-wise conditioning, and in-betweening. We introduce a new metric for physical plausibility, and evaluate dance quality generated by our method extensively through (1) multiple quantitative metrics on physical plausibility, beat alignment, and diversity benchmarks, and more importantly, (2) a large-scale user study, demonstrating a significant improvement over previous state-of-the-art methods. Qualitative samples from our model can be found at our website.",
                        "Citation Paper Authors": "Authors:Jonathan Tseng, Rodrigo Castellon, C. Karen Liu"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": ", a large collection of mo-\ntion datasets. Several deterministic methods [1, 16] as well\nas probabilistic approaches such as transformers [42, 69],\nGANs ",
                    "Citation Text": "Liang Xu, Ziyang Song, Dongliang Wang, Jing Su, Zhicheng\nFang, Chenjing Ding, Weihao Gan, Yichao Yan, Xin Jin, Xi-\naokang Yang, et al. Actformer: A gan-based transformer\ntowards general action-conditioned 3d human motion gener-\nation. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision , pages 2228\u20132238, 2023. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.07706",
                        "Citation Paper Title": "Title:ActFormer: A GAN-based Transformer towards General Action-Conditioned 3D Human Motion Generation",
                        "Citation Paper Abstract": "Abstract:We present a GAN-based Transformer for general action-conditioned 3D human motion generation, including not only single-person actions but also multi-person interactive actions. Our approach consists of a powerful Action-conditioned motion TransFormer (ActFormer) under a GAN training scheme, equipped with a Gaussian Process latent prior. Such a design combines the strong spatio-temporal representation capacity of Transformer, superiority in generative modeling of GAN, and inherent temporal correlations from the latent prior. Furthermore, ActFormer can be naturally extended to multi-person motions by alternately modeling temporal correlations and human interactions with Transformer encoders. To further facilitate research on multi-person motion generation, we introduce a new synthetic dataset of complex multi-person combat behaviors. Extensive experiments on NTU-13, NTU RGB+D 120, BABEL and the proposed combat dataset show that our method can adapt to various human motion representations and achieve superior performance over the state-of-the-art methods on both single-person and multi-person motion generation tasks, demonstrating a promising step towards a general human motion generator.",
                        "Citation Paper Authors": "Authors:Liang Xu, Ziyang Song, Dongliang Wang, Jing Su, Zhicheng Fang, Chenjing Ding, Weihao Gan, Yichao Yan, Xin Jin, Xiaokang Yang, Wenjun Zeng, Wei Wu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.12583v1": {
            "Paper Title": "Observation-Augmented Contextual Multi-Armed Bandits for Robotic\n  Exploration with Uncertain Semantic Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12333v1": {
            "Paper Title": "Path Planning for Continuum Rods Using Bernstein Surfaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12262v1": {
            "Paper Title": "Evaluating Speech-in-Speech Perception via a Humanoid Robot",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.08957v2": {
            "Paper Title": "Acceptance and Trust: Drivers' First Contact with Released Automated\n  Vehicles in Naturalistic Traffic",
            "Sentences": []
        },
        "http://arxiv.org/abs/2309.10092v2": {
            "Paper Title": "Conformal Temporal Logic Planning using Large Language Models: Knowing\n  When to Do What and When to Ask for Help",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12121v1": {
            "Paper Title": "Towards Learning-Based Gyrocompassing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.12064v1": {
            "Paper Title": "MPI Planar Correction of Pulse Based ToF Cameras",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "where a continuous wave\nmodulation ToF camera is used. Other approaches are using\nneural networks to approximate or remove noise from depth\nimages ",
                    "Citation Text": "Kilho Son, Ming-Yu Liu, and Yuichi Taguchi. Learning to remove\nmultipath distortions in time-of-\ufb02ight range images for a r obotic arm\nsetup. 2016 IEEE International Conference on Robotics and Automat ion\n(ICRA) , May 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1601.01750",
                        "Citation Paper Title": "Title:Learning to Remove Multipath Distortions in Time-of-Flight Range Images for a Robotic Arm Setup",
                        "Citation Paper Abstract": "Abstract:Range images captured by Time-of-Flight (ToF) cameras are corrupted with multipath distortions due to interaction between modulated light signals and scenes. The interaction is often complicated, which makes a model-based solution elusive. We propose a learning-based approach for removing the multipath distortions for a ToF camera in a robotic arm setup. Our approach is based on deep learning. We use the robotic arm to automatically collect a large amount of ToF range images containing various multipath distortions. The training images are automatically labeled by leveraging a high precision structured light sensor available only in the training time. In the test time, we apply the learned model to remove the multipath distortions. This allows our robotic arm setup to enjoy the speed and compact form of the ToF camera without compromising with its range measurement errors. We conduct extensive experimental validations and compare the proposed method to several baseline algorithms. The experiment results show that our method achieves 55% error reduction in range estimation and largely outperforms the baseline algorithms.",
                        "Citation Paper Authors": "Authors:Kilho Son, Ming-Yu Liu, Yuichi Taguchi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.11911v1": {
            "Paper Title": "EVI-SAM: Robust, Real-time, Tightly-coupled Event-Visual-Inertial State\n  Estimation and 3D Dense Mapping",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": "introduces\nan EIO approach by addressing the reprojection error from\nthe asynchronous events and directly incorporating IMU\nwithin a continuous-time framework. Ref. ",
                    "Citation Text": "F. Mahlknecht, D. Gehrig, J. Nash, F. M. Rockenbauer, B. Morrell,\nJ. Delaune, and D. Scaramuzza, \u201cExploring event camera-based odom-\netry for planetary robots,\u201d IEEE Robotics and Automation Letters (RA-\nL), 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.05880",
                        "Citation Paper Title": "Title:Exploring Event Camera-based Odometry for Planetary Robots",
                        "Citation Paper Abstract": "Abstract:Due to their resilience to motion blur and high robustness in low-light and high dynamic range conditions, event cameras are poised to become enabling sensors for vision-based exploration on future Mars helicopter missions. However, existing event-based visual-inertial odometry (VIO) algorithms either suffer from high tracking errors or are brittle, since they cannot cope with significant depth uncertainties caused by an unforeseen loss of tracking or other effects. In this work, we introduce EKLT-VIO, which addresses both limitations by combining a state-of-the-art event-based frontend with a filter-based backend. This makes it both accurate and robust to uncertainties, outperforming event- and frame-based VIO algorithms on challenging benchmarks by 32%. In addition, we demonstrate accurate performance in hover-like conditions (outperforming existing event-based methods) as well as high robustness in newly collected Mars-like and high-dynamic-range sequences, where existing frame-based methods fail. In doing so, we show that event-based VIO is the way forward for vision-based exploration on Mars.",
                        "Citation Paper Authors": "Authors:Florian Mahlknecht, Daniel Gehrig, Jeremy Nash, Friedrich M. Rockenbauer, Benjamin Morrell, Jeff Delaune, Davide Scaramuzza"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "extracts features from events-only data and associates it\nwith a spatio-temporal locality scheme based on exponential\ndecay. Ref. ",
                    "Citation Text": "D. Liu, A. Parra, Y . Latif, B. Chen, T.-J. Chin, and I. Reid, \u201cAsyn-\nchronous optimisation for event-based visual odometry,\u201d in 2022\nInternational Conference on Robotics and Automation (ICRA) . IEEE,\n2022, pp. 9432\u20139438.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.01037",
                        "Citation Paper Title": "Title:Asynchronous Optimisation for Event-based Visual Odometry",
                        "Citation Paper Abstract": "Abstract:Event cameras open up new possibilities for robotic perception due to their low latency and high dynamic range. On the other hand, developing effective event-based vision algorithms that fully exploit the beneficial properties of event cameras remains work in progress. In this paper, we focus on event-based visual odometry (VO). While existing event-driven VO pipelines have adopted continuous-time representations to asynchronously process event data, they either assume a known map, restrict the camera to planar trajectories, or integrate other sensors into the system. Towards map-free event-only monocular VO in SE(3), we propose an asynchronous structure-from-motion optimisation back-end. Our formulation is underpinned by a principled joint optimisation problem involving non-parametric Gaussian Process motion modelling and incremental maximum a posteriori inference. A high-performance incremental computation engine is employed to reason about the camera trajectory with every incoming event. We demonstrate the robustness of our asynchronous back-end in comparison to frame-based methods which depend on accurate temporal accumulation of measurements.",
                        "Citation Paper Authors": "Authors:Daqi Liu, Alvaro Parra, Yasir Latif, Bo Chen, Tat-Jun Chin, Ian Reid"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.11802v1": {
            "Paper Title": "IKT-BT: Indirect Knowledge Transfer Behavior Tree Framework for\n  Multi-Robot Systems Through Communication Eavesdropping",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11713v1": {
            "Paper Title": "Indoor and Outdoor 3D Scene Graph Generation via Language-Enabled\n  Spatial Ontologies",
            "Sentences": [
                {
                    "Sentence ID": 39,
                    "Sentence": "with a Velodyne mounted\non the Jackal to provide trajectory estimates for the West\nPoint dataset, and Kimera-VIO ",
                    "Citation Text": "A. Rosinol, M. Abate, Y . Chang, and L. Carlone, \u201cKimera: an open-\nsource library for real-time metric-semantic localization and mapping,\u201d\ninIEEE Intl. Conf. on Robotics and Automation (ICRA) , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.02490",
                        "Citation Paper Title": "Title:Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping",
                        "Citation Paper Abstract": "Abstract:We provide an open-source C++ library for real-time metric-semantic visual-inertial Simultaneous Localization And Mapping (SLAM). The library goes beyond existing visual and visual-inertial SLAM libraries (e.g., ORB-SLAM, VINS- Mono, OKVIS, ROVIO) by enabling mesh reconstruction and semantic labeling in 3D. Kimera is designed with modularity in mind and has four key components: a visual-inertial odometry (VIO) module for fast and accurate state estimation, a robust pose graph optimizer for global trajectory estimation, a lightweight 3D mesher module for fast mesh reconstruction, and a dense 3D metric-semantic reconstruction module. The modules can be run in isolation or in combination, hence Kimera can easily fall back to a state-of-the-art VIO or a full SLAM system. Kimera runs in real-time on a CPU and produces a 3D metric-semantic mesh from semantically labeled images, which can be obtained by modern deep learning methods. We hope that the flexibility, computational efficiency, robustness, and accuracy afforded by Kimera will build a solid basis for future metric-semantic SLAM and perception research, and will allow researchers across multiple areas (e.g., VIO, SLAM, 3D reconstruction, segmentation) to benchmark and prototype their own efforts without having to start from scratch.",
                        "Citation Paper Authors": "Authors:Antoni Rosinol, Marcus Abate, Yun Chang, Luca Carlone"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "dataset is an RGB-D dataset\nconsisting of 90 indoor scenes. We use the Habitat simula-\ntor ",
                    "Citation Text": "M. Savva, A. Kadian, O. Maksymets, Y . Zhao, E. Wijmans, B. Jain,\nJ. Straub, J. Liu, V . Koltun, J. Malik, D. Parikh, and D. Batra,\n\u201cHabitat: A Platform for Embodied AI Research,\u201d in Proceedings of\nthe IEEE/CVF International Conference on Computer Vision (ICCV) ,\n2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.01201",
                        "Citation Paper Title": "Title:Habitat: A Platform for Embodied AI Research",
                        "Citation Paper Abstract": "Abstract:We present Habitat, a platform for research in embodied artificial intelligence (AI). Habitat enables training embodied agents (virtual robots) in highly efficient photorealistic 3D simulation. Specifically, Habitat consists of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is fast -- when rendering a scene from Matterport3D, it achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU. (ii) Habitat-API: a modular high-level library for end-to-end development of embodied AI algorithms -- defining tasks (e.g., navigation, instruction following, question answering), configuring, training, and benchmarking embodied agents.\nThese large-scale engineering contributions enable us to answer scientific questions requiring experiments that were till now impracticable or 'merely' impractical. Specifically, in the context of point-goal navigation: (1) we revisit the comparison between learning and SLAM approaches from two recent works and find evidence for the opposite conclusion -- that learning outperforms SLAM if scaled to an order of magnitude more experience than previous investigations, and (2) we conduct the first cross-dataset generalization experiments {train, test} x {Matterport3D, Gibson} for multiple sensors {blind, RGB, RGBD, D} and find that only agents with depth (D) sensors generalize across datasets. We hope that our open-source platform and these findings will advance research in embodied AI.",
                        "Citation Paper Authors": "Authors:Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, Dhruv Batra"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": ", and there has been a surge applying these\nto problems such as 2D scene graph generation ",
                    "Citation Text": "A. Zareian, S. Karaman, and S.-F. Chang, \u201cBridging knowledge graphs\nto generate scene graphs,\u201d in European Conf. on Computer Vision\n(ECCV) . Springer, 2020, pp. 606\u2013623.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2001.02314",
                        "Citation Paper Title": "Title:Bridging Knowledge Graphs to Generate Scene Graphs",
                        "Citation Paper Abstract": "Abstract:Scene graphs are powerful representations that parse images into their abstract semantic elements, i.e., objects and their interactions, which facilitates visual comprehension and explainable reasoning. On the other hand, commonsense knowledge graphs are rich repositories that encode how the world is structured, and how general concepts interact. In this paper, we present a unified formulation of these two constructs, where a scene graph is seen as an image-conditioned instantiation of a commonsense knowledge graph. Based on this new perspective, we re-formulate scene graph generation as the inference of a bridge between the scene and commonsense graphs, where each entity or predicate instance in the scene graph has to be linked to its corresponding entity or predicate class in the commonsense graph. To this end, we propose a novel graph-based neural network that iteratively propagates information between the two graphs, as well as within each of them, while gradually refining their bridge in each iteration. Our Graph Bridging Network, GB-Net, successively infers edges and nodes, allowing to simultaneously exploit and refine the rich, heterogeneous structure of the interconnected scene and commonsense graphs. Through extensive experimentation, we showcase the superior accuracy of GB-Net compared to the most recent methods, resulting in a new state of the art. We publicly release the source code of our method.",
                        "Citation Paper Authors": "Authors:Alireza Zareian, Svebor Karaman, Shih-Fu Chang"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "discuss\ndetecting higher-level concept categories ( e.g., room types)\nin 3D scene graphs using a fully-supervised GNN approach\nin an online manner, and Chen et al. ",
                    "Citation Text": "W. Chen, S. Hu, R. Talak, and L. Carlone, \u201cLeveraging large lan-\nguage models for robot 3D scene understanding,\u201d arXiv preprint:\n2209.05629 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2209.05629",
                        "Citation Paper Title": "Title:Leveraging Large (Visual) Language Models for Robot 3D Scene Understanding",
                        "Citation Paper Abstract": "Abstract:Abstract semantic 3D scene understanding is a problem of critical importance in robotics. As robots still lack the common-sense knowledge about household objects and locations of an average human, we investigate the use of pre-trained language models to impart common sense for scene understanding. We introduce and compare a wide range of scene classification paradigms that leverage language only (zero-shot, embedding-based, and structured-language) or vision and language (zero-shot and fine-tuned). We find that the best approaches in both categories yield $\\sim 70\\%$ room classification accuracy, exceeding the performance of pure-vision and graph classifiers. We also find such methods demonstrate notable generalization and transfer capabilities stemming from their use of language.",
                        "Citation Paper Authors": "Authors:William Chen, Siyi Hu, Rajat Talak, Luca Carlone"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": ". More\nrecently, several approaches explore the creation of 3D scene\ngraphs in real-time ",
                    "Citation Text": "S. Wu, J. Wald, K. Tateno, N. Navab, and F. Tombari, \u201cSceneGraphFu-\nsion: Incremental 3D scene graph prediction from RGB-D sequences,\u201d\ninIEEE Conf. on Computer Vision and Pattern Recognition (CVPR) ,\n2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.14898",
                        "Citation Paper Title": "Title:SceneGraphFusion: Incremental 3D Scene Graph Prediction from RGB-D Sequences",
                        "Citation Paper Abstract": "Abstract:Scene graphs are a compact and explicit representation successfully used in a variety of 2D scene understanding tasks. This work proposes a method to incrementally build up semantic scene graphs from a 3D environment given a sequence of RGB-D frames. To this end, we aggregate PointNet features from primitive scene components by means of a graph neural network. We also propose a novel attention mechanism well suited for partial and missing graph data present in such an incremental reconstruction scenario. Although our proposed method is designed to run on submaps of the scene, we show it also transfers to entire 3D scenes. Experiments show that our approach outperforms 3D scene graph prediction methods by a large margin and its accuracy is on par with other 3D semantic and panoptic segmentation methods while running at 35 Hz.",
                        "Citation Paper Authors": "Authors:Shun-Cheng Wu, Johanna Wald, Keisuke Tateno, Nassir Navab, Federico Tombari"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "introduced the notion of 3D scene graphs\nas a hierarchical model of 3D environments. Following this,\nRosinol et al. ",
                    "Citation Text": "A. Rosinol, A. Gupta, M. Abate, J. Shi, and L. Carlone, \u201c3D dynamic\nscene graphs: Actionable spatial perception with places, objects, and\nhumans,\u201d in Robotics: Science and Systems (RSS) , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.06289",
                        "Citation Paper Title": "Title:3D Dynamic Scene Graphs: Actionable Spatial Perception with Places, Objects, and Humans",
                        "Citation Paper Abstract": "Abstract:We present a unified representation for actionable spatial perception: 3D Dynamic Scene Graphs. Scene graphs are directed graphs where nodes represent entities in the scene (e.g. objects, walls, rooms), and edges represent relations (e.g. inclusion, adjacency) among nodes. Dynamic scene graphs (DSGs) extend this notion to represent dynamic scenes with moving agents (e.g. humans, robots), and to include actionable information that supports planning and decision-making (e.g. spatio-temporal relations, topology at different levels of abstraction). Our second contribution is to provide the first fully automatic Spatial PerceptIon eNgine(SPIN) to build a DSG from visual-inertial data. We integrate state-of-the-art techniques for object and human detection and pose estimation, and we describe how to robustly infer object, robot, and human nodes in crowded scenes. To the best of our knowledge, this is the first paper that reconciles visual-inertial SLAM and dense human mesh tracking. Moreover, we provide algorithms to obtain hierarchical representations of indoor environments (e.g. places, structures, rooms) and their relations. Our third contribution is to demonstrate the proposed spatial perception engine in a photo-realistic Unity-based simulator, where we assess its robustness and expressiveness. Finally, we discuss the implications of our proposal on modern robotics applications. 3D Dynamic Scene Graphs can have a profound impact on planning and decision-making, human-robot interaction, long-term autonomy, and scene prediction. A video abstract is available at this https URL",
                        "Citation Paper Authors": "Authors:Antoni Rosinol, Arjun Gupta, Marcus Abate, Jingnan Shi, Luca Carlone"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.11424v1": {
            "Paper Title": "3D exploration-based search for multiple targets using a UAV",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11598v1": {
            "Paper Title": "SkillDiffuser: Interpretable Hierarchical Planning via Skill\n  Abstractions in Diffusion-Based Task Execution",
            "Sentences": [
                {
                    "Sentence ID": 49,
                    "Sentence": "to vision and\nlanguage descriptions [7, 10, 12, 46].\nMulti-task learning approaches often leverage shared\nrepresentations to learn a spectrum of tasks simultaneously,\nenhancing the flexibility and efficiency of the learning pro-\ncess. The Meta-World benchmark ",
                    "Citation Text": "Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian,\nKarol Hausman, Chelsea Finn, and Sergey Levine. Meta-\nworld: A benchmark and evaluation for multi-task and meta\nreinforcement learning. In Conference on robot learning ,\npages 1094\u20131100. PMLR, 2020. 2, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.10897",
                        "Citation Paper Title": "Title:Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:Meta-reinforcement learning algorithms can enable robots to acquire new skills much more quickly, by leveraging prior experience to learn how to learn. However, much of the current research on meta-reinforcement learning focuses on task distributions that are very narrow. For example, a commonly used meta-reinforcement learning benchmark uses different running velocities for a simulated robot as different tasks. When policies are meta-trained on such narrow task distributions, they cannot possibly generalize to more quickly acquire entirely new tasks. Therefore, if the aim of these methods is to enable faster acquisition of entirely new behaviors, we must evaluate them on task distributions that are sufficiently broad to enable generalization to new behaviors. In this paper, we propose an open-source simulated benchmark for meta-reinforcement learning and multi-task learning consisting of 50 distinct robotic manipulation tasks. Our aim is to make it possible to develop algorithms that generalize to accelerate the acquisition of entirely new, held-out tasks. We evaluate 7 state-of-the-art meta-reinforcement learning and multi-task learning algorithms on these tasks. Surprisingly, while each task and its variations (e.g., with different object positions) can be learned with reasonable success, these algorithms struggle to learn with multiple tasks at the same time, even with as few as ten distinct training tasks. Our analysis and open-source environments pave the way for future research in multi-task learning and meta-learning that can enable meaningful generalization, thereby unlocking the full potential of these methods.",
                        "Citation Paper Authors": "Authors:Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Avnish Narayan, Hayden Shively, Adithya Bellathur, Karol Hausman, Chelsea Finn, Sergey Levine"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "recent years and has shown promising results\nin various generative applications. A seminal work that\nperforms planning with diffusion directly is Diffuser ",
                    "Citation Text": "Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey\nLevine. Planning with diffusion for flexible behavior syn-\nthesis. In International Conference on Machine Learning ,\npages 9902\u20139915. PMLR, 2022. 1, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.09991",
                        "Citation Paper Title": "Title:Planning with Diffusion for Flexible Behavior Synthesis",
                        "Citation Paper Abstract": "Abstract:Model-based reinforcement learning methods often use learning only for the purpose of estimating an approximate dynamics model, offloading the rest of the decision-making work to classical trajectory optimizers. While conceptually simple, this combination has a number of empirical shortcomings, suggesting that learned models may not be well-suited to standard trajectory optimization. In this paper, we consider what it would look like to fold as much of the trajectory optimization pipeline as possible into the modeling problem, such that sampling from the model and planning with it become nearly identical. The core of our technical approach lies in a diffusion probabilistic model that plans by iteratively denoising trajectories. We show how classifier-guided sampling and image inpainting can be reinterpreted as coherent planning strategies, explore the unusual and useful properties of diffusion-based planning methods, and demonstrate the effectiveness of our framework in control settings that emphasize long-horizon decision-making and test-time flexibility.",
                        "Citation Paper Authors": "Authors:Michael Janner, Yilun Du, Joshua B. Tenenbaum, Sergey Levine"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": ".\nWith the development of deep learning, Eysenbach et\nal. ",
                    "Citation Text": "Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and\nSergey Levine. Diversity is all you need: Learning skills\nwithout a reward function. arXiv preprint arXiv:1802.06070 ,\n2018. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.06070",
                        "Citation Paper Title": "Title:Diversity is All You Need: Learning Skills without a Reward Function",
                        "Citation Paper Abstract": "Abstract:Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose DIAYN ('Diversity is All You Need'), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning.",
                        "Citation Paper Authors": "Authors:Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, Sergey Levine"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "showcases few-\nshot policy generalization using task-specific prompts. Ad-\nditionally, diffusion models have been explored as planners\nin multi-task settings ",
                    "Citation Text": "Haoran He, Chenjia Bai, Kang Xu, Zhuoran Yang, Weinan\nZhang, Dong Wang, Bin Zhao, and Xuelong Li. Diffusion\nmodel is an effective planner and data synthesizer for multi-\ntask reinforcement learning. Advances in neural information\nprocessing systems , 2023. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.18459",
                        "Citation Paper Title": "Title:Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:Diffusion models have demonstrated highly-expressive generative capabilities in vision and NLP. Recent studies in reinforcement learning (RL) have shown that diffusion models are also powerful in modeling complex policies or trajectories in offline datasets. However, these works have been limited to single-task settings where a generalist agent capable of addressing multi-task predicaments is absent. In this paper, we aim to investigate the effectiveness of a single diffusion model in modeling large-scale multi-task offline data, which can be challenging due to diverse and multimodal data distribution. Specifically, we propose Multi-Task Diffusion Model (\\textsc{MTDiff}), a diffusion-based method that incorporates Transformer backbones and prompt learning for generative planning and data synthesis in multi-task offline settings. \\textsc{MTDiff} leverages vast amounts of knowledge available in multi-task data and performs implicit knowledge sharing among tasks. For generative planning, we find \\textsc{MTDiff} outperforms state-of-the-art algorithms across 50 tasks on Meta-World and 8 maps on Maze2D. For data synthesis, \\textsc{MTDiff} generates high-quality data for testing tasks given a single demonstration as a prompt, which enhances the low-quality datasets for even unseen tasks.",
                        "Citation Paper Authors": "Authors:Haoran He, Chenjia Bai, Kang Xu, Zhuoran Yang, Weinan Zhang, Dong Wang, Bin Zhao, Xuelong Li"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": ", where imitators are trained across varied\ntasks, aiming for generalization to new scenarios with task\nspecifications ranging from vector states ",
                    "Citation Text": "Ashvin V Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl,\nSteven Lin, and Sergey Levine. Visual reinforcement learn-\ning with imagined goals. Advances in neural information\nprocessing systems , 31, 2018. 2\n9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.04742",
                        "Citation Paper Title": "Title:Visual Reinforcement Learning with Imagined Goals",
                        "Citation Paper Abstract": "Abstract:For an autonomous agent to fulfill a wide range of user-specified goals at test time, it must be able to learn broadly applicable and general-purpose skill repertoires. Furthermore, to provide the requisite level of generality, these skills must handle raw sensory input such as images. In this paper, we propose an algorithm that acquires such general-purpose skills by combining unsupervised representation learning and reinforcement learning of goal-conditioned policies. Since the particular goals that might be required at test-time are not known in advance, the agent performs a self-supervised \"practice\" phase where it imagines goals and attempts to achieve them. We learn a visual representation with three distinct purposes: sampling goals for self-supervised practice, providing a structured transformation of raw sensory inputs, and computing a reward signal for goal reaching. We also propose a retroactive goal relabeling scheme to further improve the sample-efficiency of our method. Our off-policy algorithm is efficient enough to learn policies that operate on raw image observations and goals for a real-world robotic system, and substantially outperforms prior techniques.",
                        "Citation Paper Authors": "Authors:Ashvin Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, Sergey Levine"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "from expert data, enhancing the abil-\nity to mimic complex behaviors. A new challenge lies in\nmulti-task IL ",
                    "Citation Text": "Avi Singh, Eric Jang, Alexander Irpan, Daniel Kappler, Mur-\ntaza Dalal, Sergey Levinev, Mohi Khansari, and Chelsea\nFinn. Scalable multi-task imitation learning with au-\ntonomous improvement. In 2020 IEEE International Confer-\nence on Robotics and Automation (ICRA) , pages 2167\u20132173.\nIEEE, 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.02636",
                        "Citation Paper Title": "Title:Scalable Multi-Task Imitation Learning with Autonomous Improvement",
                        "Citation Paper Abstract": "Abstract:While robot learning has demonstrated promising results for enabling robots to automatically acquire new skills, a critical challenge in deploying learning-based systems is scale: acquiring enough data for the robot to effectively generalize broadly. Imitation learning, in particular, has remained a stable and powerful approach for robot learning, but critically relies on expert operators for data collection. In this work, we target this challenge, aiming to build an imitation learning system that can continuously improve through autonomous data collection, while simultaneously avoiding the explicit use of reinforcement learning, to maintain the stability, simplicity, and scalability of supervised imitation. To accomplish this, we cast the problem of imitation with autonomous improvement into a multi-task setting. We utilize the insight that, in a multi-task setting, a failed attempt at one task might represent a successful attempt at another task. This allows us to leverage the robot's own trials as demonstrations for tasks other than the one that the robot actually attempted. Using an initial dataset of multi-task demonstration data, the robot autonomously collects trials which are only sparsely labeled with a binary indication of whether the trial accomplished any useful task or not. We then embed the trials into a learned latent space of tasks, trained using only the initial demonstration dataset, to draw similarities between various trials, enabling the robot to achieve one-shot generalization to new tasks. In contrast to prior imitation learning approaches, our method can autonomously collect data with sparse supervision for continuous improvement, and in contrast to reinforcement learning algorithms, our method can effectively improve from sparse, task-agnostic reward signals.",
                        "Citation Paper Authors": "Authors:Avi Singh, Eric Jang, Alexander Irpan, Daniel Kappler, Murtaza Dalal, Sergey Levine, Mohi Khansari, Chelsea Finn"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.11384v1": {
            "Paper Title": "DiffTune-MPC: Closed-Loop Learning for Model Predictive Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11374v1": {
            "Paper Title": "Mastering Stacking of Diverse Shapes with Large-Scale Iterative\n  Reinforcement Learning on Real Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11306v1": {
            "Paper Title": "Physical design optimization for automated drug dispensing systems in a\n  human-machine interaction environment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11243v1": {
            "Paper Title": "GraspLDM: Generative 6-DoF Grasp Synthesis using Latent Diffusion Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11207v1": {
            "Paper Title": "Decentralized traffic management of autonomous drones",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11194v1": {
            "Paper Title": "Learning from Imperfect Demonstrations through Dynamics Evaluation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.11163v1": {
            "Paper Title": "Energy-Aware Hierarchical Control of Joint Velocities",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": "and we use the\nchain rule for the required terms \u02d9J=dJ\ndq\u02d9qand\n\u02d9W=dW\ndq\u02d9q. We use the algorithm in ",
                    "Citation Text": "Haviland, J., Corke, P.: A systematic\napproach to computing the manipulator\njacobian and hessian using the elementary\n14transform sequence. CoRR abs/2010.08696\n(2020) 2010.08696",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.08696",
                        "Citation Paper Title": "Title:A Systematic Approach to Computing the Manipulator Jacobian and Hessian using the Elementary Transform Sequence",
                        "Citation Paper Abstract": "Abstract:The elementary transform sequence (ETS) provides a universal method of describing the kinematics of any serial-link manipulator. The ETS notation is intuitive and easy to understand, while avoiding the complexity and limitations of Denvit-Hartenberg frame assignment. In this paper, we describe a systematic method for computing the manipulator Jacobian and Hessian (differential kinematics) using the ETS notation. Differential kinematics have many applications including numerical inverse kinematics, resolved-rate motion control and manipulability motion control. Furthermore, we provide an open-source Python library which implements our algorithm and can be interfaced with any serial-link manipulator (available at this http URL).",
                        "Citation Paper Authors": "Authors:Jesse Haviland, Peter Corke"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.11032v1": {
            "Paper Title": "Robot Crowd Navigation in Dynamic Environment with Offline Reinforcement\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10986v1": {
            "Paper Title": "Long-Tailed 3D Detection via 2D Late Fusion",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": "(published in 2020) annotates 144K RGB\nimages of 23 classes, while COCO ",
                    "Citation Text": "Tsung-Yi Lin, Michael Maire, Serge J. Belongie,\nJames Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll\u00b4ar, and C. Lawrence Zitnick. Microsoft COCO:\ncommon objects in context. In ECCV , 2014. 4, 5, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1405.0312",
                        "Citation Paper Title": "Title:Microsoft COCO: Common Objects in Context",
                        "Citation Paper Abstract": "Abstract:We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
                        "Citation Paper Authors": "Authors:Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Doll\u00e1r"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": ". YOLOV7 is a real-time detector that identifies\na number of training tricks that nearly doubles the inference\nefficiency over prior work without sacrificing performance.\nDINO is a recent transformer-based detector that improves\nupon DETR ",
                    "Citation Text": "Nicolas Carion, Francisco Massa, Gabriel Synnaeve,\nNicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with trans-\nformers. In ECCV , 2020. 2, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.12872",
                        "Citation Paper Title": "Title:End-to-End Object Detection with Transformers",
                        "Citation Paper Abstract": "Abstract:We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko"
                    }
                },
                {
                    "Sentence ID": 62,
                    "Sentence": ". Recent works\nshow that feature-fusion can be more effective than input-\nfusion. PointFusion ",
                    "Citation Text": "Danfei Xu, Dragomir Anguelov, and Ashesh Jain.\nPointfusion: Deep sensor fusion for 3d bounding box\nestimation. In IEEE conference on computer vision\nand pattern recognition , 2018. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.10871",
                        "Citation Paper Title": "Title:PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation",
                        "Citation Paper Abstract": "Abstract:We present PointFusion, a generic 3D object detection method that leverages both image and 3D point cloud information. Unlike existing methods that either use multi-stage pipelines or hold sensor and dataset-specific assumptions, PointFusion is conceptually simple and application-agnostic. The image data and the raw point cloud data are independently processed by a CNN and a PointNet architecture, respectively. The resulting outputs are then combined by a novel fusion network, which predicts multiple 3D box hypotheses and their confidences, using the input 3D points as spatial anchors. We evaluate PointFusion on two distinctive datasets: the KITTI dataset that features driving scenes captured with a lidar-camera setup, and the SUN-RGBD dataset that captures indoor environments with RGB-D cameras. Our model is the first one that is able to perform better or on-par with the state-of-the-art on these diverse datasets without any dataset-specific model tuning.",
                        "Citation Paper Authors": "Authors:Danfei Xu, Dragomir Anguelov, Ashesh Jain"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "leverage 2D RGB detections\nto regress 3D bounding boxes for LiDAR points within the\n2D detection frustum using PointNets ",
                    "Citation Text": "Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J\nGuibas. Pointnet: Deep learning on point sets for\n3d classification and segmentation. In Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition , pages 652\u2013660, 2017. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.00593",
                        "Citation Paper Title": "Title:PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation",
                        "Citation Paper Abstract": "Abstract:Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds and well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.",
                        "Citation Paper Authors": "Authors:Charles R. Qi, Hao Su, Kaichun Mo, Leonidas J. Guibas"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "densifies regions of LiDAR sweeps that correspond\nwith objects in semantic segmentation masks. Frustum\nPointNets ",
                    "Citation Text": "Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, and\nLeonidas J Guibas. Frustum pointnets for 3d object\ndetection from rgb-d data. In IEEE conference on\ncomputer vision and pattern recognition , 2018. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.08488",
                        "Citation Paper Title": "Title:Frustum PointNets for 3D Object Detection from RGB-D Data",
                        "Citation Paper Abstract": "Abstract:In this work, we study 3D object detection from RGB-D data in both indoor and outdoor scenes. While previous methods focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D data, we directly operate on raw point clouds by popping up RGB-D scans. However, a key challenge of this approach is how to efficiently localize objects in point clouds of large-scale scenes (region proposal). Instead of solely relying on 3D proposals, our method leverages both mature 2D object detectors and advanced 3D deep learning for object localization, achieving efficiency as well as high recall for even small objects. Benefited from learning directly in raw point clouds, our method is also able to precisely estimate 3D bounding boxes even under strong occlusion or with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection benchmarks, our method outperforms the state of the art by remarkable margins while having real-time capability.",
                        "Citation Paper Authors": "Authors:Charles R. Qi, Wei Liu, Chenxia Wu, Hao Su, Leonidas J. Guibas"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ". Monoc-\nular RGB detectors accurately classify objects but struggle\nto estimate depth, particularly for far-field detections ",
                    "Citation Text": "Shubham Gupta, Jeet Kanjani, Mengtian Li,\nFrancesco Ferroni, James Hays, Deva Ramanan,\nand Shu Kong. Far3det: Towards far-field 3d de-\ntection. In Proceedings of the IEEE/CVF Winter\nConference on Applications of Computer Vision ,\n2023. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2211.13858",
                        "Citation Paper Title": "Title:Far3Det: Towards Far-Field 3D Detection",
                        "Citation Paper Abstract": "Abstract:We focus on the task of far-field 3D detection (Far3Det) of objects beyond a certain distance from an observer, e.g., $>$50m. Far3Det is particularly important for autonomous vehicles (AVs) operating at highway speeds, which require detections of far-field obstacles to ensure sufficient braking distances. However, contemporary AV benchmarks such as nuScenes underemphasize this problem because they evaluate performance only up to a certain distance (50m). One reason is that obtaining far-field 3D annotations is difficult, particularly for lidar sensors that produce very few point returns for far-away objects. Indeed, we find that almost 50% of far-field objects (beyond 50m) contain zero lidar points. Secondly, current metrics for 3D detection employ a \"one-size-fits-all\" philosophy, using the same tolerance thresholds for near and far objects, inconsistent with tolerances for both human vision and stereo disparities. Both factors lead to an incomplete analysis of the Far3Det task. For example, while conventional wisdom tells us that high-resolution RGB sensors should be vital for 3D detection of far-away objects, lidar-based methods still rank higher compared to RGB counterparts on the current benchmark leaderboards. As a first step towards a Far3Det benchmark, we develop a method to find well-annotated scenes from the nuScenes dataset and derive a well-annotated far-field validation set. We also propose a Far3Det evaluation protocol and explore various 3D detection methods for Far3Det. Our result convincingly justifies the long-held conventional wisdom that high-resolution RGB improves 3D detection in the far-field. We further propose a simple yet effective method that fuses detections from RGB and lidar detectors based on non-maximum suppression, which remarkably outperforms state-of-the-art 3D detectors in the far-field.",
                        "Citation Paper Authors": "Authors:Shubham Gupta, Jeet Kanjani, Mengtian Li, Francesco Ferroni, James Hays, Deva Ramanan, Shu Kong"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "introduces a polar-coordinate transformation\nthat improves near-field detection. Importantly, many of\nthese state-of-the-art 3D RGB detectors are commonly pre-\ntrained on large external datasets like DDAD ",
                    "Citation Text": "Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan\nRaventos, and Adrien Gaidon. 3d packing for self-\nsupervised monocular depth estimation. In CVPR ,\n2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.02693",
                        "Citation Paper Title": "Title:3D Packing for Self-Supervised Monocular Depth Estimation",
                        "Citation Paper Abstract": "Abstract:Although cameras are ubiquitous, robotic platforms typically rely on active sensors like LiDAR for direct 3D perception. In this work, we propose a novel self-supervised monocular depth estimation method combining geometry with a new deep network, PackNet, learned only from unlabeled monocular videos. Our architecture leverages novel symmetrical packing and unpacking blocks to jointly learn to compress and decompress detail-preserving representations using 3D convolutions. Although self-supervised, our method outperforms other self, semi, and fully supervised methods on the KITTI benchmark. The 3D inductive bias in PackNet enables it to scale with input resolution and number of parameters without overfitting, generalizing better on out-of-domain data such as the NuScenes dataset. Furthermore, it does not require large-scale supervised pretraining on ImageNet and can run in real-time. Finally, we release DDAD (Dense Depth for Automated Driving), a new urban driving dataset with more challenging and accurate depth evaluation, thanks to longer-range and denser ground-truth depth generated from high-density LiDARs mounted on a fleet of self-driving cars operating world-wide.",
                        "Citation Paper Authors": "Authors:Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raventos, Adrien Gaidon"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "3D Detection for Autonomous Vehicles (A Vs) can be\nbroadly classified based on input modalities: LiDAR-only,\nRGB-only, and multi-modal detectors. Recent work in\n3D detection is inspired by prior work in 2D detection\n[5, 38, 72]. LiDAR-based detectors like PointPillars ",
                    "Citation Text": "Alex H. Lang, Sourabh V ora, Holger Caesar, Lubing\nZhou, Jiong Yang, and Oscar Beijbom. Pointpillars:\nFast encoders for object detection from point clouds.\nInCVPR , 2019. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.05784",
                        "Citation Paper Title": "Title:PointPillars: Fast Encoders for Object Detection from Point Clouds",
                        "Citation Paper Abstract": "Abstract:Object detection in point clouds is an important aspect of many robotics applications such as autonomous driving. In this paper we consider the problem of encoding a point cloud into a format appropriate for a downstream detection pipeline. Recent literature suggests two types of encoders; fixed encoders tend to be fast but sacrifice accuracy, while encoders that are learned from data are more accurate, but slower. In this work we propose PointPillars, a novel encoder which utilizes PointNets to learn a representation of point clouds organized in vertical columns (pillars). While the encoded features can be used with any standard 2D convolutional detection architecture, we further propose a lean downstream network. Extensive experimentation shows that PointPillars outperforms previous encoders with respect to both speed and accuracy by a large margin. Despite only using lidar, our full detection pipeline significantly outperforms the state of the art, even among fusion methods, with respect to both the 3D and bird's eye view KITTI benchmarks. This detection performance is achieved while running at 62 Hz: a 2 - 4 fold runtime improvement. A faster version of our method matches the state of the art at 105 Hz. These benchmarks suggest that PointPillars is an appropriate encoding for object detection in point clouds.",
                        "Citation Paper Authors": "Authors:Alex H. Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, Oscar Beijbom"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.10941v1": {
            "Paper Title": "Behaviour Description Database for AVs in Singapore",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10932v1": {
            "Paper Title": "A Shape Detection Framework for Deformation Objects Using Clustering\n  Algorithms",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10887v1": {
            "Paper Title": "On Computing Makespan-Optimal Solutions for Generalized Sliding-Tile\n  Puzzles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.08864v4": {
            "Paper Title": "Open X-Embodiment: Robotic Learning Datasets and RT-X Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10880v1": {
            "Paper Title": "Sharable Clothoid-based Continuous Motion Planning for Connected\n  Automated Vehicles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10807v1": {
            "Paper Title": "Language-conditioned Learning for Robotic Manipulation: A Survey",
            "Sentences": [
                {
                    "Sentence ID": 85,
                    "Sentence": ", are capable\nof zero-shot learning capabilities, strong commonsense un-\nderstanding, and contextual reasoning. Thus, more and more\nrecent studies have focused on LLM for robotic manipulation.\nFor example, Ahn et al. ",
                    "Citation Text": "M. Ahn, A. Brohan, N. Brown, Y . Chebotar, O. Cortes, B. David,\nC. Finn, C. Fu, K. Gopalakrishnan, K. Hausman, et al. , \u201cDo as i can,\nnot as i say: Grounding language in robotic affordances,\u201d arXiv preprint\narXiv:2204.01691 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.01691",
                        "Citation Paper Title": "Title:Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
                        "Citation Paper Abstract": "Abstract:Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's \"hands and eyes,\" while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at this https URL.",
                        "Citation Paper Authors": "Authors:Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, Andy Zeng"
                    }
                },
                {
                    "Sentence ID": 89,
                    "Sentence": ". For example, LLMs will plan an action involving\nobjects unavailable in the real environment.\n\u2022Natural language is highly ambiguous, especially in ex-\npressing spatial and geometrical relationships such as\n\u201cmoving faster\u201d or \u201cplacing objects slightly left\u201d ",
                    "Citation Text": "J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence,\nand A. Zeng, \u201cCode as policies: Language model programs for em-\nbodied control,\u201d in 2023 IEEE International Conference on Robotics\nand Automation (ICRA) . IEEE, 2023, pp. 9493\u20139500.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2209.07753",
                        "Citation Paper Title": "Title:Code as Policies: Language Model Programs for Embodied Control",
                        "Citation Paper Abstract": "Abstract:Large language models (LLMs) trained on code completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g.,from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (\"faster\") depending on context (i.e., behavioral commonsense). This paper presents code as policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8% of problems on the HumanEval [1] benchmark. Code and videos are available at this https URL",
                        "Citation Paper Authors": "Authors:Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.10797v1": {
            "Paper Title": "Large-Scale Multi-Robot Coverage Path Planning via Local Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.15190v2": {
            "Paper Title": "Fast Path Planning for Autonomous Vehicle Parking with Safety-Guarantee\n  using Hamilton-Jacobi Reachability",
            "Sentences": []
        },
        "http://arxiv.org/abs/2303.08815v2": {
            "Paper Title": "Lane Graph as Path: Continuity-preserving Path-wise Modeling for Online\n  Lane Graph Construction",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": "adopts a coarse-to-\nfine two-stage pipeline for vectorized HD map learning.\nMapTR ",
                    "Citation Text": "Bencheng Liao, Shaoyu Chen, Xinggang Wang, Tianheng\nCheng, Qian Zhang, Wenyu Liu, and Chang Huang. MapTR:\nStructured modeling and learning for online vectorized HD\nmap construction. In ICLR , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2208.14437",
                        "Citation Paper Title": "Title:MapTR: Structured Modeling and Learning for Online Vectorized HD Map Construction",
                        "Citation Paper Abstract": "Abstract:High-definition (HD) map provides abundant and precise environmental information of the driving scene, serving as a fundamental and indispensable component for planning in autonomous driving system. We present MapTR, a structured end-to-end Transformer for efficient online vectorized HD map construction. We propose a unified permutation-equivalent modeling approach, i.e., modeling map element as a point set with a group of equivalent permutations, which accurately describes the shape of map element and stabilizes the learning process. We design a hierarchical query embedding scheme to flexibly encode structured map information and perform hierarchical bipartite matching for map element learning. MapTR achieves the best performance and efficiency with only camera input among existing vectorized map construction approaches on nuScenes dataset. In particular, MapTR-nano runs at real-time inference speed ($25.1$ FPS) on RTX 3090, $8\\times$ faster than the existing state-of-the-art camera-based method while achieving $5.0$ higher mAP. Even compared with the existing state-of-the-art multi-modality method, MapTR-nano achieves $0.7$ higher mAP, and MapTR-tiny achieves $13.5$ higher mAP and $3\\times$ faster inference speed. Abundant qualitative results show that MapTR maintains stable and robust map construction quality in complex and various driving scenes. MapTR is of great application value in autonomous driving. Code and more demos are available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Bencheng Liao, Shaoyu Chen, Xinggang Wang, Tianheng Cheng, Qian Zhang, Wenyu Liu, Chang Huang"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "proposes a DETR-like decision-making trans-\nformer network to iteratively update the global lane graph\nwith vehicle-mounted sensors. Recently, STSU ",
                    "Citation Text": "Yigit Baran Can, Alexander Liniger, Danda Pani Paudel, and\nLuc Van Gool. Structured bird\u2019s-eye-view traffic scene un-\nderstanding from onboard images. In ICCV , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.01997",
                        "Citation Paper Title": "Title:Structured Bird's-Eye-View Traffic Scene Understanding from Onboard Images",
                        "Citation Paper Abstract": "Abstract:Autonomous navigation requires structured representation of the road network and instance-wise identification of the other traffic agents. Since the traffic scene is defined on the ground plane, this corresponds to scene understanding in the bird's-eye-view (BEV). However, the onboard cameras of autonomous cars are customarily mounted horizontally for a better view of the surrounding, making this task very challenging. In this work, we study the problem of extracting a directed graph representing the local road network in BEV coordinates, from a single onboard camera image. Moreover, we show that the method can be extended to detect dynamic objects on the BEV plane. The semantics, locations, and orientations of the detected objects together with the road graph facilitates a comprehensive understanding of the scene. Such understanding becomes fundamental for the downstream tasks, such as path planning and navigation. We validate our approach against powerful baselines and show that our network achieves superior performance. We also demonstrate the effects of various design choices through ablation studies. Code: this https URL",
                        "Citation Paper Authors": "Authors:Yigit Baran Can, Alexander Liniger, Danda Pani Paudel, Luc Van Gool"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "pro-\nposes a bottom-up approach to aggregate multiple local\naerial lane graphs into a global consistent graph. Center-\nlineDet ",
                    "Citation Text": "Zhenhua Xu, Yuxuan Liu, Yuxiang Sun, Ming Liu, and Lu-\njia Wang. Centerlinedet: Road lane centerline graph detec-\ntion with vehicle-mounted sensors by transformer for high-\ndefinition map creation. arXiv preprint arXiv:2209.07734 ,\n2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2209.07734",
                        "Citation Paper Title": "Title:CenterLineDet: CenterLine Graph Detection for Road Lanes with Vehicle-mounted Sensors by Transformer for HD Map Generation",
                        "Citation Paper Abstract": "Abstract:With the fast development of autonomous driving technologies, there is an increasing demand for high-definition (HD) maps, which provide reliable and robust prior information about the static part of the traffic environments. As one of the important elements in HD maps, road lane centerline is critical for downstream tasks, such as prediction and planning. Manually annotating centerlines for road lanes in HD maps is labor-intensive, expensive and inefficient, severely restricting the wide applications of autonomous driving systems. Previous work seldom explores the lane centerline detection problem due to the complicated topology and severe overlapping issues of lane centerlines. In this paper, we propose a novel method named CenterLineDet to detect lane centerlines for automatic HD map generation. Our CenterLineDet is trained by imitation learning and can effectively detect the graph of centerlines with vehicle-mounted sensors (i.e., six cameras and one LiDAR) through iterations. Due to the use of the DETR-like transformer network, CenterLineDet can handle complicated graph topology, such as lane intersections. The proposed approach is evaluated on the large-scale public dataset NuScenes. The superiority of our CenterLineDet is demonstrated by the comparative results. Our code, supplementary materials, and video demonstrations are available at \\href{this https URL}{this https URL}.",
                        "Citation Paper Authors": "Authors:Zhenhua Xu, Yuxuan Liu, Yuxiang Sun, Ming Liu, Lujia Wang"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "proposes unified permutation-equivalent mod-\neling to exploit the undirected nature of semantic HD map\nand designs a parallel end-to-end framework. BeMap-\nNet ",
                    "Citation Text": "Limeng Qiao, Wenjie Ding, Xi Qiu, and Chi Zhang. End-\nto-end vectorized hd-map construction with piecewise bezier\ncurve. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , pages 13218\u201313228,\n2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2306.09700",
                        "Citation Paper Title": "Title:End-to-End Vectorized HD-map Construction with Piecewise Bezier Curve",
                        "Citation Paper Abstract": "Abstract:Vectorized high-definition map (HD-map) construction, which focuses on the perception of centimeter-level environmental information, has attracted significant research interest in the autonomous driving community. Most existing approaches first obtain rasterized map with the segmentation-based pipeline and then conduct heavy post-processing for downstream-friendly vectorization. In this paper, by delving into parameterization-based methods, we pioneer a concise and elegant scheme that adopts unified piecewise Bezier curve. In order to vectorize changeful map elements end-to-end, we elaborate a simple yet effective architecture, named Piecewise Bezier HD-map Network (BeMapNet), which is formulated as a direct set prediction paradigm and postprocessing-free. Concretely, we first introduce a novel IPM-PE Align module to inject 3D geometry prior into BEV features through common position encoding in Transformer. Then a well-designed Piecewise Bezier Head is proposed to output the details of each map element, including the coordinate of control points and the segment number of curves. In addition, based on the progressively restoration of Bezier curve, we also present an efficient Point-Curve-Region Loss for supervising more robust and precise HD-map modeling. Extensive comparisons show that our method is remarkably superior to other existing SOTAs by 18.0 mAP at least.",
                        "Citation Paper Authors": "Authors:Limeng Qiao, Wenjie Ding, Xi Qiu, Chi Zhang"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "follows a segmentation-\nthen-vectorization paradigm. To achieve end-to-end learn-\ning [8, 47, 13], VectorMapNet ",
                    "Citation Text": "Yicheng Liu, Yue Wang, Yilun Wang, and Hang Zhao. Vec-\ntormapnet: End-to-end vectorized hd map learning. arXiv\npreprint arXiv:2206.08920 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.08920",
                        "Citation Paper Title": "Title:VectorMapNet: End-to-end Vectorized HD Map Learning",
                        "Citation Paper Abstract": "Abstract:Autonomous driving systems require High-Definition (HD) semantic maps to navigate around urban roads. Existing solutions approach the semantic mapping problem by offline manual annotation, which suffers from serious scalability issues. Recent learning-based methods produce dense rasterized segmentation predictions to construct maps. However, these predictions do not include instance information of individual map elements and require heuristic post-processing to obtain vectorized maps. To tackle these challenges, we introduce an end-to-end vectorized HD map learning pipeline, termed VectorMapNet. VectorMapNet takes onboard sensor observations and predicts a sparse set of polylines in the bird's-eye view. This pipeline can explicitly model the spatial relation between map elements and generate vectorized maps that are friendly to downstream autonomous driving tasks. Extensive experiments show that VectorMapNet achieve strong map learning performance on both nuScenes and Argoverse2 dataset, surpassing previous state-of-the-art methods by 14.2 mAP and 14.6mAP. Qualitatively, VectorMapNet is capable of generating comprehensive maps and capturing fine-grained details of road geometry. To the best of our knowledge, VectorMapNet is the first work designed towards end-to-end vectorized map learning from onboard observations. Our project website is available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Yicheng Liu, Tianyuan Yuan, Yue Wang, Yilun Wang, Hang Zhao"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "uses a fully convolutional network to predict\nBezier lanes defined with 4 Bezier control points. Pers-\nFormer ",
                    "Citation Text": "Li Chen, Chonghao Sima, Yang Li, Zehan Zheng, Jiajie Xu,\nXiangwei Geng, Hongyang Li, Conghui He, Jianping Shi, Yu\nQiao, and Junchi Yan. Persformer: 3d lane detection via per-\nspective transformer and the openlane benchmark. In ECCV ,\n2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.11089",
                        "Citation Paper Title": "Title:PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark",
                        "Citation Paper Abstract": "Abstract:Methods for 3D lane detection have been recently proposed to address the issue of inaccurate lane layouts in many autonomous driving scenarios (uphill/downhill, bump, etc.). Previous work struggled in complex cases due to their simple designs of the spatial transformation between front view and bird's eye view (BEV) and the lack of a realistic dataset. Towards these issues, we present PersFormer: an end-to-end monocular 3D lane detector with a novel Transformer-based spatial feature transformation module. Our model generates BEV features by attending to related front-view local regions with camera parameters as a reference. PersFormer adopts a unified 2D/3D anchor design and an auxiliary task to detect 2D/3D lanes simultaneously, enhancing the feature consistency and sharing the benefits of multi-task learning. Moreover, we release one of the first large-scale real-world 3D lane datasets: OpenLane, with high-quality annotation and scenario diversity. OpenLane contains 200,000 frames, over 880,000 instance-level lanes, 14 lane categories, along with scene tags and the closed-in-path object annotations to encourage the development of lane detection and more industrial-related autonomous driving methods. We show that PersFormer significantly outperforms competitive baselines in the 3D lane detection task on our new OpenLane dataset as well as Apollo 3D Lane Synthetic dataset, and is also on par with state-of-the-art algorithms in the 2D task on OpenLane. The project page is available at this https URL and OpenLane dataset is provided at this https URL.",
                        "Citation Paper Authors": "Authors:Li Chen, Chonghao Sima, Yang Li, Zehan Zheng, Jiajie Xu, Xiangwei Geng, Hongyang Li, Conghui He, Jianping Shi, Yu Qiao, Junchi Yan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2304.01168v5": {
            "Paper Title": "DeepAccident: A Motion and Accident Prediction Benchmark for V2X\n  Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10655v1": {
            "Paper Title": "Practical Non-Intrusive GUI Exploration Testing with Visual-based\n  Robotic Arms",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": "is an effective method for Android testing with\nreinforcement learning, it trains a neural network to divide differ-\nent states at the granularity of functional scenarios. Romdhana et\nal. ",
                    "Citation Text": "Andrea Romdhana, Alessio Merlo, Mariano Ceccato, and Paolo Tonella. 2022.\nDeep reinforcement learning for black-box testing of android apps. ACM Trans-\nactions on Software Engineering and Methodology (TOSEM) 31, 4 (2022), 1\u201329.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.02636",
                        "Citation Paper Title": "Title:Deep Reinforcement Learning for Black-Box Testing of Android Apps",
                        "Citation Paper Abstract": "Abstract:The state space of Android apps is huge and its thorough exploration during testing remains a major challenge. In fact, the best exploration strategy is highly dependent on the features of the app under test. Reinforcement Learning (RL) is a machine learning technique that learns the optimal strategy to solve a task by trial and error, guided by positive or negative reward, rather than by explicit supervision. Deep RL is a recent extension of RL that takes advantage of the learning capabilities of neural networks. Such capabilities make Deep RL suitable for complex exploration spaces such as the one of Android apps. However, state of the art, publicly available tools only support basic, tabular RL. We have developed ARES, a Deep RL approach for black-box testing of Android apps. Experimental results show that it achieves higher coverage and fault revelation than the baselines, which include state of the art RL based tools, such as TimeMachine and Q-Testing. We also investigated qualitatively the reasons behind such performance and we have identified the key features of Android apps that make Deep RL particularly effective on them to be the presence of chained and blocking activities.",
                        "Citation Paper Authors": "Authors:Andrea Romdhana, Alessio Merlo, Mariano Ceccato, Paolo Tonella"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "propose that visual GUI\ntesting is an emerging technology in industrial practice that offers\ngreater flexibility and robustness to certain GUI changes than pre-\nvious high-level GUI test automation techniques. Chen et al. ",
                    "Citation Text": "Jieshan Chen, Mulong Xie, Zhenchang Xing, Chunyang Chen, Xiwei Xu, Liming\nZhu, and Guoqiang Li. 2020. Object detection for graphical user interface: Old\nfashioned or deep learning or a combination?. In proceedings of the 28th ACMICSE 2024, April 2024, Lisbon, Portugal Shengcheng and Chunrong et al.\njoint meeting on European Software Engineering Conference and Symposium on\nthe Foundations of Software Engineering . 1202\u20131214.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.05132",
                        "Citation Paper Title": "Title:Object Detection for Graphical User Interface: Old Fashioned or Deep Learning or a Combination?",
                        "Citation Paper Abstract": "Abstract:Detecting Graphical User Interface (GUI) elements in GUI images is a domain-specific object detection task. It supports many software engineering tasks, such as GUI animation and testing, GUI search and code generation. Existing studies for GUI element detection directly borrow the mature methods from computer vision (CV) domain, including old fashioned ones that rely on traditional image processing features (e.g., canny edge, contours), and deep learning models that learn to detect from large-scale GUI data. Unfortunately, these CV methods are not originally designed with the awareness of the unique characteristics of GUIs and GUI elements and the high localization accuracy of the GUI element detection task. We conduct the first large-scale empirical study of seven representative GUI element detection methods on over 50k GUI images to understand the capabilities, limitations and effective designs of these methods. This study not only sheds the light on the technical challenges to be addressed but also informs the design of new GUI element detection methods. We accordingly design a new GUI-specific old-fashioned method for non-text GUI element detection which adopts a novel top-down coarse-to-fine strategy, and incorporate it with the mature deep learning model for GUI text detection.Our evaluation on 25,000 GUI images shows that our method significantly advances the start-of-the-art performance in GUI element detection.",
                        "Citation Paper Authors": "Authors:Jieshan Chen, Mulong Xie, Zhenchang Xing, Chunyang Chen, Xiwei Xu, Liming Zhu, Guoqiang Li"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.10647v1": {
            "Paper Title": "Single-Stage Optimization of Open-loop Stable Limit Cycles with Smooth,\n  Symbolic Derivatives",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.19629v2": {
            "Paper Title": "RayDF: Neural Ray-surface Distance Fields with Multi-view Consistency",
            "Sentences": [
                {
                    "Sentence ID": 73,
                    "Sentence": ", 2) the scene-level synthetic DM-SR dataset from the\nrecent DM-NeRF paper ",
                    "Citation Text": "B. Wang, L. Chen, and B. Yang. DM-NeRF: 3D Scene Geometry Decomposition and Manipulation from\n2D Images. ICLR , 2023. 1, 5, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2208.07227",
                        "Citation Paper Title": "Title:DM-NeRF: 3D Scene Geometry Decomposition and Manipulation from 2D Images",
                        "Citation Paper Abstract": "Abstract:In this paper, we study the problem of 3D scene geometry decomposition and manipulation from 2D views. By leveraging the recent implicit neural representation techniques, particularly the appealing neural radiance fields, we introduce an object field component to learn unique codes for all individual objects in 3D space only from 2D supervision. The key to this component is a series of carefully designed loss functions to enable every 3D point, especially in non-occupied space, to be effectively optimized even without 3D labels. In addition, we introduce an inverse query algorithm to freely manipulate any specified 3D object shape in the learned scene representation. Notably, our manipulation algorithm can explicitly tackle key issues such as object collisions and visual occlusions. Our method, called DM-NeRF, is among the first to simultaneously reconstruct, decompose, manipulate and render complex 3D scenes in a single pipeline. Extensive experiments on three datasets clearly show that our method can accurately decompose all 3D objects from 2D views, allowing any interested object to be freely manipulated in 3D space such as translation, rotation, size adjustment, and deformation.",
                        "Citation Paper Authors": "Authors:Bing Wang, Lu Chen, Bo Yang"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": ". A comprehensive survey of these methods can be found in [ 6;28]. Thanks\nto the large-scale datasets ",
                    "Citation Text": "A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song,\nH. Su, J. Xiao, L. Yi, and F. Yu. ShapeNet: An Information-Rich 3D Model Repository. arXiv:1512.03012 ,\n2015. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.03012",
                        "Citation Paper Title": "Title:ShapeNet: An Information-Rich 3D Model Repository",
                        "Citation Paper Abstract": "Abstract:We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.",
                        "Citation Paper Authors": "Authors:Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, Fisher Yu"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": "Explicit 3D Shape Representations: Classic methods to recover explicit 3D geometry of objects\nand scenes mainly include SfM ",
                    "Citation Text": "O. Ozyesil, V . V oroninski, R. Basri, and A. Singer. A Survey of Structure from Motion. Acta Numerica ,\n2017. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1701.08493",
                        "Citation Paper Title": "Title:A Survey of Structure from Motion",
                        "Citation Paper Abstract": "Abstract:The structure from motion (SfM) problem in computer vision is the problem of recovering the three-dimensional ($3$D) structure of a stationary scene from a set of projective measurements, represented as a collection of two-dimensional ($2$D) images, via estimation of motion of the cameras corresponding to these images. In essence, SfM involves the three main stages of (1) extraction of features in images (e.g., points of interest, lines, etc.) and matching these features between images, (2) camera motion estimation (e.g., using relative pairwise camera positions estimated from the extracted features), and (3) recovery of the $3$D structure using the estimated motion and features (e.g., by minimizing the so-called reprojection error). This survey mainly focuses on relatively recent developments in the literature pertaining to stages (2) and (3). More specifically, after touching upon the early factorization-based techniques for motion and structure estimation, we provide a detailed account of some of the recent camera location estimation methods in the literature, followed by discussion of notable techniques for $3$D structure recovery. We also cover the basics of the simultaneous localization and mapping (SLAM) problem, which can be viewed as a specific case of the SfM problem. Further, our survey includes a review of the fundamentals of feature extraction and matching (i.e., stage (1) above), various recent methods for handling ambiguities in $3$D scenes, SfM techniques involving relatively uncommon camera models and image features, and popular sources of data and SfM software.",
                        "Citation Paper Authors": "Authors:Onur Ozyesil, Vladislav Voroninski, Ronen Basri, Amit Singer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.10571v1": {
            "Paper Title": "Multi-level Reasoning for Robotic Assembly: From Sequence Inference to\n  Contact Selection",
            "Sentences": [
                {
                    "Sentence ID": 39,
                    "Sentence": ". Second, we apply\npart-centric disassembly forces aligned with the parts\u2019 inertia\naxes, as supported by findings in ",
                    "Citation Text": "K. D. D. Willis, Y . Pu, J. Luo, H. Chu, T. Du, J. G. Lambourne,\nA. Solar-Lezama, and W. Matusik, \u201cFusion 360 gallery: A dataset and\nenvironment for programmatic cad construction from human design\nsequences,\u201d ACM Transactions on Graphics , vol. 40, no. 4, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.02392",
                        "Citation Paper Title": "Title:Fusion 360 Gallery: A Dataset and Environment for Programmatic CAD Construction from Human Design Sequences",
                        "Citation Paper Abstract": "Abstract:Parametric computer-aided design (CAD) is a standard paradigm used to design manufactured objects, where a 3D shape is represented as a program supported by the CAD software. Despite the pervasiveness of parametric CAD and a growing interest from the research community, currently there does not exist a dataset of realistic CAD models in a concise programmatic form. In this paper we present the Fusion 360 Gallery, consisting of a simple language with just the sketch and extrude modeling operations, and a dataset of 8,625 human design sequences expressed in this language. We also present an interactive environment called the Fusion 360 Gym, which exposes the sequential construction of a CAD program as a Markov decision process, making it amendable to machine learning approaches. As a use case for our dataset and environment, we define the CAD reconstruction task of recovering a CAD program from a target geometry. We report results of applying state-of-the-art methods of program synthesis with neurally guided search on this task.",
                        "Citation Paper Authors": "Authors:Karl D.D. Willis, Yewen Pu, Jieliang Luo, Hang Chu, Tao Du, Joseph G. Lambourne, Armando Solar-Lezama, Wojciech Matusik"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.10557v1": {
            "Paper Title": "Improving Environment Robustness of Deep Reinforcement Learning\n  Approaches for Autonomous Racing Using Bayesian Optimization-based Curriculum\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10525v1": {
            "Paper Title": "Runtime Architecture and Task Plan Co-Adaptation for Autonomous Robots\n  with Metaplan",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10419v1": {
            "Paper Title": "A Survey on Robotic Manipulation of Deformable Objects: Recent Advances,\n  Open Challenges and New Frontiers",
            "Sentences": [
                {
                    "Sentence ID": 154,
                    "Sentence": ". Compositional gen-\neralization for subskills is a challenging problem that requires\nfurther study.\nV. D ISCUSSION OF LARGE LANGUAGES MODELS AND\nOUTLOOKS\nLLMs recently have achieved impressive performance on\nvarious natural language processing (NLP) tasks ",
                    "Citation Text": "J. Yang, H. Jin, R. Tang, X. Han, Q. Feng, H. Jiang, B. Yin, and X. Hu,\n\u201cHarnessing the power of llms in practice: A survey on chatgpt and\nbeyond,\u201d arXiv preprint arXiv:2304.13712 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.13712",
                        "Citation Paper Title": "Title:Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond",
                        "Citation Paper Abstract": "Abstract:This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. Firstly, we offer an introduction and brief summary of current GPT- and BERT-style LLMs. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, natural language generation tasks, emergent abilities, and considerations for specific tasks.We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on LLMs and delve into other essential considerations, such as efficiency, cost, and latency, to ensure a comprehensive understanding of deploying LLMs in practice. This comprehensive guide aims to provide researchers and practitioners with valuable insights and best practices for working with LLMs, thereby enabling the successful implementation of these models in a wide range of NLP tasks. A curated list of practical guide resources of LLMs, regularly updated, can be found at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, Xia Hu"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": ". Recent research has\nmade it possible to plan actions based on current observations.\nThis is achieved by collecting and annotating a large amount of\ndemonstration data and extracting features ",
                    "Citation Text": "Y . Avigal, L. Berscheid, T. Asfour, T. Kr \u00a8oger, and K. Goldberg,\n\u201cSpeedfolding: Learning efficient bimanual folding of garments,\u201d in\n2022 IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS) . IEEE, 2022, pp. 1\u20138.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2208.10552",
                        "Citation Paper Title": "Title:SpeedFolding: Learning Efficient Bimanual Folding of Garments",
                        "Citation Paper Abstract": "Abstract:Folding garments reliably and efficiently is a long standing challenge in robotic manipulation due to the complex dynamics and high dimensional configuration space of garments. An intuitive approach is to initially manipulate the garment to a canonical smooth configuration before folding. In this work, we develop SpeedFolding, a reliable and efficient bimanual system, which given user-defined instructions as folding lines, manipulates an initially crumpled garment to (1) a smoothed and (2) a folded configuration. Our primary contribution is a novel neural network architecture that is able to predict pairs of gripper poses to parameterize a diverse set of bimanual action primitives. After learning from 4300 human-annotated and self-supervised actions, the robot is able to fold garments from a random initial configuration in under 120s on average with a success rate of 93%. Real-world experiments show that the system is able to generalize to unseen garments of different color, shape, and stiffness. While prior work achieved 3-6 Folds Per Hour (FPH), SpeedFolding achieves 30-40 FPH.",
                        "Citation Paper Authors": "Authors:Yahav Avigal, Lars Berscheid, Tamim Asfour, Torsten Kr\u00f6ger, Ken Goldberg"
                    }
                },
                {
                    "Sentence ID": 127,
                    "Sentence": ". A novel model-based RL system\nthat incorporates sensing information, named SAM-RL, was\ndeveloped by Lv et al. ",
                    "Citation Text": "J. Lv, Y . Feng, C. Zhang, S. Zhao, L. Shao, and C. Lu, \u201cSam-\nrl: Sensing-aware model-based reinforcement learning via differ-\nentiable physics-based simulation and rendering,\u201d arXiv preprint\narXiv:2210.15185 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.15185",
                        "Citation Paper Title": "Title:SAM-RL: Sensing-Aware Model-Based Reinforcement Learning via Differentiable Physics-Based Simulation and Rendering",
                        "Citation Paper Abstract": "Abstract:Model-based reinforcement learning (MBRL) is recognized with the potential to be significantly more sample-efficient than model-free RL. How an accurate model can be developed automatically and efficiently from raw sensory inputs (such as images), especially for complex environments and tasks, is a challenging problem that hinders the broad application of MBRL in the real world. In this work, we propose a sensing-aware model-based reinforcement learning system called SAM-RL. Leveraging the differentiable physics-based simulation and rendering, SAM-RL automatically updates the model by comparing rendered images with real raw images and produces the policy efficiently. With the sensing-aware learning pipeline, SAM-RL allows a robot to select an informative viewpoint to monitor the task process. We apply our framework to real world experiments for accomplishing three manipulation tasks: robotic assembly, tool manipulation, and deformable object manipulation. We demonstrate the effectiveness of SAM-RL via extensive experiments. Videos are available on our project webpage at this https URL.",
                        "Citation Paper Authors": "Authors:Jun Lv, Yunhai Feng, Cheng Zhang, Shuang Zhao, Lin Shao, Cewu Lu"
                    }
                },
                {
                    "Sentence ID": 81,
                    "Sentence": ". A Jacobian matrix is fully determined by the DOs\u2019 current state x.\nFig. 7. One-step prediction through GNN-based Model (Message-passing GNN type for manipulating a DLO as an example ",
                    "Citation Text": "Y . Li, J. Wu, J.-Y . Zhu, J. B. Tenenbaum, A. Torralba, and R. Tedrake,\n\u201cPropagation networks for model-based control under partial observa-\ntion,\u201d in 2019 International Conference on Robotics and Automation\n(ICRA) . IEEE, 2019, pp. 1205\u20131211.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.11169",
                        "Citation Paper Title": "Title:Propagation Networks for Model-Based Control Under Partial Observation",
                        "Citation Paper Abstract": "Abstract:There has been an increasing interest in learning dynamics simulators for model-based control. Compared with off-the-shelf physics engines, a learnable simulator can quickly adapt to unseen objects, scenes, and tasks. However, existing models like interaction networks only work for fully observable systems; they also only consider pairwise interactions within a single time step, both restricting their use in practical systems. We introduce Propagation Networks (PropNet), a differentiable, learnable dynamics model that handles partially observable scenarios and enables instantaneous propagation of signals beyond pairwise interactions. Experiments show that our propagation networks not only outperform current learnable physics engines in forward simulation, but also achieve superior performance on various control tasks. Compared with existing model-free deep reinforcement learning algorithms, model-based control with propagation networks is more accurate, efficient, and generalizable to new, partially observable scenes and tasks.",
                        "Citation Paper Authors": "Authors:Yunzhu Li, Jiajun Wu, Jun-Yan Zhu, Joshua B. Tenenbaum, Antonio Torralba, Russ Tedrake"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.10339v1": {
            "Paper Title": "Model-free Learning of Corridor Clearance: A Near-term Deployment\n  Perspective",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": "optimizes the above objective by sampling\ntrajectories ( s0, a0,\u00b7\u00b7\u00b7, sT\u22121, aT\u22121, sT) and following the\nupdate rule,\n\u03b8\u2190\u03b8+\u03b2T\u22121X\nt=0\u2207\u03b8log\u03c0\u03b8(st)T\u22121X\nt\u2032=t\u03b3t\u2032\u2212tr(st\u2032, at\u2032, st\u2032+1)(2)\nwhere \u03b2is the learning rate. We use the Proximal Policy\nOptimization (PPO) algorithm ",
                    "Citation Text": "J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,\n\u201cProximal policy optimization algorithms,\u201d ArXiv , vol. abs/1707.06347,\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.06347",
                        "Citation Paper Title": "Title:Proximal Policy Optimization Algorithms",
                        "Citation Paper Abstract": "Abstract:We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.",
                        "Citation Paper Authors": "Authors:John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.08684v2": {
            "Paper Title": "Stein-MAP: A Sequential Variational Inference Framework for Maximum A\n  Posteriori Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10309v1": {
            "Paper Title": "Enabling Mammography with Co-Robotic Ultrasound",
            "Sentences": []
        },
        "http://arxiv.org/abs/2308.14277v2": {
            "Paper Title": "9DTact: A Compact Vision-Based Tactile Sensor for Accurate 3D Shape\n  Reconstruction and Generalizable 6D Force Estimation",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": "as the neural\nnetwork for predicting the 6D force. We utilize the implemen-\ntation of Densenet from PyTorch library ",
                    "Citation Text": "A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al. , \u201cPytorch: An\nimperative style, high-performance deep learning library,\u201d Advances in\nneural information processing systems , vol. 32, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.01703",
                        "Citation Paper Title": "Title:PyTorch: An Imperative Style, High-Performance Deep Learning Library",
                        "Citation Paper Abstract": "Abstract:Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.\nIn this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.\nWe demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.",
                        "Citation Paper Authors": "Authors:Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "80\u00d740\u00d740 = 128000 252 508 NA 90 30\nGelSlim 3.0 ",
                    "Citation Text": "I. H. Taylor, S. Dong, and A. Rodriguez, \u201cGelslim 3.0: High-resolution\nmeasurement of shape, force and slip in a compact tactile-sensing\nfinger,\u201d in 2022 International Conference on Robotics and Automation\n(ICRA) . IEEE, 2022, pp. 10 781\u201310 787.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.12269",
                        "Citation Paper Title": "Title:GelSlim3.0: High-Resolution Measurement of Shape, Force and Slip in a Compact Tactile-Sensing Finger",
                        "Citation Paper Abstract": "Abstract:This work presents a new version of the tactile-sensing finger GelSlim 3.0, which integrates the ability to sense high-resolution shape, force, and slip in a compact form factor for use with small parallel jaw grippers in cluttered bin-picking scenarios. The novel design incorporates the capability to use real-time analytic methods to measure shape, estimate the contact 3D force distribution, and detect incipient slip. To achieve a compact integration, we optimize the optical path from illumination source to camera and other geometric variables in a optical simulation environment. In particular, we optimize the illumination sources and a light shaping lens around the constraints imposed by the photometric stereo algorithm used for depth reconstruction. The optimized optical configuration is integrated into a finger design composed of robust and easily replaceable snap-to-fit fingetip module that allow for ease of manufacture, assembly, use, and repair. To stimulate future research in tactile-sensing and provide the robotics community access to reliable and easily-reproducible tactile finger with a diversity of sensing modalities, we open-source the design and software at this https URL.",
                        "Citation Paper Authors": "Authors:Ian Taylor, Siyuan Dong, Alberto Rodriguez"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.08782v2": {
            "Paper Title": "Toward General-Purpose Robots via Foundation Models: A Survey and\n  Meta-Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.10008v1": {
            "Paper Title": "Movement Primitive Diffusion: Learning Gentle Robotic Manipulation of\n  Deformable Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.04787v2": {
            "Paper Title": "HI-SLAM: Monocular Real-time Dense Mapping with Hybrid Implicit Fields",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": "begins to incorporate this representation into the\nSLAM system to perform joint neural scene and pose opti-\nmization. Later, many follow-up methods ",
                    "Citation Text": "Z. Zhu, S. Peng, V . Larsson, W. Xu, H. Bao, Z. Cui, M. R. Oswald, and\nM. Pollefeys, \u201cNice-slam: Neural implicit scalable encoding for slam,\u201d\ninProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , 2022, pp. 12 786\u201312 796.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.12130",
                        "Citation Paper Title": "Title:NICE-SLAM: Neural Implicit Scalable Encoding for SLAM",
                        "Citation Paper Abstract": "Abstract:Neural implicit representations have recently shown encouraging results in various domains, including promising progress in simultaneous localization and mapping (SLAM). Nevertheless, existing methods produce over-smoothed scene reconstructions and have difficulty scaling up to large scenes. These limitations are mainly due to their simple fully-connected network architecture that does not incorporate local information in the observations. In this paper, we present NICE-SLAM, a dense SLAM system that incorporates multi-level local information by introducing a hierarchical scene representation. Optimizing this representation with pre-trained geometric priors enables detailed reconstruction on large indoor scenes. Compared to recent neural implicit SLAM systems, our approach is more scalable, efficient, and robust. Experiments on five challenging datasets demonstrate competitive results of NICE-SLAM in both mapping and tracking quality. Project page: this https URL",
                        "Citation Paper Authors": "Authors:Zihan Zhu, Songyou Peng, Viktor Larsson, Weiwei Xu, Hujun Bao, Zhaopeng Cui, Martin R. Oswald, Marc Pollefeys"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "denoted as h\u03b8hash with\noptimizable parameters \u03b8hash. For a sample point x, features\nat every resolution are looked up via tri-linear interpolation\nand concatenated together. This yields a coarse-to-fine feature\nencoding. To enhance the scene completion capability, inspired\nby ",
                    "Citation Text": "B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi,\nand R. Ng, \u201cNerf: Representing scenes as neural radiance fields for view\nsynthesis,\u201d in ECCV , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "to obtain poses and sparse point clouds, which are\nleveraged to regularize neural field learning. NeRF-SLAM ",
                    "Citation Text": "A. Rosinol, J. J. Leonard, and L. Carlone, \u201cNerf-slam: Real-time\ndense monocular slam with neural radiance fields,\u201d arXiv preprint\narXiv:2210.13641 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.13641",
                        "Citation Paper Title": "Title:NeRF-SLAM: Real-Time Dense Monocular SLAM with Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:We propose a novel geometric and photometric 3D mapping pipeline for accurate and real-time scene reconstruction from monocular images. To achieve this, we leverage recent advances in dense monocular SLAM and real-time hierarchical volumetric neural radiance fields. Our insight is that dense monocular SLAM provides the right information to fit a neural radiance field of the scene in real-time, by providing accurate pose estimates and depth-maps with associated uncertainty. With our proposed uncertainty-based depth loss, we achieve not only good photometric accuracy, but also great geometric accuracy. In fact, our proposed pipeline achieves better geometric and photometric accuracy than competing approaches (up to 179% better PSNR and 86% better L1 depth), while working in real-time and using only monocular images.",
                        "Citation Paper Authors": "Authors:Antoni Rosinol, John J. Leonard, Luca Carlone"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.09906v1": {
            "Paper Title": "Sample-Efficient Learning to Solve a Real-World Labyrinth Game Using\n  Data-Augmented Model-Based Reinforcement Learning",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": "In recent years, deep RL has excelled at attaining super-\nhuman performance amongst a variety of different games\nsuch as chess, Go, and shogi ",
                    "Citation Text": "J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre,\nS. Schmitt, A. Guez, E. Lockhart, D. Hassabis, T. Graepel, et al. ,\n\u201cMastering atari, go, chess and shogi by planning with a learned\nmodel,\u201d Nature , vol. 588, no. 7839, pp. 604\u2013609, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.08265",
                        "Citation Paper Title": "Title:Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",
                        "Citation Paper Abstract": "Abstract:Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.",
                        "Citation Paper Authors": "Authors:Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillicrap, David Silver"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.09822v1": {
            "Paper Title": "SeeThruFinger: See and Grasp Anything with a Soft Touch",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": ", to generate the occluded scene and\nobjects for visual perception. Then, we performed object detection using the inpainted image, i.e., Real-Time Detection\nTransformer (RT-DETR) ",
                    "Citation Text": "Wenyu Lv, Shangliang Xu, Yian Zhao, Guanzhong Wang, Jinman Wei, Cheng Cui, Yuning Du, Qingqing Dang,\nand Yi Liu. DETRs Beat YOLOs on Real-time Object Detection. 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.08069",
                        "Citation Paper Title": "Title:DETRs Beat YOLOs on Real-time Object Detection",
                        "Citation Paper Abstract": "Abstract:Recently, end-to-end transformer-based detectors~(DETRs) have achieved remarkable performance. However, the issue of the high computational cost of DETRs has not been effectively addressed, limiting their practical application and preventing them from fully exploiting the benefits of no post-processing, such as non-maximum suppression (NMS). In this paper, we first analyze the influence of NMS in modern real-time object detectors on inference speed, and establish an end-to-end speed benchmark. To avoid the inference delay caused by NMS, we propose a Real-Time DEtection TRansformer (RT-DETR), the first real-time end-to-end object detector to our best knowledge. Specifically, we design an efficient hybrid encoder to efficiently process multi-scale features by decoupling the intra-scale interaction and cross-scale fusion, and propose IoU-aware query selection to improve the initialization of object queries. In addition, our proposed detector supports flexibly adjustment of the inference speed by using different decoder layers without the need for retraining, which facilitates the practical application of real-time object detectors. Our RT-DETR-L achieves 53.0% AP on COCO val2017 and 114 FPS on T4 GPU, while RT-DETR-X achieves 54.8% AP and 74 FPS, outperforming all YOLO detectors of the same scale in both speed and accuracy. Furthermore, our RT-DETR-R50 achieves 53.1% AP and 108 FPS, outperforming DINO-Deformable-DETR-R50 by 2.2% AP in accuracy and by about 21 times in FPS. ource code and pre-trained models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Wenyu Lv, Yian Zhao, Shangliang Xu, Jinman Wei, Guanzhong Wang, Cheng Cui, Yuning Du, Qingqing Dang, Yi Liu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.09800v1": {
            "Paper Title": "Deep Event Visual Odometry",
            "Sentences": [
                {
                    "Sentence ID": 53,
                    "Sentence": "for iterative optical flow prediction\nand a differentiable bundle adjustment layer for pose es-\ntimation. DPVO ",
                    "Citation Text": "Zachary Teed, Lahav Lipson, and Jia Deng. Deep patch vi-\nsual odometry. arXiv preprint arXiv:2208.04726 , 2022. 1, 2,\n3, 4, 5, 6, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2208.04726",
                        "Citation Paper Title": "Title:Deep Patch Visual Odometry",
                        "Citation Paper Abstract": "Abstract:We propose Deep Patch Visual Odometry (DPVO), a new deep learning system for monocular Visual Odometry (VO). DPVO uses a novel recurrent network architecture designed for tracking image patches across time. Recent approaches to VO have significantly improved the state-of-the-art accuracy by using deep networks to predict dense flow between video frames. However, using dense flow incurs a large computational cost, making these previous methods impractical for many use cases. Despite this, it has been assumed that dense flow is important as it provides additional redundancy against incorrect matches. DPVO disproves this assumption, showing that it is possible to get the best accuracy and efficiency by exploiting the advantages of sparse patch-based matching over dense flow. DPVO introduces a novel recurrent update operator for patch based correspondence coupled with differentiable bundle adjustment. On Standard benchmarks, DPVO outperforms all prior work, including the learning-based state-of-the-art VO-system (DROID) using a third of the memory while running 3x faster on average. Code is available at this https URL",
                        "Citation Paper Authors": "Authors:Zachary Teed, Lahav Lipson, Jia Deng"
                    }
                },
                {
                    "Sentence ID": 70,
                    "Sentence": "employ a depth and event sensor. Due to the\nscarcity of datasets containing both modalities, they only\nevaluate on MVSEC ",
                    "Citation Text": "Alex Zihao Zhu, Dinesh Thakur, Tolga \u00a8Ozaslan, Bernd\nPfrommer, Vijay Kumar, and Kostas Daniilidis. The multi-\nvehicle stereo event camera dataset: An event camera dataset\nfor 3d perception. IEEE Robotics and Automation Letters , 3\n(3):2032\u20132039, 2018. 2, 4, 6, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.10202",
                        "Citation Paper Title": "Title:The Multi Vehicle Stereo Event Camera Dataset: An Event Camera Dataset for 3D Perception",
                        "Citation Paper Abstract": "Abstract:Event based cameras are a new passive sensing modality with a number of benefits over traditional cameras, including extremely low latency, asynchronous data acquisition, high dynamic range and very low power consumption. There has been a lot of recent interest and development in applying algorithms to use the events to perform a variety of 3D perception tasks, such as feature tracking, visual odometry, and stereo depth estimation. However, there currently lacks the wealth of labeled data that exists for traditional cameras to be used for both testing and development. In this paper, we present a large dataset with a synchronized stereo pair event based camera system, carried on a handheld rig, flown by a hexacopter, driven on top of a car and mounted on a motorcycle, in a variety of different illumination levels and environments. From each camera, we provide the event stream, grayscale images and IMU readings. In addition, we utilize a combination of IMU, a rigidly mounted lidar system, indoor and outdoor motion capture and GPS to provide accurate pose and depth images for each camera at up to 100Hz. For comparison, we also provide synchronized grayscale images and IMU readings from a frame based stereo camera system.",
                        "Citation Paper Authors": "Authors:Alex Zihao Zhu, Dinesh Thakur, Tolga Ozaslan, Bernd Pfrommer, Vijay Kumar, Kostas Daniilidis"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": ", which both fail on all sequences.\nVECtor The VECtor dataset ",
                    "Citation Text": "Ling Gao, Yuxuan Liang, Jiaqi Yang, Shaoxun Wu, Chenyu\nWang, Jiaben Chen, and Laurent Kneip. Vector: A versa-\ntile event-centric benchmark for multi-sensor slam. IEEE\nRobotics and Automation Letters , 7(3):8217\u20138224, 2022. 5,\n6, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2207.01404",
                        "Citation Paper Title": "Title:VECtor: A Versatile Event-Centric Benchmark for Multi-Sensor SLAM",
                        "Citation Paper Abstract": "Abstract:Event cameras have recently gained in popularity as they hold strong potential to complement regular cameras in situations of high dynamics or challenging illumination. An important problem that may benefit from the addition of an event camera is given by Simultaneous Localization And Mapping (SLAM). However, in order to ensure progress on event-inclusive multi-sensor SLAM, novel benchmark sequences are needed. Our contribution is the first complete set of benchmark datasets captured with a multi-sensor setup containing an event-based stereo camera, a regular stereo camera, multiple depth sensors, and an inertial measurement unit. The setup is fully hardware-synchronized and underwent accurate extrinsic calibration. All sequences come with ground truth data captured by highly accurate external reference devices such as a motion capture system. Individual sequences include both small and large-scale environments, and cover the specific challenges targeted by dynamic vision sensors.",
                        "Citation Paper Authors": "Authors:Ling Gao, Yuxuan Liang, Jiaqi Yang, Shaoxun Wu, Chenyu Wang, Jiaben Chen, Laurent Kneip"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "Mono EVIO \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 9.79 4.74\nPLEVIO ",
                    "Citation Text": "Weipeng Guan, Peiyu Chen, Yuhan Xie, and Peng Lu.\nPl-evio: Robust monocular event-based visual inertial\nodometry with point and line features. arXiv preprint\narXiv:2209.12160 , 2022. 1, 2, 5, 6, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2209.12160",
                        "Citation Paper Title": "Title:PL-EVIO: Robust Monocular Event-based Visual Inertial Odometry with Point and Line Features",
                        "Citation Paper Abstract": "Abstract:Event cameras are motion-activated sensors that capture pixel-level illumination changes instead of the intensity image with a fixed frame rate. Compared with the standard cameras, it can provide reliable visual perception during high-speed motions and in high dynamic range scenarios. However, event cameras output only a little information or even noise when the relative motion between the camera and the scene is limited, such as in a still state. While standard cameras can provide rich perception information in most scenarios, especially in good lighting conditions. These two cameras are exactly complementary. In this paper, we proposed a robust, high-accurate, and real-time optimization-based monocular event-based visual-inertial odometry (VIO) method with event-corner features, line-based event features, and point-based image features. The proposed method offers to leverage the point-based features in the nature scene and line-based features in the human-made scene to provide more additional structure or constraints information through well-design feature management. Experiments in the public benchmark datasets show that our method can achieve superior performance compared with the state-of-the-art image-based or event-based VIO. Finally, we used our method to demonstrate an onboard closed-loop autonomous quadrotor flight and large-scale outdoor experiments. Videos of the evaluations are presented on our project website: this https URL",
                        "Citation Paper Authors": "Authors:Weipeng Guan, Peiyu Chen, Yuhan Xie, Peng Lu"
                    }
                },
                {
                    "Sentence ID": 56,
                    "Sentence": "Mono VO \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013\nUSLAM ",
                    "Citation Text": "Antoni Rosinol Vidal, Henri Rebecq, Timo Horstschaefer,\nand Davide Scaramuzza. Ultimate slam? combining events,\nimages, and imu for robust visual slam in hdr and high-speed\nscenarios. IEEE RA-L , 2018. 1, 2, 5, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.06310",
                        "Citation Paper Title": "Title:Ultimate SLAM? Combining Events, Images, and IMU for Robust Visual SLAM in HDR and High Speed Scenarios",
                        "Citation Paper Abstract": "Abstract:Event cameras are bio-inspired vision sensors that output pixel-level brightness changes instead of standard intensity frames. These cameras do not suffer from motion blur and have a very high dynamic range, which enables them to provide reliable visual information during high speed motions or in scenes characterized by high dynamic range. However, event cameras output only little information when the amount of motion is limited, such as in the case of almost still motion. Conversely, standard cameras provide instant and rich information about the environment most of the time (in low-speed and good lighting scenarios), but they fail severely in case of fast motions, or difficult lighting such as high dynamic range or low light scenes. In this paper, we present the first state estimation pipeline that leverages the complementary advantages of these two sensors by fusing in a tightly-coupled manner events, standard frames, and inertial measurements. We show on the publicly available Event Camera Dataset that our hybrid pipeline leads to an accuracy improvement of 130% over event-only pipelines, and 85% over standard-frames-only visual-inertial systems, while still being computationally tractable. Furthermore, we use our pipeline to demonstrate - to the best of our knowledge - the first autonomous quadrotor flight using an event camera for state estimation, unlocking flight scenarios that were not reachable with traditional visual-inertial odometry, such as low-light environments and high-dynamic range scenes.",
                        "Citation Paper Authors": "Authors:Antoni Rosinol Vidal, Henri Rebecq, Timo Horstschaefer, Davide Scaramuzza"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "Stereo VIO 0.55 1.19 \u2013 0.36 0.77 1.02 2.18 1.53 0.49\nVINS Fusion ",
                    "Citation Text": "Tong Qin, Jie Pan, Shaozu Cao, and Shaojie Shen. A general\noptimization-based framework for local odometry estima-\ntion with multiple sensors. arXiv preprint arXiv:1901.03638 ,\n2019. 5, 6, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.03638",
                        "Citation Paper Title": "Title:A General Optimization-based Framework for Local Odometry Estimation with Multiple Sensors",
                        "Citation Paper Abstract": "Abstract:Nowadays, more and more sensors are equipped on robots to increase robustness and autonomous ability. We have seen various sensor suites equipped on different platforms, such as stereo cameras on ground vehicles, a monocular camera with an IMU (Inertial Measurement Unit) on mobile phones, and stereo cameras with an IMU on aerial robots. Although many algorithms for state estimation have been proposed in the past, they are usually applied to a single sensor or a specific sensor suite. Few of them can be employed with multiple sensor choices. In this paper, we proposed a general optimization-based framework for odometry estimation, which supports multiple sensor sets. Every sensor is treated as a general factor in our framework. Factors which share common state variables are summed together to build the optimization problem. We further demonstrate the generality with visual and inertial sensors, which form three sensor suites (stereo cameras, a monocular camera with an IMU, and stereo cameras with an IMU). We validate the performance of our system on public datasets and through real-world experiments with multiple sensors. Results are compared against other state-of-the-art algorithms. We highlight that our system is a general framework, which can easily fuse various sensors in a pose graph optimization. Our implementations are open source\\footnote{this https URL}.",
                        "Citation Paper Authors": "Authors:Tong Qin, Jie Pan, Shaozu Cao, Shaojie Shen"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "with ATE[cm] and Rrmse[deg]. Baseline numbers (except for DPVO and DPVO \u2020) are taken from ",
                    "Citation Text": "Kunping Huang, Sen Zhang, Jing Zhang, and Dacheng Tao.\nEvent-based simultaneous localization and mapping: A com-\nprehensive survey. arXiv preprint arXiv:2304.09793 , 2023.\n8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.09793",
                        "Citation Paper Title": "Title:Event-based Simultaneous Localization and Mapping: A Comprehensive Survey",
                        "Citation Paper Abstract": "Abstract:In recent decades, visual simultaneous localization and mapping (vSLAM) has gained significant interest in both academia and industry. It estimates camera motion and reconstructs the environment concurrently using visual sensors on a moving robot. However, conventional cameras are limited by hardware, including motion blur and low dynamic range, which can negatively impact performance in challenging scenarios like high-speed motion and high dynamic range illumination. Recent studies have demonstrated that event cameras, a new type of bio-inspired visual sensor, offer advantages such as high temporal resolution, dynamic range, low power consumption, and low latency. This paper presents a timely and comprehensive review of event-based vSLAM algorithms that exploit the benefits of asynchronous and irregular event streams for localization and mapping tasks. The review covers the working principle of event cameras and various event representations for preprocessing event data. It also categorizes event-based vSLAM methods into four main categories: feature-based, direct, motion-compensation, and deep learning methods, with detailed discussions and practical guidance for each approach. Furthermore, the paper evaluates the state-of-the-art methods on various benchmarks, highlighting current challenges and future opportunities in this emerging research area. A public repository will be maintained to keep track of the rapid developments in this field at {\\url{this https URL}}.",
                        "Citation Paper Authors": "Authors:Kunping Huang, Sen Zhang, Jing Zhang, Dacheng Tao"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": ", which runs on the RGB frames\nof the respective dataset, (ii) DPVO\u2020[45, 53], which is a\nre-trained DPVO model on the E2VID ",
                    "Citation Text": "Henri Rebecq, Ren \u00b4e Ranftl, Vladlen Koltun, and Davide\nScaramuzza. Events-to-video: Bringing modern computer\nvision to event cameras. In CVPR , pages 3857\u20133866, 2019.\n4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.08298",
                        "Citation Paper Title": "Title:Events-to-Video: Bringing Modern Computer Vision to Event Cameras",
                        "Citation Paper Abstract": "Abstract:Event cameras are novel sensors that report brightness changes in the form of asynchronous \"events\" instead of intensity frames. They have significant advantages over conventional cameras: high temporal resolution, high dynamic range, and no motion blur. Since the output of event cameras is fundamentally different from conventional cameras, it is commonly accepted that they require the development of specialized algorithms to accommodate the particular nature of events. In this work, we take a different view and propose to apply existing, mature computer vision techniques to videos reconstructed from event data. We propose a novel recurrent network to reconstruct videos from a stream of events, and train it on a large amount of simulated event data. Our experiments show that our approach surpasses state-of-the-art reconstruction methods by a large margin (> 20%) in terms of image quality. We further apply off-the-shelf computer vision algorithms to videos reconstructed from event data on tasks such as object classification and visual-inertial odometry, and show that this strategy consistently outperforms algorithms that were specifically designed for event data. We believe that our approach opens the door to bringing the outstanding properties of event cameras to an entirely new range of tasks. A video of the experiments is available at this https URL",
                        "Citation Paper Authors": "Authors:Henri Rebecq, Ren\u00e9 Ranftl, Vladlen Koltun, Davide Scaramuzza"
                    }
                },
                {
                    "Sentence ID": 59,
                    "Sentence": ".\nImplementation details We simulate events on all se-\nquences of the TartanAir dataset ",
                    "Citation Text": "Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu,\nYuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Se-\nbastian Scherer. Tartanair: A dataset to push the limits of\nvisual slam. 2020. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.14338",
                        "Citation Paper Title": "Title:TartanAir: A Dataset to Push the Limits of Visual SLAM",
                        "Citation Paper Abstract": "Abstract:We present a challenging dataset, the TartanAir, for robot navigation tasks and more. The data is collected in photo-realistic simulation environments with the presence of moving objects, changing light and various weather conditions. By collecting data in simulations, we are able to obtain multi-modal sensor data and precise ground truth labels such as the stereo RGB image, depth image, segmentation, optical flow, camera poses, and LiDAR point cloud. We set up large numbers of environments with various styles and scenes, covering challenging viewpoints and diverse motion patterns that are difficult to achieve by using physical data collection platforms. In order to enable data collection at such a large scale, we develop an automatic pipeline, including mapping, trajectory sampling, data processing, and data verification. We evaluate the impact of various factors on visual SLAM algorithms using our data. The results of state-of-the-art algorithms reveal that the visual SLAM problem is far from solved. Methods that show good performance on established datasets such as KITTI do not perform well in more difficult scenarios. Although we use the simulation, our goal is to push the limits of Visual SLAM algorithms in the real world by providing a challenging benchmark for testing new methods, while also using a large diverse training data for learning-based methods. Our dataset is available at \\url{this http URL}.",
                        "Citation Paper Authors": "Authors:Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, Sebastian Scherer"
                    }
                },
                {
                    "Sentence ID": 71,
                    "Sentence": ". EVO\ncan perform well on small-scale scenes, but it is very\nparameter-sensitive and requires bootstrapping, which is ei-\nther achieved through a planar, fronto-parallel scene as-\nsumption or by using a frame-based camera.\nZhu et al. ",
                    "Citation Text": "Alex Zihao Zhu, Liangzhe Yuan, Kenneth Chaney, and\nKostas Daniilidis. Unsupervised event-based learning of\noptical flow, depth, and egomotion. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 989\u2013997, 2019. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.08156",
                        "Citation Paper Title": "Title:Unsupervised Event-based Learning of Optical Flow, Depth, and Egomotion",
                        "Citation Paper Abstract": "Abstract:In this work, we propose a novel framework for unsupervised learning for event cameras that learns motion information from only the event stream. In particular, we propose an input representation of the events in the form of a discretized volume that maintains the temporal distribution of the events, which we pass through a neural network to predict the motion of the events. This motion is used to attempt to remove any motion blur in the event image. We then propose a loss function applied to the motion compensated event image that measures the motion blur in this image. We train two networks with this framework, one to predict optical flow, and one to predict egomotion and depths, and evaluate these networks on the Multi Vehicle Stereo Event Camera dataset, along with qualitative results from a variety of different scenes.",
                        "Citation Paper Authors": "Authors:Alex Zihao Zhu, Liangzhe Yuan, Kenneth Chaney, Kostas Daniilidis"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": "to the event modality. We\npropose a novel patch selection mechanism for sparse event\ndata. Event data pose unique challenges, e.g., a large sim-\nto-real gap ",
                    "Citation Text": "Timo Stoffregen, Cedric Scheerlinck, Davide Scaramuzza,\nTom Drummond, Nick Barnes, Lindsay Kleeman, and\nRobert Mahony. Reducing the sim-to-real gap for event cam-\neras. In Computer Vision\u2013ECCV 2020: 16th European Con-\nference, Glasgow, UK, August 23\u201328, 2020, Proceedings,\nPart XXVII 16 , pages 534\u2013549. Springer, 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.09078",
                        "Citation Paper Title": "Title:Reducing the Sim-to-Real Gap for Event Cameras",
                        "Citation Paper Abstract": "Abstract:Event cameras are paradigm-shifting novel sensors that report asynchronous, per-pixel brightness changes called 'events' with unparalleled low latency. This makes them ideal for high speed, high dynamic range scenes where conventional cameras would fail. Recent work has demonstrated impressive results using Convolutional Neural Networks (CNNs) for video reconstruction and optic flow with events. We present strategies for improving training data for event based CNNs that result in 20-40% boost in performance of existing state-of-the-art (SOTA) video reconstruction networks retrained with our method, and up to 15% for optic flow networks. A challenge in evaluating event based video reconstruction is lack of quality ground truth images in existing datasets. To address this, we present a new High Quality Frames (HQF) dataset, containing events and ground truth frames from a DAVIS240C that are well-exposed and minimally motion-blurred. We evaluate our method on HQF + several existing major event camera datasets.",
                        "Citation Paper Authors": "Authors:Timo Stoffregen, Cedric Scheerlinck, Davide Scaramuzza, Tom Drummond, Nick Barnes, Lindsay Kleeman, Robert Mahony"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "propose\na SLAM system that is trained and evaluated on scenes from\nthe same simulated CARLA environment ",
                    "Citation Text": "Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio\nLopez, and Vladlen Koltun. CARLA: An open urban driving\nsimulator. In Proceedings of the 1st Annual Conference on\nRobot Learning , pages 1\u201316, 2017. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.03938",
                        "Citation Paper Title": "Title:CARLA: An Open Urban Driving Simulator",
                        "Citation Paper Abstract": "Abstract:We introduce CARLA, an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use CARLA to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by CARLA, illustrating the platform's utility for autonomous driving research. The supplementary video can be viewed at this https URL",
                        "Citation Paper Authors": "Authors:Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, Vladlen Koltun"
                    }
                },
                {
                    "Sentence ID": 74,
                    "Sentence": "are the first to demonstrate event\nand depth VO on a small-scale custom dataset. Similarly,\nZuo et al. ",
                    "Citation Text": "Yi-Fan Zuo, Jiaqi Yang, Jiaben Chen, Xia Wang, Yifu Wang,\nand Laurent Kneip. Devo: Depth-event camera visual odom-\netry in challenging conditions. In 2022 International Confer-\nence on Robotics and Automation (ICRA) , pages 2179\u20132185.\nIEEE, 2022. 2\n11",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.02556",
                        "Citation Paper Title": "Title:DEVO: Depth-Event Camera Visual Odometry in Challenging Conditions",
                        "Citation Paper Abstract": "Abstract:We present a novel real-time visual odometry framework for a stereo setup of a depth and high-resolution event camera. Our framework balances accuracy and robustness against computational efficiency towards strong performance in challenging scenarios. We extend conventional edge-based semi-dense visual odometry towards time-surface maps obtained from event streams. Semi-dense depth maps are generated by warping the corresponding depth values of the extrinsically calibrated depth camera. The tracking module updates the camera pose through efficient, geometric semi-dense 3D-2D edge alignment. Our approach is validated on both public and self-collected datasets captured under various conditions. We show that the proposed method performs comparable to state-of-the-art RGB-D camera-based alternatives in regular conditions, and eventually outperforms in challenging conditions such as high dynamics or low illumination.",
                        "Citation Paper Authors": "Authors:Yi-Fan Zuo, Jiaqi Yang, Jiaben Chen, Xia Wang, Yifu Wang, Laurent Kneip"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.09786v1": {
            "Paper Title": "Drones Guiding Drones: Cooperative Navigation of a Less-Equipped Micro\n  Aerial Vehicle in Cluttered Environments",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": ", a terrain\nmap was constructed on a ground station using a learning-\nbased approach and sent to the UGV, which used it for\npath planning. In ",
                    "Citation Text": "T. Miki, P. Khrapchenkov, and K. Hori, \u201cUA V/UGV Autonomous\nCooperation: UA V assists UGV to climb a cliff by attaching a tether,\u201dinInternational Conference on Robotics and Automation (ICRA) ,\n2019, pp. 8041\u20138047.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.04898",
                        "Citation Paper Title": "Title:UAV/UGV Autonomous Cooperation: UAV Assists UGV to Climb a Cliff by Attaching a Tether",
                        "Citation Paper Abstract": "Abstract:This paper proposes a novel cooperative system for an Unmanned Aerial Vehicle (UAV) and an Unmanned Ground Vehicle (UGV) which utilizes the UAV not only as a flying sensor but also as a tether attachment device. Two robots are connected with a tether, allowing the UAV to anchor the tether to a structure located at the top of a steep terrain, impossible to reach for UGVs. Thus, enhancing the poor traversability of the UGV by not only providing a wider range of scanning and mapping from the air, but also by allowing the UGV to climb steep terrains with the winding of the tether. In addition, we present an autonomous framework for the collaborative navigation and tether attachment in an unknown environment. The UAV employs visual inertial navigation with 3D voxel mapping and obstacle avoidance planning. The UGV makes use of the voxel map and generates an elevation map to execute path planning based on a traversability analysis. Furthermore, we compared the pros and cons of possible methods for the tether anchoring from multiple points of view. To increase the probability of successful anchoring, we evaluated the anchoring strategy with an experiment. Finally, the feasibility and capability of our proposed system were demonstrated by an autonomous mission experiment in the field with an obstacle and a cliff.",
                        "Citation Paper Authors": "Authors:Takahiro Miki, Petr Khrapchenkov, Koichi Hori"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2309.07473v2": {
            "Paper Title": "Where2Explore: Few-shot Affordance Learning for Unseen Novel Categories\n  of Articulated Objects",
            "Sentences": [
                {
                    "Sentence ID": 44,
                    "Sentence": ": an affordance learning method that explores test-time few-shot adaptation. During\nfew-shot exploration, the interactions are proposed by a curiosity module that is optimized for\ndiscovering the dynamic information of a specific object.\n\u2022PointEncoder ",
                    "Citation Text": "Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, and Jiwen Lu. Point-bert:\nPre-training 3d point cloud transformers with masked point modeling. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 19313\u201319322, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.14819",
                        "Citation Paper Title": "Title:Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling",
                        "Citation Paper Abstract": "Abstract:We present Point-BERT, a new paradigm for learning Transformers to generalize the concept of BERT to 3D point cloud. Inspired by BERT, we devise a Masked Point Modeling (MPM) task to pre-train point cloud Transformers. Specifically, we first divide a point cloud into several local point patches, and a point cloud Tokenizer with a discrete Variational AutoEncoder (dVAE) is designed to generate discrete point tokens containing meaningful local information. Then, we randomly mask out some patches of input point clouds and feed them into the backbone Transformers. The pre-training objective is to recover the original point tokens at the masked locations under the supervision of point tokens obtained by the Tokenizer. Extensive experiments demonstrate that the proposed BERT-style pre-training strategy significantly improves the performance of standard point cloud Transformers. Equipped with our pre-training strategy, we show that a pure Transformer architecture attains 93.8% accuracy on ModelNet40 and 83.1% accuracy on the hardest setting of ScanObjectNN, surpassing carefully designed point cloud models with much fewer hand-made designs. We also demonstrate that the representations learned by Point-BERT transfer well to new tasks and domains, where our models largely advance the state-of-the-art of few-shot point cloud classification task. The code and pre-trained models are available at this https URL",
                        "Citation Paper Authors": "Authors:Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, Jiwen Lu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.09757v1": {
            "Paper Title": "Benchmarking the Full-Order Model Optimization Based Imitation in the\n  Humanoid Robot Reinforcement Learning Walk",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09750v1": {
            "Paper Title": "Attention-Based VR Facial Animation with Visual Mouth Camera Guidance\n  for Immersive Telepresence Avatars",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09740v1": {
            "Paper Title": "VITA: A Multi-modal LLM-based System for Longitudinal, Autonomous, and\n  Adaptive Robotic Mental Well-being Coaching",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09734v1": {
            "Paper Title": "Learning of Hamiltonian Dynamics with Reproducing Kernel Hilbert Spaces",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "by including constraints that enfo rce\nprior knowledge of the region of attraction. The stability\nregion was enforced using a Lyapunov function, and the\nhypothesis space for the learned system was an RKHS. In ",
                    "Citation Text": "A. J. Thorpe, C. Neary, F. Djeumou, M. M. K. Oishi, and\nU. Topcu, \u201cPhysics-Informed Kernel Embeddings: Integrati ng Prior\nSystem Knowledge with Data-Driven Control,\u201d 2023, arXiv pr eprint\narXiv:2301.03565 [eess.SY].",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2301.03565",
                        "Citation Paper Title": "Title:Physics-Informed Kernel Embeddings: Integrating Prior System Knowledge with Data-Driven Control",
                        "Citation Paper Abstract": "Abstract:Data-driven control algorithms use observations of system dynamics to construct an implicit model for the purpose of control. However, in practice, data-driven techniques often require excessive sample sizes, which may be infeasible in real-world scenarios where only limited observations of the system are available. Furthermore, purely data-driven methods often neglect useful a priori knowledge, such as approximate models of the system dynamics. We present a method to incorporate such prior knowledge into data-driven control algorithms using kernel embeddings, a nonparametric machine learning technique based in the theory of reproducing kernel Hilbert spaces. Our proposed approach incorporates prior knowledge of the system dynamics as a bias term in the kernel learning problem. We formulate the biased learning problem as a least-squares problem with a regularization term that is informed by the dynamics, that has an efficiently computable, closed-form solution. Through numerical experiments, we empirically demonstrate the improved sample efficiency and out-of-sample generalization of our approach over a purely data-driven baseline. We demonstrate an application of our method to control through a target tracking problem with nonholonomic dynamics, and on spring-mass-damper and F-16 aircraft state prediction tasks.",
                        "Citation Paper Authors": "Authors:Adam J. Thorpe, Cyrus Neary, Franck Djeumou, Meeko M. K. Oishi, Ufuk Topcu"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": ", where the need for higher-order\nderivatives of the generalized coordinate was eliminated, and\nthe option for energy-based control was included. In ",
                    "Citation Text": "Z. Chen, J. Zhang, M. Arjovsky, and L. Bottou, \u201cSymplect ic Recurrent\nNeural Networks,\u201d in International Conference on Learning Represen-\ntations , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.13334",
                        "Citation Paper Title": "Title:Symplectic Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:We propose Symplectic Recurrent Neural Networks (SRNNs) as learning algorithms that capture the dynamics of physical systems from observed trajectories. An SRNN models the Hamiltonian function of the system by a neural network and furthermore leverages symplectic integration, multiple-step training and initial state optimization to address the challenging numerical issues associated with Hamiltonian systems. We show that SRNNs succeed reliably on complex and noisy Hamiltonian systems. We also show how to augment the SRNN integration scheme in order to handle stiff dynamical systems such as bouncing billiards.",
                        "Citation Paper Authors": "Authors:Zhengdao Chen, Jianyu Zhang, Martin Arjovsky, L\u00e9on Bottou"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.09688v1": {
            "Paper Title": "Shaping and Being Shaped by Drones: Supporting Perception-Action Loops",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09676v1": {
            "Paper Title": "nuScenes Knowledge Graph -- A comprehensive semantic representation of\n  traffic scenes for trajectory prediction",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": "\u2713 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717\nPGP ",
                    "Citation Text": "Nachiket Deo, Eric M. Wolff, and Oscar Beijbom. Mul-\ntimodal trajectory prediction conditioned on lane-graph\ntraversals. In Conference on Robot Learning , 2021. 3, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.15004",
                        "Citation Paper Title": "Title:Multimodal Trajectory Prediction Conditioned on Lane-Graph Traversals",
                        "Citation Paper Abstract": "Abstract:Accurately predicting the future motion of surrounding vehicles requires reasoning about the inherent uncertainty in driving behavior. This uncertainty can be loosely decoupled into lateral (e.g., keeping lane, turning) and longitudinal (e.g., accelerating, braking). We present a novel method that combines learned discrete policy rollouts with a focused decoder on subsets of the lane graph. The policy rollouts explore different goals given current observations, ensuring that the model captures lateral variability. Longitudinal variability is captured by our latent variable model decoder that is conditioned on various subsets of the lane graph. Our model achieves state-of-the-art performance on the nuScenes motion prediction dataset, and qualitatively demonstrates excellent scene compliance. Detailed ablations highlight the importance of the policy rollouts and the decoder architecture.",
                        "Citation Paper Authors": "Authors:Nachiket Deo, Eric M. Wolff, Oscar Beijbom"
                    }
                },
                {
                    "Sentence ID": 71,
                    "Sentence": ". It is a small ontology with only seven concepts\nthat explores the feasibility of using ontologies for driverassistance functions. The map structures that it models are\nlanes, traffic signs and road pieces. On the other extreme, ",
                    "Citation Text": "Lukas Westhofen, Christian Neurohr, Martin Butz, Maike\nScholtes, and Michael Schuldes. Using ontologies for the\nformalization and recognition of criticality for automated\ndriving. arXiv preprint arXiv:2205.01532 , 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.01532",
                        "Citation Paper Title": "Title:Using Ontologies for the Formalization and Recognition of Criticality for Automated Driving",
                        "Citation Paper Abstract": "Abstract:Knowledge representation and reasoning has a long history of examining how knowledge can be formalized, interpreted, and semantically analyzed by machines. In the area of automated vehicles, recent advances suggest the ability to formalize and leverage relevant knowledge as a key enabler in handling the inherently open and complex context of the traffic world. This paper demonstrates ontologies to be a powerful tool for a) modeling and formalization of and b) reasoning about factors associated with criticality in the environment of automated vehicles. For this, we leverage the well-known 6-Layer Model to create a formal representation of the environmental context. Within this representation, an ontology models domain knowledge as logical axioms, enabling deduction on the presence of critical factors within traffic scenes and scenarios. For executing automated analyses, a joint description logic and rule reasoner is used in combination with an a-priori predicate augmentation. We elaborate on the modular approach, present a publicly available implementation, and evaluate the method by means of a large-scale drone data set of urban traffic scenarios.",
                        "Citation Paper Authors": "Authors:Lukas Westhofen, Christian Neurohr, Martin Butz, Maike Scholtes, Michael Schuldes"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "\u2717 \u2717 \u2713 \u2717 \u2717 \u2717 \u2713 \u2713 \u2717 \u2717 \u2717\nLaneGCN ",
                    "Citation Text": "Ming Liang, Binh Yang, Rui Hu, Yun Chen, Renjie Liao,\nSong Feng, and Raquel Urtasun. Learning lane graph rep-\nresentations for motion forecasting. ArXiv , abs/2007.13732,\n2020. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.13732",
                        "Citation Paper Title": "Title:Learning Lane Graph Representations for Motion Forecasting",
                        "Citation Paper Abstract": "Abstract:We propose a motion forecasting model that exploits a novel structured map representation as well as actor-map interactions. Instead of encoding vectorized maps as raster images, we construct a lane graph from raw map data to explicitly preserve the map structure. To capture the complex topology and long range dependencies of the lane graph, we propose LaneGCN which extends graph convolutions with multiple adjacency matrices and along-lane dilation. To capture the complex interactions between actors and maps, we exploit a fusion network consisting of four types of interactions, actor-to-lane, lane-to-lane, lane-to-actor and actor-to-actor. Powered by LaneGCN and actor-map interactions, our model is able to predict accurate and realistic multi-modal trajectories. Our approach significantly outperforms the state-of-the-art on the large scale Argoverse motion forecasting benchmark.",
                        "Citation Paper Authors": "Authors:Ming Liang, Bin Yang, Rui Hu, Yun Chen, Renjie Liao, Song Feng, Raquel Urtasun"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "\u2713 \u2717 \u2717 \u2717 \u2717 \u2713 \u2717 \u2717 \u2717 \u2717 \u2717\nLAformer ",
                    "Citation Text": "Mengmeng Liu, Hao Cheng, Lin Chen, Hellward\nBroszio, Jiangtao Li, Runjiang Zhao, Monika Sester,\nand Michael Ying Yang. Laformer: Trajectory prediction\nfor autonomous driving with lane-aware scene constraints.\narXiv preprint arXiv:2302.13933 , 2023. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.13933",
                        "Citation Paper Title": "Title:LAformer: Trajectory Prediction for Autonomous Driving with Lane-Aware Scene Constraints",
                        "Citation Paper Abstract": "Abstract:Trajectory prediction for autonomous driving must continuously reason the motion stochasticity of road agents and comply with scene constraints. Existing methods typically rely on one-stage trajectory prediction models, which condition future trajectories on observed trajectories combined with fused scene information. However, they often struggle with complex scene constraints, such as those encountered at intersections. To this end, we present a novel method, called LAformer. It uses a temporally dense lane-aware estimation module to select only the top highly potential lane segments in an HD map, which effectively and continuously aligns motion dynamics with scene information, reducing the representation requirements for the subsequent attention-based decoder by filtering out irrelevant lane segments. Additionally, unlike one-stage prediction models, LAformer utilizes predictions from the first stage as anchor trajectories and adds a second-stage motion refinement module to further explore temporal consistency across the complete time horizon. Extensive experiments on Argoverse 1 and nuScenes demonstrate that LAformer achieves excellent performance for multimodal trajectory prediction.",
                        "Citation Paper Authors": "Authors:Mengmeng Liu, Hao Cheng, Lin Chen, Hellward Broszio, Jiangtao Li, Runjiang Zhao, Monika Sester, Michael Ying Yang"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "\u2713 \u2717 \u2717 \u2717 \u2713 \u2717 \u2717 \u2713 \u2717 \u2717 \u2717\nHDGT ",
                    "Citation Text": "Xiaosong Jia, Peng Wu, Li Chen, Hongyang Li, Yu Sen Liu,\nand Junchi Yan. Hdgt: Heterogeneous driving graph trans-\nformer for multi-agent trajectory prediction via scene encod-\ning. ArXiv , abs/2205.09753, 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.09753",
                        "Citation Paper Title": "Title:HDGT: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory Prediction via Scene Encoding",
                        "Citation Paper Abstract": "Abstract:Encoding a driving scene into vector representations has been an essential task for autonomous driving that can benefit downstream tasks e.g. trajectory prediction. The driving scene often involves heterogeneous elements such as the different types of objects (agents, lanes, traffic signs) and the semantic relations between objects are rich and diverse. Meanwhile, there also exist relativity across elements, which means that the spatial relation is a relative concept and need be encoded in a ego-centric manner instead of in a global coordinate system. Based on these observations, we propose Heterogeneous Driving Graph Transformer (HDGT), a backbone modelling the driving scene as a heterogeneous graph with different types of nodes and edges. For heterogeneous graph construction, we connect different types of nodes according to diverse semantic relations. For spatial relation encoding, the coordinates of the node as well as its in-edges are in the local node-centric coordinate system. For the aggregation module in the graph neural network (GNN), we adopt the transformer structure in a hierarchical way to fit the heterogeneous nature of inputs. Experimental results show that HDGT achieves state-of-the-art performance for the task of trajectory prediction, on INTERACTION Prediction Challenge and Waymo Open Motion Challenge.",
                        "Citation Paper Authors": "Authors:Xiaosong Jia, Penghao Wu, Li Chen, Yu Liu, Hongyang Li, Junchi Yan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.09588v1": {
            "Paper Title": "NeuroFlow: Development of lightweight and efficient model integration\n  scheduling strategy for autonomous driving system",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09565v1": {
            "Paper Title": "Optimized Control Invariance Conditions for Uncertain Input-Constrained\n  Nonlinear Control Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.06371v2": {
            "Paper Title": "BAT: Behavior-Aware Human-Like Trajectory Prediction for Autonomous\n  Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2305.07493v2": {
            "Paper Title": "A Virtual Reality Framework for Human-Robot Collaboration in Cloth\n  Folding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09517v1": {
            "Paper Title": "A wearable Gait Assessment Method for Lumbar Disc Herniation Based on\n  Adaptive Kalman Filtering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09394v1": {
            "Paper Title": "HiER: Highlight Experience Replay and Easy2Hard Curriculum Learning for\n  Boosting Off-Policy Reinforcement Learning Agents",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": "introduced\nself-imitation learning (SIL) for on-policy RL. The priority\nis computed based on the discounted cumulative rewards.\nFurthermore, the technique of clipped advantage is utilized\nto incentivize positive experiences. By modifying the Bell-\nmann optimality operator, Ferret et al. ",
                    "Citation Text": "J. Ferret, O. Pietquin, and M. Geist, \u201cSelf-Imitation Advantage\nLearning,\u201d Dec. 2020, arXiv:2012.11989 [cs]. [Online]. Available:\nhttp://doi.org/10.48550/arXiv.2012.11989",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.11989",
                        "Citation Paper Title": "Title:Self-Imitation Advantage Learning",
                        "Citation Paper Abstract": "Abstract:Self-imitation learning is a Reinforcement Learning (RL) method that encourages actions whose returns were higher than expected, which helps in hard exploration and sparse reward problems. It was shown to improve the performance of on-policy actor-critic methods in several discrete control tasks. Nevertheless, applying self-imitation to the mostly action-value based off-policy RL methods is not straightforward. We propose SAIL, a novel generalization of self-imitation learning for off-policy RL, based on a modification of the Bellman optimality operator that we connect to Advantage Learning. Crucially, our method mitigates the problem of stale returns by choosing the most optimistic return estimate between the observed return and the current action-value for self-imitation. We demonstrate the empirical effectiveness of SAIL on the Arcade Learning Environment, with a focus on hard exploration games.",
                        "Citation Paper Authors": "Authors:Johan Ferret, Olivier Pietquin, Matthieu Geist"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "and\nthus, instead of uniformly, they are sampled according to\ntheir priority. Additionally, as high-priority samples would\nbias the training, importance sampling is applied.\nAs a form of prioritization, Oh et al. ",
                    "Citation Text": "J. Oh, Y . Guo, S. Singh, and H. Lee, \u201cSelf-Imitation Learning,\u201d\nJun. 2018, arXiv:1806.05635 [cs, stat]. [Online]. Available: http:\n//doi.org/10.48550/arXiv.1806.05635",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.05635",
                        "Citation Paper Title": "Title:Self-Imitation Learning",
                        "Citation Paper Abstract": "Abstract:This paper proposes Self-Imitation Learning (SIL), a simple off-policy actor-critic algorithm that learns to reproduce the agent's past good decisions. This algorithm is designed to verify our hypothesis that exploiting past good experiences can indirectly drive deep exploration. Our empirical results show that SIL significantly improves advantage actor-critic (A2C) on several hard exploration Atari games and is competitive to the state-of-the-art count-based exploration methods. We also show that SIL improves proximal policy optimization (PPO) on MuJoCo tasks.",
                        "Citation Paper Authors": "Authors:Junhyuk Oh, Yijie Guo, Satinder Singh, Honglak Lee"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.09348v1": {
            "Paper Title": "LLM-MARS: Large Language Model for Behavior Tree Generation and\n  NLP-enhanced Dialogue in Multi-Agent Robot Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09337v1": {
            "Paper Title": "Promptable Behaviors: Personalizing Multi-Objective Rewards from Human\n  Preferences",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": "2.1. Multi-Objective Reinforcement Learning\nExisting MORL algorithms are categorized into two main\ntypes ",
                    "Citation Text": "Conor F Hayes, Roxana R \u02d8adulescu, Eugenio Bargiacchi,\nJohan K \u00a8allstr \u00a8om, Matthew Macfarlane, Mathieu Reymond,\nTimothy Verstraeten, Luisa M Zintgraf, Richard Dazeley,\nFredrik Heintz, et al. A practical guide to multi-objective\nreinforcement learning and planning. Autonomous Agents\nand Multi-Agent Systems , 36(1):26, 2022. 2, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.09568",
                        "Citation Paper Title": "Title:A Practical Guide to Multi-Objective Reinforcement Learning and Planning",
                        "Citation Paper Abstract": "Abstract:Real-world decision-making tasks are generally complex, requiring trade-offs between multiple, often conflicting, objectives. Despite this, the majority of research in reinforcement learning and decision-theoretic planning either assumes only a single objective, or that multiple objectives can be adequately handled via a simple linear combination. Such approaches may oversimplify the underlying problem and hence produce suboptimal results. This paper serves as a guide to the application of multi-objective methods to difficult problems, and is aimed at researchers who are already familiar with single-objective reinforcement learning and planning methods who wish to adopt a multi-objective perspective on their research, as well as practitioners who encounter multi-objective decision problems in practice. It identifies the factors that may influence the nature of the desired solution, and illustrates by example how these influence the design of multi-objective decision-making systems for complex problems.",
                        "Citation Paper Authors": "Authors:Conor F. Hayes, Roxana R\u0103dulescu, Eugenio Bargiacchi, Johan K\u00e4llstr\u00f6m, Matthew Macfarlane, Mathieu Reymond, Timothy Verstraeten, Luisa M. Zintgraf, Richard Dazeley, Fredrik Heintz, Enda Howley, Athirai A. Irissappane, Patrick Mannion, Ann Now\u00e9, Gabriel Ramos, Marcello Restelli, Peter Vamplew, Diederik M. Roijers"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.09302v1": {
            "Paper Title": "Detecting Grasping Sites in a Martian Lava Tube: Multi-Stage Perception\n  Trade Study for ReachBo",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": "for\nan eight-boom prototype, but had assumed booms of trivial\nmass; and 30 kg had been assigned in even earlier work ",
                    "Citation Text": "S. Schneider, A. Bylard, T. G. Chen, P. Wang,\nM. Cutkosky, M. Lap \u02c6otre, and M. Pavone, \u201cReachbot:\nA small robot for large mobile manipulation tasks,\u201d in\n2022 IEEE Aerospace Conference . IEEE, 2022, pp.\n1\u201312.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.10829",
                        "Citation Paper Title": "Title:ReachBot: A Small Robot for Large Mobile Manipulation Tasks",
                        "Citation Paper Abstract": "Abstract:Robots are widely deployed in space environments because of their versatility and robustness. However, adverse gravity conditions and challenging terrain geometry expose the limitations of traditional robot designs, which are often forced to sacrifice one of mobility or manipulation capabilities to attain the other. Prospective climbing operations in these environments reveals a need for small, compact robots capable of versatile mobility and manipulation. We propose a novel robotic concept called ReachBot that fills this need by combining two existing technologies: extendable booms and mobile manipulation. ReachBot leverages the reach and tensile strength of extendable booms to achieve an outsized reachable workspace and wrench capability. Through their lightweight, compactable structure, these booms also reduce mass and complexity compared to traditional rigid-link articulated-arm designs. Using these advantages, ReachBot excels in mobile manipulation missions in low gravity or that require climbing, particularly when anchor points are sparse. After introducing the ReachBot concept, we discuss modeling approaches and strategies for increasing stability and robustness. We then develop a 2D analytical model for ReachBot's dynamics inspired by grasp models for dexterous manipulators. Next, we introduce a waypoint-tracking controller for a planar ReachBot in microgravity. Our simulation results demonstrate the controller's robustness to disturbances and modeling error. Finally, we briefly discuss next steps that build on these initially promising results to realize the full potential of ReachBot.",
                        "Citation Paper Authors": "Authors:Stephanie Schneider, Andrew Bylard, Tony G. Chen, Preston Wang, Mark Cutkosky, Marco Pavone"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2311.18126v2": {
            "Paper Title": "DisMech: A Discrete Differential Geometry-based Physical Simulator for\n  Soft Robots and Structures",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09159v1": {
            "Paper Title": "WIT-UAS: A Wildland-fire Infrared Thermal Dataset to Detect Crew Assets\n  From Aerial Views",
            "Sentences": [
                {
                    "Sentence ID": 22,
                    "Sentence": ". Suo et al. proposed a dataset of real thermal images\ncollected from high-altitude UA Vs for urban objects (such as\npeople, vehicles, bicycles, etc) ",
                    "Citation Text": "J. Suo, T. Wang, X. Zhang, H. Chen, W. Zhou, and W. Shi. Hit-uav:\nA high-altitude infrared thermal dataset for unmanned aerial vehicles,\n2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.03245",
                        "Citation Paper Title": "Title:HIT-UAV: A high-altitude infrared thermal dataset for Unmanned Aerial Vehicle-based object detection",
                        "Citation Paper Abstract": "Abstract:We present the HIT-UAV dataset, a high-altitude infrared thermal dataset for object detection applications on Unmanned Aerial Vehicles (UAVs). The dataset comprises 2,898 infrared thermal images extracted from 43,470 frames in hundreds of videos captured by UAVs in various scenarios including schools, parking lots, roads, and playgrounds. Moreover, the HIT-UAV provides essential flight data for each image, such as flight altitude, camera perspective, date, and daylight intensity. For each image, we have manually annotated object instances with bounding boxes of two types (oriented and standard) to tackle the challenge of significant overlap of object instances in aerial images. To the best of our knowledge, the HIT-UAV is the first publicly available high-altitude UAV-based infrared thermal dataset for detecting persons and vehicles. We have trained and evaluated well-established object detection algorithms on the HIT-UAV. Our results demonstrate that the detection algorithms perform exceptionally well on the HIT-UAV compared to visual light datasets since infrared thermal images do not contain significant irrelevant information about objects. We believe that the HIT-UAV will contribute to various UAV-based applications and researches. The dataset is freely available at this https URL.",
                        "Citation Paper Authors": "Authors:Jiashun Suo, Tianyi Wang, Xingzhou Zhang, Haiyang Chen, Wei Zhou, Weisong Shi"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": ", which employs multi-scale receptive field in\nits network design; and SAHI ",
                    "Citation Text": "F. C. Akyon, S. O. Altinuc, and A. Temizel. Slicing aided hyper\ninference and fine-tuning for small object detection. 2022 IEEE\nInternational Conference on Image Processing (ICIP) , pages 966\u2013970,\n2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.06934",
                        "Citation Paper Title": "Title:Slicing Aided Hyper Inference and Fine-tuning for Small Object Detection",
                        "Citation Paper Abstract": "Abstract:Detection of small objects and objects far away in the scene is a major challenge in surveillance applications. Such objects are represented by small number of pixels in the image and lack sufficient details, making them difficult to detect using conventional detectors. In this work, an open-source framework called Slicing Aided Hyper Inference (SAHI) is proposed that provides a generic slicing aided inference and fine-tuning pipeline for small object detection. The proposed technique is generic in the sense that it can be applied on top of any available object detector without any fine-tuning. Experimental evaluations, using object detection baselines on the Visdrone and xView aerial object detection datasets show that the proposed inference method can increase object detection AP by 6.8%, 5.1% and 5.3% for FCOS, VFNet and TOOD detectors, respectively. Moreover, the detection accuracy can be further increased with a slicing aided fine-tuning, resulting in a cumulative increase of 12.7%, 13.4% and 14.5% AP in the same order. Proposed technique has been integrated with Detectron2, MMDetection and YOLOv5 models and it is publicly available at this https URL .",
                        "Citation Paper Authors": "Authors:Fatih Cagatay Akyon, Sinan Onur Altinuc, Alptekin Temizel"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "are\nthree general object detection datasets that are widely used\nas benchmark datasets for general object detection. Thesedatasets have given rise to novel model architectures such as\nSSD ",
                    "Citation Text": "W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y . Fu, and\nA. C. Berg. SSD: Single shot MultiBox detector. In Computer Vision\n\u2013 ECCV 2016 , pages 21\u201337. Springer International Publishing, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.02325",
                        "Citation Paper Title": "Title:SSD: Single Shot MultiBox Detector",
                        "Citation Paper Abstract": "Abstract:We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For $300\\times 300$ input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for $500\\times 500$ input, SSD achieves 75.1% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at this https URL .",
                        "Citation Paper Authors": "Authors:Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2306.10007v2": {
            "Paper Title": "Robot Learning with Sensorimotor Pre-training",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": "trains language-conditioned\nTransformer policies from human or expert demonstrations. Likewise, we leverage Transformer\nmodels but focus on self-supervised learning from sensorimotor trajectories. ",
                    "Citation Text": "D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson,\nQ. Vuong, T. Yu, et al. Palm-e: An embodied multimodal language model. arXiv:2303.03378 ,\n2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.03378",
                        "Citation Paper Title": "Title:PaLM-E: An Embodied Multimodal Language Model",
                        "Citation Paper Abstract": "Abstract:Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.",
                        "Citation Paper Authors": "Authors:Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, Pete Florence"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "trains a generalist\nagent on trajectories from different tasks jointly. In contrast, we use a masked objective, operate on\nlatent visual representations, and focus on real-world trajectories. ",
                    "Citation Text": "A. Brohan, N. Brown, J. Carbajal, Y . Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Haus-\nman, A. Herzog, J. Hsu, et al. Rt-1: Robotics transformer for real-world control at scale.\narXiv:2212.06817 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.06817",
                        "Citation Paper Title": "Title:RT-1: Robotics Transformer for Real-World Control at Scale",
                        "Citation Paper Abstract": "Abstract:By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks. The project's website and videos can be found at this http URL",
                        "Citation Paper Authors": "Authors:Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, Brianna Zitkovich"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "simultaneously collects robot trajectories\nfor multiple tasks and learns corresponding policies; ",
                    "Citation Text": "A. Zhan, R. Zhao, L. Pinto, P. Abbeel, and M. Laskin. Learning visual robotic control effi-\nciently with contrastive pre-training and data augmentation. In IROS , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.07975",
                        "Citation Paper Title": "Title:Learning Visual Robotic Control Efficiently with Contrastive Pre-training and Data Augmentation",
                        "Citation Paper Abstract": "Abstract:Recent advances in unsupervised representation learning significantly improved the sample efficiency of training Reinforcement Learning policies in simulated environments. However, similar gains have not yet been seen for real-robot reinforcement learning. In this work, we focus on enabling data-efficient real-robot learning from pixels. We present Contrastive Pre-training and Data Augmentation for Efficient Robotic Learning (CoDER), a method that utilizes data augmentation and unsupervised learning to achieve sample-efficient training of real-robot arm policies from sparse rewards. While contrastive pre-training, data augmentation, demonstrations, and reinforcement learning are alone insufficient for efficient learning, our main contribution is showing that the combination of these disparate techniques results in a simple yet data-efficient method. We show that, given only 10 demonstrations, a single robotic arm can learn sparse-reward manipulation policies from pixels, such as reaching, picking, moving, pulling a large object, flipping a switch, and opening a drawer in just 30 minutes of mean real-world training time. We include videos and code on the project website: this https URL",
                        "Citation Paper Authors": "Authors:Albert Zhan, Ruihan Zhao, Lerrel Pinto, Pieter Abbeel, Michael Laskin"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "Self-supervised learning in robotics. There is a rich body of work on self-supervised learning in\nrobotics. [16, 17] learn grasping policies from large-scale self-supervision; ",
                    "Citation Text": "P. Florence, L. Manuelli, and R. Tedrake. Self-supervised correspondence in visuomotor policy\nlearning. RA-L , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.06933",
                        "Citation Paper Title": "Title:Self-Supervised Correspondence in Visuomotor Policy Learning",
                        "Citation Paper Abstract": "Abstract:In this paper we explore using self-supervised correspondence for improving the generalization performance and sample efficiency of visuomotor policy learning. Prior work has primarily used approaches such as autoencoding, pose-based losses, and end-to-end policy optimization in order to train the visual portion of visuomotor policies. We instead propose an approach using self-supervised dense visual correspondence training, and show this enables visuomotor policy learning with surprisingly high generalization performance with modest amounts of data: using imitation learning, we demonstrate extensive hardware validation on challenging manipulation tasks with as few as 50 demonstrations. Our learned policies can generalize across classes of objects, react to deformable object configurations, and manipulate textureless symmetrical objects in a variety of backgrounds, all with closed-loop, real-time vision-based policies. Simulated imitation learning experiments suggest that correspondence training offers sample complexity and generalization benefits compared to autoencoding and end-to-end training.",
                        "Citation Paper Authors": "Authors:Peter Florence, Lucas Manuelli, Russ Tedrake"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.09120v1": {
            "Paper Title": "Less is more -- the Dispatcher/ Executor principle for multi-task\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09073v1": {
            "Paper Title": "Optimal Motion Planning using Finite Fourier Series in a Learning-based\n  Collision Field",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": "implement the neural network for a warm start to\nimprove the planner reliability in various scenarios. Except\nfor the implementation in the numerical method, ",
                    "Citation Text": "B. Ichter, J. Harrison, and M. Pavone, \u201cLearning sampling distributions\nfor robot motion planning,\u201d in 2018 IEEE International Conference\non Robotics and Automation (ICRA) . IEEE, 2018, pp. 7087\u20137094.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.05448",
                        "Citation Paper Title": "Title:Learning Sampling Distributions for Robot Motion Planning",
                        "Citation Paper Abstract": "Abstract:A defining feature of sampling-based motion planning is the reliance on an implicit representation of the state space, which is enabled by a set of probing samples. Traditionally, these samples are drawn either probabilistically or deterministically to uniformly cover the state space. Yet, the motion of many robotic systems is often restricted to \"small\" regions of the state space, due to, for example, differential constraints or collision-avoidance constraints. To accelerate the planning process, it is thus desirable to devise non-uniform sampling strategies that favor sampling in those regions where an optimal solution might lie. This paper proposes a methodology for non-uniform sampling, whereby a sampling distribution is learned from demonstrations, and then used to bias sampling. The sampling distribution is computed through a conditional variational autoencoder, allowing sample generation from the latent space conditioned on the specific planning problem. This methodology is general, can be used in combination with any sampling-based planner, and can effectively exploit the underlying structure of a planning problem while maintaining the theoretical guarantees of sampling-based approaches. Specifically, on several planning problems, the proposed methodology is shown to effectively learn representations for the relevant regions of the state space, resulting in an order of magnitude improvement in terms of success rate and convergence to the optimal cost.",
                        "Citation Paper Authors": "Authors:Brian Ichter, James Harrison, Marco Pavone"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.09067v1": {
            "Paper Title": "Holodeck: Language Guided Generation of 3D Embodied AI Environments",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": "showcases its potential to generate large-\nscale interactive environments for training embodied agents.\nPhone2Proc ",
                    "Citation Text": "Matt Deitke, Rose Hendrix, Ali Farhadi, Kiana Ehsani, and\nAniruddha Kembhavi. Phone2proc: Bringing robust robots\ninto our chaotic world. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 9665\u20139675, 2023. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.04819",
                        "Citation Paper Title": "Title:Phone2Proc: Bringing Robust Robots Into Our Chaotic World",
                        "Citation Paper Abstract": "Abstract:Training embodied agents in simulation has become mainstream for the embodied AI community. However, these agents often struggle when deployed in the physical world due to their inability to generalize to real-world environments. In this paper, we present Phone2Proc, a method that uses a 10-minute phone scan and conditional procedural generation to create a distribution of training scenes that are semantically similar to the target environment. The generated scenes are conditioned on the wall layout and arrangement of large objects from the scan, while also sampling lighting, clutter, surface textures, and instances of smaller objects with randomized placement and materials. Leveraging just a simple RGB camera, training with Phone2Proc shows massive improvements from 34.7% to 70.7% success rate in sim-to-real ObjectNav performance across a test suite of over 200 trials in diverse real-world environments, including homes, offices, and RoboTHOR. Furthermore, Phone2Proc's diverse distribution of generated scenes makes agents remarkably robust to changes in the real world, such as human movement, object rearrangement, lighting changes, or clutter.",
                        "Citation Paper Authors": "Authors:Matt Deitke, Rose Hendrix, Luca Weihs, Ali Farhadi, Kiana Ehsani, Aniruddha Kembhavi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.09056v1": {
            "Paper Title": "ReCoRe: Regularized Contrastive Representation Learning of World Model",
            "Sentences": [
                {
                    "Sentence ID": 37,
                    "Sentence": "to evaluate out-of-distribution\n(OoD) generalization, and include the Gibson dataset ",
                    "Citation Text": "Fei Xia, Amir R. Zamir, Zhi-Yang He, Alexander Sax, Jiten-\ndra Malik, and Silvio Savarese. Gibson env: real-world per-\nception for embodied agents. In IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR) . IEEE, 2018.\n2, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.10654",
                        "Citation Paper Title": "Title:Gibson Env: Real-World Perception for Embodied Agents",
                        "Citation Paper Abstract": "Abstract:Developing visual perception models for active agents and sensorimotor control are cumbersome to be done in the physical world, as existing algorithms are too slow to efficiently learn in real-time and robots are fragile and costly. This has given rise to learning-in-simulation which consequently casts a question on whether the results transfer to real-world. In this paper, we are concerned with the problem of developing real-world perception for active agents, propose Gibson Virtual Environment for this purpose, and showcase sample perceptual tasks learned therein. Gibson is based on virtualizing real spaces, rather than using artificially designed ones, and currently includes over 1400 floor spaces from 572 full buildings. The main characteristics of Gibson are: I. being from the real-world and reflecting its semantic complexity, II. having an internal synthesis mechanism, \"Goggles\", enabling deploying the trained models in real-world without needing further domain adaptation, III. embodiment of agents and making them subject to constraints of physics and space.",
                        "Citation Paper Authors": "Authors:Fei Xia, Amir Zamir, Zhi-Yang He, Alexander Sax, Jitendra Malik, Silvio Savarese"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": ". We have used five\nlayer encoder with starting number of feature maps equal to\n32, then doubled in every consecutive layers. To encode the\ntask observations we used two dense layers of size 32 with\nELU activations ",
                    "Citation Text": "Djork-Arn \u00b4e Clevert, Thomas Unterthiner, and Sepp Hochre-\niter. Fast and accurate deep network learning by exponential\nlinear units (elus). arXiv preprint arXiv:1511.07289 , 2015.\n5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.07289",
                        "Citation Paper Title": "Title:Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)",
                        "Citation Paper Abstract": "Abstract:We introduce the \"exponential linear unit\" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10% classification error for a single crop, single model network.",
                        "Citation Paper Authors": "Authors:Djork-Arn\u00e9 Clevert, Thomas Unterthiner, Sepp Hochreiter"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "added contrastive loss as an auxiliary task and outperformed\nthe state-of-the-art model-based RL method called Dreamer ",
                    "Citation Text": "Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Moham-\nmad Norouzi. Dream to control: Learning behaviors by la-\ntent imagination. In International Conference on Learning\nRepresentations , 2020. 4, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.01603",
                        "Citation Paper Title": "Title:Dream to Control: Learning Behaviors by Latent Imagination",
                        "Citation Paper Abstract": "Abstract:Learned world models summarize an agent's experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.",
                        "Citation Paper Authors": "Authors:Danijar Hafner, Timothy Lillicrap, Jimmy Ba, Mohammad Norouzi"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": ". Masked World Models (MWM) combines the trans-\nformer based masked auto-encoder with DreamerV2. Other\nDreamer variants DayDreamer ",
                    "Citation Text": "Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter\nAbbeel, and Ken Goldberg. Daydreamer: World models for\nphysical robot learning. In Conference on Robot Learning ,\npages 2226\u20132240. PMLR, 2023. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.14176",
                        "Citation Paper Title": "Title:DayDreamer: World Models for Physical Robot Learning",
                        "Citation Paper Abstract": "Abstract:To solve tasks in complex environments, robots need to learn from experience. Deep reinforcement learning is a common approach to robot learning but requires a large amount of trial and error to learn, limiting its deployment in the physical world. As a consequence, many advances in robot learning rely on simulators. On the other hand, learning inside of simulators fails to capture the complexity of the real world, is prone to simulator inaccuracies, and the resulting behaviors do not adapt to changes in the world. The Dreamer algorithm has recently shown great promise for learning from small amounts of interaction by planning within a learned world model, outperforming pure reinforcement learning in video games. Learning a world model to predict the outcomes of potential actions enables planning in imagination, reducing the amount of trial and error needed in the real environment. However, it is unknown whether Dreamer can facilitate faster learning on physical robots. In this paper, we apply Dreamer to 4 robots to learn online and directly in the real world, without simulators. Dreamer trains a quadruped robot to roll off its back, stand up, and walk from scratch and without resets in only 1 hour. We then push the robot and find that Dreamer adapts within 10 minutes to withstand perturbations or quickly roll over and stand back up. On two different robotic arms, Dreamer learns to pick and place multiple objects directly from camera images and sparse rewards, approaching human performance. On a wheeled robot, Dreamer learns to navigate to a goal position purely from camera images, automatically resolving ambiguity about the robot orientation. Using the same hyperparameters across all experiments, we find that Dreamer is capable of online learning in the real world, establishing a strong baseline. We release our infrastructure for future applications of world models to robot learning.",
                        "Citation Paper Authors": "Authors:Philipp Wu, Alejandro Escontrela, Danijar Hafner, Ken Goldberg, Pieter Abbeel"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.09033v1": {
            "Paper Title": "Using Surprise Index for Competency Assessment in Autonomous\n  Decision-Making",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.09024v1": {
            "Paper Title": "Bayes Net based highbrid Monte Carlo Optimization for Redundant\n  Manipulator",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.08991v1": {
            "Paper Title": "A Sim-to-Real Deep Learning-based Framework for Autonomous Nano-drone\n  Racing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.08961v1": {
            "Paper Title": "Contact-Implicit MPC: Controlling Diverse Quadruped Motions Without\n  Pre-Planned Contact Modes or Trajectories",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.08958v1": {
            "Paper Title": "LiFT: Unsupervised Reinforcement Learning with Foundation Models as\n  Teachers",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": ", and language labels [ 18,5]; or\nuse pre-trained VLMs to obtain visual feedback [ 19,25,44,45]. Closest to ours, ELLM ",
                    "Citation Text": "Yuqing Du, Olivia Watkins, Zihan Wang, C\u00e9dric Colas, Trevor Darrell, Pieter Abbeel, Abhishek\nGupta, and Jacob Andreas. Guiding pretraining in reinforcement learning with large language\nmodels. International Conference on Machine Learning , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.06692",
                        "Citation Paper Title": "Title:Guiding Pretraining in Reinforcement Learning with Large Language Models",
                        "Citation Paper Abstract": "Abstract:Reinforcement learning algorithms typically struggle in the absence of a dense, well-shaped reward function. Intrinsically motivated exploration methods address this limitation by rewarding agents for visiting novel states or transitions, but these methods offer limited benefits in large environments where most discovered novelty is irrelevant for downstream tasks. We describe a method that uses background knowledge from text corpora to shape exploration. This method, called ELLM (Exploring with LLMs) rewards an agent for achieving goals suggested by a language model prompted with a description of the agent's current state. By leveraging large-scale language model pretraining, ELLM guides agents toward human-meaningful and plausibly useful behaviors without requiring a human in the loop. We evaluate ELLM in the Crafter game environment and the Housekeep robotic simulator, showing that ELLM-trained agents have better coverage of common-sense behaviors during pretraining and usually match or improve performance on a range of downstream tasks. Code available at this https URL.",
                        "Citation Paper Authors": "Authors:Yuqing Du, Olivia Watkins, Zihan Wang, C\u00e9dric Colas, Trevor Darrell, Pieter Abbeel, Abhishek Gupta, Jacob Andreas"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.08851v1": {
            "Paper Title": "Achelous++: Power-Oriented Water-Surface Panoptic Perception Framework\n  on Edge Devices based on Vision-Radar Fusion and Pruning of Heterogeneous\n  Modalities",
            "Sentences": [
                {
                    "Sentence ID": 78,
                    "Sentence": "C 1 3.71 5.29 - - - 73.5 99.4 72.9 - 41.6 46.3 89.3 184.7\nPSPNet ",
                    "Citation Text": "H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \u201cPyramid scene parsing\nnetwork,\u201d in Proceedings of the IEEE conference on computer vision\nand pattern recognition , pp. 2881\u20132890, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.01105",
                        "Citation Paper Title": "Title:Pyramid Scene Parsing Network",
                        "Citation Paper Abstract": "Abstract:Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes.",
                        "Citation Paper Authors": "Authors:Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia"
                    }
                },
                {
                    "Sentence ID": 77,
                    "Sentence": "C+R 1 23.54 - 41.8 71.2 44.5 - - - - - - - -\nSemantic Segmentation\nSegformer-B0 ",
                    "Citation Text": "E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo,\n\u201cSegformer: Simple and efficient design for semantic segmentation\nwith transformers,\u201d Advances in Neural Information Processing Systems ,\nvol. 34, pp. 12077\u201312090, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.15203",
                        "Citation Paper Title": "Title:SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers",
                        "Citation Paper Abstract": "Abstract:We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perception (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5x smaller and 2.2% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code will be released at: this http URL.",
                        "Citation Paper Authors": "Authors:Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo"
                    }
                },
                {
                    "Sentence ID": 75,
                    "Sentence": "C131 3.01 2.05 41.9 71.8 44.0 - - - - 29.8 51.9 117.8 135.7\nYOLOv7-Tiny ",
                    "Citation Text": "C.-Y . Wang, A. Bochkovskiy, and H.-Y . M. Liao, \u201cYolov7: Trainable\nbag-of-freebies sets new state-of-the-art for real-time object detectors,\u201d\ninProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pp. 7464\u20137475, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2207.02696",
                        "Citation Paper Title": "Title:YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors",
                        "Citation Paper Abstract": "Abstract:YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56.8% AP among all known real-time object detectors with 30 FPS or higher on GPU V100. YOLOv7-E6 object detector (56 FPS V100, 55.9% AP) outperforms both transformer-based detector SWIN-L Cascade-Mask R-CNN (9.2 FPS A100, 53.9% AP) by 509% in speed and 2% in accuracy, and convolutional-based detector ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2% AP) by 551% in speed and 0.7% AP in accuracy, as well as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors in speed and accuracy. Moreover, we train YOLOv7 only on MS COCO dataset from scratch without using any other datasets or pre-trained weights. Source code is released in this https URL.",
                        "Citation Paper Authors": "Authors:Chien-Yao Wang, Alexey Bochkovskiy, Hong-Yuan Mark Liao"
                    }
                },
                {
                    "Sentence ID": 70,
                    "Sentence": ". As for\npoint cloud semantic segmentation, we employ Negative Log\nLikelihood (NLL) loss ",
                    "Citation Text": "D. Zhu, H. Yao, B. Jiang, and P. Yu, \u201cNegative log likelihood\nratio loss for deep neural network classification,\u201d arXiv preprint\narXiv:1804.10690 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.10690",
                        "Citation Paper Title": "Title:Negative Log Likelihood Ratio Loss for Deep Neural Network Classification",
                        "Citation Paper Abstract": "Abstract:In deep neural network, the cross-entropy loss function is commonly used for classification. Minimizing cross-entropy is equivalent to maximizing likelihood under assumptions of uniform feature and class distributions. It belongs to generative training criteria which does not directly discriminate correct class from competing classes. We propose a discriminative loss function with negative log likelihood ratio between correct and competing classes. It significantly outperforms the cross-entropy loss on the CIFAR-10 image classification task.",
                        "Citation Paper Authors": "Authors:Donglai Zhu, Hengshuai Yao, Bei Jiang, Peng Yu"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "has had a significant\nimpact in the pruning domain, its effects on structural pruning\nare less pronounced and might even extend training times ",
                    "Citation Text": "Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, \u201cRethinking\nthe value of network pruning,\u201d International Conference on Learning\nRepresentations , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.05270",
                        "Citation Paper Title": "Title:Rethinking the Value of Network Pruning",
                        "Citation Paper Abstract": "Abstract:Network pruning is widely used for reducing the heavy inference cost of deep models in low-resource settings. A typical pruning algorithm is a three-stage pipeline, i.e., training (a large model), pruning and fine-tuning. During pruning, according to a certain criterion, redundant weights are pruned and important weights are kept to best preserve the accuracy. In this work, we make several surprising observations which contradict common beliefs. For all state-of-the-art structured pruning algorithms we examined, fine-tuning a pruned model only gives comparable or worse performance than training that model with randomly initialized weights. For pruning algorithms which assume a predefined target network architecture, one can get rid of the full pipeline and directly train the target network from scratch. Our observations are consistent for multiple network architectures, datasets, and tasks, which imply that: 1) training a large, over-parameterized model is often not necessary to obtain an efficient final model, 2) learned \"important\" weights of the large model are typically not useful for the small pruned model, 3) the pruned architecture itself, rather than a set of inherited \"important\" weights, is more crucial to the efficiency in the final model, which suggests that in some cases pruning can be useful as an architecture search paradigm. Our results suggest the need for more careful baseline evaluations in future research on structured pruning methods. We also compare with the \"Lottery Ticket Hypothesis\" (Frankle & Carbin 2019), and find that with optimal learning rate, the \"winning ticket\" initialization as used in Frankle & Carbin (2019) does not bring improvement over random initialization.",
                        "Citation Paper Authors": "Authors:Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, Trevor Darrell"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "proposes an asymmetric fusion mecha-\nnism to improve the performances of detection and segmen-\ntation bilaterally. UniAD ",
                    "Citation Text": "Y . Hu, J. Yang, L. Chen, K. Li, C. Sima, X. Zhu, S. Chai, S. Du,\nT. Lin, W. Wang, et al. , \u201cPlanning-oriented autonomous driving,\u201d in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pp. 17853\u201317862, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.10156",
                        "Citation Paper Title": "Title:Planning-oriented Autonomous Driving",
                        "Citation Paper Abstract": "Abstract:Modern autonomous driving system is characterized as modular tasks in sequential order, i.e., perception, prediction, and planning. In order to perform a wide diversity of tasks and achieve advanced-level intelligence, contemporary approaches either deploy standalone models for individual tasks, or design a multi-task paradigm with separate heads. However, they might suffer from accumulative errors or deficient task coordination. Instead, we argue that a favorable framework should be devised and optimized in pursuit of the ultimate goal, i.e., planning of the self-driving car. Oriented at this, we revisit the key components within perception and prediction, and prioritize the tasks such that all these tasks contribute to planning. We introduce Unified Autonomous Driving (UniAD), a comprehensive framework up-to-date that incorporates full-stack driving tasks in one network. It is exquisitely devised to leverage advantages of each module, and provide complementary feature abstractions for agent interaction from a global perspective. Tasks are communicated with unified query interfaces to facilitate each other toward planning. We instantiate UniAD on the challenging nuScenes benchmark. With extensive ablations, the effectiveness of using such a philosophy is proven by substantially outperforming previous state-of-the-arts in all aspects. Code and models are public.",
                        "Citation Paper Authors": "Authors:Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu Qiao, Hongyang Li"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.08805v1": {
            "Paper Title": "Zoom in on the Plant: Fine-grained Analysis of Leaf, Stem and Vein\n  Instances",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.08718v1": {
            "Paper Title": "Motion Planning and Control of Hybrid Flying-Crawling Quadrotors",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "A. Motion Planning\nAlthough there is limited research dedicated to motion\nplanning for HyFCQs, we can still reference planning meth-\nods designed for other terrestrial-aerial robots with different\nstructures and motion modes. Fan et al . ",
                    "Citation Text": "D. D. Fan, R. Thakker, T. Bartlett, M. B. Miled, L. Kim, E. Theodorou,\nand A.-a. Agha-mohammadi, \u201cAutonomous hybrid ground/aerial mo-\nbility in unknown environments,\u201d in 2019 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS) . IEEE, 2019,\npp. 3070\u20133077.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.05631",
                        "Citation Paper Title": "Title:Autonomous Hybrid Ground/Aerial Mobility in Unknown Environments",
                        "Citation Paper Abstract": "Abstract:Hybrid ground and aerial vehicles can possess distinct advantages over ground-only or flight-only designs in terms of energy savings and increased mobility. In this work we outline our unified framework for controls, planning, and autonomy of hybrid ground/air vehicles. Our contribution is three-fold: 1) We develop a control scheme for the control of passive two-wheeled hybrid ground/aerial vehicles. 2) We present a unified planner for both rolling and flying by leveraging differential flatness mappings. 3) We conduct experiments leveraging mapping and global planning for hybrid mobility in unknown environments, showing that hybrid mobility uses up to five times less energy than flying only.",
                        "Citation Paper Authors": "Authors:David D. Fan, Rohan Thakker, Tara Bartlett, Meriem Ben Miled, Leon Kim, Evangelos Theodorou, Ali-akbar Agha-mohammadi"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ", non-uniform B-spline trajectory optimization based\non the Euclidean Signed Distance Field (ESDF) map ",
                    "Citation Text": "V . Usenko, L. von Stumberg, A. Pangercic, and D. Cremers, \u201cReal-\ntime trajectory replanning for MA Vs using uniform b-splines and a\n3d circular buffer,\u201d in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. ,\n2017, pp. 215\u2013222.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.01416",
                        "Citation Paper Title": "Title:Real-Time Trajectory Replanning for MAVs using Uniform B-splines and a 3D Circular Buffer",
                        "Citation Paper Abstract": "Abstract:In this paper, we present a real-time approach to local trajectory replanning for microaerial vehicles (MAVs). Current trajectory generation methods for multicopters achieve high success rates in cluttered environments, but assume that the environment is static and require prior knowledge of the map. In the presented study, we use the results of such planners and extend them with a local replanning algorithm that can handle unmodeled (possibly dynamic) obstacles while keeping the MAV close to the global trajectory. To ensure that the proposed approach is real-time capable, we maintain information about the environment around the MAV in an occupancy grid stored in a three-dimensional circular buffer, which moves together with a drone, and represent the trajectories by using uniform B-splines. This representation ensures that the trajectory is sufficiently smooth and simultaneously allows for efficient optimization.",
                        "Citation Paper Authors": "Authors:Vladyslav Usenko, Lukas von Stumberg, Andrej Pangercic, Daniel Cremers"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.08689v1": {
            "Paper Title": "Safety-Critical Coordination of Legged Robots via Layered Controllers\n  and Forward Reachable Set based Control Barrier Functions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2303.13630v2": {
            "Paper Title": "Safety-Critical Coordination for Cooperative Legged Locomotion via\n  Control Barrier Functions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.07953v2": {
            "Paper Title": "Enhancing Robotic Navigation: An Evaluation of Single and\n  Multi-Objective Reinforcement Learning Strategies",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.08668v1": {
            "Paper Title": "Versatile Telescopic-Wheeled-Legged Locomotion of Tachyon 3 via\n  Full-Centroidal Nonlinear Model Predictive Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.08604v1": {
            "Paper Title": "Verification of Neural Reachable Tubes via Scenario Optimization and\n  Conformal Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2303.02904v4": {
            "Paper Title": "Social Cue Detection and Analysis Using Transfer Entropy",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": ". Jaques et al. use MI as a social\ninfluence reward for multi-agent deep reinforcement learning in\nSequential Social Dilemmas (SSDs) ",
                    "Citation Text": "Joel Z. Leibo, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, and Thore Grae-\npel. 2017. Multi-Agent Reinforcement Learning in Sequential Social Dilemmas.\nInProceedings of the 16th Conference on Autonomous Agents and MultiAgent Sys-\ntems (S\u00e3o Paulo, Brazil) (AAMAS \u201917) . International Foundation for Autonomous\nAgents and Multiagent Systems, Richland, SC, 464\u2013473.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.03037",
                        "Citation Paper Title": "Title:Multi-agent Reinforcement Learning in Sequential Social Dilemmas",
                        "Citation Paper Abstract": "Abstract:Matrix games like Prisoner's Dilemma have guided research on social dilemmas for decades. However, they necessarily treat the choice to cooperate or defect as an atomic action. In real-world social dilemmas these choices are temporally extended. Cooperativeness is a property that applies to policies, not elementary actions. We introduce sequential social dilemmas that share the mixed incentive structure of matrix game social dilemmas but also require agents to learn policies that implement their strategic intentions. We analyze the dynamics of policies learned by multiple self-interested independent learning agents, each using its own deep Q-network, on two Markov games we introduce here: 1. a fruit Gathering game and 2. a Wolfpack hunting game. We characterize how learned behavior in each domain changes as a function of environmental factors including resource abundance. Our experiments show how conflict can emerge from competition over shared resources and shed light on how the sequential nature of real world social dilemmas affects cooperation.",
                        "Citation Paper Authors": "Authors:Joel Z. Leibo, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, Thore Graepel"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.08566v1": {
            "Paper Title": "Learning adaptive planning representations with natural language\n  guidance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.08554v1": {
            "Paper Title": "Adaptive Robot Coordination: A Subproblem-based Approach for Hybrid\n  Multi-Robot Motion Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2311.02558v2": {
            "Paper Title": "Multi-Agent 3D Map Reconstruction and Change Detection in Microgravity\n  with Free-Flying Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2311.18044v2": {
            "Paper Title": "Transfer Learning in Robotics: An Upcoming Breakthrough? A Review of\n  Promises and Challenges",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.08344v1": {
            "Paper Title": "FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": "leverage\nstructure-from-motion (SfM) for object modeling and pre-\ntrain 2D-3D matching networks to solve the pose from cor-\nrespondences. FS6D ",
                    "Citation Text": "Yisheng He, Yao Wang, Haoqiang Fan, Jian Sun, and Qifeng\nChen. FS6D: Few-shot 6D pose estimation of novel objects.\nInProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) , pages 6814\u20136824,\n2022. 2, 3, 6, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.14628",
                        "Citation Paper Title": "Title:FS6D: Few-Shot 6D Pose Estimation of Novel Objects",
                        "Citation Paper Abstract": "Abstract:6D object pose estimation networks are limited in their capability to scale to large numbers of object instances due to the close-set assumption and their reliance on high-fidelity object CAD models. In this work, we study a new open set problem; the few-shot 6D object poses estimation: estimating the 6D pose of an unknown object by a few support views without extra training. To tackle the problem, we point out the importance of fully exploring the appearance and geometric relationship between the given support views and query scene patches and propose a dense prototypes matching framework by extracting and matching dense RGBD prototypes with transformers. Moreover, we show that the priors from diverse appearances and shapes are crucial to the generalization capability under the problem setting and thus propose a large-scale RGBD photorealistic dataset (ShapeNet6D) for network pre-training. A simple and effective online texture blending approach is also introduced to eliminate the domain gap from the synthesis dataset, which enriches appearance diversity at a low cost. Finally, we discuss possible solutions to this problem and establish benchmarks on popular datasets to facilitate future research. The project page is at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Yisheng He, Yao Wang, Haoqiang Fan, Jian Sun, Qifeng Chen"
                    }
                },
                {
                    "Sentence ID": 61,
                    "Sentence": ",\nthe SDF representation \u2126provides higher quality depth ren-\ndering while removing the need to manually select a density\nthreshold.\nField Learning. For texture learning, we follow the volu-\nmetric rendering over truncated near-surface regions ",
                    "Citation Text": "Bowen Wen, Jonathan Tremblay, Valts Blukis, Stephen\nTyree, Thomas M\u00fcller, Alex Evans, Dieter Fox, Jan Kautz,\nand Stan Birchfield. BundleSDF: Neural 6-DoF tracking and\n3D reconstruction of unknown objects. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) , pages 606\u2013617, 2023. 2, 4, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.14158",
                        "Citation Paper Title": "Title:BundleSDF: Neural 6-DoF Tracking and 3D Reconstruction of Unknown Objects",
                        "Citation Paper Abstract": "Abstract:We present a near real-time method for 6-DoF tracking of an unknown object from a monocular RGBD video sequence, while simultaneously performing neural 3D reconstruction of the object. Our method works for arbitrary rigid objects, even when visual texture is largely absent. The object is assumed to be segmented in the first frame only. No additional information is required, and no assumption is made about the interaction agent. Key to our method is a Neural Object Field that is learned concurrently with a pose graph optimization process in order to robustly accumulate information into a consistent 3D representation capturing both geometry and appearance. A dynamic pool of posed memory frames is automatically maintained to facilitate communication between these threads. Our approach handles challenging sequences with large pose changes, partial and full occlusion, untextured surfaces, and specular highlights. We show results on HO3D, YCBInEOAT, and BEHAVE datasets, demonstrating that our method significantly outperforms existing approaches. Project page: this https URL",
                        "Citation Paper Authors": "Authors:Bowen Wen, Jonathan Tremblay, Valts Blukis, Stephen Tyree, Thomas Muller, Alex Evans, Dieter Fox, Jan Kautz, Stan Birchfield"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "+ ICP \u2713 48.2 - 57.2 -\n(PPF, Sift) + Zephyr ",
                    "Citation Text": "Brian Okorn, Qiao Gu, Martial Hebert, and David Held.\nZephyr: Zero-shot pose hypothesis rating. In IEEE Inter-\nnational Conference on Robotics and Automation (ICRA) ,\npages 14141\u201314148, 2021. 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.13526",
                        "Citation Paper Title": "Title:ZePHyR: Zero-shot Pose Hypothesis Rating",
                        "Citation Paper Abstract": "Abstract:Pose estimation is a basic module in many robot manipulation pipelines. Estimating the pose of objects in the environment can be useful for grasping, motion planning, or manipulation. However, current state-of-the-art methods for pose estimation either rely on large annotated training sets or simulated data. Further, the long training times for these methods prohibit quick interaction with novel objects. To address these issues, we introduce a novel method for zero-shot object pose estimation in clutter. Our approach uses a hypothesis generation and scoring framework, with a focus on learning a scoring function that generalizes to objects not used for training. We achieve zero-shot generalization by rating hypotheses as a function of unordered point differences. We evaluate our method on challenging datasets with both textured and untextured objects in cluttered scenes and demonstrate that our method significantly outperforms previous methods on this task. We also demonstrate how our system can be used by quickly scanning and building a model of a novel object, which can immediately be used by our method for pose estimation. Our work allows users to estimate the pose of novel objects without requiring any retraining. Additional information can be found on our website this https URL",
                        "Citation Paper Authors": "Authors:Brian Okorn, Qiao Gu, Martial Hebert, David Held"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "RGB \u2713 200 31.2 97.3 88.0 89.8 70.4 92.5 42.3 99.7 48.0 69.7 97.4 97.8 76.0 76.9\nLatentFusion ",
                    "Citation Text": "Keunhong Park, Arsalan Mousavian, Yu Xiang, and Dieter\nFox. LatentFusion: End-to-end differentiable reconstruction\nand rendering for unseen object pose estimation. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR) , pages 10710\u201310719, 2020. 2,\n7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.00416",
                        "Citation Paper Title": "Title:LatentFusion: End-to-End Differentiable Reconstruction and Rendering for Unseen Object Pose Estimation",
                        "Citation Paper Abstract": "Abstract:Current 6D object pose estimation methods usually require a 3D model for each object. These methods also require additional training in order to incorporate new objects. As a result, they are difficult to scale to a large number of objects and cannot be directly applied to unseen objects.\nWe propose a novel framework for 6D pose estimation of unseen objects. We present a network that reconstructs a latent 3D representation of an object using a small number of reference views at inference time. Our network is able to render the latent 3D representation from arbitrary views. Using this neural renderer, we directly optimize for pose given an input image. By training our network with a large number of 3D shapes for reconstruction and rendering, our network generalizes well to unseen objects. We present a new dataset for unseen object pose estimation--MOPED. We evaluate the performance of our method for unseen object pose estimation on MOPED as well as the ModelNet and LINEMOD datasets. Our method performs competitively to supervised methods that are trained on those objects. Code and data is available at this https URL.",
                        "Citation Paper Authors": "Authors:Keunhong Park, Arsalan Mousavian, Yu Xiang, Dieter Fox"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": "designs a detection, re-\ntrieval and refinement pipeline. However, to avoid difficul-\nties with out-of-distribution test set, it requires fine-tuning.\nOnePose ",
                    "Citation Text": "Jiaming Sun, Zihao Wang, Siyu Zhang, Xingyi He,\nHongcheng Zhao, Guofeng Zhang, and Xiaowei Zhou.\nOnePose: One-shot object pose estimation without CAD\nmodels. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) , pages\n6825\u20136834, 2022. 1, 2, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.12257",
                        "Citation Paper Title": "Title:OnePose: One-Shot Object Pose Estimation without CAD Models",
                        "Citation Paper Abstract": "Abstract:We propose a new method named OnePose for object pose estimation. Unlike existing instance-level or category-level methods, OnePose does not rely on CAD models and can handle objects in arbitrary categories without instance- or category-specific network training. OnePose draws the idea from visual localization and only requires a simple RGB video scan of the object to build a sparse SfM model of the object. Then, this model is registered to new query images with a generic feature matching network. To mitigate the slow runtime of existing visual localization methods, we propose a new graph attention network that directly matches 2D interest points in the query image with the 3D points in the SfM model, resulting in efficient and robust pose estimation. Combined with a feature-based pose tracker, OnePose is able to stably detect and track 6D poses of everyday household objects in real-time. We also collected a large-scale dataset that consists of 450 sequences of 150 objects.",
                        "Citation Paper Authors": "Authors:Jiaming Sun, Zihao Wang, Siyu Zhang, Xingyi He, Hongcheng Zhao, Guofeng Zhang, Xiaowei Zhou"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": ".\n\u2022Recall of ADD that is less than 0.1 of the object diameter\n(ADD-0.1d), as used in [17, 20].\n\u2022Average recall (AR) of VSD, MSSD and MSPD metrics\nintroduced in the BOP challenge ",
                    "Citation Text": "Tomas Hodan, Frank Michel, Eric Brachmann, Wadim Kehl,\nAnders GlentBuch, Dirk Kraft, Bertram Drost, Joel Vidal,\nStephan Ihrke, Xenophon Zabulis, et al. BOP: Benchmarkfor 6D object pose estimation. In Proceedings of the Euro-\npean Conference on Computer Vision (ECCV) , pages 19\u201334,\n2018. 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.08319",
                        "Citation Paper Title": "Title:BOP: Benchmark for 6D Object Pose Estimation",
                        "Citation Paper Abstract": "Abstract:We propose a benchmark for 6D pose estimation of a rigid object from a single RGB-D input image. The training data consists of a texture-mapped 3D object model or images of the object in known 6D poses. The benchmark comprises of: i) eight datasets in a unified format that cover different practical scenarios, including two new datasets focusing on varying lighting conditions, ii) an evaluation methodology with a pose-error function that deals with pose ambiguities, iii) a comprehensive evaluation of 15 diverse recent methods that captures the status quo of the field, and iv) an online evaluation system that is open for continuous submission of new results. The evaluation shows that methods based on point-pair features currently perform best, outperforming template matching methods, learning-based methods and methods based on 3D local features. The project website is available at this http URL.",
                        "Citation Paper Authors": "Authors:Tomas Hodan, Frank Michel, Eric Brachmann, Wadim Kehl, Anders Glent Buch, Dirk Kraft, Bertram Drost, Joel Vidal, Stephan Ihrke, Xenophon Zabulis, Caner Sahin, Fabian Manhardt, Federico Tombari, Tae-Kyun Kim, Jiri Matas, Carsten Rother"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": "with position embed-\nding. Finally, the network predicts the translation update\n\u2206t\u2208R3and rotation update \u2206R\u2208SO(3), each individu-\nally processed by a transformer encoder ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in Neural\nInformation Processing Systems (NeurIPS) , 30, 2017. 5, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": ":\nc(r) =Zz(r)+0.5\u03bb\nz(r)\u2212\u03bbw(xi)\u03a6(f\u2126(xi), n(xi), d(xi))dt, (1)\nw(xi) =1\n1 +e\u2212\u03b1\u2126(xi)1\n1 +e\u03b1\u2126(xi), (2)\nwhere w(xi)is the bell-shaped probability density function ",
                    "Citation Text": "Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. NeuS: Learning neural im-\nplicit surfaces by volume rendering for multi-view recon-\nstruction. In Advances in Neural Information Processing\nSystems (NeurIPS) , 2021. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.10689",
                        "Citation Paper Title": "Title:NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction",
                        "Citation Paper Abstract": "Abstract:We present a novel neural surface reconstruction method, called NeuS, for reconstructing objects and scenes with high fidelity from 2D image inputs. Existing neural surface reconstruction approaches, such as DVR and IDR, require foreground mask as supervision, easily get trapped in local minima, and therefore struggle with the reconstruction of objects with severe self-occlusion or thin structures. Meanwhile, recent neural methods for novel view synthesis, such as NeRF and its variants, use volume rendering to produce a neural scene representation with robustness of optimization, even for highly complex objects. However, extracting high-quality surfaces from this learned implicit representation is difficult because there are not sufficient surface constraints in the representation. In NeuS, we propose to represent a surface as the zero-level set of a signed distance function (SDF) and develop a new volume rendering method to train a neural SDF representation. We observe that the conventional volume rendering method causes inherent geometric errors (i.e. bias) for surface reconstruction, and therefore propose a new formulation that is free of bias in the first order of approximation, thus leading to more accurate surface reconstruction even without the mask supervision. Experiments on the DTU dataset and the BlendedMVS dataset show that NeuS outperforms the state-of-the-arts in high-quality surface reconstruction, especially for objects and scenes with complex structures and self-occlusion.",
                        "Citation Paper Authors": "Authors:Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, Wenping Wang"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "we chose the objects from the Objaverse-\nLVIS subset that consists of more than 40K objects belong-\ning to 1156 LVIS ",
                    "Citation Text": "Agrim Gupta, Piotr Dollar, and Ross Girshick. LVIS: A\ndataset for large vocabulary instance segmentation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR) , pages 5356\u20135364, 2019. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.03195",
                        "Citation Paper Title": "Title:LVIS: A Dataset for Large Vocabulary Instance Segmentation",
                        "Citation Paper Abstract": "Abstract:Progress on object detection is enabled by datasets that focus the research community's attention on open challenges. This process led us from simple images to complex scenes and from bounding boxes to segmentation masks. In this work, we introduce LVIS (pronounced `el-vis'): a new dataset for Large Vocabulary Instance Segmentation. We plan to collect ~2 million high-quality instance segmentation masks for over 1000 entry-level object categories in 164k images. Due to the Zipfian distribution of categories in natural images, LVIS naturally has a long tail of categories with few training samples. Given that state-of-the-art deep learning methods for object detection perform poorly in the low-sample regime, we believe that our dataset poses an important and exciting new scientific challenge. LVIS is available at this http URL.",
                        "Citation Paper Authors": "Authors:Agrim Gupta, Piotr Doll\u00e1r, Ross Girshick"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.08250v1": {
            "Paper Title": "Enhancing Robot Program Synthesis Through Environmental Context",
            "Sentences": [
                {
                    "Sentence ID": 54,
                    "Sentence": ", while Ye et al. enhance neural networks using a loss function\nbased on program compilation and execution information ",
                    "Citation Text": "He Ye, Matias Martinez, and Martin Monperrus. Neural program repair with execution-based\nbackpropagation. In Proceedings of ICSE , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.04123",
                        "Citation Paper Title": "Title:Neural Program Repair with Execution-based Backpropagation",
                        "Citation Paper Abstract": "Abstract:Neural machine translation (NMT) architectures have achieved promising results for automatic program repair. Yet, they have the limitation of generating low-quality patches (e.g., not compilable patches). This is because the existing works only optimize a purely syntactic loss function based on characters and tokens without incorporating program-specific information during neural network weight optimization. In this paper, we propose a novel program repair model called RewardRepair. The core novelty of RewardRepair is to improve NMT-based program repair with a loss function based on program compilation and test execution information, rewarding the network to produce patches that compile and that do not overfit. We conduct several experiments to evaluate RewardRepair showing that it is feasible and effective to use compilation and test execution results to optimize the underlying neural repair model. RewardRepair correctly repairs 207 bugs over four benchmarks. we report on repair success for 121 bugs that are fixed for the first time in the literature. Also, RewardRepair produces up to 45.3% of compilable patches, an improvement over the 39% by the state-of-the-art.",
                        "Citation Paper Authors": "Authors:He Ye, Matias Martinez, Martin Monperrus"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": ", and avoided the problem of Program\nAliasing by defining proper rewards; Trivedi et al. considered a two-stage learning scheme on this\nbasis ",
                    "Citation Text": "Dweep Trivedi, Jesse Zhang, Shao-Hua Sun, and Joseph J Lim. Learning to synthesize programs\nas interpretable and generalizable policies. In Proceedings of NeurIPS , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.13643",
                        "Citation Paper Title": "Title:Learning to Synthesize Programs as Interpretable and Generalizable Policies",
                        "Citation Paper Abstract": "Abstract:Recently, deep reinforcement learning (DRL) methods have achieved impressive performance on tasks in a variety of domains. However, neural network policies produced with DRL methods are not human-interpretable and often have difficulty generalizing to novel scenarios. To address these issues, prior works explore learning programmatic policies that are more interpretable and structured for generalization. Yet, these works either employ limited policy representations (e.g. decision trees, state machines, or predefined program templates) or require stronger supervision (e.g. input/output state pairs or expert demonstrations). We present a framework that instead learns to synthesize a program, which details the procedure to solve a task in a flexible and expressive manner, solely from reward signals. To alleviate the difficulty of learning to compose programs to induce the desired agent behavior from scratch, we propose to first learn a program embedding space that continuously parameterizes diverse behaviors in an unsupervised manner and then search over the learned program embedding space to yield a program that maximizes the return for a given task. Experimental results demonstrate that the proposed framework not only learns to reliably synthesize task-solving programs but also outperforms DRL and program synthesis baselines while producing interpretable and more generalizable policies. We also justify the necessity of the proposed two-stage learning scheme as well as analyze various methods for learning the program embedding.",
                        "Citation Paper Authors": "Authors:Dweep Trivedi, Jesse Zhang, Shao-Hua Sun, Joseph J. Lim"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ". Additionally, reinforcement learning has been\nintegrated into synthetic neural network training in numerous studies [ 7,16,9,48,19]. Bunel et al.\ncombined program synthesis with reinforcement learning ",
                    "Citation Text": "Rudy Bunel, Matthew Hausknecht, Jacob Devlin, Rishabh Singh, and Pushmeet Kohli. Lever-\naging grammar and reinforcement learning for neural program synthesis. In Proceedings of\nICLR , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.04276",
                        "Citation Paper Title": "Title:Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis",
                        "Citation Paper Abstract": "Abstract:Program synthesis is the task of automatically generating a program consistent with a specification. Recent years have seen proposal of a number of neural approaches for program synthesis, many of which adopt a sequence generation paradigm similar to neural machine translation, in which sequence-to-sequence models are trained to maximize the likelihood of known reference programs. While achieving impressive results, this strategy has two key limitations. First, it ignores Program Aliasing: the fact that many different programs may satisfy a given specification (especially with incomplete specifications such as a few input-output examples). By maximizing the likelihood of only a single reference program, it penalizes many semantically correct programs, which can adversely affect the synthesizer performance. Second, this strategy overlooks the fact that programs have a strict syntax that can be efficiently checked. To address the first limitation, we perform reinforcement learning on top of a supervised model with an objective that explicitly maximizes the likelihood of generating semantically correct programs. For addressing the second limitation, we introduce a training procedure that directly maximizes the probability of generating syntactically correct programs that fulfill the specification. We show that our contributions lead to improved accuracy of the models, especially in cases where the training data is limited.",
                        "Citation Paper Authors": "Authors:Rudy Bunel, Matthew Hausknecht, Jacob Devlin, Rishabh Singh, Pushmeet Kohli"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": ". Moreover, Spoc has recognized the\nsignificance of pseudocode and explores its potential for aiding code search, focusing on alternative\ntranslations of pseudocode ",
                    "Citation Text": "Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, and\nPercy S Liang. Spoc: Search-based pseudocode to code. In Proceedings of NeurIPS , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.04908",
                        "Citation Paper Title": "Title:SPoC: Search-based Pseudocode to Code",
                        "Citation Paper Abstract": "Abstract:We consider the task of mapping pseudocode to long programs that are functionally correct. Given test cases as a mechanism to validate programs, we search over the space of possible translations of the pseudocode to find a program that passes the validation. However, without proper credit assignment to localize the sources of program failures, it is difficult to guide search toward more promising programs. We propose to perform credit assignment based on signals from compilation errors, which constitute 88.7% of program failures. Concretely, we treat the translation of each pseudocode line as a discrete portion of the program, and whenever a synthesized program fails to compile, an error localization method tries to identify the portion of the program responsible for the failure. We then focus search over alternative translations of the pseudocode for those portions. For evaluation, we collected the SPoC dataset (Search-based Pseudocode to Code) containing 18,356 programs with human-authored pseudocode and test cases. Under a budget of 100 program compilations, performing search improves the synthesis success rate over using the top-one translation of the pseudocode from 25.6% to 44.7%.",
                        "Citation Paper Authors": "Authors:Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, Percy Liang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.08240v1": {
            "Paper Title": "CenterGrasp: Object-Aware Implicit Representation Learning for\n  Simultaneous Shape Reconstruction and 6-DoF Grasp Estimation",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": "topology. We then concatenate both representations and feed\nthem into a ResNet18-FPN backbone ",
                    "Citation Text": "A. Kirillov, R. Girshick, K. He, and P. Doll \u00b4ar, \u201cPanoptic feature pyramid\nnetworks,\u201d in Proc. of the IEEE Conf. on Computer Vision and Pattern\nRecognition , 2019, pp. 6399\u20136408.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.02446",
                        "Citation Paper Title": "Title:Panoptic Feature Pyramid Networks",
                        "Citation Paper Abstract": "Abstract:The recently introduced panoptic segmentation task has renewed our community's interest in unifying the tasks of instance segmentation (for thing classes) and semantic segmentation (for stuff classes). However, current state-of-the-art methods for this joint task use separate and dissimilar networks for instance and semantic segmentation, without performing any shared computation. In this work, we aim to unify these methods at the architectural level, designing a single network for both tasks. Our approach is to endow Mask R-CNN, a popular instance segmentation method, with a semantic segmentation branch using a shared Feature Pyramid Network (FPN) backbone. Surprisingly, this simple baseline not only remains effective for instance segmentation, but also yields a lightweight, top-performing method for semantic segmentation. In this work, we perform a detailed study of this minimally extended version of Mask R-CNN with FPN, which we refer to as Panoptic FPN, and show it is a robust and accurate baseline for both tasks. Given its effectiveness and conceptual simplicity, we hope our method can serve as a strong baseline and aid future research in panoptic segmentation.",
                        "Citation Paper Authors": "Authors:Alexander Kirillov, Ross Girshick, Kaiming He, Piotr Doll\u00e1r"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "adopt it to encode\ngeometric and semantic descriptors in a 3D volume, which\ncan then be queried to sample task-relevant keypoints ",
                    "Citation Text": "J. O. von Hartz, E. Chisari, T. Welschehold, W. Burgard, J. Boedecker,\nand A. Valada, \u201cThe treachery of images: Bayesian scene keypoints\nfor deep policy learning in robotic manipulation,\u201d IEEE Robotics and\nAutomation Letters , vol. 8, no. 11, pp. 6931\u20136938, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.04718",
                        "Citation Paper Title": "Title:The Treachery of Images: Bayesian Scene Keypoints for Deep Policy Learning in Robotic Manipulation",
                        "Citation Paper Abstract": "Abstract:In policy learning for robotic manipulation, sample efficiency is of paramount importance. Thus, learning and extracting more compact representations from camera observations is a promising avenue. However, current methods often assume full observability of the scene and struggle with scale invariance. In many tasks and settings, this assumption does not hold as objects in the scene are often occluded or lie outside the field of view of the camera, rendering the camera observation ambiguous with regard to their location. To tackle this problem, we present BASK, a Bayesian approach to tracking scale-invariant keypoints over time. Our approach successfully resolves inherent ambiguities in images, enabling keypoint tracking on symmetrical objects and occluded and out-of-view objects. We employ our method to learn challenging multi-object robot manipulation tasks from wrist camera observations and demonstrate superior utility for policy learning compared to other representation learning techniques. Furthermore, we show outstanding robustness towards disturbances such as clutter, occlusions, and noisy depth measurements, as well as generalization to unseen objects both in simulation and real-world robotic experiments.",
                        "Citation Paper Authors": "Authors:Jan Ole von Hartz, Eugenio Chisari, Tim Welschehold, Wolfram Burgard, Joschka Boedecker, Abhinav Valada"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "employs this model for scene reconstruction\nfrom partial observations. NDFs ",
                    "Citation Text": "A. Simeonov, Y . Du, A. Tagliasacchi, J. B. Tenenbaum, A. Rodriguez,\nP. Agrawal, and V . Sitzmann, \u201cNeural descriptor fields: Se(3)-equivariant\nobject representations for manipulation,\u201d in Int. Conf. on Robotics and\nAutomation , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.05124",
                        "Citation Paper Title": "Title:Neural Descriptor Fields: SE(3)-Equivariant Object Representations for Manipulation",
                        "Citation Paper Abstract": "Abstract:We present Neural Descriptor Fields (NDFs), an object representation that encodes both points and relative poses between an object and a target (such as a robot gripper or a rack used for hanging) via category-level descriptors. We employ this representation for object manipulation, where given a task demonstration, we want to repeat the same task on a new object instance from the same category. We propose to achieve this objective by searching (via optimization) for the pose whose descriptor matches that observed in the demonstration. NDFs are conveniently trained in a self-supervised fashion via a 3D auto-encoding task that does not rely on expert-labeled keypoints. Further, NDFs are SE(3)-equivariant, guaranteeing performance that generalizes across all possible 3D object translations and rotations. We demonstrate learning of manipulation tasks from few (5-10) demonstrations both in simulation and on a real robot. Our performance generalizes across both object instances and 6-DoF object poses, and significantly outperforms a recent baseline that relies on 2D descriptors. Project website: this https URL.",
                        "Citation Paper Authors": "Authors:Anthony Simeonov, Yilun Du, Andrea Tagliasacchi, Joshua B. Tenenbaum, Alberto Rodriguez, Pulkit Agrawal, Vincent Sitzmann"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "led to the development of many center-based\nobject detection methods ",
                    "Citation Text": "K. Duan, S. Bai, L. Xie, H. Qi, Q. Huang, and Q. Tian, \u201cCenterNet:\nKeypoint Triplets for Object Detection,\u201d in Int. Conf. on Computer\nVision , 2019, pp. 6568\u20136577.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.08189",
                        "Citation Paper Title": "Title:CenterNet: Keypoint Triplets for Object Detection",
                        "Citation Paper Abstract": "Abstract:In object detection, keypoint-based approaches often suffer a large number of incorrect object bounding boxes, arguably due to the lack of an additional look into the cropped regions. This paper presents an efficient solution which explores the visual patterns within each cropped region with minimal costs. We build our framework upon a representative one-stage keypoint-based detector named CornerNet. Our approach, named CenterNet, detects each object as a triplet, rather than a pair, of keypoints, which improves both precision and recall. Accordingly, we design two customized modules named cascade corner pooling and center pooling, which play the roles of enriching information collected by both top-left and bottom-right corners and providing more recognizable information at the central regions, respectively. On the MS-COCO dataset, CenterNet achieves an AP of 47.0%, which outperforms all existing one-stage detectors by at least 4.9%. Meanwhile, with a faster inference speed, CenterNet demonstrates quite comparable performance to the top-ranked two-stage detectors. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qingming Huang, Qi Tian"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": ", where multiple grasp\ncandidates are sampled using geometric heuristics from a point\ncloud, and a learned classifier is trained to evaluate their\nquality. It is later extended as GPD (Grasp Pose Detection) ",
                    "Citation Text": "A. Ten Pas, M. Gualtieri, K. Saenko, and R. Platt, \u201cGrasp pose detection\nin point clouds,\u201d Int. Journal of Robotics Research , vol. 36, no. 13-14,\npp. 1455\u20131473, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.09911",
                        "Citation Paper Title": "Title:Grasp Pose Detection in Point Clouds",
                        "Citation Paper Abstract": "Abstract:Recently, a number of grasp detection methods have been proposed that can be used to localize robotic grasp configurations directly from sensor data without estimating object pose. The underlying idea is to treat grasp perception analogously to object detection in computer vision. These methods take as input a noisy and partially occluded RGBD image or point cloud and produce as output pose estimates of viable grasps, without assuming a known CAD model of the object. Although these methods generalize grasp knowledge to new objects well, they have not yet been demonstrated to be reliable enough for wide use. Many grasp detection methods achieve grasp success rates (grasp successes as a fraction of the total number of grasp attempts) between 75% and 95% for novel objects presented in isolation or in light clutter. Not only are these success rates too low for practical grasping applications, but the light clutter scenarios that are evaluated often do not reflect the realities of real world grasping. This paper proposes a number of innovations that together result in a significant improvement in grasp detection performance. The specific improvement in performance due to each of our contributions is quantitatively measured either in simulation or on robotic hardware. Ultimately, we report a series of robotic experiments that average a 93% end-to-end grasp success rate for novel objects presented in dense clutter.",
                        "Citation Paper Authors": "Authors:Andreas ten Pas, Marcus Gualtieri, Kate Saenko, Robert Platt"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.08146v1": {
            "Paper Title": "High-accuracy Vision-Based Attitude Estimation System for Air-Bearing\n  Spacecraft Simulators",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.08411v1": {
            "Paper Title": "Pose and shear-based tactile servoing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.08070v1": {
            "Paper Title": "Laser Powered Harvesting System for Table-Top Grown Strawberries",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.08047v1": {
            "Paper Title": "Trust and Acceptance of Multi-Robot Systems \"in the Wild\". A Roadmap\n  exemplified within the EU-Project BugWright2",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.09053v3": {
            "Paper Title": "DATT: Deep Adaptive Trajectory Tracking for Quadrotor Control",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": "only focuses on feasible trajectories while in this work, we aim to track infeasible\ntrajectories as accurately as possible. Simulation-based learning with imitation learning to an expert\nMPC controller is used to generate acrobatic maneuvers in ",
                    "Citation Text": "E. Kaufmann, A. Loquercio, R. Ranftl, M. M \u00a8uller, V . Koltun, and D. Scaramuzza. Deep\nDrone Acrobatics. In Robotics: Science and Systems XVI . Robotics: Science and Systems\nFoundation, July 2020. ISBN 978-0-9923747-6-1. doi:10.15607/RSS.2020.XVI.040. URL\nhttp://www.roboticsproceedings.org/rss16/p040.pdf .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.05768",
                        "Citation Paper Title": "Title:Deep Drone Acrobatics",
                        "Citation Paper Abstract": "Abstract:Performing acrobatic maneuvers with quadrotors is extremely challenging. Acrobatic flight requires high thrust and extreme angular accelerations that push the platform to its physical limits. Professional drone pilots often measure their level of mastery by flying such maneuvers in competitions. In this paper, we propose to learn a sensorimotor policy that enables an autonomous quadrotor to fly extreme acrobatic maneuvers with only onboard sensing and computation. We train the policy entirely in simulation by leveraging demonstrations from an optimal controller that has access to privileged information. We use appropriate abstractions of the visual input to enable transfer to a real quadrotor. We show that the resulting policy can be directly deployed in the physical world without any fine-tuning on real data. Our methodology has several favorable properties: it does not require a human expert to provide demonstrations, it cannot harm the physical system during training, and it can be used to learn maneuvers that are challenging even for the best human pilots. Our approach enables a physical quadrotor to fly maneuvers such as the Power Loop, the Barrel Roll, and the Matty Flip, during which it incurs accelerations of up to 3g.",
                        "Citation Paper Authors": "Authors:Elia Kaufmann, Antonio Loquercio, Ren\u00e9 Ranftl, Matthias M\u00fcller, Vladlen Koltun, Davide Scaramuzza"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "uses domain randomization to show policy transfer between multiple quadrotors. Kaufmann et al. ",
                    "Citation Text": "E. Kaufmann, L. Bauersfeld, and D. Scaramuzza. A Benchmark Comparison of Learned Con-\ntrol Policies for Agile Quadrotor Flight, Feb. 2022. URL http://arxiv.org/abs/2202.\n10796 . arXiv:2202.10796 [cs].",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.10796",
                        "Citation Paper Title": "Title:A Benchmark Comparison of Learned Control Policies for Agile Quadrotor Flight",
                        "Citation Paper Abstract": "Abstract:Quadrotors are highly nonlinear dynamical systems that require carefully tuned controllers to be pushed to their physical limits. Recently, learning-based control policies have been proposed for quadrotors, as they would potentially allow learning direct mappings from high-dimensional raw sensory observations to actions. Due to sample inefficiency, training such learned controllers on the real platform is impractical or even impossible. Training in simulation is attractive but requires to transfer policies between domains, which demands trained policies to be robust to such domain gap. In this work, we make two contributions: (i) we perform the first benchmark comparison of existing learned control policies for agile quadrotor flight and show that training a control policy that commands body-rates and thrust results in more robust sim-to-real transfer compared to a policy that directly specifies individual rotor thrusts, (ii) we demonstrate for the first time that such a control policy trained via deep reinforcement learning can control a quadrotor in real-world experiments at speeds over 45km/h.",
                        "Citation Paper Authors": "Authors:Elia Kaufmann, Leonard Bauersfeld, Davide Scaramuzza"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2302.02367v6": {
            "Paper Title": "FastPillars: A Deployment-friendly Pillar-based 3D Detector",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.07964v1": {
            "Paper Title": "Three-Filters-to-Normal+: Revisiting Discontinuity Discrimination in\n  Depth-to-Normal Translation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2303.06510v2": {
            "Paper Title": "E2CoPre: Energy Efficient and Cooperative Collision Avoidance for UAV\n  Swarms with Trajectory Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.07843v1": {
            "Paper Title": "Foundation Models in Robotics: Applications, Challenges, and the Future",
            "Sentences": [
                {
                    "Sentence ID": 167,
                    "Sentence": "to generate textual descriptions\nwhile using SAM to extract objects of interest from visual\ninput. Then, Anything-3D lifts the extracted objects into a\nNeural Radiance Field (NeRF) ",
                    "Citation Text": "Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T\nBarron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing sc enes\nas neural radiance \ufb01elds for view synthesis. Communications of the\nACM , 65(1):99\u2013106, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                },
                {
                    "Sentence ID": 108,
                    "Sentence": "employs a collection of visual-\nlanguage models and SAMs to elevate objects into the realm\nof 3D. It uses BLIP ",
                    "Citation Text": "Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. BL IP:\nBootstrapping language-image pre-training for uni\ufb01ed vis ion-language\nunderstanding and generation. In ICML , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.12086",
                        "Citation Paper Title": "Title:BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation",
                        "Citation Paper Abstract": "Abstract:Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at this https URL.",
                        "Citation Paper Authors": "Authors:Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi"
                    }
                },
                {
                    "Sentence ID": 62,
                    "Sentence": "achieve compara-\nble performance to SAM at faster inference speeds. The\nTrack Anything Model (TAM) ",
                    "Citation Text": "Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang , and\nFeng Zheng. Track anything: Segment anything meets videos. arXiv\npreprint arXiv:2304.11968 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.11968",
                        "Citation Paper Title": "Title:Track Anything: Segment Anything Meets Videos",
                        "Citation Paper Abstract": "Abstract:Recently, the Segment Anything Model (SAM) gains lots of attention rapidly due to its impressive segmentation performance on images. Regarding its strong ability on image segmentation and high interactivity with different prompts, we found that it performs poorly on consistent segmentation in videos. Therefore, in this report, we propose Track Anything Model (TAM), which achieves high-performance interactive tracking and segmentation in videos. To be detailed, given a video sequence, only with very little human participation, i.e., several clicks, people can track anything they are interested in, and get satisfactory results in one-pass inference. Without additional training, such an interactive design performs impressively on video object tracking and segmentation. All resources are available on {this https URL}. We hope this work can facilitate related research.",
                        "Citation Paper Authors": "Authors:Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, Feng Zheng"
                    }
                },
                {
                    "Sentence ID": 114,
                    "Sentence": "as an image encoder\nwhile using a text encoder from CLIP ",
                    "Citation Text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ram esh,\nGabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askel l, Pamela\nMishkin, Jack Clark, et al. Learning transferable visual mo dels from\nnatural language supervision. In ICML , pages 8748\u20138763, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.00020",
                        "Citation Paper Title": "Title:Learning Transferable Visual Models From Natural Language Supervision",
                        "Citation Paper Abstract": "Abstract:State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",
                        "Citation Paper Authors": "Authors:Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever"
                    }
                },
                {
                    "Sentence ID": 164,
                    "Sentence": "that associates semantically similar labels to\nsimilar regions in an embedding space. LSeg uses a text\nencoder based on the CLIP architecture to compute text\nembeddings and an image encoder with the underlying\narchitecture of Dense Prediction Transformer (DPT) ",
                    "Citation Text": "Ren\u00b4 e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun . Vision trans-\nformers for dense prediction. In ICCV , pages 12179\u201312188, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.13413",
                        "Citation Paper Title": "Title:Vision Transformers for Dense Prediction",
                        "Citation Paper Abstract": "Abstract:We introduce dense vision transformers, an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks. We assemble tokens from various stages of the vision transformer into image-like representations at various resolutions and progressively combine them into full-resolution predictions using a convolutional decoder. The transformer backbone processes representations at a constant and relatively high resolution and has a global receptive field at every stage. These properties allow the dense vision transformer to provide finer-grained and more globally coherent predictions when compared to fully-convolutional networks. Our experiments show that this architecture yields substantial improvements on dense prediction tasks, especially when a large amount of training data is available. For monocular depth estimation, we observe an improvement of up to 28% in relative performance when compared to a state-of-the-art fully-convolutional network. When applied to semantic segmentation, dense vision transformers set a new state of the art on ADE20K with 49.02% mIoU. We further show that the architecture can be fine-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets the new state of the art. Our models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Ren\u00e9 Ranftl, Alexey Bochkovskiy, Vladlen Koltun"
                    }
                },
                {
                    "Sentence ID": 56,
                    "Sentence": "uses a transformer-based architecture to extract features from\npoint clouds, generalizing the concept of BERT into 3D point\nclouds.\nUnlike PointCLIP which converts the task of matching point\nclouds and text to image-text alignment, ULIP ",
                    "Citation Text": "Le Xue, Mingfei Gao, Chen Xing, Roberto Mart\u00b4 \u0131n-Mart\u00b4 \u0131 n, Jiajun Wu,\nCaiming Xiong, Ran Xu, Juan Carlos Niebles, and Silvio Savar ese.\nULIP: Learning uni\ufb01ed representation of language, image an d point\ncloud for 3D understanding. arXiv preprint arXiv:2212.05171 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.05171",
                        "Citation Paper Title": "Title:ULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding",
                        "Citation Paper Abstract": "Abstract:The recognition capabilities of current state-of-the-art 3D models are limited by datasets with a small number of annotated data and a pre-defined set of categories. In its 2D counterpart, recent advances have shown that similar problems can be significantly alleviated by employing knowledge from other modalities, such as language. Inspired by this, leveraging multimodal information for 3D modality could be promising to improve 3D understanding under the restricted data regime, but this line of research is not well studied. Therefore, we introduce ULIP to learn a unified representation of images, texts, and 3D point clouds by pre-training with object triplets from the three modalities. To overcome the shortage of training triplets, ULIP leverages a pre-trained vision-language model that has already learned a common visual and textual space by training with massive image-text pairs. Then, ULIP learns a 3D representation space aligned with the common image-text space, using a small number of automatically synthesized triplets. ULIP is agnostic to 3D backbone networks and can easily be integrated into any 3D architecture. Experiments show that ULIP effectively improves the performance of multiple recent 3D backbones by simply pre-training them on ShapeNet55 using our framework, achieving state-of-the-art performance in both standard 3D classification and zero-shot 3D classification on ModelNet40 and ScanObjectNN. ULIP also improves the performance of PointMLP by around 3% in 3D classification on ScanObjectNN, and outperforms PointCLIP by 28.8% on top-1 accuracy for zero-shot 3D classification on ModelNet40. Our code and pre-trained models are released at this https URL.",
                        "Citation Paper Authors": "Authors:Le Xue, Mingfei Gao, Chen Xing, Roberto Mart\u00edn-Mart\u00edn, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, Silvio Savarese"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": "as the open-\nvocabulary object detector to \ufb01nd \u201centities of interest\u201d (e .g.,\nvase or drawer handles) and ultimately de\ufb01ne value maps for\noptimizing manipulation trajectories.\nGrounding DINO ",
                    "Citation Text": "Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zha ng, Jie\nYang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grou nding\nDINO: Marrying DINO with grounded pre-training for open-se t object\ndetection. arXiv preprint arXiv:2303.05499 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.05499",
                        "Citation Paper Title": "Title:Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection",
                        "Citation Paper Abstract": "Abstract:In this paper, we present an open-set object detector, called Grounding DINO, by marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector for open-set concept generalization. To effectively fuse language and vision modalities, we conceptually divide a closed-set detector into three phases and propose a tight fusion solution, which includes a feature enhancer, a language-guided query selection, and a cross-modality decoder for cross-modality fusion. While previous works mainly evaluate open-set object detection on novel categories, we propose to also perform evaluations on referring expression comprehension for objects specified with attributes. Grounding DINO performs remarkably well on all three settings, including benchmarks on COCO, LVIS, ODinW, and RefCOCO/+/g. Grounding DINO achieves a $52.5$ AP on the COCO detection zero-shot transfer benchmark, i.e., without any training data from COCO. It sets a new record on the ODinW zero-shot benchmark with a mean $26.1$ AP. Code will be available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang"
                    }
                },
                {
                    "Sentence ID": 160,
                    "Sentence": "decoder-only transformer 11M bidirectional\ndynamics prediction\nand masked hindsight\ncontrol1 Hz 8 Nvidia V100 GPUs\nCOMPASS ",
                    "Citation Text": "Shuang Ma, Sai Vemprala, Wenshan Wang, Jayesh K Gupta, Yale\nSong, Daniel McDufft, and Ashish Kapoor. COMPASS:: Contras tive\nmultimodal pretraining for autonomous systems. In IROS , pages 1000\u2013\n1007. IEEE, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.15788",
                        "Citation Paper Title": "Title:COMPASS: Contrastive Multimodal Pretraining for Autonomous Systems",
                        "Citation Paper Abstract": "Abstract:Learning representations that generalize across tasks and domains is challenging yet necessary for autonomous systems. Although task-driven approaches are appealing, designing models specific to each application can be difficult in the face of limited data, especially when dealing with highly variable multimodal input spaces arising from different tasks in different environments.We introduce the first general-purpose pretraining pipeline, COntrastive Multimodal Pretraining for AutonomouS Systems (COMPASS), to overcome the limitations of task-specific models and existing pretraining approaches. COMPASS constructs a multimodal graph by considering the essential information for autonomous systems and the properties of different modalities. Through this graph, multimodal signals are connected and mapped into two factorized spatio-temporal latent spaces: a \"motion pattern space\" and a \"current state space.\" By learning from multimodal correspondences in each latent space, COMPASS creates state representations that models necessary information such as temporal dynamics, geometry, and semantics. We pretrain COMPASS on a large-scale multimodal simulation dataset TartanAir \\cite{tartanair2020iros} and evaluate it on drone navigation, vehicle racing, and visual odometry tasks. The experiments indicate that COMPASS can tackle all three scenarios and can also generalize to unseen environments and real-world data.",
                        "Citation Paper Authors": "Authors:Shuang Ma, Sai Vemprala, Wenshan Wang, Jayesh K. Gupta, Yale Song, Daniel McDuff, Ashish Kapoor"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "a temporal convolution layer, a\nResNet 62 image processing\nstack, and residual unmasked\nattention layers,0.5B embodied agent in\nMinecraft20Hz 9 days on 720 V100 GPUs\nRT-1 ",
                    "Citation Text": "Anthony Brohan, Noah Brown, Justice Carbajal, and et al . RT-1:\nRobotics transformer for real-world control at scale. In RSS, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.06817",
                        "Citation Paper Title": "Title:RT-1: Robotics Transformer for Real-World Control at Scale",
                        "Citation Paper Abstract": "Abstract:By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks. The project's website and videos can be found at this http URL",
                        "Citation Paper Authors": "Authors:Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, Brianna Zitkovich"
                    }
                },
                {
                    "Sentence ID": 72,
                    "Sentence": "Ef\ufb01cientNet+ decoder transformer 31M visual navigation 4 Hz variety of GPU con\ufb01gurations is\nused including 2\u00d74090, 3\u00d7Titan\nXp, 4\u00d7P100, 8\u00d71080Ti, 8\u00d7V100,\nand 8\u00d7A100\nVPT ",
                    "Citation Text": "Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga , Jie Tang,\nAdrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff C lune.\nVideo PreTraining (VPT): Learning to act by watching unlabe led online\nvideos. In NeurIPS , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.11795",
                        "Citation Paper Title": "Title:Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos",
                        "Citation Paper Abstract": "Abstract:Pretraining on noisy, internet-scale datasets has been heavily studied as a technique for training models with broad, general capabilities for text, images, and other modalities. However, for many sequential decision domains such as robotics, video games, and computer use, publicly available data does not contain the labels required to train behavioral priors in the same way. We extend the internet-scale pretraining paradigm to sequential decision domains through semi-supervised imitation learning wherein agents learn to act by watching online unlabeled videos. Specifically, we show that with a small amount of labeled data we can train an inverse dynamics model accurate enough to label a huge unlabeled source of online data -- here, online videos of people playing Minecraft -- from which we can then train a general behavioral prior. Despite using the native human interface (mouse and keyboard at 20Hz), we show that this behavioral prior has nontrivial zero-shot capabilities and that it can be fine-tuned, with both imitation learning and reinforcement learning, to hard-exploration tasks that are impossible to learn from scratch via reinforcement learning. For many tasks our models exhibit human-level performance, and we are the first to report computer agents that can craft diamond tools, which can take proficient humans upwards of 20 minutes (24,000 environment actions) of gameplay to accomplish.",
                        "Citation Paper Authors": "Authors:Bowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, Jeff Clune"
                    }
                },
                {
                    "Sentence ID": 154,
                    "Sentence": "is a self-improving\nAI agent. It uses a 1.18B-parameter decoder-only transform er.\nIt learns to operate different robotic arms, solves tasks fr om as\nfew as 100 demonstrations, and improves from self-generate d\ndata. RoboCat is based on Gato ",
                    "Citation Text": "Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Go mez Col-\nmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gim enez,\nYury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A g eneralist\nagent. arXiv preprint arXiv:2205.06175 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.06175",
                        "Citation Paper Title": "Title:A Generalist Agent",
                        "Citation Paper Abstract": "Abstract:Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.",
                        "Citation Paper Authors": "Authors:Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, Nando de Freitas"
                    }
                },
                {
                    "Sentence ID": 158,
                    "Sentence": ". DALL-E-Bot autonomous object rearrange-\nment does not require any further data collection or train-\ning. First, the initial observation image (of the disorgani zed\nscene) is converted into a per-object representation inclu ding\na segmentation mask using Mask R-CNN ",
                    "Citation Text": "Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4 ar, and Ross Girshick. Mask\nR-CNN. In ICCV , pages 2980\u20132988, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.06870",
                        "Citation Paper Title": "Title:Mask R-CNN",
                        "Citation Paper Abstract": "Abstract:We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: this https URL",
                        "Citation Paper Authors": "Authors:Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, Ross Girshick"
                    }
                },
                {
                    "Sentence ID": 88,
                    "Sentence": "per-forms zero-shot autonomous rearrangement in the scene in\na human-like way using pretrained image diffusion model\nDALL-E2 ",
                    "Citation Text": "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey C hu, and\nMark Chen. Hierarchical text-conditional image generatio n with CLIP\nlatents. arXiv preprint arXiv:2204.06125 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.06125",
                        "Citation Paper Title": "Title:Hierarchical Text-Conditional Image Generation with CLIP Latents",
                        "Citation Paper Abstract": "Abstract:Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.",
                        "Citation Paper Authors": "Authors:Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen"
                    }
                },
                {
                    "Sentence ID": 155,
                    "Sentence": "architecture and is\ntrained with a self-improvement cycle.\nFor robots to operate effectively in the real world they\nmust be able to manipulate previously unseen objects. Liu\net al. present StructDiffusion ",
                    "Citation Text": "Weiyu Liu, Yilun Du, Tucker Hermans, Sonia Chernova, a nd Chris\nPaxton. StructDiffusion: Language-guided creation of phy sically-valid\nstructures using unseen objects. In RSS, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2211.04604",
                        "Citation Paper Title": "Title:StructDiffusion: Language-Guided Creation of Physically-Valid Structures using Unseen Objects",
                        "Citation Paper Abstract": "Abstract:Robots operating in human environments must be able to rearrange objects into semantically-meaningful configurations, even if these objects are previously unseen. In this work, we focus on the problem of building physically-valid structures without step-by-step instructions. We propose StructDiffusion, which combines a diffusion model and an object-centric transformer to construct structures given partial-view point clouds and high-level language goals, such as \"set the table\". Our method can perform multiple challenging language-conditioned multi-step 3D planning tasks using one model. StructDiffusion even improves the success rate of assembling physically-valid structures out of unseen objects by on average 16% over an existing multi-modal transformer model trained on specific structures. We show experiments on held-out objects in both simulation and on real-world rearrangement tasks. Importantly, we show how integrating both a diffusion model and a collision-discriminator model allows for improved generalization over other methods when rearranging previously-unseen objects. For videos and additional results, see our website: this https URL.",
                        "Citation Paper Authors": "Authors:Weiyu Liu, Yilun Du, Tucker Hermans, Sonia Chernova, Chris Paxton"
                    }
                },
                {
                    "Sentence ID": 149,
                    "Sentence": "enhances visual target navigation\nby constructing an environment map and selecting long-term\ngoals using the inference capabilities of large language mo dels.\nThe system can determine appropriate long-term goals for\nnavigation by leveraging pretrained language models such\nas RoBERTa-large ",
                    "Citation Text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar J oshi,\nDanqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Ves elin\nStoyanov. RoBERTa: A robustly optimized BERT pretraining a pproach.\narXiv preprint arXiv:1907.11692 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.11692",
                        "Citation Paper Title": "Title:RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                        "Citation Paper Abstract": "Abstract:Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
                        "Citation Paper Authors": "Authors:Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov"
                    }
                },
                {
                    "Sentence ID": 144,
                    "Sentence": "to compute\nthe cosine similarity between an image and a user-speci\ufb01ed\ndescription.\nCommon datasets and benchmarks for these types of prob-\nlems are Matterport3D ",
                    "Citation Text": "Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Ha lber,\nMatthias Niebner, Manolis Savva, Shuran Song, Andy Zeng, an d\nYinda Zhang. Matterport3D: Learning from RGB-D data in indo or\nenvironments. In 3DV, pages 667\u2013676, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.06158",
                        "Citation Paper Title": "Title:Matterport3D: Learning from RGB-D Data in Indoor Environments",
                        "Citation Paper Abstract": "Abstract:Access to large, diverse RGB-D datasets is critical for training RGB-D scene understanding algorithms. However, existing datasets still cover only a limited number of views or a restricted scale of spaces. In this paper, we introduce Matterport3D, a large-scale RGB-D dataset containing 10,800 panoramic views from 194,400 RGB-D images of 90 building-scale scenes. Annotations are provided with surface reconstructions, camera poses, and 2D and 3D semantic segmentations. The precise global alignment and comprehensive, diverse panoramic set of views over entire buildings enable a variety of supervised and self-supervised computer vision tasks, including keypoint matching, view overlap prediction, normal prediction from color, semantic segmentation, and region classification.",
                        "Citation Paper Authors": "Authors:Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Nie\u00dfner, Manolis Savva, Shuran Song, Andy Zeng, Yinda Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.07826v1": {
            "Paper Title": "Integrated Path Tracking with DYC and MPC using LSTM Based Tire Force\n  Estimator for Four-wheel Independent Steering and Driving Vehicle",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.07786v1": {
            "Paper Title": "A Data-driven Method for Safety-critical Control: Designing Control\n  Barrier Functions from State Constraints",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": ". For more complex systems like legged\nrobots, which have intricate models, defining CBFs can be\nchallenging. Hence, reduced-order models combined with\nCBFs have been used to safely control legged robots such\nas quadruped and bipedal robots ",
                    "Citation Text": "J. Lee, J. Kim, and A. D. Ames, \u201cHierarchical relaxation of safety-\ncritical controllers: Mitigating contradictory safety conditions withapplication to quadruped robots,\u201d arXiv preprint arXiv:2305.03929 ,\n2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.03929",
                        "Citation Paper Title": "Title:Hierarchical Relaxation of Safety-critical Controllers: Mitigating Contradictory Safety Conditions with Application to Quadruped Robots",
                        "Citation Paper Abstract": "Abstract:The safety-critical control of robotic systems often must account for multiple, potentially conflicting, safety constraints. This paper proposes novel relaxation techniques to address safety-critical control problems in the presence of conflicting safety conditions. In particular, Control Barrier Function (CBFs) provide a means to encode safety as constraints in a Quadratic Program (QP), wherein multiple safety conditions yield multiple constraints. However, the QP problem becomes infeasible when the safety conditions cannot be simultaneously satisfied. To resolve this potential infeasibility, we introduce a hierarchy between the safety conditions and employ an additional variable to relax the less important safety conditions (Relaxed-CBF-QP), and formulate a cascaded structure to achieve smaller violations of lower-priority safety conditions (Hierarchical-CBF-QP). The proposed approach, therefore, ensures the existence of at least one solution to the QP problem with the CBFs while dynamically balancing enforcement of additional safety constraints. Importantly, this paper evaluates the impact of different weighting factors in the Hierarchical-CBF-QP and, due to the sensitivity of these weightings in the observed behavior, proposes a method to determine the weighting factors via a sampling-based technique. The validity of the proposed approach is demonstrated through simulations and experiments on a quadrupedal robot navigating to a goal through regions with different levels of danger.",
                        "Citation Paper Authors": "Authors:Jaemin Lee, Jeeseop Kim, Aaron D. Ames"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": ", particularly due\nto the simplicity of the system model. CBFs have also\nbeen effectively applied to control simpler systems such as\nSegway platforms and drones, enabling obstacle avoidance ",
                    "Citation Text": "A. Taylor, A. Singletary, Y . Yue, and A. Ames, \u201cLearning for safety-\ncritical control with control barrier functions,\u201d in Learning for Dy-\nnamics and Control . PMLR, 2020, pp. 708\u2013717.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.10099",
                        "Citation Paper Title": "Title:Learning for Safety-Critical Control with Control Barrier Functions",
                        "Citation Paper Abstract": "Abstract:Modern nonlinear control theory seeks to endow systems with properties of stability and safety, and have been deployed successfully in multiple domains. Despite this success, model uncertainty remains a significant challenge in synthesizing safe controllers, leading to degradation in the properties provided by the controllers. This paper develops a machine learning framework utilizing Control Barrier Functions (CBFs) to reduce model uncertainty as it impact the safe behavior of a system. This approach iteratively collects data and updates a controller, ultimately achieving safe behavior. We validate this method in simulation and experimentally on a Segway platform.",
                        "Citation Paper Authors": "Authors:Andrew Taylor, Andrew Singletary, Yisong Yue, Aaron Ames"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "Recently, CBF-based frameworks for safety-critical con-\ntrol have found applications in a diverse range of autonomous\nsystems. For example, the most prominent application of\nCBFs is in autonomous vehicles ",
                    "Citation Text": "J. Choi, F. Castaneda, C. J. Tomlin, and K. Sreenath, \u201cReinforcement\nlearning for safety-critical control under model uncertainty, using con-\ntrol lyapunov functions and control barrier functions,\u201d arXiv preprint\narXiv:2004.07584 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.07584",
                        "Citation Paper Title": "Title:Reinforcement Learning for Safety-Critical Control under Model Uncertainty, using Control Lyapunov Functions and Control Barrier Functions",
                        "Citation Paper Abstract": "Abstract:In this paper, the issue of model uncertainty in safety-critical control is addressed with a data-driven approach. For this purpose, we utilize the structure of an input-ouput linearization controller based on a nominal model along with a Control Barrier Function and Control Lyapunov Function based Quadratic Program (CBF-CLF-QP). Specifically, we propose a novel reinforcement learning framework which learns the model uncertainty present in the CBF and CLF constraints, as well as other control-affine dynamic constraints in the quadratic program. The trained policy is combined with the nominal model-based CBF-CLF-QP, resulting in the Reinforcement Learning-based CBF-CLF-QP (RL-CBF-CLF-QP), which addresses the problem of model uncertainty in the safety constraints. The performance of the proposed method is validated by testing it on an underactuated nonlinear bipedal robot walking on randomly spaced stepping stones with one step preview, obtaining stable and safe walking under model uncertainty.",
                        "Citation Paper Authors": "Authors:Jason Choi, Fernando Casta\u00f1eda, Claire J. Tomlin, Koushil Sreenath"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.07778v1": {
            "Paper Title": "Safety-critical Control of Quadrupedal Robots with Rolling Arms for\n  Autonomous Inspection of Complex Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.07745v1": {
            "Paper Title": "High-density Electromyography for Effective Gesture-based Control of\n  Physically Assistive Mobile Manipulators",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.07744v1": {
            "Paper Title": "How Does Perception Affect Safety: New Metrics and Strategy",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": "as the baseline\nobject detector, which utilizes the COCO dataset ",
                    "Citation Text": "T.-Y . Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays,\nP. Perona, D. Ramanan, C. L. Zitnick, and P. Doll \u00b4ar, \u201cMicrosoft\ncoco: Common objects in context,\u201d 2014. [Online]. Available:\nhttps://arxiv.org/abs/1405.0312",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1405.0312",
                        "Citation Paper Title": "Title:Microsoft COCO: Common Objects in Context",
                        "Citation Paper Abstract": "Abstract:We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
                        "Citation Paper Authors": "Authors:Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Doll\u00e1r"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.07724v1": {
            "Paper Title": "Restorebot: Towards an Autonomous Robotics Platform for Degraded\n  Rangeland Restoration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.07671v1": {
            "Paper Title": "Reacting like Humans: Incorporating Intrinsic Human Behaviors into NAO\n  through Sound-Based Reactions for Enhanced Sociability",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.07523v1": {
            "Paper Title": "Self-Healing Distributed Swarm Formation Control Using Image Moments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.07491v1": {
            "Paper Title": "On Robot Acceptance and Trust: A Review and Unanswered Questions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.07457v1": {
            "Paper Title": "Dynamics Harmonic Analysis of Robotic Systems: Application in\n  Data-Driven Koopman Modelling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.07454v1": {
            "Paper Title": "\"You Might Like It\": How People Respond to Small Talk in Human-Robot\n  Collaboration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2309.08769v3": {
            "Paper Title": "The Use of Multi-Scale Fiducial Markers To Aid Takeoff and Landing\n  Navigation by Rotorcraft",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.07368v1": {
            "Paper Title": "Sequential Planning in Large Partially Observable Environments guided by\n  LLMs",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": "There is a rich literature on sequential planning in large \nenvironments. DRRN ",
                    "Citation Text": "He, J., Chen, J., He, X., Gao, J., Li, L., Deng, L., & Ostendorf, M. \n(2015). \u201cDeep reinforcement learning with a natural language action \nspace \u201d. arXiv preprint arXiv:1511.04636.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.04636",
                        "Citation Paper Title": "Title:Deep Reinforcement Learning with a Natural Language Action Space",
                        "Citation Paper Abstract": "Abstract:This paper introduces a novel architecture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language, as found in text-based games. Termed a deep reinforcement relevance network (DRRN), the architecture represents action and state spaces with separate embedding vectors, which are combined with an interaction function to approximate the Q-function in reinforcement learning. We evaluate the DRRN on two popular text games, showing superior performance over other deep Q-learning architectures. Experiments with paraphrased action descriptions show that the model is extracting meaning rather than simply memorizing strings of text.",
                        "Citation Paper Authors": "Authors:Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, Mari Ostendorf"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.07337v1": {
            "Paper Title": "RMS: Redundancy-Minimizing Point Cloud Sampling for Real-Time Pose\n  Estimation in Degenerated Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.07292v1": {
            "Paper Title": "Statistically Distinct Plans for Multi-Objective Task Assignment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.07290v1": {
            "Paper Title": "Underwater motions analysis and control of a coupling-tiltable unmanned\n  aerial-aquatic quadrotor",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.07234v1": {
            "Paper Title": "Designing Heterogeneous Robot Fleets for Task Allocation and Sequencing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.06643v2": {
            "Paper Title": "Gaze Detection and Analysis for Initiating Joint Activity in Industrial\n  Human-Robot Collaboration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.07227v1": {
            "Paper Title": "Scalarizing Multi-Objective Robot Planning Problems using Weighted\n  Maximization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.07214v1": {
            "Paper Title": "Exploring Large Language Models to Facilitate Variable Autonomy for\n  Human-Robot Teaming",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.07192v1": {
            "Paper Title": "waveSLAM: Empowering Accurate Indoor Mapping Using Off-the-Shelf\n  Millimeter-wave Self-sensing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.07146v1": {
            "Paper Title": "CompdVision: Combining Near-Field 3D Visual and Tactile Sensing Using a\n  Compact Compound-Eye Imaging System",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "presented a sensor that employed a novel compound eye\nimaging system comprised of an array of pinholes on a\nsingle complementary metal oxide semiconductor (CMOS)\nsensor, reducing the thickness to just 5 mm. Chen et al. ",
                    "Citation Text": "X. Chen, G. Zhang, M. Y . Wang, and H. Yu, \u201cA thin format vision-\nbased tactile sensor with a microlens array (mla),\u201d IEEE Sensors\nJournal , vol. 22, no. 22, pp. 22 069\u201322 076, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.08691",
                        "Citation Paper Title": "Title:A Thin Format Vision-Based Tactile Sensor with A Micro Lens Array (MLA)",
                        "Citation Paper Abstract": "Abstract:Vision-based tactile sensors have been widely studied in the robotics field for high spatial resolution and compatibility with machine learning algorithms. However, the currently employed sensor's imaging system is bulky limiting its further application. Here we present a micro lens array (MLA) based vison system to achieve a low thickness format of the sensor package with high tactile sensing performance. Multiple micromachined micro lens units cover the whole elastic touching layer and provide a stitched clear tactile image, enabling high spatial resolution with a thin thickness of 5 mm. The thermal reflow and soft lithography method ensure the uniform spherical profile and smooth surface of micro lens. Both optical and mechanical characterization demonstrated the sensor's stable imaging and excellent tactile sensing, enabling precise 3D tactile information, such as displacement mapping and force distribution with an ultra compact-thin structure.",
                        "Citation Paper Authors": "Authors:Xia Chen, Guanlan Zhang, Michael Yu Wang, Hongyu Yu"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "developed a sensor using an infrared\ncamera for tactile sensing and dual cameras for stereo depth\nestimation. Despite these advancements, the sensor cannot\nprovide contact details due to its rigid surface. Roberge et al. ",
                    "Citation Text": "E. Roberge, G. Fornes, and J.-P. Roberge, \u201cStereotac: a novel visuotac-\ntile sensor that combines tactile sensing with 3d vision,\u201d arXiv preprint\narXiv:2303.06542 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.06542",
                        "Citation Paper Title": "Title:StereoTac: a Novel Visuotactile Sensor that Combines Tactile Sensing with 3D Vision",
                        "Citation Paper Abstract": "Abstract:Combining 3D vision with tactile sensing could unlock a greater level of dexterity for robots and improve several manipulation tasks. However, obtaining a close-up 3D view of the location where manipulation contacts occur can be challenging, particularly in confined spaces, cluttered environments, or without installing more sensors on the end effector. In this context, this paper presents StereoTac, a novel vision-based sensor that combines tactile sensing with 3D vision. The proposed sensor relies on stereoscopic vision to capture a 3D representation of the environment before contact and uses photometric stereo to reconstruct the tactile imprint generated by an object during contact. To this end, two cameras were integrated in a single sensor, whose interface is made of a transparent elastomer coated with a thin layer of paint with a level of transparency that can be adjusted by varying the sensor's internal lighting conditions. We describe the sensor's fabrication and evaluate its performance for both tactile perception and 3D vision. Our results show that the proposed sensor can reconstruct a 3D view of a scene just before grasping and perceive the tactile imprint after grasping, allowing for monitoring of the contact during manipulation.",
                        "Citation Paper Authors": "Authors:Etienne Roberge, Guillaume Fornes, Jean-Philippe Roberge"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "leveraged\ninternal pins to track and record surface deformations, offer-\ning another innovative approach to tactile sensing. Lambeta\net al. ",
                    "Citation Text": "M. Lambeta, P.-W. Chou, S. Tian, B. Yang, B. Maloon, V . R. Most,\nD. Stroud, R. Santos, A. Byagowi, G. Kammerer et al. , \u201cDigit: A\nnovel design for a low-cost compact high-resolution tactile sensor with\napplication to in-hand manipulation,\u201d IEEE Robotics and Automation\nLetters , vol. 5, no. 3, pp. 3838\u20133845, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.14679",
                        "Citation Paper Title": "Title:DIGIT: A Novel Design for a Low-Cost Compact High-Resolution Tactile Sensor with Application to In-Hand Manipulation",
                        "Citation Paper Abstract": "Abstract:Despite decades of research, general purpose in-hand manipulation remains one of the unsolved challenges of robotics. One of the contributing factors that limit current robotic manipulation systems is the difficulty of precisely sensing contact forces -- sensing and reasoning about contact forces are crucial to accurately control interactions with the environment. As a step towards enabling better robotic manipulation, we introduce DIGIT, an inexpensive, compact, and high-resolution tactile sensor geared towards in-hand manipulation. DIGIT improves upon past vision-based tactile sensors by miniaturizing the form factor to be mountable on multi-fingered hands, and by providing several design improvements that result in an easier, more repeatable manufacturing process, and enhanced reliability. We demonstrate the capabilities of the DIGIT sensor by training deep neural network model-based controllers to manipulate glass marbles in-hand with a multi-finger robotic hand. To provide the robotic community access to reliable and low-cost tactile sensors, we open-source the DIGIT design at this https URL.",
                        "Citation Paper Authors": "Authors:Mike Lambeta, Po-Wei Chou, Stephen Tian, Brian Yang, Benjamin Maloon, Victoria Rose Most, Dave Stroud, Raymond Santos, Ahmad Byagowi, Gregg Kammerer, Dinesh Jayaraman, Roberto Calandra"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2306.06871v3": {
            "Paper Title": "Improving Offline-to-Online Reinforcement Learning with Q-Ensembles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2306.13509v2": {
            "Paper Title": "Exploring AI-enhanced Shared Control for an Assistive Robotic Arm",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.07075v1": {
            "Paper Title": "Motion Planning and Control of A Morphing Quadrotor in Restricted\n  Scenarios",
            "Sentences": []
        },
        "http://arxiv.org/abs/2308.09103v3": {
            "Paper Title": "Efficient collision avoidance for autonomous vehicles in polygonal\n  domains",
            "Sentences": []
        },
        "http://arxiv.org/abs/2310.19797v2": {
            "Paper Title": "DEFT: Dexterous Fine-Tuning for Real-World Hand Policies",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": "for its focus on kitchen tasks similar to our robot\u2019s. We learn a task-conditioned\naffordance model fthat produces (\u02c6\u00b5,\u02c6\u03b8wrist,\u02c6P) =f(v\u2032\n1, T). We predict \u02c6\u00b5in similar fashion to ",
                    "Citation Text": "S. Bahl, R. Mendonca, L. Chen, U. Jain, and D. Pathak. Affordances from human videos as a\nversatile representation for robotics. 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.08488",
                        "Citation Paper Title": "Title:Affordances from Human Videos as a Versatile Representation for Robotics",
                        "Citation Paper Abstract": "Abstract:Building a robot that can understand and learn to interact by watching humans has inspired several vision problems. However, despite some successful results on static datasets, it remains unclear how current models can be used on a robot directly. In this paper, we aim to bridge this gap by leveraging videos of human interactions in an environment centric manner. Utilizing internet videos of human behavior, we train a visual affordance model that estimates where and how in the scene a human is likely to interact. The structure of these behavioral affordances directly enables the robot to perform many complex tasks. We show how to seamlessly integrate our affordance model with four robot learning paradigms including offline imitation learning, exploration, goal-conditioned learning, and action parameterization for reinforcement learning. We show the efficacy of our approach, which we call VRB, across 4 real world environments, over 10 different tasks, and 2 robotic platforms operating in the wild. Results, visualizations and videos at this https URL",
                        "Citation Paper Authors": "Authors:Shikhar Bahl, Russell Mendonca, Lili Chen, Unnat Jain, Deepak Pathak"
                    }
                },
                {
                    "Sentence ID": 68,
                    "Sentence": ". Similar to previous approaches\n[3,4,7,5], a set of contact points are extracted to fit a Gaussian Mixture Model (GMM) with centers\n\u00b5={\u00b51, \u00b52, . . . , \u00b5 k}. Detic ",
                    "Citation Text": "X. Zhou, R. Girdhar, A. Joulin, P. Kr \u00a8ahenb \u00a8uhl, and I. Misra. Detecting twenty-thousand classes\nusing image-level supervision. arXiv preprint arXiv:2201.02605 , 2022.\n12",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.02605",
                        "Citation Paper Title": "Title:Detecting Twenty-thousand Classes using Image-level Supervision",
                        "Citation Paper Abstract": "Abstract:Current object detectors are limited in vocabulary size due to the small scale of detection datasets. Image classifiers, on the other hand, reason about much larger vocabularies, as their datasets are larger and easier to collect. We propose Detic, which simply trains the classifiers of a detector on image classification data and thus expands the vocabulary of detectors to tens of thousands of concepts. Unlike prior work, Detic does not need complex assignment schemes to assign image labels to boxes based on model predictions, making it much easier to implement and compatible with a range of detection architectures and backbones. Our results show that Detic yields excellent detectors even for classes without box annotations. It outperforms prior work on both open-vocabulary and long-tail detection benchmarks. Detic provides a gain of 2.4 mAP for all classes and 8.3 mAP for novel classes on the open-vocabulary LVIS benchmark. On the standard LVIS benchmark, Detic obtains 41.7 mAP when evaluated on all classes, or only rare classes, hence closing the gap in performance for object categories with few samples. For the first time, we train a detector with all the twenty-one-thousand classes of the ImageNet dataset and show that it generalizes to new datasets without finetuning. Code is available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Kr\u00e4henb\u00fchl, Ishan Misra"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "of axes aligned with the wrist and a 10-dimensional shape\nvector. MANOtorch from ",
                    "Citation Text": "L. Yang, X. Zhan, K. Li, W. Xu, J. Li, and C. Lu. CPF: Learning a contact potential field to\nmodel the hand-object interaction. In ICCV , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.00924",
                        "Citation Paper Title": "Title:CPF: Learning a Contact Potential Field to Model the Hand-Object Interaction",
                        "Citation Paper Abstract": "Abstract:Modeling the hand-object (HO) interaction not only requires estimation of the HO pose, but also pays attention to the contact due to their interaction. Significant progress has been made in estimating hand and object separately with deep learning methods, simultaneous HO pose estimation and contact modeling has not yet been fully explored. In this paper, we present an explicit contact representation namely Contact Potential Field (CPF), and a learning-fitting hybrid framework namely MIHO to Modeling the Interaction of Hand and Object. In CPF, we treat each contacting HO vertex pair as a spring-mass system. Hence the whole system forms a potential field with minimal elastic energy at the grasp position. Extensive experiments on the two commonly used benchmarks have demonstrated that our method can achieve state-of-the-art in several reconstruction metrics, and allow us to produce more physically plausible HO pose even when the ground-truth exhibits severe interpenetration or disjointedness. Our code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Lixin Yang, Xinyu Zhan, Kailin Li, Wenqiang Xu, Jiefeng Li, Cewu Lu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.06991v1": {
            "Paper Title": "Attacking the Loop: Adversarial Attacks on Graph-based Loop Closure\n  Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.06928v1": {
            "Paper Title": "Blockchain-Based Security Architecture for Unmanned Aerial Vehicles in\n  B5G/6G Services and Beyond: A Comprehensive Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2308.14815v3": {
            "Paper Title": "Distributionally Robust Statistical Verification with Imprecise Neural\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.06876v1": {
            "Paper Title": "Interactive Planning Using Large Language Models for Partially\n  Observable Robotics Tasks",
            "Sentences": [
                {
                    "Sentence ID": 47,
                    "Sentence": ".\nIn order to get the required data to fine-tune a model as a planner in interactive planning under partial\nobservation, we follow the procedure shown in Figure 3b, using self-instruct ",
                    "Citation Text": "Y . Wang, Y . Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct:\nAligning language model with self generated instructions, 2022.\n12",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.10560",
                        "Citation Paper Title": "Title:Self-Instruct: Aligning Language Models with Self-Generated Instructions",
                        "Citation Paper Abstract": "Abstract:Large \"instruction-tuned\" language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning. Our code and data are available at this https URL.",
                        "Citation Paper Authors": "Authors:Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ", explore how pre-trained models can be improved for general\nand specific tasks [ 34,29,39,49]. Researchers have proposed strategies to prompt LLMs to perform\nhigh-quality synthetic data by in-context learning ",
                    "Citation Text": "T. Kojima, S. S. Gu, M. Reid, Y . Matsuo, and Y . Iwasawa. Large language models are zero-shot\nreasoners, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.11916",
                        "Citation Paper Title": "Title:Large Language Models are Zero-Shot Reasoners",
                        "Citation Paper Abstract": "Abstract:Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding \"Let's think step by step\" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.",
                        "Citation Paper Authors": "Authors:Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, Yusuke Iwasawa"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.06848v1": {
            "Paper Title": "Data-Driven Modeling and Verification of Perception-Based Autonomous\n  Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2303.00855v2": {
            "Paper Title": "Grounded Decoding: Guiding Text Generation with Grounded Models for\n  Embodied Agents",
            "Sentences": []
        },
        "http://arxiv.org/abs/2303.08476v2": {
            "Paper Title": "Bayesian Learning for the Robust Verification of Autonomous Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.06802v1": {
            "Paper Title": "On the Feasibility of Fingerprinting Collaborative Robot Traffic",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": ".\nIoT device fingerprinting. IoT devices typically exhibit regular\nand predictable traffic patterns which are only partially shrouded by\nencryption, enabling eavesdroppers to infer which specific devices\noperate within a smart-household ",
                    "Citation Text": "Noah Apthorpe, Dillon Reisman, and Nick Feamster. 2016. A smart home is\nno castle: Privacy vulnerabilities of encrypted IoT traffic. In Proceedings of the\nWorkshop on Data and Algorithmic Transparency .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.06805",
                        "Citation Paper Title": "Title:A Smart Home is No Castle: Privacy Vulnerabilities of Encrypted IoT Traffic",
                        "Citation Paper Abstract": "Abstract:The increasing popularity of specialized Internet-connected devices and appliances, dubbed the Internet-of-Things (IoT), promises both new conveniences and new privacy concerns. Unlike traditional web browsers, many IoT devices have always-on sensors that constantly monitor fine-grained details of users' physical environments and influence the devices' network communications. Passive network observers, such as Internet service providers, could potentially analyze IoT network traffic to infer sensitive details about users. Here, we examine four IoT smart home devices (a Sense sleep monitor, a Nest Cam Indoor security camera, a WeMo switch, and an Amazon Echo) and find that their network traffic rates can reveal potentially sensitive user interactions even when the traffic is encrypted. These results indicate that a technological solution is needed to protect IoT device owner privacy, and that IoT-specific concerns must be considered in the ongoing policy debate around ISP data collection and usage.",
                        "Citation Paper Authors": "Authors:Noah Apthorpe, Dillon Reisman, Nick Feamster"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.06801v1": {
            "Paper Title": "ADOD: Adaptive Domain-Aware Object Detection with Residual Attention for\n  Underwater Environments",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "toughens models by exposing them to domain shifts, and\ndata augmentation techniques, like CrossGrad ",
                    "Citation Text": "S. Shankar, V . Piratla, S. Chakrabarti, S. Chaudhuri, P. Jyothi, and\nS. Sarawagi, \u201cGeneralizing across domains via cross-gradient train-\ning,\u201d arXiv preprint arXiv:1804.10745 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.10745",
                        "Citation Paper Title": "Title:Generalizing Across Domains via Cross-Gradient Training",
                        "Citation Paper Abstract": "Abstract:We present CROSSGRAD, a method to use multi-domain training data to learn a classifier that generalizes to new domains. CROSSGRAD does not need an adaptation phase via labeled or unlabeled data, or domain features in the new domain. Most existing domain adaptation methods attempt to erase domain signals using techniques like domain adversarial training. In contrast, CROSSGRAD is free to use domain signals for predicting labels, if it can prevent overfitting on training domains. We conceptualize the task in a Bayesian setting, in which a sampling step is implemented as data augmentation, based on domain-guided perturbations of input instances. CROSSGRAD parallelly trains a label and a domain classifier on examples perturbed by loss gradients of each other's objectives. This enables us to directly perturb inputs, without separating and re-mixing domain signals while making various distributional assumptions. Empirical evaluation on three different applications where this setting is natural establishes that (1) domain-guided perturbation provides consistently better generalization to unseen domains, compared to generic instance perturbation methods, and that (2) data augmentation is a more stable and accurate method than domain adversarial training.",
                        "Citation Paper Authors": "Authors:Shiv Shankar, Vihari Piratla, Soumen Chakrabarti, Siddhartha Chaudhuri, Preethi Jyothi, Sunita Sarawagi"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "Consequently, Domain shift occurs when the training\ndata\u2019s distribution doesn\u2019t match the testing data, leading to\na drop in model performance in real-world scenarios ",
                    "Citation Text": "B. Recht, R. Roelofs, L. Schmidt, and V . Shankar, \u201cDo imagenet\nclassifiers generalize to imagenet?\u201d in International conference on\nmachine learning . PMLR, 2019, pp. 5389\u20135400.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.10811",
                        "Citation Paper Title": "Title:Do ImageNet Classifiers Generalize to ImageNet?",
                        "Citation Paper Abstract": "Abstract:We build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively re-used test sets. By closely following the original dataset creation processes, we test to what extent current classification models generalize to new data. We evaluate a broad range of models and find accuracy drops of 3% - 15% on CIFAR-10 and 11% - 14% on ImageNet. However, accuracy gains on the original test sets translate to larger gains on the new test sets. Our results suggest that the accuracy drops are not caused by adaptivity, but by the models' inability to generalize to slightly \"harder\" images than those found in the original test sets.",
                        "Citation Paper Authors": "Authors:Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, Vaishaal Shankar"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2305.12032v4": {
            "Paper Title": "The Waymo Open Sim Agents Challenge",
            "Sentences": [
                {
                    "Sentence ID": 53,
                    "Sentence": ". Previous evaluation methods such as the Inception Score (IS) ",
                    "Citation Text": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved\ntechniques for training gans. In Proceedings of the 30th International Conference on Neural Information\nProcessing Systems , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.03498",
                        "Citation Paper Title": "Title:Improved Techniques for Training GANs",
                        "Citation Paper Abstract": "Abstract:We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.",
                        "Citation Paper Authors": "Authors:Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": ".\nEvaluation of Generative Models Distribution matching has become a common way to evaluate\ngenerative models [ 18,20,27,30,31,45,46,49,52,77], through the Fr\u00e9chet Inception Distance\n(FID) ",
                    "Citation Text": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. In Proceedings of the 31st\nInternational Conference on Neural Information Processing Systems , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.08500",
                        "Citation Paper Title": "Title:GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
                        "Citation Paper Abstract": "Abstract:Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the \"Fr\u00e9chet Inception Distance\" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.",
                        "Citation Paper Authors": "Authors:Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": ". Other methods\ndiffer in the type of input, whether rasterized [ 3,74] or provided in a vector format [ 28,79]. Some\nworks focus specifically on generating challenging scenarios ",
                    "Citation Text": "Davis Rempe, Jonah Philion, Leonidas J Guibas, Sanja Fidler, and Or Litany. Generating useful accident-\nprone driving scenarios via a learned traffic prior. In CVPR , June 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.05077",
                        "Citation Paper Title": "Title:Generating Useful Accident-Prone Driving Scenarios via a Learned Traffic Prior",
                        "Citation Paper Abstract": "Abstract:Evaluating and improving planning for autonomous vehicles requires scalable generation of long-tail traffic scenarios. To be useful, these scenarios must be realistic and challenging, but not impossible to drive through safely. In this work, we introduce STRIVE, a method to automatically generate challenging scenarios that cause a given planner to produce undesirable behavior, like collisions. To maintain scenario plausibility, the key idea is to leverage a learned model of traffic motion in the form of a graph-based conditional VAE. Scenario generation is formulated as an optimization in the latent space of this traffic model, perturbing an initial real-world scene to produce trajectories that collide with a given planner. A subsequent optimization is used to find a \"solution\" to the scenario, ensuring it is useful to improve the given planner. Further analysis clusters generated scenarios based on collision type. We attack two planners and show that STRIVE successfully generates realistic, challenging scenarios in both cases. We additionally \"close the loop\" and use these scenarios to optimize hyperparameters of a rule-based planner.",
                        "Citation Paper Authors": "Authors:Davis Rempe, Jonah Philion, Leonidas J. Guibas, Sanja Fidler, Or Litany"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.06639v1": {
            "Paper Title": "Harmonic Mobile Manipulation",
            "Sentences": [
                {
                    "Sentence ID": 48,
                    "Sentence": "have created diverse\nhouses for training embodied agents to navigate through\nvarious everyday scenes. Yenamandra et al. ",
                    "Citation Text": "Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav,\nAustin Wang, Mukul Khanna, Theophile Gervet, Tsung-Yen\nYang, Vidhi Jain, Alexander William Clegg, John Turner,\nZsolt Kira, Manolis Savva, Angel Chang, Devendra Singh\nChaplot, Dhruv Batra, Roozbeh Mottaghi, Yonatan Bisk, and\nChris Paxton. Homerobot: Open-vocabulary mobile manip-\nulation, 2023. 2, 3, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2306.11565",
                        "Citation Paper Title": "Title:HomeRobot: Open-Vocabulary Mobile Manipulation",
                        "Citation Paper Abstract": "Abstract:HomeRobot (noun): An affordable compliant robot that navigates homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future research work improve performance. See videos on our website: this https URL.",
                        "Citation Paper Authors": "Authors:Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav, Austin Wang, Mukul Khanna, Theophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander William Clegg, John Turner, Zsolt Kira, Manolis Savva, Angel Chang, Devendra Singh Chaplot, Dhruv Batra, Roozbeh Mottaghi, Yonatan Bisk, Chris Paxton"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "have underscored the effectiveness of using pre-trained vi-\nsual backbones to capture task-specific semantic informa-\ntion from visual observations and to transfer effectively to\nthe real world. Inspired by these prior efforts, we utilize a\nfrozen DINOv2 ",
                    "Citation Text": "Maxime Oquab, Timoth\u00e9e Darcet, Theo Moutakanni, Huy V .\nV o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Rus-sell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-\nWen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nico-\nlas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou,\nJulien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bo-\njanowski. Dinov2: Learning robust visual features without\nsupervision, 2023. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.07193",
                        "Citation Paper Title": "Title:DINOv2: Learning Robust Visual Features without Supervision",
                        "Citation Paper Abstract": "Abstract:The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.",
                        "Citation Paper Authors": "Authors:Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv\u00e9 Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "fo-\ncus on tracking a given target pose with a whole-body mo-\nbile manipulator. Yokoyama et al. ",
                    "Citation Text": "Naoki Yokoyama, Alex Clegg, Joanne Truong, Eric Under-\nsander, Tsung-Yen Yang, Sergio Arnaud, Sehoon Ha, Dhruv\nBatra, and Akshara Rai. Asc: Adaptive skill coordination for\nrobotic mobile manipulation, 2023. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.00410",
                        "Citation Paper Title": "Title:ASC: Adaptive Skill Coordination for Robotic Mobile Manipulation",
                        "Citation Paper Abstract": "Abstract:We present Adaptive Skill Coordination (ASC) -- an approach for accomplishing long-horizon tasks like mobile pick-and-place (i.e., navigating to an object, picking it, navigating to another location, and placing it). ASC consists of three components -- (1) a library of basic visuomotor skills (navigation, pick, place), (2) a skill coordination policy that chooses which skill to use when, and (3) a corrective policy that adapts pre-trained skills in out-of-distribution states. All components of ASC rely only on onboard visual and proprioceptive sensing, without requiring detailed maps with obstacle layouts or precise object locations, easing real-world deployment. We train ASC in simulated indoor environments, and deploy it zero-shot (without any real-world experience or fine-tuning) on the Boston Dynamics Spot robot in eight novel real-world environments (one apartment, one lab, two microkitchens, two lounges, one office space, one outdoor courtyard). In rigorous quantitative comparisons in two environments, ASC achieves near-perfect performance (59/60 episodes, or 98%), while sequentially executing skills succeeds in only 44/60 (73%) episodes. Extensive perturbation experiments show that ASC is robust to hand-off errors, changes in the environment layout, dynamic obstacles (e.g., people), and unexpected disturbances. Supplementary videos at this http URL.",
                        "Citation Paper Authors": "Authors:Naoki Yokoyama, Alex Clegg, Joanne Truong, Eric Undersander, Tsung-Yen Yang, Sergio Arnaud, Sehoon Ha, Dhruv Batra, Akshara Rai"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "have demon-\nstrated how robots can clean tables using imitation learn-\ning in a tabletop setup. Urakami et al. ",
                    "Citation Text": "Yusuke Urakami, Alec Hodgkinson, Casey Carlin, Randall\nLeu, Luca Rigazio, and Pieter Abbeel. Doorgym: A scalable\ndoor opening environment and baseline agent, 2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.01887",
                        "Citation Paper Title": "Title:DoorGym: A Scalable Door Opening Environment And Baseline Agent",
                        "Citation Paper Abstract": "Abstract:In order to practically implement the door opening task, a policy ought to be robust to a wide distribution of door types and environment settings. Reinforcement Learning (RL) with Domain Randomization (DR) is a promising technique to enforce policy generalization, however, there are only a few accessible training environments that are inherently designed to train agents in domain randomized environments. We introduce DoorGym, an open-source door opening simulation framework designed to utilize domain randomization to train a stable policy. We intend for our environment to lie at the intersection of domain transfer, practical tasks, and realism. We also provide baseline Proximal Policy Optimization and Soft Actor-Critic implementations, which achieves success rates between 0% up to 95% for opening various types of doors in this environment. Moreover, the real-world transfer experiment shows the trained policy is able to work in the real world. Environment kit available here: this https URL",
                        "Citation Paper Authors": "Authors:Yusuke Urakami, Alec Hodgkinson, Casey Carlin, Randall Leu, Luca Rigazio, Pieter Abbeel"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "have developed benchmarks for straightforward mobile ma-\nnipulation tasks in open spaces. Gu et al. ",
                    "Citation Text": "Jiayuan Gu, Sean Kirmani, Paul Wohlhart, Yao Lu, Montser-\nrat Gonzalez Arenas, Kanishka Rao, Wenhao Yu, Chuyuan\nFu, Keerthana Gopalakrishnan, Zhuo Xu, Priya Sundare-\nsan, Peng Xu, Hao Su, Karol Hausman, Chelsea Finn, Quan\nVuong, and Ted Xiao. Rt-trajectory: Robotic task general-\nization via hindsight trajectory sketches, 2023. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2311.01977",
                        "Citation Paper Title": "Title:RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches",
                        "Citation Paper Abstract": "Abstract:Generalization remains one of the most important desiderata for robust robot learning systems. While recently proposed approaches show promise in generalization to novel objects, semantic concepts, or visual distribution shifts, generalization to new tasks remains challenging. For example, a language-conditioned policy trained on pick-and-place tasks will not be able to generalize to a folding task, even if the arm trajectory of folding is similar to pick-and-place. Our key insight is that this kind of generalization becomes feasible if we represent the task through rough trajectory sketches. We propose a policy conditioning method using such rough trajectory sketches, which we call RT-Trajectory, that is practical, easy to specify, and allows the policy to effectively perform new tasks that would otherwise be challenging to perform. We find that trajectory sketches strike a balance between being detailed enough to express low-level motion-centric guidance while being coarse enough to allow the learned policy to interpret the trajectory sketch in the context of situational visual observations. In addition, we show how trajectory sketches can provide a useful interface to communicate with robotic policies: they can be specified through simple human inputs like drawings or videos, or through automated methods such as modern image-generating or waypoint-generating methods. We evaluate RT-Trajectory at scale on a variety of real-world robotic tasks, and find that RT-Trajectory is able to perform a wider range of tasks compared to language-conditioned and goal-conditioned policies, when provided the same training data.",
                        "Citation Paper Authors": "Authors:Jiayuan Gu, Sean Kirmani, Paul Wohlhart, Yao Lu, Montserrat Gonzalez Arenas, Kanishka Rao, Wenhao Yu, Chuyuan Fu, Keerthana Gopalakrishnan, Zhuo Xu, Priya Sundaresan, Peng Xu, Hao Su, Karol Hausman, Chelsea Finn, Quan Vuong, Ted Xiao"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.02396v2": {
            "Paper Title": "Unsupervised Change Detection for Space Habitats Using 3D Point Clouds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.06741v1": {
            "Paper Title": "Gaussian Splatting SLAM",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": "has recently been pop-\nularised with Neural Radiance Fields (NeRF) ",
                    "Citation Text": "Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In Proceedings of the European Conference on Com-\nputer Vision (ECCV) , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08934",
                        "Citation Paper Title": "Title:NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
                        "Citation Paper Abstract": "Abstract:We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",
                        "Citation Paper Authors": "Authors:Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.06583v1": {
            "Paper Title": "3D Hand Pose Estimation in Egocentric Images in the Wild",
            "Sentences": [
                {
                    "Sentence ID": 37,
                    "Sentence": "backbone pretrained on ImageNet to obtain a 7\u00d77\u00d72048\nfeature representation for the hand crop. We process this\nfeature map through 3 convolutional and 2 linear layers to\nmap into a 2048 -dimensional feature vector. This is passed\nto a HMR ",
                    "Citation Text": "Angjoo Kanazawa, Shubham Tulsiani, Alexei A Efros, and Ji-\ntendra Malik. Learning category-specific mesh reconstruction\nfrom image collections. In ECCV , 2018. 2, 4, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.07549",
                        "Citation Paper Title": "Title:Learning Category-Specific Mesh Reconstruction from Image Collections",
                        "Citation Paper Abstract": "Abstract:We present a learning framework for recovering the 3D shape, camera, and texture of an object from a single image. The shape is represented as a deformable 3D mesh model of an object category where a shape is parameterized by a learned mean shape and per-instance predicted deformation. Our approach allows leveraging an annotated image collection for training, where the deformable model and the 3D prediction mechanism are learned without relying on ground-truth 3D or multi-view supervision. Our representation enables us to go beyond existing 3D prediction approaches by incorporating texture inference as prediction of an image in a canonical appearance space. Additionally, we show that semantic keypoints can be easily associated with the predicted shapes. We present qualitative and quantitative results of our approach on CUB and PASCAL3D datasets and show that we can learn to predict diverse shapes and textures across objects using only annotated image collections. The project website can be found at this https URL.",
                        "Citation Paper Authors": "Authors:Angjoo Kanazawa, Shubham Tulsiani, Alexei A. Efros, Jitendra Malik"
                    }
                },
                {
                    "Sentence ID": 63,
                    "Sentence": ".\nDifferent from these datasets with 3D poses, [ 8,11,63] pro-\nvide annotations for segmentation masks [ 8,11], 2D bound-\ning boxes ",
                    "Citation Text": "Dandan Shan, Jiaqi Geng, Michelle Shu, and David F Fouhey.\nUnderstanding human hands in contact at internet scale. In\nCVPR , 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.06669",
                        "Citation Paper Title": "Title:Understanding Human Hands in Contact at Internet Scale",
                        "Citation Paper Abstract": "Abstract:Hands are the central means by which humans manipulate their world and being able to reliably extract hand state information from Internet videos of humans engaged in their hands has the potential to pave the way to systems that can learn from petabytes of video data. This paper proposes steps towards this by inferring a rich representation of hands engaged in interaction method that includes: hand location, side, contact state, and a box around the object in contact. To support this effort, we gather a large-scale dataset of hands in contact with objects consisting of 131 days of footage as well as a 100K annotated hand-contact video frame dataset. The learned model on this dataset can serve as a foundation for hand-contact understanding in videos. We quantitatively evaluate it both on its own and in service of predicting and learning from 3D meshes of human hands.",
                        "Citation Paper Authors": "Authors:Dandan Shan, Jiaqi Geng, Michelle Shu, David F. Fouhey"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.07606v1": {
            "Paper Title": "Pedestrian and Passenger Interaction with Autonomous Vehicles: Field\n  Study in a Crosswalk Scenario",
            "Sentences": []
        },
        "http://arxiv.org/abs/2311.15643v2": {
            "Paper Title": "A Survey on Monocular Re-Localization: From the Perspective of Scene Map\n  Representation",
            "Sentences": [
                {
                    "Sentence ID": 292,
                    "Sentence": "minimizes the photometric errors between query image and virtual image\nrendered at estimated pose while DFNet ",
                    "Citation Text": "S. Chen, X. Li, Z. Wang, V. A. Prisacariu, Dfnet: Enhance absolute\npose regression with direct feature matching, in: Proceedings of Eu-\nropean Conference on Computer Vision (ECCV), Springer, 2022, pp.\n1\u201317.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.00559",
                        "Citation Paper Title": "Title:DFNet: Enhance Absolute Pose Regression with Direct Feature Matching",
                        "Citation Paper Abstract": "Abstract:We introduce a camera relocalization pipeline that combines absolute pose regression (APR) and direct feature matching. By incorporating exposure-adaptive novel view synthesis, our method successfully addresses photometric distortions in outdoor environments that existing photometric-based methods fail to handle. With domain-invariant feature matching, our solution improves pose regression accuracy using semi-supervised learning on unlabeled data. In particular, the pipeline consists of two components: Novel View Synthesizer and DFNet. The former synthesizes novel views compensating for changes in exposure and the latter regresses camera poses and extracts robust features that close the domain gap between real images and synthetic ones. Furthermore, we introduce an online synthetic data generation scheme. We show that these approaches effectively enhance camera pose estimation both in indoor and outdoor scenes. Hence, our method achieves a state-of-the-art accuracy by outperforming existing single-image APR methods by as much as 56%, comparable to 3D structure-based methods.",
                        "Citation Paper Authors": "Authors:Shuai Chen, Xinghui Li, Zirui Wang, Victor Adrian Prisacariu"
                    }
                },
                {
                    "Sentence ID": 291,
                    "Sentence": "as an offline data augmentation pipeline to\nenhance APR model. As an online augmentation way, Direct-PoseNet ",
                    "Citation Text": "S. Chen, Z. Wang, V. Prisacariu, Direct-posenet: Absolute pose re-\ngression with photometric consistency, in: Proceedings of International\nConference on 3D Vision (3DV), 2021, pp. 1175\u20131185. doi:10.1109/\n3DV53792.2021.00125 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.04073",
                        "Citation Paper Title": "Title:Direct-PoseNet: Absolute Pose Regression with Photometric Consistency",
                        "Citation Paper Abstract": "Abstract:We present a relocalization pipeline, which combines an absolute pose regression (APR) network with a novel view synthesis based direct matching module, offering superior accuracy while maintaining low inference time. Our contribution is twofold: i) we design a direct matching module that supplies a photometric supervision signal to refine the pose regression network via differentiable rendering; ii) we modify the rotation representation from the classical quaternion to SO(3) in pose regression, removing the need for balancing rotation and translation loss terms. As a result, our network Direct-PoseNet achieves state-of-the-art performance among all other single-image APR methods on the 7-Scenes benchmark and the LLFF dataset.",
                        "Citation Paper Authors": "Authors:Shuai Chen, Zirui Wang, Victor Prisacariu"
                    }
                },
                {
                    "Sentence ID": 289,
                    "Sentence": ", a pre-\ntrained NeRF model is integrated into a Monte Carlo localization to assigns\nweights to particles by image similarity between query image and rendered\nimage. In NeRF-Loc ",
                    "Citation Text": "J. Liu, Q. Nie, Y. Liu, C. Wang, Nerf-loc: Visual localization with con-\nditional neural radiance field, arXiv preprint arXiv:2304.07979 (2023).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.07979",
                        "Citation Paper Title": "Title:NeRF-Loc: Visual Localization with Conditional Neural Radiance Field",
                        "Citation Paper Abstract": "Abstract:We propose a novel visual re-localization method based on direct matching between the implicit 3D descriptors and the 2D image with transformer. A conditional neural radiance field(NeRF) is chosen as the 3D scene representation in our pipeline, which supports continuous 3D descriptors generation and neural rendering. By unifying the feature matching and the scene coordinate regression to the same framework, our model learns both generalizable knowledge and scene prior respectively during two training stages. Furthermore, to improve the localization robustness when domain gap exists between training and testing phases, we propose an appearance adaptation layer to explicitly align styles between the 3D model and the query image. Experiments show that our method achieves higher localization accuracy than other learning-based approaches on multiple benchmarks. Code is available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Jianlin Liu, Qiang Nie, Yong Liu, Chengjie Wang"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "minimizes the photometric error between the observed image and rendered\nimage by optimizing the pose on the tangent plane. In Loc-NeRF ",
                    "Citation Text": "D. Maggio, M. Abate, J. Shi, C. Mario, L. Carlone, Loc-nerf: Monte\ncarlo localization using neural radiance fields, in: Proceedings of IEEE\nInternational Conference on Robotics and Automation (ICRA), IEEE,\n2023, pp. 4018\u20134025.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2209.09050",
                        "Citation Paper Title": "Title:Loc-NeRF: Monte Carlo Localization using Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:We present Loc-NeRF, a real-time vision-based robot localization approach that combines Monte Carlo localization and Neural Radiance Fields (NeRF). Our system uses a pre-trained NeRF model as the map of an environment and can localize itself in real-time using an RGB camera as the only exteroceptive sensor onboard the robot. While neural radiance fields have seen significant applications for visual rendering in computer vision and graphics, they have found limited use in robotics. Existing approaches for NeRF-based localization require both a good initial pose guess and significant computation, making them impractical for real-time robotics applications. By using Monte Carlo localization as a workhorse to estimate poses using a NeRF map model, Loc-NeRF is able to perform localization faster than the state of the art and without relying on an initial pose estimate. In addition to testing on synthetic data, we also run our system using real data collected by a Clearpath Jackal UGV and demonstrate for the first time the ability to perform real-time global localization with neural radiance fields. We make our code publicly available at this https URL.",
                        "Citation Paper Authors": "Authors:Dominic Maggio, Marcus Abate, Jingnan Shi, Courtney Mario, Luca Carlone"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": ":\n\u02c6xq= arg min\nxq\u2208SE(3)L(Iq,xq|\u0398 freezed) (23)\nSuch a joint pose estimation and NeRF training scheme then motivates pose-\nfree NeRF [287, 288]. In LATITUDE ",
                    "Citation Text": "Z. Zhu, Y. Chen, Z. Wu, C. Hou, Y. Shi, C. Li, P. Li, H. Zhao, G. Zhou,\nLatitude: Robotic global localization with truncated dynamic low-pass\nfilter in city-scale nerf, in: Proceedings of IEEE International Conference\non Robotics and Automation (ICRA), IEEE, 2023, pp. 8326\u20138332.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2209.08498",
                        "Citation Paper Title": "Title:LATITUDE: Robotic Global Localization with Truncated Dynamic Low-pass Filter in City-scale NeRF",
                        "Citation Paper Abstract": "Abstract:Neural Radiance Fields (NeRFs) have made great success in representing complex 3D scenes with high-resolution details and efficient memory. Nevertheless, current NeRF-based pose estimators have no initial pose prediction and are prone to local optima during optimization. In this paper, we present LATITUDE: Global Localization with Truncated Dynamic Low-pass Filter, which introduces a two-stage localization mechanism in city-scale NeRF. In place recognition stage, we train a regressor through images generated from trained NeRFs, which provides an initial value for global localization. In pose optimization stage, we minimize the residual between the observed image and rendered image by directly optimizing the pose on tangent plane. To avoid convergence to local optimum, we introduce a Truncated Dynamic Low-pass Filter (TDLF) for coarse-to-fine pose registration. We evaluate our method on both synthetic and real-world data and show its potential applications for high-precision navigation in large-scale city scenes. Codes and data will be publicly available at this https URL.",
                        "Citation Paper Authors": "Authors:Zhenxin Zhu, Yuantao Chen, Zirui Wu, Chao Hou, Yongliang Shi, Chuxuan Li, Pengfei Li, Hao Zhao, Guyue Zhou"
                    }
                },
                {
                    "Sentence ID": 284,
                    "Sentence": ",\nauthors proposed a Dense Scene Matching (DSM) module. The DSM mod-\nule predicts scene coordinates by cost volume constructed between query im-\nage and reference image with its scene coordinates. Following this way, the\nTransformer-based SAReg ",
                    "Citation Text": "J. Revaud, Y. Cabon, R. Br\u2019egier, J. Lee, P. Weinzaepfel, Sacreg:\nScene-agnostic coordinate regression for visual localization, ArXiv\nabs/2307.11702 (2023).\nURL https://api.semanticscholar.org/CorpusID:260091307",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2307.11702",
                        "Citation Paper Title": "Title:SACReg: Scene-Agnostic Coordinate Regression for Visual Localization",
                        "Citation Paper Abstract": "Abstract:Scene coordinates regression (SCR), i.e., predicting 3D coordinates for every pixel of a given image, has recently shown promising potential. However, existing methods remain limited to small scenes memorized during training, and thus hardly scale to realistic datasets and scenarios. In this paper, we propose a generalized SCR model trained once to be deployed in new test scenes, regardless of their scale, without any finetuning. Instead of encoding the scene coordinates into the network weights, our model takes as input a database image with some sparse 2D pixel to 3D coordinate annotations, extracted from e.g. off-the-shelf Structure-from-Motion or RGB-D data, and a query image for which are predicted a dense 3D coordinate map and its confidence, based on cross-attention. At test time, we rely on existing off-the-shelf image retrieval systems and fuse the predictions from a shortlist of relevant database images w.r.t. the query. Afterwards camera pose is obtained using standard Perspective-n-Point (PnP). Starting from selfsupervised CroCo pretrained weights, we train our model on diverse datasets to ensure generalizabilty across various scenarios, and significantly outperform other scene regression approaches, including scene-specific models, on multiple visual localization benchmarks. Finally, we show that the database representation of images and their 2D-3D annotations can be highly compressed with negligible loss of localization performance.",
                        "Citation Paper Authors": "Authors:Jerome Revaud, Yohann Cabon, Romain Br\u00e9gier, JongMin Lee, Philippe Weinzaepfel"
                    }
                },
                {
                    "Sentence ID": 261,
                    "Sentence": "proposed a new loss function with learnable\nweight of orientation error and translation error so that manual fine-tunning\ncan be avoided, which is popularly used in later works. In ",
                    "Citation Text": "S. Brahmbhatt, J. Gu, K. Kim, J. Hays, J. Kautz, Geometry-aware\nlearning of maps for camera localization, in: Proceedings of IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), 2018,\npp. 2616\u20132625. doi:10.1109/CVPR.2018.00277 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.03342",
                        "Citation Paper Title": "Title:Geometry-Aware Learning of Maps for Camera Localization",
                        "Citation Paper Abstract": "Abstract:Maps are a key component in image-based camera localization and visual SLAM systems: they are used to establish geometric constraints between images, correct drift in relative pose estimation, and relocalize cameras after lost tracking. The exact definitions of maps, however, are often application-specific and hand-crafted for different scenarios (e.g. 3D landmarks, lines, planes, bags of visual words). We propose to represent maps as a deep neural net called MapNet, which enables learning a data-driven map representation. Unlike prior work on learning maps, MapNet exploits cheap and ubiquitous sensory inputs like visual odometry and GPS in addition to images and fuses them together for camera localization. Geometric constraints expressed by these inputs, which have traditionally been used in bundle adjustment or pose-graph optimization, are formulated as loss terms in MapNet training and also used during inference. In addition to directly improving localization accuracy, this allows us to update the MapNet (i.e., maps) in a self-supervised manner using additional unlabeled video sequences from the scene. We also propose a novel parameterization for camera rotation which is better suited for deep-learning based camera pose regression. Experimental results on both the indoor 7-Scenes dataset and the outdoor Oxford RobotCar dataset show significant performance improvement over prior work. The MapNet project webpage is this https URL.",
                        "Citation Paper Authors": "Authors:Samarth Brahmbhatt, Jinwei Gu, Kihwan Kim, James Hays, Jan Kautz"
                    }
                },
                {
                    "Sentence ID": 260,
                    "Sentence": ", the scale\nbetween orientation error and translation error varies in different scenes, e.g.,\n120 to 750 for indoor scenes while 250 to 2000 for outdoor scenes, which is\ntricky. Kendall et al. ",
                    "Citation Text": "A. Kendall, R. Cipolla, Geometric loss functions for camera pose re-\ngression with deep learning, in: Proceedings of IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2017, pp. 6555\u2013\n6564. doi:10.1109/CVPR.2017.694 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.00390",
                        "Citation Paper Title": "Title:Geometric Loss Functions for Camera Pose Regression with Deep Learning",
                        "Citation Paper Abstract": "Abstract:Deep learning has shown to be effective for robust and real-time monocular image relocalisation. In particular, PoseNet is a deep convolutional neural network which learns to regress the 6-DOF camera pose from a single image. It learns to localize using high level features and is robust to difficult lighting, motion blur and unknown camera intrinsics, where point based SIFT registration fails. However, it was trained using a naive loss function, with hyper-parameters which require expensive tuning. In this paper, we give the problem a more fundamental theoretical treatment. We explore a number of novel loss functions for learning camera pose which are based on geometry and scene reprojection error. Additionally we show how to automatically learn an optimal weighting to simultaneously regress position and orientation. By leveraging geometry, we demonstrate that our technique significantly improves PoseNet's performance across datasets ranging from indoor rooms to a small city.",
                        "Citation Paper Authors": "Authors:Alex Kendall, Roberto Cipolla"
                    }
                },
                {
                    "Sentence ID": 254,
                    "Sentence": "is composed by a feature en-\ncoder, i.e., a GoogLeNet backbone ",
                    "Citation Text": "C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Er-\nhan, V. Vanhoucke, A. Rabinovich, Going deeper with convolutions,\nin: Proceedings of IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 2015, pp. 1\u20139. doi:10.1109/CVPR.2015.\n7298594 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.4842",
                        "Citation Paper Title": "Title:Going Deeper with Convolutions",
                        "Citation Paper Abstract": "Abstract:We propose a deep convolutional neural network architecture codenamed \"Inception\", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.",
                        "Citation Paper Authors": "Authors:Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich"
                    }
                },
                {
                    "Sentence ID": 250,
                    "Sentence": "estimates 3 DOF pose in a 2D map, Open-\nStreetMap (OSM), by proposed neural map matching. But its localization\naccuracy is only limited to sub-meter level due to the low precision of used\nOSM map. BEV-Locator ",
                    "Citation Text": "Z. Zhang, M. Xu, W. Zhou, T. Peng, L. Li, S. Poslad, Bev-locator: An\nend-to-end visual semantic localization network using multi-view images,\nArXiv abs/2211.14927 (2022).\nURL https://api.semanticscholar.org/CorpusID:254044186",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2211.14927",
                        "Citation Paper Title": "Title:BEV-Locator: An End-to-end Visual Semantic Localization Network Using Multi-View Images",
                        "Citation Paper Abstract": "Abstract:Accurate localization ability is fundamental in autonomous driving. Traditional visual localization frameworks approach the semantic map-matching problem with geometric models, which rely on complex parameter tuning and thus hinder large-scale deployment. In this paper, we propose BEV-Locator: an end-to-end visual semantic localization neural network using multi-view camera images. Specifically, a visual BEV (Birds-Eye-View) encoder extracts and flattens the multi-view images into BEV space. While the semantic map features are structurally embedded as map queries sequence. Then a cross-model transformer associates the BEV features and semantic map queries. The localization information of ego-car is recursively queried out by cross-attention modules. Finally, the ego pose can be inferred by decoding the transformer outputs. We evaluate the proposed method in large-scale nuScenes and Qcraft datasets. The experimental results show that the BEV-locator is capable to estimate the vehicle poses under versatile scenarios, which effectively associates the cross-model information from multi-view images and global semantic maps. The experiments report satisfactory accuracy with mean absolute errors of 0.052m, 0.135m and 0.251$^\\circ$ in lateral, longitudinal translation and heading angle degree.",
                        "Citation Paper Authors": "Authors:Zhihuang Zhang, Meng Xu, Wenqiang Zhou, Tao Peng, Liang Li, Stefan Poslad"
                    }
                },
                {
                    "Sentence ID": 232,
                    "Sentence": "is differentiable not only w.r.t. the camera pose but also w.r.t.\nthe descriptors. PixLoc ",
                    "Citation Text": "P.-E. Sarlin, A. Unagar, M. Larsson, H. Germain, C. Toft, V. Larsson,\nM. Pollefeys, V. Lepetit, L. Hammarstrand, F. Kahl, T. Sattler, Back\n93to the feature: Learning robust camera localization from pixels to pose,\nin: Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 2021, pp. 3247\u20133257.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.09213",
                        "Citation Paper Title": "Title:Back to the Feature: Learning Robust Camera Localization from Pixels to Pose",
                        "Citation Paper Abstract": "Abstract:Camera pose estimation in known scenes is a 3D geometry task recently tackled by multiple learning algorithms. Many regress precise geometric quantities, like poses or 3D points, from an input image. This either fails to generalize to new viewpoints or ties the model parameters to a specific scene. In this paper, we go Back to the Feature: we argue that deep networks should focus on learning robust and invariant visual features, while the geometric estimation should be left to principled algorithms. We introduce PixLoc, a scene-agnostic neural network that estimates an accurate 6-DoF pose from an image and a 3D model. Our approach is based on the direct alignment of multiscale deep features, casting camera localization as metric learning. PixLoc learns strong data priors by end-to-end training from pixels to pose and exhibits exceptional generalization to new scenes by separating model parameters and scene geometry. The system can localize in large environments given coarse pose priors but also improve the accuracy of sparse feature matching by jointly refining keypoints and poses with little overhead. The code will be publicly available at this https URL.",
                        "Citation Paper Authors": "Authors:Paul-Edouard Sarlin, Ajaykumar Unagar, M\u00e5ns Larsson, Hugo Germain, Carl Toft, Viktor Larsson, Marc Pollefeys, Vincent Lepetit, Lars Hammarstrand, Fredrik Kahl, Torsten Sattler"
                    }
                },
                {
                    "Sentence ID": 198,
                    "Sentence": "use multi-scale dense CNN features for both image description and feature\nmatching. GN-Net ",
                    "Citation Text": "L. von Stumberg, P. Wenzel, Q. A. Khan, D. Cremers, Gn-net: The\ngauss-newton loss for multi-weather relocalization, IEEE Robotics and\nAutomation Letters 5 (2019) 890\u2013897.\nURL https://api.semanticscholar.org/CorpusID:202542796",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.11932",
                        "Citation Paper Title": "Title:GN-Net: The Gauss-Newton Loss for Multi-Weather Relocalization",
                        "Citation Paper Abstract": "Abstract:Direct SLAM methods have shown exceptional performance on odometry tasks. However, they are susceptible to dynamic lighting and weather changes while also suffering from a bad initialization on large baselines. To overcome this, we propose GN-Net: a network optimized with the novel Gauss-Newton loss for training weather invariant deep features, tailored for direct image alignment. Our network can be trained with pixel correspondences between images taken from different sequences. Experiments on both simulated and real-world datasets demonstrate that our approach is more robust against bad initialization, variations in day-time, and weather changes thereby outperforming state-of-the-art direct and indirect methods. Furthermore, we release an evaluation benchmark for relocalization tracking against different types of weather. Our benchmark is available at this https URL.",
                        "Citation Paper Authors": "Authors:Lukas von Stumberg, Patrick Wenzel, Qadeer Khan, Daniel Cremers"
                    }
                },
                {
                    "Sentence ID": 230,
                    "Sentence": "to calculate the\ngradients of the back-end PnP-based pose estimation process, enabling the\nmodel to be trained end-to-end with the supervision on estimated pose. More\nrecently, I2PNet ",
                    "Citation Text": "G. Wang, Y. Zheng, Y. Guo, Z. Liu, Y. Zhu, W. Burgard, H. Wang,\nEnd-to-end 2d-3d registration between image and lidar point cloud for\nvehicle localization, ArXiv abs/2306.11346 (2023).\nURL https://api.semanticscholar.org/CorpusID:259202501",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2306.11346",
                        "Citation Paper Title": "Title:End-to-end 2D-3D Registration between Image and LiDAR Point Cloud for Vehicle Localization",
                        "Citation Paper Abstract": "Abstract:Robot localization using a previously built map is essential for a variety of tasks including highly accurate navigation and mobile manipulation. A popular approach to robot localization is based on image-to-point cloud registration, which combines illumination-invariant LiDAR-based mapping with economical image-based localization. However, the recent works for image-to-point cloud registration either divide the registration into separate modules or project the point cloud to the depth image to register the RGB and depth images. In this paper, we present I2PNet, a novel end-to-end 2D-3D registration network. I2PNet directly registers the raw 3D point cloud with the 2D RGB image using differential modules with a unique target. The 2D-3D cost volume module for differential 2D-3D association is proposed to bridge feature extraction and pose regression. 2D-3D cost volume module implicitly constructs the soft point-to-pixel correspondence on the intrinsic-independent normalized plane of the pinhole camera model. Moreover, we introduce an outlier mask prediction module to filter the outliers in the 2D-3D association before pose regression. Furthermore, we propose the coarse-to-fine 2D-3D registration architecture to increase localization accuracy. We conduct extensive localization experiments on the KITTI Odometry and nuScenes datasets. The results demonstrate that I2PNet outperforms the state-of-the-art by a large margin. In addition, I2PNet has a higher efficiency than the previous works and can perform the localization in real-time. Moreover, we extend the application of I2PNet to the camera-LiDAR online calibration and demonstrate that I2PNet outperforms recent approaches on the online calibration task.",
                        "Citation Paper Authors": "Authors:Guangming Wang, Yu Zheng, Yanfeng Guo, Zhe Liu, Yixiang Zhu, Wolfram Burgard, Hesheng Wang"
                    }
                },
                {
                    "Sentence ID": 225,
                    "Sentence": "achieves improved\nperformance with significant margin, indicating that matching-based meth-\nods should be a more reasonable choice than regression-based alternates for\nI2P localization. DeepI2P ",
                    "Citation Text": "J. Li, G. Hee Lee, Deepi2p: Image-to-point cloud registration via deep\nclassification, in: Proceedings of IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2021, pp. 15955\u201315964. doi:\n10.1109/CVPR46437.2021.01570 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.03501",
                        "Citation Paper Title": "Title:DeepI2P: Image-to-Point Cloud Registration via Deep Classification",
                        "Citation Paper Abstract": "Abstract:This paper presents DeepI2P: a novel approach for cross-modality registration between an image and a point cloud. Given an image (e.g. from a rgb-camera) and a general point cloud (e.g. from a 3D Lidar scanner) captured at different locations in the same scene, our method estimates the relative rigid transformation between the coordinate frames of the camera and Lidar. Learning common feature descriptors to establish correspondences for the registration is inherently challenging due to the lack of appearance and geometric correlations across the two modalities. We circumvent the difficulty by converting the registration problem into a classification and inverse camera projection optimization problem. A classification neural network is designed to label whether the projection of each point in the point cloud is within or beyond the camera frustum. These labeled points are subsequently passed into a novel inverse camera projection solver to estimate the relative pose. Extensive experimental results on Oxford Robotcar and KITTI datasets demonstrate the feasibility of our approach. Our source code is available at this https URL",
                        "Citation Paper Authors": "Authors:Jiaxin Li, Gim Hee Lee"
                    }
                },
                {
                    "Sentence ID": 221,
                    "Sentence": ". Such\nI2P-RPR networks simply utilize stacked convolution layer or fully connec-\ntion layer as pose regressor and cannot fully exploit pose-related information\nfrom image-point cloud cost volumes. Therefore, POET ",
                    "Citation Text": "J. Miao, K. Jiang, Y. Wang, T. Wen, Z. Xiao, Z. Fu, M. Yang, M. Liu,\nD. Yang, Poses as queries: Image-to-lidar map localization with trans-\nformers, arXiv preprint arXiv:2305.04298 (2023).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.04298",
                        "Citation Paper Title": "Title:Poses as Queries: Image-to-LiDAR Map Localization with Transformers",
                        "Citation Paper Abstract": "Abstract:High-precision vehicle localization with commercial setups is a crucial technique for high-level autonomous driving tasks. Localization with a monocular camera in LiDAR map is a newly emerged approach that achieves promising balance between cost and accuracy, but estimating pose by finding correspondences between such cross-modal sensor data is challenging, thereby damaging the localization accuracy. In this paper, we address the problem by proposing a novel Transformer-based neural network to register 2D images into 3D LiDAR map in an end-to-end manner. Poses are implicitly represented as high-dimensional feature vectors called pose queries and can be iteratively updated by interacting with the retrieved relevant information from cross-model features using attention mechanism in a proposed POse Estimator Transformer (POET) module. Moreover, we apply a multiple hypotheses aggregation method that estimates the final poses by performing parallel optimization on multiple randomly initialized pose queries to reduce the network uncertainty. Comprehensive analysis and experimental results on public benchmark conclude that the proposed image-to-LiDAR map localization network could achieve state-of-the-art performances in challenging cross-modal localization tasks.",
                        "Citation Paper Authors": "Authors:Jinyu Miao, Kun Jiang, Yunlong Wang, Tuopu Wen, Zhongyang Xiao, Zheng Fu, Mengmeng Yang, Maolin Liu, Diange Yang"
                    }
                },
                {
                    "Sentence ID": 219,
                    "Sentence": "that calcu-\nlates the cost volume between the image feature and LiDAR feature by a\noptical flow network ",
                    "Citation Text": "D. Sun, X. Yang, M.-Y. Liu, J. Kautz, Pwc-net: Cnns for optical flow\nusing pyramid, warping, and cost volume, in: Proceedings of IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), 2018,\npp. 8934\u20138943. doi:10.1109/CVPR.2018.00931 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.02371",
                        "Citation Paper Title": "Title:PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume",
                        "Citation Paper Abstract": "Abstract:We present a compact but effective CNN model for optical flow, called PWC-Net. PWC-Net has been designed according to simple and well-established principles: pyramidal processing, warping, and the use of a cost volume. Cast in a learnable feature pyramid, PWC-Net uses the cur- rent optical flow estimate to warp the CNN features of the second image. It then uses the warped features and features of the first image to construct a cost volume, which is processed by a CNN to estimate the optical flow. PWC-Net is 17 times smaller in size and easier to train than the recent FlowNet2 model. Moreover, it outperforms all published optical flow methods on the MPI Sintel final pass and KITTI 2015 benchmarks, running at about 35 fps on Sintel resolution (1024x436) images. Our models are available on this https URL.",
                        "Citation Paper Authors": "Authors:Deqing Sun, Xiaodong Yang, Ming-Yu Liu, Jan Kautz"
                    }
                },
                {
                    "Sentence ID": 216,
                    "Sentence": "proposes to project images and point\nclouds into unit spheres and extract features through sphere CNN, which\nrequires multiple images as input. Towards I2P localization robust to in-\nconsistent environmental conditions, i3dLoc ",
                    "Citation Text": "P. Yin, L. Xu, J. Zhang, H. Choset, S. Scherer, i3dloc: Image-to-\nrange cross-domain localization robust to inconsistent environmental\n91conditions, in: Proceedings of Robotics: Science and Systems (RSS),\nRobotics: Science and Systems 2021, 2021.\nURL https://arxiv.org/abs/2105.12883",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.12883",
                        "Citation Paper Title": "Title:i3dLoc: Image-to-range Cross-domain Localization Robust to Inconsistent Environmental Conditions",
                        "Citation Paper Abstract": "Abstract:We present a method for localizing a single camera with respect to a point cloud map in indoor and outdoor scenes. The problem is challenging because correspondences of local invariant features are inconsistent across the domains between image and 3D. The problem is even more challenging as the method must handle various environmental conditions such as illumination, weather, and seasonal changes. Our method can match equirectangular images to the 3D range projections by extracting cross-domain symmetric place descriptors. Our key insight is to retain condition-invariant 3D geometry features from limited data samples while eliminating the condition-related features by a designed Generative Adversarial Network. Based on such features, we further design a spherical convolution network to learn viewpoint-invariant symmetric place descriptors. We evaluate our method on extensive self-collected datasets, which involve \\textit{Long-term} (variant appearance conditions), \\textit{Large-scale} (up to $2km$ structure/unstructured environment), and \\textit{Multistory} (four-floor confined space). Our method surpasses other current state-of-the-arts by achieving around $3$ times higher place retrievals to inconsistent environments, and above $3$ times accuracy on online localization. To highlight our method's generalization capabilities, we also evaluate the recognition across different datasets. With a single trained model, i3dLoc can demonstrate reliable visual localization in random conditions.",
                        "Citation Paper Authors": "Authors:Peng Yin, Lingyun Xu, Ji Zhang, Howie Choset, Sebastian Scherer"
                    }
                },
                {
                    "Sentence ID": 215,
                    "Sentence": ". However, this approach does not generalize\nwell to unseen environments. ",
                    "Citation Text": "L. Bernreiter, L. Ott, J. Nieto, R. Siegwart, C. Cadena, Spherical multi-\nmodal place recognition for heterogeneous sensor systems, in: Proceed-\nings of IEEE International Conference on Robotics and Automation\n(ICRA), IEEE Press, 2021, p. 1743\u20131750. doi:10.1109/ICRA48506.\n2021.9561078 .\nURL https://doi.org/10.1109/ICRA48506.2021.9561078",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.10067",
                        "Citation Paper Title": "Title:Spherical Multi-Modal Place Recognition for Heterogeneous Sensor Systems",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose a robust end-to-end multi-modal pipeline for place recognition where the sensor systems can differ from the map building to the query. Our approach operates directly on images and LiDAR scans without requiring any local feature extraction modules. By projecting the sensor data onto the unit sphere, we learn a multi-modal descriptor of partially overlapping scenes using a spherical convolutional neural network. The employed spherical projection model enables the support of arbitrary LiDAR and camera systems readily without losing information. Loop closure candidates are found using a nearest-neighbor lookup in the embedding space. We tackle the problem of correctly identifying the closest place by correlating the candidates' power spectra, obtaining a confidence value per prospect. Our estimate for the correct place corresponds then to the candidate with the highest confidence. We evaluate our proposal w.r.t. state-of-the-art approaches in place recognition using real-world data acquired using different sensors. Our approach can achieve a recall that is up to 10% and 5% higher than for a LiDAR- and vision-based system, respectively, when the sensor setup differs between model training and deployment. Additionally, our place selection can correctly identify up to 95% matches from the candidate set.",
                        "Citation Paper Authors": "Authors:Lukas Bernreiter, Lionel Ott, Juan Nieto, Roland Siegwart, Cesar Cadena"
                    }
                },
                {
                    "Sentence ID": 206,
                    "Sentence": ", and learning-based b) I2P-VPR, c) I2P-RPR, and d) I2P-\nMLoc.\nin ",
                    "Citation Text": "H. Yu, W. Zhen, W. Yang, J. Zhang, S. Scherer, Monocular camera lo-\ncalization in prior lidar maps with 2d-3d line correspondences, in: Pro-\nceedings of IEEE/RSJ International Conference on Intelligent Robots\nand Systems (IROS), 2020, pp. 4588\u20134594. doi:10.1109/IROS45743.\n2020.9341690 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.00740",
                        "Citation Paper Title": "Title:Monocular Camera Localization in Prior LiDAR Maps with 2D-3D Line Correspondences",
                        "Citation Paper Abstract": "Abstract:Light-weight camera localization in existing maps is essential for vision-based navigation. Currently, visual and visual-inertial odometry (VO\\&VIO) techniques are well-developed for state estimation but with inevitable accumulated drifts and pose jumps upon loop closure. To overcome these problems, we propose an efficient monocular camera localization method in prior LiDAR maps using direct 2D-3D line correspondences. To handle the appearance differences and modality gaps between LiDAR point clouds and images, geometric 3D lines are extracted offline from LiDAR maps while robust 2D lines are extracted online from video sequences. With the pose prediction from VIO, we can efficiently obtain coarse 2D-3D line correspondences. Then the camera poses and 2D-3D correspondences are iteratively optimized by minimizing the projection error of correspondences and rejecting outliers. Experimental results on the EurocMav dataset and our collected dataset demonstrate that the proposed method can efficiently estimate camera poses without accumulated drifts or pose jumps in structured environments.",
                        "Citation Paper Authors": "Authors:Huai Yu, Weikun Zhen, Wen Yang, Ji Zhang, Sebastian Scherer"
                    }
                },
                {
                    "Sentence ID": 211,
                    "Sentence": "proposed to associate the local visual landmarks\nreconstructed from the monocular Visual Odometry (VO) ",
                    "Citation Text": "R. Mur-Artal, J. M. M. Montiel, J. D. Tard\u00b4 os, Orb-slam: A versatile\nand accurate monocular slam system, IEEE Transactions on Robotics\n31 (5) (2015) 1147\u20131163. doi:10.1109/TRO.2015.2463671 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1502.00956",
                        "Citation Paper Title": "Title:ORB-SLAM: a Versatile and Accurate Monocular SLAM System",
                        "Citation Paper Abstract": "Abstract:This paper presents ORB-SLAM, a feature-based monocular SLAM system that operates in real time, in small and large, indoor and outdoor environments. The system is robust to severe motion clutter, allows wide baseline loop closing and relocalization, and includes full automatic initialization. Building on excellent algorithms of recent years, we designed from scratch a novel system that uses the same features for all SLAM tasks: tracking, mapping, relocalization, and loop closing. A survival of the fittest strategy that selects the points and keyframes of the reconstruction leads to excellent robustness and generates a compact and trackable map that only grows if the scene content changes, allowing lifelong operation. We present an exhaustive evaluation in 27 sequences from the most popular datasets. ORB-SLAM achieves unprecedented performance with respect to other state-of-the-art monocular SLAM approaches. For the benefit of the community, we make the source code public.",
                        "Citation Paper Authors": "Authors:Raul Mur-Artal, J. M. M. Montiel, Juan D. Tardos"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": ", illumination [14, 29], seasons [13, 32], and day-night\nchanges ",
                    "Citation Text": "T. Sattler, W. Maddern, C. Toft, A. Torii, L. Hammarstrand, E. Sten-\nborg, D. Safari, M. Okutomi, M. Pollefeys, J. Sivic, et al., Benchmarking\n6dof outdoor visual localization in changing conditions, in: Proceed-\nings of the IEEE conference on computer vision and pattern recognition\n(ICCV), 2018, pp. 8601\u20138610.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.09092",
                        "Citation Paper Title": "Title:Benchmarking 6DOF Outdoor Visual Localization in Changing Conditions",
                        "Citation Paper Abstract": "Abstract:Visual localization enables autonomous vehicles to navigate in their surroundings and augmented reality applications to link virtual to real worlds. Practical visual localization approaches need to be robust to a wide variety of viewing condition, including day-night changes, as well as weather and seasonal variations, while providing highly accurate 6 degree-of-freedom (6DOF) camera pose estimates. In this paper, we introduce the first benchmark datasets specifically designed for analyzing the impact of such factors on visual localization. Using carefully created ground truth poses for query images taken under a wide variety of conditions, we evaluate the impact of various factors on 6DOF camera pose estimation accuracy through extensive experiments with state-of-the-art localization approaches. Based on our results, we draw conclusions about the difficulty of different conditions, showing that long-term localization is far from solved, and propose promising avenues for future work, including sequence-based localization approaches and the need for better local features. Our benchmark is available at this http URL.",
                        "Citation Paper Authors": "Authors:Torsten Sattler, Will Maddern, Carl Toft, Akihiko Torii, Lars Hammarstrand, Erik Stenborg, Daniel Safari, Masatoshi Okutomi, Marc Pollefeys, Josef Sivic, Fredrik Kahl, Tomas Pajdla"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ", the size of scene map is reduced to 31 MB compared to 7828 MB\nof HLoc ",
                    "Citation Text": "P.-E. Sarlin, C. Cadena, R. Siegwart, M. Dymczyk, From coarse to\nfine: Robust hierarchical localization at large scale, in: Proceedings of\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 2019, pp. 12708\u201312717. doi:10.1109/CVPR.2019.01300 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.03506",
                        "Citation Paper Title": "Title:From Coarse to Fine: Robust Hierarchical Localization at Large Scale",
                        "Citation Paper Abstract": "Abstract:Robust and accurate visual localization is a fundamental capability for numerous applications, such as autonomous driving, mobile robotics, or augmented reality. It remains, however, a challenging task, particularly for large-scale environments and in presence of significant appearance changes. State-of-the-art methods not only struggle with such scenarios, but are often too resource intensive for certain real-time applications. In this paper we propose HF-Net, a hierarchical localization approach based on a monolithic CNN that simultaneously predicts local features and global descriptors for accurate 6-DoF localization. We exploit the coarse-to-fine localization paradigm: we first perform a global retrieval to obtain location hypotheses and only later match local features within those candidate places. This hierarchical approach incurs significant runtime savings and makes our system suitable for real-time operation. By leveraging learned descriptors, our method achieves remarkable localization robustness across large variations of appearance and sets a new state-of-the-art on two challenging benchmarks for large-scale localization.",
                        "Citation Paper Authors": "Authors:Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, Marcin Dymczyk"
                    }
                },
                {
                    "Sentence ID": 195,
                    "Sentence": "to estimate\nmore robustly and precisely [22, 125].\n4.4. Further Improvements\nThere are several other attempts to improve VL-MRL methods, except\nfeature extraction and matching, and pose solver. Here we introduce some\nworks with noticeable improvements.\nCross Descriptor matching ",
                    "Citation Text": "M. Dusmanu, O. Miksik, J. L. Sch\u00a8 onberger, M. Pollefeys, Cross-\ndescriptor visual localization and mapping, in: Proceedings of\nIEEE/CVF International Conference on Computer Vision (ICCV),\n2021, pp. 6038\u20136047. doi:10.1109/ICCV48922.2021.00600 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.01377",
                        "Citation Paper Title": "Title:Cross-Descriptor Visual Localization and Mapping",
                        "Citation Paper Abstract": "Abstract:Visual localization and mapping is the key technology underlying the majority of mixed reality and robotics systems. Most state-of-the-art approaches rely on local features to establish correspondences between images. In this paper, we present three novel scenarios for localization and mapping which require the continuous update of feature representations and the ability to match across different feature types. While localization and mapping is a fundamental computer vision problem, the traditional setup supposes the same local features are used throughout the evolution of a map. Thus, whenever the underlying features are changed, the whole process is repeated from scratch. However, this is typically impossible in practice, because raw images are often not stored and re-building the maps could lead to loss of the attached digital content. To overcome the limitations of current approaches, we present the first principled solution to cross-descriptor localization and mapping. Our data-driven approach is agnostic to the feature descriptor type, has low computational requirements, and scales linearly with the number of description algorithms. Extensive experiments demonstrate the effectiveness of our approach on state-of-the-art benchmarks for a variety of handcrafted and learned features.",
                        "Citation Paper Authors": "Authors:Mihai Dusmanu, Ondrej Miksik, Johannes L. Sch\u00f6nberger, Marc Pollefeys"
                    }
                },
                {
                    "Sentence ID": 194,
                    "Sentence": ", the VL-MRL after tracking lost or kidnapping is implemented by first\nusing MLPnP solver ",
                    "Citation Text": "S. Urban, J. Leitloff, S. Hinz, Mlpnp - a real-time maximum likeli-\nhood solution to the perspective-n-point problem, ArXiv abs/1607.08112\n(2016).\nURL https://api.semanticscholar.org/CorpusID:14704843",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.08112",
                        "Citation Paper Title": "Title:MLPnP - A Real-Time Maximum Likelihood Solution to the Perspective-n-Point Problem",
                        "Citation Paper Abstract": "Abstract:In this paper, a statistically optimal solution to the Perspective-n-Point (PnP) problem is presented. Many solutions to the PnP problem are geometrically optimal, but do not consider the uncertainties of the observations. In addition, it would be desirable to have an internal estimation of the accuracy of the estimated rotation and translation parameters of the camera pose. Thus, we propose a novel maximum likelihood solution to the PnP problem, that incorporates image observation uncertainties and remains real-time capable at the same time. Further, the presented method is general, as is works with 3D direction vectors instead of 2D image points and is thus able to cope with arbitrary central camera models. This is achieved by projecting (and thus reducing) the covariance matrices of the observations to the corresponding vector tangent space.",
                        "Citation Paper Authors": "Authors:Steffen Urban, Jens Leitloff, Stefan Hinz"
                    }
                },
                {
                    "Sentence ID": 102,
                    "Sentence": "applies zooming in method\nto obtain more accurate matching. LoFTR ",
                    "Citation Text": "J. Sun, Z. Shen, Y. Wang, H. Bao, X. Zhou, Loftr: Detector-free local\nfeature matching with transformers, in: Proceedings of IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition (CVPR), 2021, pp.\n8918\u20138927. doi:10.1109/CVPR46437.2021.00881 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.00680",
                        "Citation Paper Title": "Title:LoFTR: Detector-Free Local Feature Matching with Transformers",
                        "Citation Paper Abstract": "Abstract:We present a novel method for local image feature matching. Instead of performing image feature detection, description, and matching sequentially, we propose to first establish pixel-wise dense matches at a coarse level and later refine the good matches at a fine level. In contrast to dense methods that use a cost volume to search correspondences, we use self and cross attention layers in Transformer to obtain feature descriptors that are conditioned on both images. The global receptive field provided by Transformer enables our method to produce dense matches in low-texture areas, where feature detectors usually struggle to produce repeatable interest points. The experiments on indoor and outdoor datasets show that LoFTR outperforms state-of-the-art methods by a large margin. LoFTR also ranks first on two public benchmarks of visual localization among the published methods.",
                        "Citation Paper Authors": "Authors:Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, Xiaowei Zhou"
                    }
                },
                {
                    "Sentence ID": 173,
                    "Sentence": "to enhance the representative ability of feature descrip-\ntors. It achieves accurate matching performance even in textureless region,\nas shown in Fig. 10, effectively reducing the requirements of scene texture in\nVL-MRL methods. As follow-up methods, QuadTree LoFTR ",
                    "Citation Text": "S. Tang, J. Zhang, S. Zhu, P. Tan, Quadtree attention for vision trans-\nformers, ArXiv abs/2201.02767 (2022).\nURL https://api.semanticscholar.org/CorpusID:245837195",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.02767",
                        "Citation Paper Title": "Title:QuadTree Attention for Vision Transformers",
                        "Citation Paper Abstract": "Abstract:Transformers have been successful in many vision tasks, thanks to their capability of capturing long-range dependency. However, their quadratic computational complexity poses a major obstacle for applying them to vision tasks requiring dense predictions, such as object detection, feature matching, stereo, etc. We introduce QuadTree Attention, which reduces the computational complexity from quadratic to linear. Our quadtree transformer builds token pyramids and computes attention in a coarse-to-fine manner. At each level, the top K patches with the highest attention scores are selected, such that at the next level, attention is only evaluated within the relevant regions corresponding to these top K patches. We demonstrate that quadtree attention achieves state-of-the-art performance in various vision tasks, e.g. with 4.0% improvement in feature matching on ScanNet, about 50% flops reduction in stereo matching, 0.4-1.5% improvement in top-1 accuracy on ImageNet classification, 1.2-1.8% improvement on COCO object detection, and 0.7-2.4% improvement on semantic segmentation over previous state-of-the-art transformers. The codes are available at this https URL.",
                        "Citation Paper Authors": "Authors:Shitao Tang, Jiahui Zhang, Siyu Zhu, Ping Tan"
                    }
                },
                {
                    "Sentence ID": 98,
                    "Sentence": "improves by detecting on feature\nmap with original resolution. ALIKE ",
                    "Citation Text": "X. Zhao, X. Wu, J. Miao, W. Chen, P. C. Y. Chen, Z. Li, ALIKE:\nAccurate and lightweight keypoint detection and descriptor extraction,\nIEEE Transactions on Multimedia (2022) 1\u20131 doi:10.1109/TMM.2022.\n3155927 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.02906",
                        "Citation Paper Title": "Title:ALIKE: Accurate and Lightweight Keypoint Detection and Descriptor Extraction",
                        "Citation Paper Abstract": "Abstract:Existing methods detect the keypoints in a non-differentiable way, therefore they can not directly optimize the position of keypoints through back-propagation. To address this issue, we present a partially differentiable keypoint detection module, which outputs accurate sub-pixel keypoints. The reprojection loss is then proposed to directly optimize these sub-pixel keypoints, and the dispersity peak loss is presented for accurate keypoints regularization. We also extract the descriptors in a sub-pixel way, and they are trained with the stable neural reprojection error loss. Moreover, a lightweight network is designed for keypoint detection and descriptor extraction, which can run at 95 frames per second for 640x480 images on a commercial GPU. On homography estimation, camera pose estimation, and visual (re-)localization tasks, the proposed method achieves equivalent performance with the state-of-the-art approaches, while greatly reduces the inference time.",
                        "Citation Paper Authors": "Authors:Xiaoming Zhao, Xingming Wu, Jinyu Miao, Weihai Chen, Peter C. Y. Chen, Zhengguo Li"
                    }
                },
                {
                    "Sentence ID": 97,
                    "Sentence": "further combine attention information\nand semantic information to select stable and discriminative local features.\nDifferent from the methods trained with metric learning scheme, DISK ",
                    "Citation Text": "M. J. Tyszkiewicz, P. Fua, E. Trulls, DISK: Learning local features\nwith policy gradient, in: Proceedings of Advances in Neural Information\nProcessing Systems (NeurIPS), 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.13566",
                        "Citation Paper Title": "Title:DISK: Learning local features with policy gradient",
                        "Citation Paper Abstract": "Abstract:Local feature frameworks are difficult to learn in an end-to-end fashion, due to the discreteness inherent to the selection and matching of sparse keypoints. We introduce DISK (DIScrete Keypoints), a novel method that overcomes these obstacles by leveraging principles from Reinforcement Learning (RL), optimizing end-to-end for a high number of correct feature matches. Our simple yet expressive probabilistic model lets us keep the training and inference regimes close, while maintaining good enough convergence properties to reliably train from scratch. Our features can be extracted very densely while remaining discriminative, challenging commonly held assumptions about what constitutes a good keypoint, as showcased in Fig. 1, and deliver state-of-the-art results on three public benchmarks.",
                        "Citation Paper Authors": "Authors:Micha\u0142 J. Tyszkiewicz, Pascal Fua, Eduard Trulls"
                    }
                },
                {
                    "Sentence ID": 134,
                    "Sentence": "is optimize by apply metric learning between two\nco-visible patches and achieve better performance, but it extracts local fea-\nture on a feature map with reduced resolution, so the accuracy of matching\nand localization is limited. ASLFeat ",
                    "Citation Text": "Z. Luo, L. Zhou, X. Bai, H. Chen, J. Zhang, Y. Yao, S. Li, T. Fang,\nL. Quan, Aslfeat: Learning local features of accurate shape and local-\nization, in: Proceedings of IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), 2020, pp. 6588\u20136597. doi:\n10.1109/CVPR42600.2020.00662 .\n81",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.10071",
                        "Citation Paper Title": "Title:ASLFeat: Learning Local Features of Accurate Shape and Localization",
                        "Citation Paper Abstract": "Abstract:This work focuses on mitigating two limitations in the joint learning of local feature detectors and descriptors. First, the ability to estimate the local shape (scale, orientation, etc.) of feature points is often neglected during dense feature extraction, while the shape-awareness is crucial to acquire stronger geometric invariance. Second, the localization accuracy of detected keypoints is not sufficient to reliably recover camera geometry, which has become the bottleneck in tasks such as 3D reconstruction. In this paper, we present ASLFeat, with three light-weight yet effective modifications to mitigate above issues. First, we resort to deformable convolutional networks to densely estimate and apply local transformation. Second, we take advantage of the inherent feature hierarchy to restore spatial resolution and low-level details for accurate keypoint localization. Finally, we use a peakiness measurement to relate feature responses and derive more indicative detection scores. The effect of each modification is thoroughly studied, and the evaluation is extensively conducted across a variety of practical scenarios. State-of-the-art results are reported that demonstrate the superiority of our methods.",
                        "Citation Paper Authors": "Authors:Zixin Luo, Lei Zhou, Xuyang Bai, Hongkai Chen, Jiahui Zhang, Yao Yao, Shiwei Li, Tian Fang, Long Quan"
                    }
                },
                {
                    "Sentence ID": 122,
                    "Sentence": ", authors combined two type of PRP methods and designed a net-\nwork to predict 3D-3D correspondence between two images, adopting Key-\npointNet ",
                    "Citation Text": "S. Suwajanakorn, N. Snavely, J. Tompson, M. Norouzi, Discovery of\nlatent 3d keypoints via end-to-end geometric reasoning, in: Proceedings\nof the International Conference on Neural Information Processing Sys-\ntems (NeurIPS), NIPS\u201918, Curran Associates Inc., Red Hook, NY, USA,\n2018, p. 2063\u20132074.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.03146",
                        "Citation Paper Title": "Title:Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning",
                        "Citation Paper Abstract": "Abstract:This paper presents KeypointNet, an end-to-end geometric reasoning framework to learn an optimal set of category-specific 3D keypoints, along with their detectors. Given a single image, KeypointNet extracts 3D keypoints that are optimized for a downstream task. We demonstrate this framework on 3D pose estimation by proposing a differentiable objective that seeks the optimal set of keypoints for recovering the relative pose between two views of an object. Our model discovers geometrically and semantically consistent keypoints across viewing angles and instances of an object category. Importantly, we find that our end-to-end framework using no ground-truth keypoint annotations outperforms a fully supervised baseline using the same neural network architecture on the task of pose estimation. The discovered 3D keypoints on the car, chair, and plane categories of ShapeNet are visualized at this http URL.",
                        "Citation Paper Authors": "Authors:Supasorn Suwajanakorn, Noah Snavely, Jonathan Tompson, Mohammad Norouzi"
                    }
                },
                {
                    "Sentence ID": 93,
                    "Sentence": "utilized semantic ob-\njects as features, and built a semantic topological graph based on the nearby\nrelationship of semantic objects in images. The graph can be compactly\nrepresented by a global feature based on random walk ",
                    "Citation Text": "B. Perozzi, R. Al-Rfou, S. Skiena, Deepwalk: Online learning of social\nrepresentations, in: Proceedings of the ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining, KDD \u201914, Associ-\nation for Computing Machinery, New York, NY, USA, 2014, p. 701\u2013710.\ndoi:10.1145/2623330.2623732 .\nURL https://doi.org/10.1145/2623330.2623732",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1403.6652",
                        "Citation Paper Title": "Title:DeepWalk: Online Learning of Social Representations",
                        "Citation Paper Abstract": "Abstract:We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide $F_1$ scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.",
                        "Citation Paper Authors": "Authors:Bryan Perozzi, Rami Al-Rfou, Steven Skiena"
                    }
                },
                {
                    "Sentence ID": 83,
                    "Sentence": "applies Bayesian framework by measuring the score of local features oc-\ncurred in current image. LiPo-LCD ",
                    "Citation Text": "J. P. Company-Corcoles, E. Garcia-Fidalgo, A. Ortiz, Msc-vo: Exploit-\ning manhattan and structural constraints for visual odometry, IEEE\nRobotics and Automation Letters 7 (2) (2022) 2803\u20132810.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.03408",
                        "Citation Paper Title": "Title:MSC-VO: Exploiting Manhattan and Structural Constraints for Visual Odometry",
                        "Citation Paper Abstract": "Abstract:Visual odometry algorithms tend to degrade when facing low-textured scenes -from e.g. human-made environments-, where it is often difficult to find a sufficient number of point features. Alternative geometrical visual cues, such as lines, which can often be found within these scenarios, can become particularly useful. Moreover, these scenarios typically present structural regularities, such as parallelism or orthogonality, and hold the Manhattan World assumption. Under these premises, in this work, we introduce MSC-VO, an RGB-D -based visual odometry approach that combines both point and line features and leverages, if exist, those structural regularities and the Manhattan axes of the scene. Within our approach, these structural constraints are initially used to estimate accurately the 3D position of the extracted lines. These constraints are also combined next with the estimated Manhattan axes and the reprojection errors of points and lines to refine the camera pose by means of local map optimization. Such a combination enables our approach to operate even in the absence of the aforementioned constraints, allowing the method to work for a wider variety of scenarios. Furthermore, we propose a novel multi-view Manhattan axes estimation procedure that mainly relies on line features. MSC-VO is assessed using several public datasets, outperforming other state-of-the-art solutions, and comparing favourably even with some SLAM methods.",
                        "Citation Paper Authors": "Authors:Joan P. Company-Corcoles, Emilio Garcia-Fidalgo, Alberto Ortiz"
                    }
                },
                {
                    "Sentence ID": 82,
                    "Sentence": "across consecutive frames to incrementally\ngenerate vocabulary, and perform VPR by a likelihood function. iBoW-LCD ",
                    "Citation Text": "E. Garcia-Fidalgo, A. Ortiz, iBoW-LCD: An appearance-based loop-\nclosure detection approach using incremental bags of binary words,\nIEEE Robotics and Automation Letters 3 (4) (2018) 3051\u20133057. doi:\n10.1109/LRA.2018.2849609 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.05909",
                        "Citation Paper Title": "Title:iBoW-LCD: An Appearance-based Loop Closure Detection Approach using Incremental Bags of Binary Words",
                        "Citation Paper Abstract": "Abstract:In this paper, we introduce iBoW-LCD, a novel appearance-based loop closure detection method. The presented approach makes use of an incremental Bag-of-Words (BoW) scheme based on binary descriptors to retrieve previously seen similar images, avoiding any vocabulary training stage usually required by classic BoW models. In addition, to detect loop closures, iBoW-LCD builds on the concept of dynamic islands, a simple but effective mechanism to group similar images close in time, which reduces the computational times typically associated to Bayesian frameworks. Our approach is validated using several indoor and outdoor public datasets, taken under different environmental conditions, achieving a high accuracy and outperforming other state-of-the-art solutions.",
                        "Citation Paper Authors": "Authors:Emilio Garcia-Fidalgo, Alberto Ortiz"
                    }
                },
                {
                    "Sentence ID": 60,
                    "Sentence": "designs a trainable generalized-\nmean pooling layer that generalizes maximum and average pooling for fea-\nture aggregation. TransVPR ",
                    "Citation Text": "R. Wang, Y. Shen, W. Zuo, S. Zhou, N. Zheng, Transvpr: Transformer-\nbased place recognition with multi-level attention aggregation, in: Pro-\nceedings of IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 2022, pp. 13638\u201313647. doi:10.1109/\nCVPR52688.2022.01328 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.02001",
                        "Citation Paper Title": "Title:TransVPR: Transformer-based place recognition with multi-level attention aggregation",
                        "Citation Paper Abstract": "Abstract:Visual place recognition is a challenging task for applications such as autonomous driving navigation and mobile robot localization. Distracting elements presenting in complex scenes often lead to deviations in the perception of visual place. To address this problem, it is crucial to integrate information from only task-relevant regions into image representations. In this paper, we introduce a novel holistic place recognition model, TransVPR, based on vision Transformers. It benefits from the desirable property of the self-attention operation in Transformers which can naturally aggregate task-relevant features. Attentions from multiple levels of the Transformer, which focus on different regions of interest, are further combined to generate a global image representation. In addition, the output tokens from Transformer layers filtered by the fused attention mask are considered as key-patch descriptors, which are used to perform spatial matching to re-rank the candidates retrieved by the global image features. The whole model allows end-to-end training with a single objective and image-level supervision. TransVPR achieves state-of-the-art performance on several real-world benchmarks while maintaining low computational time and storage requirements.",
                        "Citation Paper Authors": "Authors:Ruotong Wang, Yanqing Shen, Weiliang Zuo, Sanping Zhou, Nanning Zheng"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": "presented general place recognition challenges. However, place recognition\nmethods could only serve as a coarse localization that provide a rough pose\napproximation, which only cover few part of visual localization researches.\nIn ",
                    "Citation Text": "E. Mahdi, H. Xinming, A survey on visual map localization using lidars\nand cameras, arXiv preprint arXiv:2208.03376 (2022).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2208.03376",
                        "Citation Paper Title": "Title:A Survey on Visual Map Localization Using LiDARs and Cameras",
                        "Citation Paper Abstract": "Abstract:As the autonomous driving industry is slowly maturing, visual map localization is quickly becoming the standard approach to localize cars as accurately as possible. Owing to the rich data returned by visual sensors such as cameras or LiDARs, researchers are able to build different types of maps with various levels of details, and use them to achieve high levels of vehicle localization accuracy and stability in urban environments. Contrary to the popular SLAM approaches, visual map localization relies on pre-built maps, and is focused solely on improving the localization accuracy by avoiding error accumulation or drift. We define visual map localization as a two-stage process. At the stage of place recognition, the initial position of the vehicle in the map is determined by comparing the visual sensor output with a set of geo-tagged map regions of interest. Subsequently, at the stage of map metric localization, the vehicle is tracked while it moves across the map by continuously aligning the visual sensors' output with the current area of the map that is being traversed. In this paper, we survey, discuss and compare the latest methods for LiDAR based, camera based and cross-modal visual map localization for both stages, in an effort to highlight the strength and weakness of each approach.",
                        "Citation Paper Authors": "Authors:Elhousni Mahdi, Huang Xinming"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "reviewed\nrecently proposed solutions from the deep learning perspective. Yin et al. ",
                    "Citation Text": "P. Yin, S. Zhao, I. Cisneros, A. Abuduweili, G. P. Huang, M. Milford,\nC. Liu, H. Choset, S. A. Scherer, General place recognition survey: To-\nwards the real-world autonomy age, ArXiv abs/2209.04497 (2022).\nURL https://api.semanticscholar.org/CorpusID:252199702",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2209.04497",
                        "Citation Paper Title": "Title:General Place Recognition Survey: Towards the Real-world Autonomy Age",
                        "Citation Paper Abstract": "Abstract:Place recognition is the fundamental module that can assist Simultaneous Localization and Mapping (SLAM) in loop-closure detection and re-localization for long-term navigation. The place recognition community has made astonishing progress over the last $20$ years, and this has attracted widespread research interest and application in multiple fields such as computer vision and robotics. However, few methods have shown promising place recognition performance in complex real-world scenarios, where long-term and large-scale appearance changes usually result in failures. Additionally, there is a lack of an integrated framework amongst the state-of-the-art methods that can handle all of the challenges in place recognition, which include appearance changes, viewpoint differences, robustness to unknown areas, and efficiency in real-world applications. In this work, we survey the state-of-the-art methods that target long-term localization and discuss future directions and opportunities.\nWe start by investigating the formulation of place recognition in long-term autonomy and the major challenges in real-world environments. We then review the recent works in place recognition for different sensor modalities and current strategies for dealing with various place recognition challenges. Finally, we review the existing datasets for long-term localization and introduce our datasets and evaluation API for different approaches. This paper can be a tutorial for researchers new to the place recognition community and those who care about long-term robotics autonomy. We also provide our opinion on the frequently asked question in robotics: Do robots need accurate localization for long-term autonomy? A summary of this work and our datasets and evaluation API is publicly available to the robotics community at: this https URL.",
                        "Citation Paper Authors": "Authors:Peng Yin, Shiqi Zhao, Ivan Cisneros, Abulikemu Abuduweili, Guoquan Huang, Micheal Milford, Changliu Liu, Howie Choset, Sebastian Scherer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.06475v1": {
            "Paper Title": "NetROS-5G: Enhancing Personalization through 5G Network Slicing and Edge\n  Computing in Human-Robot Interactions",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": "discuss\nthe main advantages combining the use of a Kubernetes and real,\nheterogeneous, robotic hardware, the main shortcomings we en-\ncountered with networking and sharing GPUs to support deep\nlearning workloads. More specifically, Peng Huang et al. in ",
                    "Citation Text": "Peng Huang, Liekang Zeng, Xu Chen, Ke Luo, Zhi Zhou, and Shuai Yu. 2021. Edge\nRobotics: Edge-Computing-Accelerated Multi-Robot Simultaneous Localization\nand Mapping. https://doi.org/10.48550/ARXIV.2112.13222",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.13222",
                        "Citation Paper Title": "Title:Edge Robotics: Edge-Computing-Accelerated Multi-Robot Simultaneous Localization and Mapping",
                        "Citation Paper Abstract": "Abstract:With the wide penetration of smart robots in multifarious fields, Simultaneous Localization and Mapping (SLAM) technique in robotics has attracted growing attention in the community. Yet collaborating SLAM over multiple robots still remains challenging due to performance contradiction between the intensive graphics computation of SLAM and the limited computing capability of robots. While traditional solutions resort to the powerful cloud servers acting as an external computation provider, we show by real-world measurements that the significant communication overhead in data offloading prevents its practicability to real deployment. To tackle these challenges, this paper promotes the emerging edge computing paradigm into multi-robot SLAM and proposes RecSLAM, a multi-robot laser SLAM system that focuses on accelerating map construction process under the robot-edge-cloud architecture. In contrast to conventional multi-robot SLAM that generates graphic maps on robots and completely merges them on the cloud, RecSLAM develops a hierarchical map fusion technique that directs robots' raw data to edge servers for real-time fusion and then sends to the cloud for global merging. To optimize the overall pipeline, an efficient multi-robot SLAM collaborative processing framework is introduced to adaptively optimize robot-to-edge offloading tailored to heterogeneous edge resource conditions, meanwhile ensuring the workload balancing among the edge servers. Extensive evaluations show RecSLAM can achieve up to 39% processing latency reduction over the state-of-the-art. Besides, a proof-of-concept prototype is developed and deployed in real scenes to demonstrate its effectiveness.",
                        "Citation Paper Authors": "Authors:Peng Huang, Liekang Zeng, Xu Chen, Ke Luo, Zhi Zhou, Shuai Yu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.06445v1": {
            "Paper Title": "Towards a Unified Naming Scheme for Thermo-Active Soft Actuators: A\n  Review of Materials, Working Principles, and Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.06408v1": {
            "Paper Title": "DiffVL: Scaling Up Soft Body Manipulation using Vision-Language Driven\n  Differentiable Physics",
            "Sentences": [
                {
                    "Sentence ID": 44,
                    "Sentence": ", which could be differentiable through various\ntechniques [ 7,16,30,57]. It has been shown that soft bodies have smoother gradients compared\nto rigid bodies [ 6,30,32,82,93], enabling better optimization. However, the solver may still\nsuffer from non-convexity ",
                    "Citation Text": "Sizhe Li, Zhiao Huang, Tao Du, Hao Su, Joshua B Tenenbaum, and Chuang Gan. Contact\npoints discovery for soft-body manipulations with differentiable physics. arXiv preprint\narXiv:2205.02835 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.02835",
                        "Citation Paper Title": "Title:Contact Points Discovery for Soft-Body Manipulations with Differentiable Physics",
                        "Citation Paper Abstract": "Abstract:Differentiable physics has recently been shown as a powerful tool for solving soft-body manipulation tasks. However, the differentiable physics solver often gets stuck when the initial contact points of the end effectors are sub-optimal or when performing multi-stage tasks that require contact point switching, which often leads to local minima. To address this challenge, we propose a contact point discovery approach (CPDeform) that guides the stand-alone differentiable physics solver to deform various soft-body plasticines. The key idea of our approach is to integrate optimal transport-based contact points discovery into the differentiable physics solver to overcome the local minima from initial contact points or contact switching. On single-stage tasks, our method can automatically find suitable initial contact points based on transport priorities. On complex multi-stage tasks, we can iteratively switch the contact points of end-effectors based on transport priorities. To evaluate the effectiveness of our method, we introduce PlasticineLab-M that extends the existing differentiable physics benchmark PlasticineLab to seven new challenging multi-stage soft-body manipulation tasks. Extensive experimental results suggest that: 1) on multi-stage tasks that are infeasible for the vanilla differentiable physics solver, our approach discovers contact points that efficiently guide the solver to completion; 2) on tasks where the vanilla solver performs sub-optimally or near-optimally, our contact point discovery method performs better than or on par with the manipulation performance obtained with handcrafted contact points.",
                        "Citation Paper Authors": "Authors:Sizhe Li, Zhiao Huang, Tao Du, Hao Su, Joshua B. Tenenbaum, Chuang Gan"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": ". We set a\nthreshold for each scene\u2019s target IoU score to measure successful task completion, accommodating\nvariations in IoU scores across different scenes.\nWe compare our method against previous baselines, including two representative reinforcement\nlearning algorithms, Soft Actor-Critic (SAC) ",
                    "Citation Text": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-\npolicy maximum entropy deep reinforcement learning with a stochastic actor. In International\nconference on machine learning , pages 1861\u20131870. PMLR, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.01290",
                        "Citation Paper Title": "Title:Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
                        "Citation Paper Abstract": "Abstract:Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",
                        "Citation Paper Authors": "Authors:Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "Differentiable simulation for soft bodies Differentiable physics [ 19,29,30,69,93] has brought\nunique opportunities in manipulating deformable objects of various types [ 23,27,28,33,47,49,53,\n56,58,63,74,83,92,95,95,100], including plasticine ",
                    "Citation Text": "Zhiao Huang, Yuanming Hu, Tao Du, Siyuan Zhou, Hao Su, Joshua B Tenenbaum, and\nChuang Gan. Plasticinelab: A soft-body manipulation benchmark with differentiable physics.\nInInternational Conference on Learning Representations , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.03311",
                        "Citation Paper Title": "Title:PlasticineLab: A Soft-Body Manipulation Benchmark with Differentiable Physics",
                        "Citation Paper Abstract": "Abstract:Simulated virtual environments serve as one of the main driving forces behind developing and evaluating skill learning algorithms. However, existing environments typically only simulate rigid body physics. Additionally, the simulation process usually does not provide gradients that might be useful for planning and control optimizations. We introduce a new differentiable physics benchmark called PasticineLab, which includes a diverse collection of soft body manipulation tasks. In each task, the agent uses manipulators to deform the plasticine into the desired configuration. The underlying physics engine supports differentiable elastic and plastic deformation using the DiffTaichi system, posing many under-explored challenges to robotic agents. We evaluate several existing reinforcement learning (RL) methods and gradient-based methods on this benchmark. Experimental results suggest that 1) RL-based approaches struggle to solve most of the tasks efficiently; 2) gradient-based approaches, by optimizing open-loop control sequences with the built-in differentiable physics engine, can rapidly find a solution within tens of iterations, but still fall short on multi-stage tasks that require long-term planning. We expect that PlasticineLab will encourage the development of novel algorithms that combine differentiable physics and RL for more complex physics-based skill learning tasks.",
                        "Citation Paper Authors": "Authors:Zhiao Huang, Yuanming Hu, Tao Du, Siyuan Zhou, Hao Su, Joshua B. Tenenbaum, Chuang Gan"
                    }
                },
                {
                    "Sentence ID": 97,
                    "Sentence": "for elastoplastic material simulation. To enhance the user experience and\nsupport scene creation, we integrate it into SAPIEN ",
                    "Citation Text": "Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu,\nHanxiao Jiang, Yifu Yuan, He Wang, Li Yi, Angel X. Chang, Leonidas J. Guibas, and Hao\nSu. SAPIEN: A simulated part-based interactive environment. In The IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) , June 2020.\n16",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08515",
                        "Citation Paper Title": "Title:SAPIEN: A SimulAted Part-based Interactive ENvironment",
                        "Citation Paper Abstract": "Abstract:Building home assistant robots has long been a pursuit for vision and robotics researchers. To achieve this task, a simulated environment with physically realistic simulation, sufficient articulated objects, and transferability to the real robot is indispensable. Existing environments achieve these requirements for robotics simulation with different levels of simplification and focus. We take one step further in constructing an environment that supports household tasks for training robot learning algorithm. Our work, SAPIEN, is a realistic and physics-rich simulated environment that hosts a large-scale set for articulated objects. Our SAPIEN enables various robotic vision and interaction tasks that require detailed part-level understanding.We evaluate state-of-the-art vision algorithms for part detection and motion attribute recognition as well as demonstrate robotic interaction tasks using heuristic approaches and reinforcement learning algorithms. We hope that our SAPIEN can open a lot of research directions yet to be explored, including learning cognition through interaction, part motion discovery, and construction of robotics-ready simulated game environment.",
                        "Citation Paper Authors": "Authors:Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, Li Yi, Angel X. Chang, Leonidas J. Guibas, Hao Su"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": "turns languages into constraints to correct trajectories. Our method\ndistinguishes itself by integrating language with differentiable physics through machine-interpretable\noptimization programs. Recent works aim at leveraging large language models [ 4,13,15,50,94]\nto facilitate robot learning. Most use language or code ",
                    "Citation Text": "Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence,\nand Andy Zeng. Code as policies: Language model programs for embodied control. arXiv\npreprint arXiv:2209.07753 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2209.07753",
                        "Citation Paper Title": "Title:Code as Policies: Language Model Programs for Embodied Control",
                        "Citation Paper Abstract": "Abstract:Large language models (LLMs) trained on code completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g.,from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (\"faster\") depending on context (i.e., behavioral commonsense). This paper presents code as policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8% of problems on the HumanEval [1] benchmark. Code and videos are available at this https URL",
                        "Citation Paper Authors": "Authors:Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.06406v1": {
            "Paper Title": "Partial End-to-end Reinforcement Learning for Robustness Against\n  Modelling Error in Autonomous Racing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.06398v1": {
            "Paper Title": "NVFi: Neural Velocity Fields for 3D Physics Learning from Dynamic Videos",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": "Static 3D Representations: Conventional representations for static 3D objects and scenes include\nvoxels [ 13;71;72], point clouds ",
                    "Citation Text": "H. Fan, H. Su, and L. Guibas. A Point Set Generation Network for 3D Object Reconstruction\nfrom a Single Image. CVPR , 2017. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.00603",
                        "Citation Paper Title": "Title:A Point Set Generation Network for 3D Object Reconstruction from a Single Image",
                        "Citation Paper Abstract": "Abstract:Generation of 3D data by deep neural network has been attracting increasing attention in the research community. The majority of extant works resort to regular representations such as volumetric grids or collection of images; however, these representations obscure the natural invariance of 3D shapes under geometric transformations and also suffer from a number of other issues. In this paper we address the problem of 3D reconstruction from a single image, generating a straight-forward form of output -- point cloud coordinates. Along with this problem arises a unique and interesting issue, that the groundtruth shape for an input image may be ambiguous. Driven by this unorthodox output form and the inherent ambiguity in groundtruth, we design architecture, loss function and learning paradigm that are novel and effective. Our final solution is a conditional shape sampler, capable of predicting multiple plausible 3D point clouds from an input image. In experiments not only can our system outperform state-of-the-art methods on single image based 3d reconstruction benchmarks; but it also shows a strong performance for 3d shape completion and promising ability in making multiple plausible predictions.",
                        "Citation Paper Authors": "Authors:Haoqiang Fan, Hao Su, Leonidas Guibas"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2308.01654v2": {
            "Paper Title": "Towards a Safe Real-Time Motion Planning Framework for Autonomous\n  Driving Systems: An MPPI Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.06330v1": {
            "Paper Title": "Navigating Open Set Scenarios for Skeleton-based Action Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.06314v1": {
            "Paper Title": "DMS*: Minimizing Makespan for Multi-Agent Combinatorial Path Finding",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": "J. Pearl, Heuristics: intelligent search strategies for computer problem\nsolving . Addison-Wesley Longman Publishing Co., Inc., 1984. ",
                    "Citation Text": "R. Stern, N. Sturtevant, A. Felner, S. Koenig, H. Ma, T. Walker, J. Li,\nD. Atzmon, L. Cohen, T. Kumar et al. , \u201cMulti-agent pathfinding: Def-\ninitions, variants, and benchmarks,\u201d arXiv preprint arXiv:1906.08291 ,\n2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.08291",
                        "Citation Paper Title": "Title:Multi-Agent Pathfinding: Definitions, Variants, and Benchmarks",
                        "Citation Paper Abstract": "Abstract:The MAPF problem is the fundamental problem of planning paths for multiple agents, where the key constraint is that the agents will be able to follow these paths concurrently without colliding with each other. Applications of MAPF include automated warehouses and autonomous vehicles. Research on MAPF has been flourishing in the past couple of years. Different MAPF research papers make different assumptions, e.g., whether agents can traverse the same road at the same time, and have different objective functions, e.g., minimize makespan or sum of agents' actions costs. These assumptions and objectives are sometimes implicitly assumed or described informally. This makes it difficult to establish appropriate baselines for comparison in research papers, as well as making it difficult for practitioners to find the papers relevant to their concrete application. This paper aims to fill this gap and support researchers and practitioners by providing a unifying terminology for describing common MAPF assumptions and objectives. In addition, we also provide pointers to two MAPF benchmarks. In particular, we introduce a new grid-based benchmark for MAPF, and demonstrate experimentally that it poses a challenge to contemporary MAPF algorithms.",
                        "Citation Paper Authors": "Authors:Roni Stern, Nathan Sturtevant, Ariel Felner, Sven Koenig, Hang Ma, Thayne Walker, Jiaoyang Li, Dor Atzmon, Liron Cohen, T. K. Satish Kumar, Eli Boyarski, Roman Bartak"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "X. Zhong, J. Li, S. Koenig, and H. Ma, \u201cOptimal and bounded-\nsuboptimal multi-goal task assignment and path finding,\u201d in 2022\nInternational Conference on Robotics and Automation (ICRA) . IEEE,\n2022, pp. 10 731\u201310 737. ",
                    "Citation Text": "H. Ma, J. Li, T. K. S. Kumar, and S. Koenig, \u201cLifelong multi-agent\npath finding for online pickup and delivery tasks,\u201d in Conference on\nAutonomous Agents & Multiagent Systems , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.10868",
                        "Citation Paper Title": "Title:Lifelong Multi-Agent Path Finding for Online Pickup and Delivery Tasks",
                        "Citation Paper Abstract": "Abstract:The multi-agent path-finding (MAPF) problem has recently received a lot of attention. However, it does not capture important characteristics of many real-world domains, such as automated warehouses, where agents are constantly engaged with new tasks. In this paper, we therefore study a lifelong version of the MAPF problem, called the multi-agent pickup and delivery (MAPD) problem. In the MAPD problem, agents have to attend to a stream of delivery tasks in an online setting. One agent has to be assigned to each delivery task. This agent has to first move to a given pickup location and then to a given delivery location while avoiding collisions with other agents. We present two decoupled MAPD algorithms, Token Passing (TP) and Token Passing with Task Swaps (TPTS). Theoretically, we show that they solve all well-formed MAPD instances, a realistic subclass of MAPD instances. Experimentally, we compare them against a centralized strawman MAPD algorithm without this guarantee in a simulated warehouse system. TP can easily be extended to a fully distributed MAPD algorithm and is the best choice when real-time computation is of primary concern since it remains efficient for MAPD instances with hundreds of agents and tasks. TPTS requires limited communication among agents and balances well between TP and the centralized MAPD algorithm.",
                        "Citation Paper Authors": "Authors:Hang Ma, Jiaoyang Li, T. K. Satish Kumar, Sven Koenig"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "of mobile\nrobots for manufacturing . US Department of Commerce, National\nInstitute of Standards and Technology . . . , 2015. ",
                    "Citation Text": "K. Brown, O. Peltzer, M. A. Sehr, M. Schwager, and M. J. Kochenderfer,\n\u201cOptimal sequential task assignment and path finding for multi-agent\nrobotic assembly planning,\u201d in 2020 IEEE International Conference on\nRobotics and Automation (ICRA) , 2020, pp. 441\u2013447.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.08845",
                        "Citation Paper Title": "Title:Optimal Sequential Task Assignment and Path Finding for Multi-Agent Robotic Assembly Planning",
                        "Citation Paper Abstract": "Abstract:We study the problem of sequential task assignment and collision-free routing for large teams of robots in applications with inter-task precedence constraints (e.g., task $A$ and task $B$ must both be completed before task $C$ may begin). Such problems commonly occur in assembly planning for robotic manufacturing applications, in which sub-assemblies must be completed before they can be combined to form the final product. We propose a hierarchical algorithm for computing makespan-optimal solutions to the problem. The algorithm is evaluated on a set of randomly generated problem instances where robots must transport objects between stations in a \"factory \"grid world environment. In addition, we demonstrate in high-fidelity simulation that the output of our algorithm can be used to generate collision-free trajectories for non-holonomic differential-drive robots.",
                        "Citation Paper Authors": "Authors:Kyle Brown, Oriana Peltzer, Martin A. Sehr, Mac Schwager, Mykel J. Kochenderfer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2312.06310v1": {
            "Paper Title": "Development of the Lifelike Head Unit for a Humanoid Cybernetic Avatar\n  `Yui' and Its Operation Interface",
            "Sentences": []
        },
        "http://arxiv.org/abs/2312.06292v1": {
            "Paper Title": "HoLLiE C -- A Multifunctional Bimanual Mobile Robot Supporting Versatile\n  Care Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.02021v3": {
            "Paper Title": "When Robotics Meets Wireless Communications: An Introductory Tutorial",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.04883v2": {
            "Paper Title": "Neural-Rendezvous: Provably Robust Guidance and Control to Encounter\n  Interstellar Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.04554v5": {
            "Paper Title": "Diagnosis-guided Attack Recovery for Securing Robotic Vehicles from\n  Sensor Deception Attacks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.03986v3": {
            "Paper Title": "Leveraging Randomized Smoothing for Optimal Control of Nonsmooth\n  Dynamical Systems",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": ".\nIt is common to address these issues by making the hypothesis of bilateral\ncontacts either between feet and the ground for locomotion ",
                    "Citation Text": "F. Farshidian, M. Neunert, A. W. Winkler, G. Rey, and J. Buchli, \u201cAn\nefficient optimal planning and control framework for quadrupedal loco-\nmotion,\u201d in 2017 IEEE International Conference on Robotics and Au-\ntomation (ICRA) . IEEE, 2017, pp. 93\u2013100.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.09861",
                        "Citation Paper Title": "Title:An Efficient Optimal Planning and Control Framework For Quadrupedal Locomotion",
                        "Citation Paper Abstract": "Abstract:In this paper, we present an efficient Dynamic Programing framework for optimal planning and control of legged robots. First we formulate this problem as an optimal control problem for switched systems. Then we propose a multi--level optimization approach to find the optimal switching times and the optimal continuous control inputs. Through this scheme, the decomposed optimization can potentially be done more efficiently than the combined approach. Finally, we present a continuous-time constrained LQR algorithm which simultaneously optimizes the feedforward and feedback controller with $O(n)$ time-complexity. In order to validate our approach, we show the performance of our framework on a quadrupedal robot. We choose the Center of Mass dynamics and the full kinematic formulation as the switched system model where the switching times as well as the contact forces and the joint velocities are optimized for different locomotion tasks such as gap crossing, walking and trotting.",
                        "Citation Paper Authors": "Authors:Farbod Farshidian, Michael Neunert, Alexander W. Winkler, Gonzalo Rey, Jonas Buchli"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.14471v5": {
            "Paper Title": "UTIL: An Ultra-wideband Time-difference-of-arrival Indoor Localization\n  Dataset",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.08627v2": {
            "Paper Title": "Long Horizon Planning through Contact using Discrete Search and\n  Continuous Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.06758v2": {
            "Paper Title": "Exploring Contextual Representation and Multi-Modality for End-to-End\n  Autonomous Driving",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.15169v3": {
            "Paper Title": "Multimotion Visual Odometry (MVO)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.14408v3": {
            "Paper Title": "RALACs: Action Recognition in Autonomous Vehicles using Interaction\n  Encoding and Optical Flow",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": ". This seems counter-intuitive for\na dataset meant to cater action recognition for autonomous\nvehicles, which requires an accurate, online solution to action\nlocalization and classification.\nIn particular, the winning entry Argus++ ",
                    "Citation Text": "Lijun Yu, Yijun Qian, Wenhe Liu, and Alexander G Hauptmann.\nArgus++: Robust real-time activity detection for unconstrained video\nstreams with overlapping cube proposals. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer Vision ,\npages 112\u2013121, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.05290",
                        "Citation Paper Title": "Title:Argus++: Robust Real-time Activity Detection for Unconstrained Video Streams with Overlapping Cube Proposals",
                        "Citation Paper Abstract": "Abstract:Activity detection is one of the attractive computer vision tasks to exploit the video streams captured by widely installed cameras. Although achieving impressive performance, conventional activity detection algorithms are usually designed under certain constraints, such as using trimmed and/or object-centered video clips as inputs. Therefore, they failed to deal with the multi-scale multi-instance cases in real-world unconstrained video streams, which are untrimmed and have large field-of-views. Real-time requirements for streaming analysis also mark brute force expansion of them unfeasible.\nTo overcome these issues, we propose Argus++, a robust real-time activity detection system for analyzing unconstrained video streams. The design of Argus++ introduces overlapping spatio-temporal cubes as an intermediate concept of activity proposals to ensure coverage and completeness of activity detection through over-sampling. The overall system is optimized for real-time processing on standalone consumer-level hardware. Extensive experiments on different surveillance and driving scenarios demonstrated its superior performance in a series of activity detection benchmarks, including CVPR ActivityNet ActEV 2021, NIST ActEV SDL UF/KF, TRECVID ActEV 2020/2021, and ICCV ROAD 2021.",
                        "Citation Paper Authors": "Authors:Lijun Yu, Yijun Qian, Wenhe Liu, Alexander G. Hauptmann"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "architecture which\nintroduced the use of a recurrent operator that iteratively\nupdates the flow field.\nThe estimated optical flow can be used with raw RGB\nframes for various detection and segmentation tasks. ",
                    "Citation Text": "Hazem Rashed, Senthil Yogamani, Ahmad El-Sallab, Pavel Krizek, and\nMohamed El-Helw. Optical flow augmented semantic segmentation\nnetworks for automated driving. 1 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.07355",
                        "Citation Paper Title": "Title:Optical Flow augmented Semantic Segmentation networks for Automated Driving",
                        "Citation Paper Abstract": "Abstract:Motion is a dominant cue in automated driving systems. Optical flow is typically computed to detect moving objects and to estimate depth using triangulation. In this paper, our motivation is to leverage the existing dense optical flow to improve the performance of semantic segmentation. To provide a systematic study, we construct four different architectures which use RGB only, flow only, RGBF concatenated and two-stream RGB + flow. We evaluate these networks on two automotive datasets namely Virtual KITTI and Cityscapes using the state-of-the-art flow estimator FlowNet v2. We also make use of the ground truth optical flow in Virtual KITTI to serve as an ideal estimator and a standard Farneback optical flow algorithm to study the effect of noise. Using the flow ground truth in Virtual KITTI, two-stream architecture achieves the best results with an improvement of 4% IoU. As expected, there is a large improvement for moving objects like trucks, vans and cars with 38%, 28% and 6% increase in IoU. FlowNet produces an improvement of 2.4% in average IoU with larger improvement in the moving objects corresponding to 26%, 11% and 5% in trucks, vans and cars. In Cityscapes, flow augmentation provided an improvement for moving objects like motorcycle and train with an increase of 17% and 7% in IoU.",
                        "Citation Paper Authors": "Authors:Hazem Rashed, Senthil Yogamani, Ahmad El-Sallab, Pavel Krizek, Mohamed El-Helw"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.03224v4": {
            "Paper Title": "Factor Graph-Based Planning as Inference for Autonomous Vehicle Racing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.03395v2": {
            "Paper Title": "Unified Learning from Demonstrations, Corrections, and Preferences\n  during Physical Human-Robot Interaction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.01982v5": {
            "Paper Title": "Lifelong Ensemble Learning based on Multiple Representations for\n  Few-Shot Object Recognition",
            "Sentences": [
                {
                    "Sentence ID": 74,
                    "Sentence": ".View-based\nmethods render different 2D images of a 3D object by pro-\njectingtheobject\u2019spointsonto2Dplanes ",
                    "Citation Text": "Su, H., Maji, S., Kalogerakis, E., Learned-Miller, E., 2015. Multi-\nview convolutional neural networks for 3d shape recognition, in:\nProceedingsoftheIEEEinternationalconferenceoncomputervision,\npp. 945\u2013953.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1505.00880",
                        "Citation Paper Title": "Title:Multi-view Convolutional Neural Networks for 3D Shape Recognition",
                        "Citation Paper Abstract": "Abstract:A longstanding question in computer vision concerns the representation of 3D shapes for recognition: should 3D shapes be represented with descriptors operating on their native 3D formats, such as voxel grid or polygon mesh, or can they be effectively represented with view-based descriptors? We address this question in the context of learning to recognize 3D shapes from a collection of their rendered views on 2D images. We first present a standard CNN architecture trained to recognize the shapes' rendered views independently of each other, and show that a 3D shape can be recognized even from a single view at an accuracy far higher than using state-of-the-art 3D shape descriptors. Recognition rates further increase when multiple views of the shapes are provided. In addition, we present a novel CNN architecture that combines information from multiple views of a 3D shape into a single and compact shape descriptor offering even better recognition performance. The same architecture can be applied to accurately recognize human hand-drawn sketches of shapes. We conclude that a collection of 2D views can be highly informative for 3D shape recognition and is amenable to emerging CNN architectures and their derivatives.",
                        "Citation Paper Authors": "Authors:Hang Su, Subhransu Maji, Evangelos Kalogerakis, Erik Learned-Miller"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "extended Latent Dirich-\nlet Allocation (LDA) and introduced Local-LDA, which is\nable to learn structural semantic features for each category\nindependently. Ayoubi et al., ",
                    "Citation Text": "Ayoobi, H., Kasaei, H., Cao, M., Verbrugge, R., Verheij, B., 2022.\nLocal-hdp: Interactive open-ended 3d object category recognition in\nreal-time robotic scenarios. Robotics and Autonomous Systems 147,\n103911.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.01152",
                        "Citation Paper Title": "Title:Local-HDP: Interactive Open-Ended 3D Object Categorization in Real-Time Robotic Scenarios",
                        "Citation Paper Abstract": "Abstract:We introduce a non-parametric hierarchical Bayesian approach for open-ended 3D object categorization, named the Local Hierarchical Dirichlet Process (Local-HDP). This method allows an agent to learn independent topics for each category incrementally and to adapt to the environment in time. Hierarchical Bayesian approaches like Latent Dirichlet Allocation (LDA) can transform low-level features to high-level conceptual topics for 3D object categorization. However, the efficiency and accuracy of LDA-based approaches depend on the number of topics that is chosen manually. Moreover, fixing the number of topics for all categories can lead to overfitting or underfitting of the model. In contrast, the proposed Local-HDP can autonomously determine the number of topics for each category. Furthermore, the online variational inference method has been adapted for fast posterior approximation in the Local-HDP model. Experiments show that the proposed Local-HDP method outperforms other state-of-the-art approaches in terms of accuracy, scalability, and memory efficiency by a large margin. Moreover, two robotic experiments have been conducted to show the applicability of the proposed approach in real-time applications.",
                        "Citation Paper Authors": "Authors:H. Ayoobi, H. Kasaei, M. Cao, R. Verbrugge, B. Verheij"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.08742v2": {
            "Paper Title": "Attentiveness Map Estimation for Haptic Teleoperation of Mobile Robot\n  Obstacle Avoidance and Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.10981v5": {
            "Paper Title": "Adaptive Sampling-based Particle Filter for Visual-inertial Gimbal in\n  the Wild",
            "Sentences": [
                {
                    "Sentence ID": 39,
                    "Sentence": ", then the warped parts are blended\nto synthesize the stabilized image. The framework by J.\nChoi et al. ",
                    "Citation Text": "J. Choi, J. Park, I. S & Kweon. (2021). Self-Supervised Real-time\nVideo Stabilization. arXiv preprint arXiv:2111.05980.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.05980",
                        "Citation Paper Title": "Title:Self-Supervised Real-time Video Stabilization",
                        "Citation Paper Abstract": "Abstract:Videos are a popular media form, where online video streaming has recently gathered much popularity. In this work, we propose a novel method of real-time video stabilization - transforming a shaky video to a stabilized video as if it were stabilized via gimbals in real-time. Our framework is trainable in a self-supervised manner, which does not require data captured with special hardware setups (i.e., two cameras on a stereo rig or additional motion sensors). Our framework consists of a transformation estimator between given frames for global stability adjustments, followed by scene parallax reduction module via spatially smoothed optical flow for further stability. Then, a margin inpainting module fills in the missing margin regions created during stabilization to reduce the amount of post-cropping. These sequential steps reduce distortion and margin cropping to a minimum while enhancing stability. Hence, our approach outperforms state-of-the-art real-time video stabilization methods as well as offline methods that require camera trajectory optimization. Our method procedure takes approximately 24.3 ms yielding 41 fps regardless of resolution (e.g., 480p or 1080p).",
                        "Citation Paper Authors": "Authors:Jinsoo Choi, Jaesik Park, In So Kweon"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": ". To tackle the problem of video\nstabilization in dynamic scenes, a dense warping field as\nscene representation was trained from consecutive video\nframes by Liu et al. ",
                    "Citation Text": "Yu-Lun Liu, et al. \u201cHybrid neural fusion for full-frame video stabi-\nlization.\u201d Proceedings of the IEEE/CVF International Conference on\nComputer Vision. 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.06205",
                        "Citation Paper Title": "Title:Hybrid Neural Fusion for Full-frame Video Stabilization",
                        "Citation Paper Abstract": "Abstract:Existing video stabilization methods often generate visible distortion or require aggressive cropping of frame boundaries, resulting in smaller field of views. In this work, we present a frame synthesis algorithm to achieve full-frame video stabilization. We first estimate dense warp fields from neighboring frames and then synthesize the stabilized frame by fusing the warped contents. Our core technical novelty lies in the learning-based hybrid-space fusion that alleviates artifacts caused by optical flow inaccuracy and fast-moving objects. We validate the effectiveness of our method on the NUS, selfie, and DeepStab video datasets. Extensive experiment results demonstrate the merits of our approach over prior video stabilization methods.",
                        "Citation Paper Authors": "Authors:Yu-Lun Liu, Wei-Sheng Lai, Ming-Hsuan Yang, Yung-Yu Chuang, Jia-Bin Huang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.04567v2": {
            "Paper Title": "A general class of combinatorial filters that can be minimized\n  efficiently",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.09935v2": {
            "Paper Title": "CAPE: Corrective Actions from Precondition Errors using Large Language\n  Models",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": "that extract plans from LLMs\nusing prompting strategies assume access to extra informa-\ntion such as: 1) predefined skills with preconditions ",
                    "Citation Text": "M. Ahn, A. Brohan, N. Brown, Y . Chebotar, O. Cortes,\nB. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Haus-\nman, A. Herzog, D. Ho, J. Hsu, J. Ibarz, B. Ichter,\nA. Irpan, E. Jang, R. J. Ruano, K. Jeffrey, S. Jesmonth,\nN. Joshi, R. Julian, D. Kalashnikov, Y . Kuang, K.-H.\nLee, S. Levine, Y . Lu, L. Luu, C. Parada, P. Pastor,\nJ. Quiambao, K. Rao, J. Rettinghouse, D. Reyes, P. Ser-\nmanet, N. Sievers, C. Tan, A. Toshev, V . Vanhoucke,\nF. Xia, T. Xiao, P. Xu, S. Xu, M. Yan, and A. Zeng,\n\u201cDo As I Can, Not As I Say: Grounding Language\nin Robotic Affordances,\u201d in Proceedings of The 6th\nConference on Robot Learning , ser. Proceedings of\nMachine Learning Research, K. Liu, D. Kulic, and\nJ. Ichnowski, Eds., vol. 205. PMLR, 14\u201318 Dec 2022,\npp. 287\u2013318.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.01691",
                        "Citation Paper Title": "Title:Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
                        "Citation Paper Abstract": "Abstract:Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's \"hands and eyes,\" while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at this https URL.",
                        "Citation Paper Authors": "Authors:Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, Andy Zeng"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": ") are then used to\nground admissible skills to spatial points for navigation or\ngrasping in the physical environment, similar to approaches\nlike NLMap-SayCan ",
                    "Citation Text": "B. Chen, F. Xia, B. Ichter, K. Rao, K. Gopalakrish-\nnan, M. S. Ryoo, A. Stone, and D. Kappler, \u201cOpen-\nvocabulary Queryable Scene Representations for Real\nWorld Planning,\u201d in 2023 IEEE International Confer-\nence on Robotics and Automation (ICRA) , 2023, pp.\n11 509\u201311 522.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2209.09874",
                        "Citation Paper Title": "Title:Open-vocabulary Queryable Scene Representations for Real World Planning",
                        "Citation Paper Abstract": "Abstract:Large language models (LLMs) have unlocked new capabilities of task planning from human instructions. However, prior attempts to apply LLMs to real-world robotic tasks are limited by the lack of grounding in the surrounding scene. In this paper, we develop NLMap, an open-vocabulary and queryable scene representation to address this problem. NLMap serves as a framework to gather and integrate contextual information into LLM planners, allowing them to see and query available objects in the scene before generating a context-conditioned plan. NLMap first establishes a natural language queryable scene representation with Visual Language models (VLMs). An LLM based object proposal module parses instructions and proposes involved objects to query the scene representation for object availability and location. An LLM planner then plans with such information about the scene. NLMap allows robots to operate without a fixed list of objects nor executable options, enabling real robot operation unachievable by previous methods. Project website: this https URL",
                        "Citation Paper Authors": "Authors:Boyuan Chen, Fei Xia, Brian Ichter, Kanishka Rao, Keerthana Gopalakrishnan, Michael S. Ryoo, Austin Stone, Daniel Kappler"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ", whereas PIandPEcan work with task and\nmotion planning approaches ",
                    "Citation Text": "C. R. Garrett, R. Chitnis, R. Holladay, B. Kim, T. Sil-\nver, L. P. Kaelbling, and T. Lozano-P\u00e9rez, \u201cIntegrated\ntask and motion planning,\u201d Annual Review of Control,\nRobotics, and Autonomous Systems , vol. 4, pp. 265\u2013\n293, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.01083",
                        "Citation Paper Title": "Title:Integrated Task and Motion Planning",
                        "Citation Paper Abstract": "Abstract:The problem of planning for a robot that operates in environments containing a large number of objects, taking actions to move itself through the world as well as to change the state of the objects, is known as task and motion planning (TAMP). TAMP problems contain elements of discrete task planning, discrete-continuous mathematical programming, and continuous motion planning, and thus cannot be effectively addressed by any of these fields directly. In this paper, we define a class of TAMP problems and survey algorithms for solving them, characterizing the solution methods in terms of their strategies for solving the continuous-space subproblems and their techniques for integrating the discrete and continuous components of the search.",
                        "Citation Paper Authors": "Authors:Caelan Reed Garrett, Rohan Chitnis, Rachel Holladay, Beomjoon Kim, Tom Silver, Leslie Pack Kaelbling, Tom\u00e1s Lozano-P\u00e9rez"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.06273v2": {
            "Paper Title": "Continuous Occupancy Mapping in Dynamic Environments Using Particles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.07162v2": {
            "Paper Title": "Category-Agnostic 6D Pose Estimation with Conditional Neural Processes",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": ". We further compare max aggregation\nwith the cross-attention module proposed in Attentive Neu-\nral Processes (ANPs) ",
                    "Citation Text": "Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Gar-\nnelo, Ali Eslami, Dan Rosenbaum, Oriol Vinyals, and\nYee Whye Teh. Attentive neural processes. In International\nConference on Learning Representations , 2019. 3, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.05761",
                        "Citation Paper Title": "Title:Attentive Neural Processes",
                        "Citation Paper Abstract": "Abstract:Neural Processes (NPs) (Garnelo et al 2018a;b) approach regression by learning to map a context set of observed input-output pairs to a distribution over regression functions. Each function models the distribution of the output given an input, conditioned on the context. NPs have the benefit of fitting observed data efficiently with linear complexity in the number of context input-output pairs, and can learn a wide family of conditional distributions; they learn predictive distributions conditioned on context sets of arbitrary size. Nonetheless, we show that NPs suffer a fundamental drawback of underfitting, giving inaccurate predictions at the inputs of the observed data they condition on. We address this issue by incorporating attention into NPs, allowing each input location to attend to the relevant context points for the prediction. We show that this greatly improves the accuracy of predictions, results in noticeably faster training, and expands the range of functions that can be modelled.",
                        "Citation Paper Authors": "Authors:Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Garnelo, Ali Eslami, Dan Rosenbaum, Oriol Vinyals, Yee Whye Teh"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": ". Our sec-\nond dataset can be further divided into a non-occluded and\nan occluded version, called PBR-MCMS and Occlusion-\nMCMS. To create these datasets, we extend the open-source\nphysics-based rendering (PBR) pipeline ",
                    "Citation Text": "Maximilian Denninger, Martin Sundermeyer, Dominik\nWinkelbauer, Youssef Zidan, Dmitry Olefir, Mohamad El-\nbadrawy, Ahsan Lodhi, and Harinandan Katam. Blender-\nproc. CoRR , abs/1911.01911, 2019. 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.01911",
                        "Citation Paper Title": "Title:BlenderProc",
                        "Citation Paper Abstract": "Abstract:BlenderProc is a modular procedural pipeline, which helps in generating real looking images for the training of convolutional neural networks. These can be used in a variety of use cases including segmentation, depth, normal and pose estimation and many others. A key feature of our extension of blender is the simple to use modular pipeline, which was designed to be easily extendable. By offering standard modules, which cover a variety of scenarios, we provide a starting point on which new modules can be created.",
                        "Citation Paper Authors": "Authors:Maximilian Denninger, Martin Sundermeyer, Dominik Winkelbauer, Youssef Zidan, Dmitry Olefir, Mohamad Elbadrawy, Ahsan Lodhi, Harinandan Katam"
                    }
                },
                {
                    "Sentence ID": 55,
                    "Sentence": ", which contain various objects from Multiple\nCategories inMultiple Scenes (MCMS) . The simple ver-\nsion of MCMS, named Toy-MCMS, is composed of images\ncontaining a single object with backgrounds randomly sam-\npled from the real-world image dataset SUN ",
                    "Citation Text": "Shuran Song, Fisher Yu, Andy Zeng, Angel X. Chang,\nManolis Savva, and Thomas Funkhouser. Semantic scene\ncompletion from a single depth image. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR) , July 2017. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.08974",
                        "Citation Paper Title": "Title:Semantic Scene Completion from a Single Depth Image",
                        "Citation Paper Abstract": "Abstract:This paper focuses on semantic scene completion, a task for producing a complete 3D voxel representation of volumetric occupancy and semantic labels for a scene from a single-view depth map observation. Previous work has considered scene completion and semantic labeling of depth maps separately. However, we observe that these two problems are tightly intertwined. To leverage the coupled nature of these two tasks, we introduce the semantic scene completion network (SSCNet), an end-to-end 3D convolutional network that takes a single depth image as input and simultaneously outputs occupancy and semantic labels for all voxels in the camera view frustum. Our network uses a dilation-based 3D context module to efficiently expand the receptive field and enable 3D context learning. To train our network, we construct SUNCG - a manually created large-scale dataset of synthetic 3D scenes with dense volumetric annotations. Our experiments demonstrate that the joint model outperforms methods addressing each task in isolation and outperforms alternative approaches on the semantic scene completion task.",
                        "Citation Paper Authors": "Authors:Shuran Song, Fisher Yu, Andy Zeng, Angel X. Chang, Manolis Savva, Thomas Funkhouser"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ",\nobject detection [6, 12, 13, 46, 72], robotic grasping ",
                    "Citation Text": "Ning Gao, Jingyu Zhang, Ruijie Chen, Ngo Anh Vien,\nHanna Ziesche, and Gerhard Neumann. Meta-learning re-\ngrasping strategies for physical-agnostic objects. ArXiv ,\nabs/2205.11110, 2023. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.11110",
                        "Citation Paper Title": "Title:Meta-Learning Regrasping Strategies for Physical-Agnostic Objects",
                        "Citation Paper Abstract": "Abstract:Grasping inhomogeneous objects in real-world applications remains a challenging task due to the unknown physical properties such as mass distribution and coefficient of friction. In this study, we propose a meta-learning algorithm called ConDex, which incorporates Conditional Neural Processes (CNP) with DexNet-2.0 to autonomously discern the underlying physical properties of objects using depth images. ConDex efficiently acquires physical embeddings from limited trials, enabling precise grasping point estimation. Furthermore, ConDex is capable of updating the predicted grasping quality iteratively from new trials in an online fashion. To the best of our knowledge, we are the first who generate two object datasets focusing on inhomogeneous physical properties with varying mass distributions and friction coefficients. Extensive evaluations in simulation demonstrate ConDex's superior performance over DexNet-2.0 and existing meta-learning-based grasping pipelines. Furthermore, ConDex shows robust generalization to previously unseen real-world objects despite training solely in the simulation. The synthetic and real-world datasets will be published as well.",
                        "Citation Paper Authors": "Authors:Ning Gao, Jingyu Zhang, Ruijie Chen, Ngo Anh Vien, Hanna Ziesche, Gerhard Neumann"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "generates 3D point clouds\nin the canonical space using a variational autoencoder\n(V AE). FS-Net ",
                    "Citation Text": "Wei Chen, Xi Jia, Hyung Jin Chang, Jinming Duan, Linlin\nShen, and Ales Leonardis. Fs-net: Fast shape-based network\nfor category-level 6d object pose estimation with decou-\npled rotation mechanism. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR) , pages 1581\u20131590, June 2021. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.07054",
                        "Citation Paper Title": "Title:FS-Net: Fast Shape-based Network for Category-Level 6D Object Pose Estimation with Decoupled Rotation Mechanism",
                        "Citation Paper Abstract": "Abstract:In this paper, we focus on category-level 6D pose and size estimation from monocular RGB-D image. Previous methods suffer from inefficient category-level pose feature extraction which leads to low accuracy and inference speed. To tackle this problem, we propose a fast shape-based network (FS-Net) with efficient category-level feature extraction for 6D pose estimation. First, we design an orientation aware autoencoder with 3D graph convolution for latent feature extraction. The learned latent feature is insensitive to point shift and object size thanks to the shift and scale-invariance properties of the 3D graph convolution. Then, to efficiently decode category-level rotation information from the latent feature, we propose a novel decoupled rotation mechanism that employs two decoders to complementarily access the rotation information. Meanwhile, we estimate translation and size by two residuals, which are the difference between the mean of object points and ground truth translation, and the difference between the mean size of the category and ground truth size, respectively. Finally, to increase the generalization ability of FS-Net, we propose an online box-cage based 3D deformation mechanism to augment the training data. Extensive experiments on two benchmark datasets show that the proposed method achieves state-of-the-art performance in both category- and instance-level 6D object pose estimation. Especially in category-level pose estimation, without extra synthetic data, our method outperforms existing methods by 6.3% on the NOCS-REAL dataset.",
                        "Citation Paper Authors": "Authors:Wei Chen, Xi Jia, Hyung Jin Chang, Jinming Duan, Linlin Shen, Ales Leonardis"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "share a canonical representation for all possible\nobject instances within a category using Normalized Object\nCoordinate Space (NOCS). However, inferring the object\npose by predicting only the NOCS representation is not easy\ngiven large intra-category variations ",
                    "Citation Text": "Zhaoxin Fan, Yazhi Zhu, Yulin He, Qi Sun, Hongyan Liu,\nand Jun He. Deep learning on monocular object pose de-\ntection and tracking: A comprehensive overview. ArXiv ,\nabs/2105.14291, 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.14291",
                        "Citation Paper Title": "Title:Deep Learning on Monocular Object Pose Detection and Tracking: A Comprehensive Overview",
                        "Citation Paper Abstract": "Abstract:Object pose detection and tracking has recently attracted increasing attention due to its wide applications in many areas, such as autonomous driving, robotics, and augmented reality. Among methods for object pose detection and tracking, deep learning is the most promising one that has shown better performance than others. However, survey study about the latest development of deep learning-based methods is lacking. Therefore, this study presents a comprehensive review of recent progress in object pose detection and tracking that belongs to the deep learning technical route. To achieve a more thorough introduction, the scope of this study is limited to methods taking monocular RGB/RGBD data as input and covering three kinds of major tasks: instance-level monocular object pose detection, category-level monocular object pose detection, and monocular object pose tracking. In our work, metrics, datasets, and methods of both detection and tracking are presented in detail. Comparative results of current state-of-the-art methods on several publicly available datasets are also presented, together with insightful observations and inspiring future research directions.",
                        "Citation Paper Authors": "Authors:Zhaoxin Fan, Yazhi Zhu, Yulin He, Qi Sun, Hongyan Liu, Jun He"
                    }
                },
                {
                    "Sentence ID": 64,
                    "Sentence": ".\nRecently, category-level 6D object pose estimation has\ngained increasing attention [4, 7, 8, 62, 64]. Wang et\nal. ",
                    "Citation Text": "He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin,\nShuran Song, and Leonidas J. Guibas. Normalized object\ncoordinate space for category-level 6d object pose and size\nestimation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR) , June\n2019. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.02970",
                        "Citation Paper Title": "Title:Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation",
                        "Citation Paper Abstract": "Abstract:The goal of this paper is to estimate the 6D pose and dimensions of unseen object instances in an RGB-D image. Contrary to \"instance-level\" 6D pose estimation tasks, our problem assumes that no exact object CAD models are available during either training or testing time. To handle different and unseen object instances in a given category, we introduce a Normalized Object Coordinate Space (NOCS)---a shared canonical representation for all possible object instances within a category. Our region-based neural network is then trained to directly infer the correspondence from observed pixels to this shared object representation (NOCS) along with other object information such as class label and instance mask. These predictions can be combined with the depth map to jointly estimate the metric 6D pose and dimensions of multiple objects in a cluttered scene. To train our network, we present a new context-aware technique to generate large amounts of fully annotated mixed reality data. To further improve our model and evaluate its performance on real data, we also provide a fully annotated real-world dataset with large environment and instance variation. Extensive experiments demonstrate that the proposed method is able to robustly estimate the pose and size of unseen object instances in real environments while also achieving state-of-the-art performance on standard 6D pose estimation benchmarks.",
                        "Citation Paper Authors": "Authors:He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, Leonidas J. Guibas"
                    }
                },
                {
                    "Sentence ID": 56,
                    "Sentence": ". Template-based methods, on the other hand,\nmatch the inputs to templates, which can be either explicit\npose-aware images [28, 29] or templates learned implicitly\nby neural networks ",
                    "Citation Text": "Martin Sundermeyer, Zoltan-Csaba Marton, Maximilian\nDurner, Manuel Brucker, and Rudolph Triebel. Implicit 3d\norientation learning for 6d object detection from rgb images.\nInThe European Conference on Computer Vision (ECCV) ,\nSeptember 2018. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.01275",
                        "Citation Paper Title": "Title:Implicit 3D Orientation Learning for 6D Object Detection from RGB Images",
                        "Citation Paper Abstract": "Abstract:We propose a real-time RGB-based pipeline for object detection and 6D pose estimation. Our novel 3D orientation estimation is based on a variant of the Denoising Autoencoder that is trained on simulated views of a 3D model using Domain Randomization. This so-called Augmented Autoencoder has several advantages over existing methods: It does not require real, pose-annotated training data, generalizes to various test sensors and inherently handles object and view symmetries. Instead of learning an explicit mapping from input images to object poses, it provides an implicit representation of object orientations defined by samples in a latent space. Our pipeline achieves state-of-the-art performance on the T-LESS dataset both in the RGB and RGB-D domain. We also evaluate on the LineMOD dataset where we can compete with other synthetically trained approaches. We further increase performance by correcting 3D orientation estimates to account for perspective errors when the object deviates from the image center and show extended results.",
                        "Citation Paper Authors": "Authors:Martin Sundermeyer, Zoltan-Csaba Marton, Maximilian Durner, Manuel Brucker, Rudolph Triebel"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "6D Pose Estimation. For instance-level 6D pose esti-\nmation, methods can be categorized into three classes:\ncorrespondence-based, template-based and voting-based\nmethods ",
                    "Citation Text": "Guoguang Du, Kai Wang, and Shiguo Lian. Vision-based\nrobotic grasping from object localization, pose estimation,\ngrasp detection to motion planning: A review. CoRR ,\nabs/1905.06658, 2019. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.06658",
                        "Citation Paper Title": "Title:Vision-based Robotic Grasping From Object Localization, Object Pose Estimation to Grasp Estimation for Parallel Grippers: A Review",
                        "Citation Paper Abstract": "Abstract:This paper presents a comprehensive survey on vision-based robotic grasping. We conclude three key tasks during vision-based robotic grasping, which are object localization, object pose estimation and grasp estimation. In detail, the object localization task contains object localization without classification, object detection and object instance segmentation. This task provides the regions of the target object in the input data. The object pose estimation task mainly refers to estimating the 6D object pose and includes correspondence-based methods, template-based methods and voting-based methods, which affords the generation of grasp poses for known objects. The grasp estimation task includes 2D planar grasp methods and 6DoF grasp methods, where the former is constrained to grasp from one direction. These three tasks could accomplish the robotic grasping with different combinations. Lots of object pose estimation methods need not object localization, and they conduct object localization and object pose estimation jointly. Lots of grasp estimation methods need not object localization and object pose estimation, and they conduct grasp estimation in an end-to-end manner. Both traditional methods and latest deep learning-based methods based on the RGB-D image inputs are reviewed elaborately in this survey. Related datasets and comparisons between state-of-the-art methods are summarized as well. In addition, challenges about vision-based robotic grasping and future directions in addressing these challenges are also pointed out.",
                        "Citation Paper Authors": "Authors:Guoguang Du, Kai Wang, Shiguo Lian, Kaiyong Zhao"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.08517v2": {
            "Paper Title": "ECTLO: Effective Continuous-time Odometry Using Range Image for LiDAR\n  with Small FoV",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.01041v7": {
            "Paper Title": "Probabilistic Safeguard for Reinforcement Learning Using Safety Index\n  Guided Gaussian Process Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.11524v4": {
            "Paper Title": "Control Barrier Functions in UGVs for Kinematic Obstacle Avoidance: A\n  Collision Cone Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.07870v3": {
            "Paper Title": "Scene Graph for Embodied Exploration in Cluttered Scenario",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": ". Meanwhile, other works take scene\ngraphs as intermediate results for high-level comprehension\nor task planning. Kumar et al. introduce a learning framework\nwith GNN-based scene generation to teach a robotic agent\nto interactively explore cluttered scenes ",
                    "Citation Text": "K. N. Kumar, I. Essa, and S. Ha, \u201cGraph-based cluttered scene gener-\nation and interactive exploration using deep reinforcement learning,\u201d\narXiv preprint arXiv:2109.10460 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.10460",
                        "Citation Paper Title": "Title:Graph-based Cluttered Scene Generation and Interactive Exploration using Deep Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:We introduce a novel method to teach a robotic agent to interactively explore cluttered yet structured scenes, such as kitchen pantries and grocery shelves, by leveraging the physical plausibility of the scene. We propose a novel learning framework to train an effective scene exploration policy to discover hidden objects with minimal interactions. First, we define a novel scene grammar to represent structured clutter. Then we train a Graph Neural Network (GNN) based Scene Generation agent using deep reinforcement learning (deep RL), to manipulate this Scene Grammar to create a diverse set of stable scenes, each containing multiple hidden objects. Given such cluttered scenes, we then train a Scene Exploration agent, using deep RL, to uncover hidden objects by interactively rearranging the scene. We show that our learned agents hide and discover significantly more objects than the baselines. We present quantitative results that prove the generalization capabilities of our agents. We also demonstrate sim-to-real transfer by successfully deploying the learned policy on a real UR10 robot to explore real-world cluttered scenes. The supplemental video can be found at this https URL.",
                        "Citation Paper Authors": "Authors:K. Niranjan Kumar, Irfan Essa, Sehoon Ha"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": ". Some works\nexplore the utilization of more information, including seman-\ntics and spatial relationship of objects in clutter. Therefore,\nmanipulation tasks in the form of linguistic interaction have\nbeen raised in recent years ",
                    "Citation Text": "A. Das, S. Kottur, K. Gupta, A. Singh, D. Yadav, J. M. Moura,\nD. Parikh, and D. Batra, \u201cVisual dialog,\u201d in Proceedings of the IEEEConference on Computer Vision and Pattern Recognition , pp. 326\u2013\n335, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.08669",
                        "Citation Paper Title": "Title:Visual Dialog",
                        "Citation Paper Abstract": "Abstract:We introduce the task of Visual Dialog, which requires an AI agent to hold a meaningful dialog with humans in natural, conversational language about visual content. Specifically, given an image, a dialog history, and a question about the image, the agent has to ground the question in image, infer context from history, and answer the question accurately. Visual Dialog is disentangled enough from a specific downstream task so as to serve as a general test of machine intelligence, while being grounded in vision enough to allow objective evaluation of individual responses and benchmark progress. We develop a novel two-person chat data-collection protocol to curate a large-scale Visual Dialog dataset (VisDial). VisDial v0.9 has been released and contains 1 dialog with 10 question-answer pairs on ~120k images from COCO, with a total of ~1.2M dialog question-answer pairs.\nWe introduce a family of neural encoder-decoder models for Visual Dialog with 3 encoders -- Late Fusion, Hierarchical Recurrent Encoder and Memory Network -- and 2 decoders (generative and discriminative), which outperform a number of sophisticated baselines. We propose a retrieval-based evaluation protocol for Visual Dialog where the AI agent is asked to sort a set of candidate answers and evaluated on metrics such as mean-reciprocal-rank of human response. We quantify gap between machine and human performance on the Visual Dialog task via human studies. Putting it all together, we demonstrate the first 'visual chatbot'! Our dataset, code, trained models and visual chatbot are available on this https URL",
                        "Citation Paper Authors": "Authors:Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos\u00e9 M. F. Moura, Devi Parikh, Dhruv Batra"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": ". Another challenging task in\ncluttered surroundings is object layout rearrangement. Suc-\ncessfully accomplishing this task necessitates the robot to\nnot only accurately detect and localize every object but also\ncomprehend their spatial relationships ",
                    "Citation Text": "S. H. Cheong, B. Y . Cho, J. Lee, C. Kim, and C. Nam, \u201cWhere\nto relocate?: Object rearrangement inside cluttered and confined en-\nvironments for robotic manipulation,\u201d in 2020 IEEE International\nConference on Robotics and Automation (ICRA) , pp. 7791\u20137797,\nIEEE, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.10863",
                        "Citation Paper Title": "Title:Where to relocate?: Object rearrangement inside cluttered and confined environments for robotic manipulation",
                        "Citation Paper Abstract": "Abstract:We present an algorithm determining where to relocate objects inside a cluttered and confined space while rearranging objects to retrieve a target object. Although methods that decide what to remove have been proposed, planning for the placement of removed objects inside a workspace has not received much attention. Rather, removed objects are often placed outside the workspace, which incurs additional laborious work (e.g., motion planning and execution of the manipulator and the mobile base, perception of other areas). Some other methods manipulate objects only inside the workspace but without a principle so the rearrangement becomes inefficient.\nIn this work, we consider both monotone (each object is moved only once) and non-monotone arrangement problems which have shown to be NP-hard. Once the sequence of objects to be relocated is given by any existing algorithm, our method aims to minimize the number of pick-and-place actions to place the objects until the target becomes accessible. From extensive experiments, we show that our method reduces the number of pick-and-place actions and the total execution time (the reduction is up to 23.1% and 28.1% respectively) compared to baseline methods while achieving higher success rates.",
                        "Citation Paper Authors": "Authors:Sang Hun Cheong, Brian Y. Cho, Jinhwi Lee, ChangHwan Kim, Changjoo Nam"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.05154v3": {
            "Paper Title": "Optimal Control for Quadruped Locomotion using LTV MPC",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.05952v4": {
            "Paper Title": "Efficient Domain Coverage for Vehicles with Second-Order Dynamics via\n  Multi-Agent Reinforcement Learning",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": "mechanism to encode the state\ninformation into an ideal format as input for MLP networks.\nMeanwhile, the value decomposition (VD) ",
                    "Citation Text": "P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V . Zambaldi,\nM. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls, and\nT. Graepel, \u201cValue-decomposition networks for cooperative multi-\nagent learning based on team reward,\u201d in Int. Conf. Autonomous Agents\nand MultiAgent Systems , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.05296",
                        "Citation Paper Title": "Title:Value-Decomposition Networks For Cooperative Multi-Agent Learning",
                        "Citation Paper Abstract": "Abstract:We study the problem of cooperative multi-agent reinforcement learning with a single joint reward signal. This class of learning problems is difficult because of the often large combined action and observation spaces. In the fully centralized and decentralized approaches, we find the problem of spurious rewards and a phenomenon we call the \"lazy agent\" problem, which arises due to partial observability. We address these problems by training individual agents with a novel value decomposition network architecture, which learns to decompose the team value function into agent-wise value functions. We perform an experimental evaluation across a range of partially-observable multi-agent domains and show that learning such value-decompositions leads to superior results, in particular when combined with weight sharing, role information and information channels.",
                        "Citation Paper Authors": "Authors:Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl Tuyls, Thore Graepel"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ", an actor-critic algorithm for the\nMARL problem. The main feature of MAPPO is to extend\nthe PPO algorithm ",
                    "Citation Text": "J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,\n\u201cProximal policy optimization algorithms,\u201d 2017. arXiv preprint:\n1707.06347.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.06347",
                        "Citation Paper Title": "Title:Proximal Policy Optimization Algorithms",
                        "Citation Paper Abstract": "Abstract:We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.",
                        "Citation Paper Authors": "Authors:John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "applies\nan actor-critic method involving a specific value function\nformulation to multi-agent continuous coverage control. In ",
                    "Citation Text": "M. Kouzehgar, M. Meghjani, and R. Bouffanais, \u201cMulti-agent rein-\nforcement learning for dynamic ocean monitoring by a swarm of\nbuoys,\u201d in Global Oceans 2020: Singapore \u2013 U.S. Gulf Coast , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.11641",
                        "Citation Paper Title": "Title:Multi-Agent Reinforcement Learning for Dynamic Ocean Monitoring by a Swarm of Buoys",
                        "Citation Paper Abstract": "Abstract:Autonomous marine environmental monitoring problem traditionally encompasses an area coverage problem which can only be effectively carried out by a multi-robot system. In this paper, we focus on robotic swarms that are typically operated and controlled by means of simple swarming behaviors obtained from a subtle, yet ad hoc combination of bio-inspired strategies. We propose a novel and structured approach for area coverage using multi-agent reinforcement learning (MARL) which effectively deals with the non-stationarity of environmental features. Specifically, we propose two dynamic area coverage approaches: (1) swarm-based MARL, and (2) coverage-range-based MARL. The former is trained using the multi-agent deep deterministic policy gradient (MADDPG) approach whereas, a modified version of MADDPG is introduced for the latter with a reward function that intrinsically leads to a collective behavior. Both methods are tested and validated with different geometric shaped regions with equal surface area (square vs. rectangle) yielding acceptable area coverage, and benefiting from the structured learning in non-stationary environments. Both approaches are advantageous compared to a na\u00efve swarming method. However, coverage-range-based MARL outperforms the swarm-based MARL with stronger convergence features in learning criteria and higher spreading of agents for area coverage.",
                        "Citation Paper Authors": "Authors:Maryam Kouzehgar, Malika Meghjani, Roland Bouffanais"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.12403v2": {
            "Paper Title": "ZSON: Zero-Shot Object-Goal Navigation using Multimodal Goal Embeddings",
            "Sentences": [
                {
                    "Sentence ID": 33,
                    "Sentence": ". First, we sample 9k ImageNav episodes for each HM3D scan, split\nequally between 3 difficulty levels corresponding with path length: EASY (1.5-3m), MEDIUM (3-\n5m), and HARD (5-10m). We follow the episode generation approach from ",
                    "Citation Text": "Lina Mezghani, Sainbayar Sukhbaatar, Thibaut Lavril, Oleksandr Maksymets, Dhruv Batra, Piotr Bo-\njanowski, and Karteek Alahari. Memory-Augmented Reinforcement Learning for Image-Goal Navigation.\narXiv preprint arXiv:2101.05181 , 2021. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.05181",
                        "Citation Paper Title": "Title:Memory-Augmented Reinforcement Learning for Image-Goal Navigation",
                        "Citation Paper Abstract": "Abstract:In this work, we present a memory-augmented approach for image-goal navigation. Earlier attempts, including RL-based and SLAM-based approaches have either shown poor generalization performance, or are heavily-reliant on pose/depth sensors. Our method is based on an attention-based end-to-end model that leverages an episodic memory to learn to navigate. First, we train a state-embedding network in a self-supervised fashion, and then use it to embed previously-visited states into the agent's memory. Our navigation policy takes advantage of this information through an attention mechanism. We validate our approach with extensive evaluations, and show that our model establishes a new state of the art on the challenging Gibson dataset. Furthermore, we achieve this impressive performance from RGB input alone, without access to additional information such as position or depth, in stark contrast to related work.",
                        "Citation Paper Authors": "Authors:Lina Mezghani, Sainbayar Sukhbaatar, Thibaut Lavril, Oleksandr Maksymets, Dhruv Batra, Piotr Bojanowski, Karteek Alahari"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "encoder, which is pretrained on the Omnidata Starter Dataset (OSD) ",
                    "Citation Text": "Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. Omnidata: A Scalable Pipeline for\nMaking Multi-Task Mid-Level Vision Datasets From 3D Scans. In ICCV , 2021. 5\n11",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.04994",
                        "Citation Paper Title": "Title:Omnidata: A Scalable Pipeline for Making Multi-Task Mid-Level Vision Datasets from 3D Scans",
                        "Citation Paper Abstract": "Abstract:This paper introduces a pipeline to parametrically sample and render multi-task vision datasets from comprehensive 3D scans from the real world. Changing the sampling parameters allows one to \"steer\" the generated datasets to emphasize specific information. In addition to enabling interesting lines of research, we show the tooling and generated data suffice to train robust vision models.\nCommon architectures trained on a generated starter dataset reached state-of-the-art performance on multiple common vision tasks and benchmarks, despite having seen no benchmark or non-pipeline data. The depth estimation network outperforms MiDaS and the surface normal estimation network is the first to achieve human-level performance for in-the-wild surface normal estimation -- at least according to one metric on the OASIS benchmark.\nThe Dockerized pipeline with CLI, the (mostly python) code, PyTorch dataloaders for the generated data, the generated starter dataset, download scripts and other utilities are available through our project website, https://omnidata.vision.",
                        "Citation Paper Authors": "Authors:Ainaz Eftekhar, Alexander Sax, Roman Bachmann, Jitendra Malik, Amir Zamir"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.14791v3": {
            "Paper Title": "ViNL: Visual Navigation and Locomotion Over Obstacles",
            "Sentences": [
                {
                    "Sentence ID": 41,
                    "Sentence": "in Stage 3 of our approach\n(described below), in which we distill the knowledge from\nthe privileged map MLP encoder, into an encoder \u03c4that uses\negocentric depth observations instead.\nThe policy is trained using Proximal Policy Optimization\n(PPO) ",
                    "Citation Text": "J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,\n\u201cProximal Policy Optimization Algorithms,\u201d arXiv:1707.06347 [cs] ,\nAug. 2017, arXiv: 1707.06347. [Online]. Available: http://arxiv.org/\nabs/1707.06347",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.06347",
                        "Citation Paper Title": "Title:Proximal Policy Optimization Algorithms",
                        "Citation Paper Abstract": "Abstract:We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.",
                        "Citation Paper Authors": "Authors:John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.10404v3": {
            "Paper Title": "GP-net: Flexible Viewpoint Grasp Proposal",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": ",\nwhere grasp configurations are sampled and subsequently\nranked according to a quality metric ",
                    "Citation Text": "J. Mahler, J. Liang, S. Niyaz, M. Laskey, R. Doan, X. Liu, J. A. Ojea,\nand K. Goldberg, \u201cDex-Net 2.0: Deep Learning to Plan Robust Grasps\nwith Synthetic Point Clouds and Analytic Grasp Metrics,\u201d in Robotics:\nScience and Systems (RSS) , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.09312",
                        "Citation Paper Title": "Title:Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics",
                        "Citation Paper Abstract": "Abstract:To reduce data collection time for deep learning of robust robotic grasp plans, we explore training from a synthetic dataset of 6.7 million point clouds, grasps, and analytic grasp metrics generated from thousands of 3D models from Dex-Net 1.0 in randomized poses on a table. We use the resulting dataset, Dex-Net 2.0, to train a Grasp Quality Convolutional Neural Network (GQ-CNN) model that rapidly predicts the probability of success of grasps from depth images, where grasps are specified as the planar position, angle, and depth of a gripper relative to an RGB-D sensor. Experiments with over 1,000 trials on an ABB YuMi comparing grasp planning methods on singulated objects suggest that a GQ-CNN trained with only synthetic data from Dex-Net 2.0 can be used to plan grasps in 0.8sec with a success rate of 93% on eight known objects with adversarial geometry and is 3x faster than registering point clouds to a precomputed dataset of objects and indexing grasps. The Dex-Net 2.0 grasp planner also has the highest success rate on a dataset of 10 novel rigid objects and achieves 99% precision (one false positive out of 69 grasps classified as robust) on a dataset of 40 novel household objects, some of which are articulated or deformable. Code, datasets, videos, and supplementary material are available at this http URL .",
                        "Citation Paper Authors": "Authors:Jeffrey Mahler, Jacky Liang, Sherdil Niyaz, Michael Laskey, Richard Doan, Xinyu Liu, Juan Aparicio Ojea, Ken Goldberg"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.05167v3": {
            "Paper Title": "LF-VISLAM: A SLAM Framework for Large Field-of-View Cameras with\n  Negative Imaging Plane on Mobile Agents",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.14116v2": {
            "Paper Title": "Coordination of Drones at Scale: Decentralized Energy-aware Swarm\n  Intelligence for Spatio-temporal Sensing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.07758v2": {
            "Paper Title": "Game-theoretic Objective Space Planning",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": "approximates an\nadvantage-like function as a proxy for regret in a single agent\nsetting. Brown et al. ",
                    "Citation Text": "Noam Brown, Adam Lerer, Sam Gross, and Tuomas Sandholm. 2018. Deep\nCounterfactual Regret Minimization. (Nov. 2018). https://doi.org/10.48550/arXiv.\n1811.00164",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.00164",
                        "Citation Paper Title": "Title:Deep Counterfactual Regret Minimization",
                        "Citation Paper Abstract": "Abstract:Counterfactual Regret Minimization (CFR) is the leading framework for solving large imperfect-information games. It converges to an equilibrium by iteratively traversing the game tree. In order to deal with extremely large games, abstraction is typically applied before running CFR. The abstracted game is solved with tabular CFR, and its solution is mapped back to the full game. This process can be problematic because aspects of abstraction are often manual and domain specific, abstraction algorithms may miss important strategic nuances of the game, and there is a chicken-and-egg problem because determining a good abstraction requires knowledge of the equilibrium of the game. This paper introduces Deep Counterfactual Regret Minimization, a form of CFR that obviates the need for abstraction by instead using deep neural networks to approximate the behavior of CFR in the full game. We show that Deep CFR is principled and achieves strong performance in large poker games. This is the first non-tabular variant of CFR to be successful in large games.",
                        "Citation Paper Authors": "Authors:Noam Brown, Adam Lerer, Sam Gross, Tuomas Sandholm"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "learns the latent dynamics of complex dynamic\nsystems and trains agents in latent imagination for traditionally\ndifficult control tasks. Schwarting et al. ",
                    "Citation Text": "Wilko Schwarting, Tim Seyde, Igor Gilitschenski, Lucas Liebenwein, Ryan Sander,\nSertac Karaman, and Daniela Rus. 2021. Deep Latent Competition: Learning to\nRace Using Visual Control Policies in Latent Space. https://doi.org/10.48550/\narXiv.2102.09812 arXiv:2102.09812 [cs].",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.09812",
                        "Citation Paper Title": "Title:Deep Latent Competition: Learning to Race Using Visual Control Policies in Latent Space",
                        "Citation Paper Abstract": "Abstract:Learning competitive behaviors in multi-agent settings such as racing requires long-term reasoning about potential adversarial interactions. This paper presents Deep Latent Competition (DLC), a novel reinforcement learning algorithm that learns competitive visual control policies through self-play in imagination. The DLC agent imagines multi-agent interaction sequences in the compact latent space of a learned world model that combines a joint transition function with opponent viewpoint prediction. Imagined self-play reduces costly sample generation in the real world, while the latent representation enables planning to scale gracefully with observation dimensionality. We demonstrate the effectiveness of our algorithm in learning competitive behaviors on a novel multi-agent racing benchmark that requires planning from image observations. Code and videos available at this https URL.",
                        "Citation Paper Authors": "Authors:Wilko Schwarting, Tim Seyde, Igor Gilitschenski, Lucas Liebenwein, Ryan Sander, Sertac Karaman, Daniela Rus"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ". These approaches allow the agents to train themselves\ninside \u201challucinated dreams\" generated by these models. Similarly,\nHafner et al. ",
                    "Citation Text": "Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. 2020.\nDream to Control: Learning Behaviors by Latent Imagination. https://doi.org/\n10.48550/arXiv.1912.01603 arXiv:1912.01603 [cs].",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.01603",
                        "Citation Paper Title": "Title:Dream to Control: Learning Behaviors by Latent Imagination",
                        "Citation Paper Abstract": "Abstract:Learned world models summarize an agent's experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.",
                        "Citation Paper Authors": "Authors:Danijar Hafner, Timothy Lillicrap, Jimmy Ba, Mohammad Norouzi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.03729v2": {
            "Paper Title": "Flexible Attention-Based Multi-Policy Fusion for Efficient Deep\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.10765v2": {
            "Paper Title": "A Low-Cost Lane-Following Algorithm for Cyber-Physical Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.13175v2": {
            "Paper Title": "Coordination of multiple mobile manipulators for ordered sorting of\n  cluttered objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.01069v2": {
            "Paper Title": "Image-based Navigation in Real-World Environments via Multiple Mid-level\n  Representations: Fusion Models, Benchmark and Efficient Evaluation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.12000v2": {
            "Paper Title": "Data-driven Predictive Tracking Control based on Koopman Operators",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": ".\nThe advantage of this form of tracking MPC is that including\na decision variable of optimal reachable outputs (determined\nonline) improves recursive feasibility and guarantees conver-\ngence close to the specified reference. In ",
                    "Citation Text": "J. Berberich, J. K \u00a8ohler, M. A. Muller, and F. Allgower, \u201cLinear tracking\nMPC for nonlinear systems part i: The model-based case,\u201d IEEE\nTransactions on Automatic Control , vol. 67, no. 9, pp. 4390\u20134405, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.08560",
                        "Citation Paper Title": "Title:Linear tracking MPC for nonlinear systems Part I: The model-based case",
                        "Citation Paper Abstract": "Abstract:We develop a tracking model predictive control (MPC) scheme for nonlinear systems using the linearized dynamics at the current state as a prediction model. Under reasonable assumptions on the linearized dynamics, we prove that the proposed MPC scheme exponentially stabilizes the optimal reachable equilibrium w.r.t. a desired target setpoint. Our theoretical results rely on the fact that, close to the steady-state manifold, the prediction error of the linearization is small and hence, we can slide along the steady-state manifold towards the optimal reachable equilibrium. The closed-loop stability properties mainly depend on a cost matrix which allows us to trade off performance, robustness, and the size of the region of attraction. In an application to a nonlinear continuous stirred tank reactor, we show that the scheme, which only requires solving a convex quadratic program online, has comparable performance to a nonlinear MPC scheme while being computationally significantly more efficient. Further, our results provide the basis for controlling nonlinear systems based on data-dependent linear prediction models, which we explore in our companion paper.",
                        "Citation Paper Authors": "Authors:Julian Berberich, Johannes K\u00f6hler, Matthias A. M\u00fcller, Frank Allg\u00f6wer"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "has been attracting at-\ntention as a modeling technique for known and unknown\nnonlinear dynamics, primarily through a process of expressing\na nonlinear system as a lifted linear model with nonlinear\nbasis functions, see e.g., ",
                    "Citation Text": "S. L. Brunton, B. W. Brunton, J. L. Proctor, and J. N. Kutz, \u201cKoop-\nman invariant subspaces and finite linear representations of nonlinear\ndynamical systems for control,\u201d PloS one , vol. 11, no. 2, p. e0150171,\n2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1510.03007",
                        "Citation Paper Title": "Title:Koopman invariant subspaces and finite linear representations of nonlinear dynamical systems for control",
                        "Citation Paper Abstract": "Abstract:In this work, we explore finite-dimensional linear representations of nonlinear dynamical systems by restricting the Koopman operator to an invariant subspace. The Koopman operator is an infinite-dimensional linear operator that evolves observable functions of the state-space of a dynamical system [Koopman 1931, PNAS]. Dominant terms in the Koopman expansion are typically computed using dynamic mode decomposition (DMD). DMD uses linear measurements of the state variables, and it has recently been shown that this may be too restrictive for nonlinear systems [Williams et al. 2015, JNLS]. Choosing nonlinear observable functions to form an invariant subspace where it is possible to obtain linear models, especially those that are useful for control, is an open challenge.\nHere, we investigate the choice of observable functions for Koopman analysis that enable the use of optimal linear control techniques on nonlinear problems. First, to include a cost on the state of the system, as in linear quadratic regulator (LQR) control, it is helpful to include these states in the observable subspace, as in DMD. However, we find that this is only possible when there is a single isolated fixed point, as systems with multiple fixed points or more complicated attractors are not globally topologically conjugate to a finite-dimensional linear system, and cannot be represented by a finite-dimensional linear Koopman subspace that includes the state. We then present a data-driven strategy to identify relevant observable functions for Koopman analysis using a new algorithm to determine terms in a dynamical system by sparse regression of the data in a nonlinear function space [Brunton et al. 2015, arxiv]; we show how this algorithm is related to DMD. Finally, we demonstrate how to design optimal control laws for nonlinear systems using techniques from linear optimal control on Koopman invariant subspaces.",
                        "Citation Paper Authors": "Authors:Steven L. Brunton, Bingni W. Brunton, Joshua L. Proctor, J. Nathan Kutz"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.01774v2": {
            "Paper Title": "ElasticROS: An Elastically Collaborative Robot Operation System for Fog\n  and Cloud Robotics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.02685v3": {
            "Paper Title": "A Real2Sim2Real Method for Robust Object Grasping with Neural Surface\n  Reconstruction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.12114v2": {
            "Paper Title": "An Optical Control Environment for Benchmarking Reinforcement Learning\n  Algorithms",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.09997v2": {
            "Paper Title": "Bag All You Need: Learning a Generalizable Bagging Strategy for\n  Heterogeneous Objects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.01672v2": {
            "Paper Title": "Bringing robotics taxonomies to continuous domains via GPLVM on\n  hyperbolic manifolds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.13091v2": {
            "Paper Title": "WaterNeRF: Neural Radiance Fields for Underwater Scenes",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": ".We specifically leverage mip-NeRF, which replaces rays with\nconical frustums to avoid aliasing issues ",
                    "Citation Text": "J. T. Barron, B. Mildenhall, M. Tancik, P. Hedman, R. Martin-Brualla,\nand P. P. Srinivasan, \u201cMip-nerf: A multiscale representation for anti-\naliasing neural radiance fields,\u201d ICCV , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.13415",
                        "Citation Paper Title": "Title:Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields",
                        "Citation Paper Abstract": "Abstract:The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call \"mip-NeRF\" (a la \"mipmap\"), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF's ability to represent fine details, while also being 7% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with NeRF and by 60% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22x faster.",
                        "Citation Paper Authors": "Authors:Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, Pratul P. Srinivasan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.09337v3": {
            "Paper Title": "CoBRA: A Composable Benchmark for Robotics Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.09916v3": {
            "Paper Title": "Online Distribution Shift Detection via Recency Prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.06129v3": {
            "Paper Title": "Safe Reinforcement Learning with Probabilistic Guarantees Satisfying\n  Temporal Logic Specifications in Continuous Action Spaces",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": ".\nProbabilistic Controller Verification As expressed in Sec-\ntion 3 in ",
                    "Citation Text": "A. Corso, R. Moss, M. Koren, R. Lee, and M. Kochenderfer, \u201cA surveyof algorithms for black-box safety validation of cyber-physical systems,\u201d\nJournal of Artificial Intelligence Research , vol. 72, pp. 377\u2013428, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.02979",
                        "Citation Paper Title": "Title:A Survey of Algorithms for Black-Box Safety Validation of Cyber-Physical Systems",
                        "Citation Paper Abstract": "Abstract:Autonomous cyber-physical systems (CPS) can improve safety and efficiency for safety-critical applications, but require rigorous testing before deployment. The complexity of these systems often precludes the use of formal verification and real-world testing can be too dangerous during development. Therefore, simulation-based techniques have been developed that treat the system under test as a black box operating in a simulated environment. Safety validation tasks include finding disturbances in the environment that cause the system to fail (falsification), finding the most-likely failure, and estimating the probability that the system fails. Motivated by the prevalence of safety-critical artificial intelligence, this work provides a survey of state-of-the-art safety validation techniques for CPS with a focus on applied algorithms and their modifications for the safety validation problem. We present and discuss algorithms in the domains of optimization, path planning, reinforcement learning, and importance sampling. Problem decomposition techniques are presented to help scale algorithms to large state spaces, which are common for CPS. A brief overview of safety-critical applications is given, including autonomous vehicles and aircraft collision avoidance systems. Finally, we present a survey of existing academic and commercially available safety validation tools.",
                        "Citation Paper Authors": "Authors:Anthony Corso, Robert J. Moss, Mark Koren, Ritchie Lee, Mykel J. Kochenderfer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.00982v2": {
            "Paper Title": "Assuring Safety of Vision-Based Swarm Formation Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.05324v4": {
            "Paper Title": "Delving into the Devils of Bird's-eye-view Perception: A Review,\n  Evaluation and Recipe",
            "Sentences": [
                {
                    "Sentence ID": 49,
                    "Sentence": "arXiv 2022 MC ODet \u2717 nuS Implicit BEV Pos Embed\nBEVDepth ",
                    "Citation Text": "Y. Li, Z. Ge, G. Yu, J. Yang, Z. Wang, Y. Shi, J. Sun, and\nZ. Li, \u201cBEVDepth: Acquisition of reliable depth for multi-\nview 3d object detection,\u201d arXiv preprint arXiv:2206.10092 ,\n2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.10092",
                        "Citation Paper Title": "Title:BEVDepth: Acquisition of Reliable Depth for Multi-view 3D Object Detection",
                        "Citation Paper Abstract": "Abstract:In this research, we propose a new 3D object detector with a trustworthy depth estimation, dubbed BEVDepth, for camera-based Bird's-Eye-View (BEV) 3D object detection. Our work is based on a key observation -- depth estimation in recent approaches is surprisingly inadequate given the fact that depth is essential to camera 3D detection. Our BEVDepth resolves this by leveraging explicit depth supervision. A camera-awareness depth estimation module is also introduced to facilitate the depth predicting capability. Besides, we design a novel Depth Refinement Module to counter the side effects carried by imprecise feature unprojection. Aided by customized Efficient Voxel Pooling and multi-frame mechanism, BEVDepth achieves the new state-of-the-art 60.9% NDS on the challenging nuScenes test set while maintaining high efficiency. For the first time, the NDS score of a camera model reaches 60%.",
                        "Citation Paper Authors": "Authors:Yinhao Li, Zheng Ge, Guanyi Yu, Jinrong Yang, Zengran Wang, Yukang Shi, Jianjian Sun, Zeming Li"
                    }
                },
                {
                    "Sentence ID": 117,
                    "Sentence": "[44, 66, 68, 84, 106, 107] [66, 67, 69, 108, 109]\n[45, 110, 111, 112, 113, 114, 115, 116] ",
                    "Citation Text": "K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual\nlearning for image recognition,\u201d in CVPR , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.03385",
                        "Citation Paper Title": "Title:Deep Residual Learning for Image Recognition",
                        "Citation Paper Abstract": "Abstract:Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
                        "Citation Paper Authors": "Authors:Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"
                    }
                },
                {
                    "Sentence ID": 132,
                    "Sentence": "can be\nemployed as the image backbone. The neck could be\nFPN ",
                    "Citation Text": "T.-Y. Lin, P . Doll \u00b4ar, R. B. Girshick, K. He, B. Hariharan,\nand S. J. Belongie, \u201cFeature pyramid networks for object\ndetection,\u201d CVPR , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.03144",
                        "Citation Paper Title": "Title:Feature Pyramid Networks for Object Detection",
                        "Citation Paper Abstract": "Abstract:Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.",
                        "Citation Paper Authors": "Authors:Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, Serge Belongie"
                    }
                },
                {
                    "Sentence ID": 64,
                    "Sentence": "-\u2713 -\u2713 - - - - 0.552 0.422 - - - - -\nBEVDet4D ",
                    "Citation Text": "J. Huang and G. Huang, \u201cBEVDet4D: Exploit temporal\ncues in multi-camera 3d object detection,\u201d arXiv preprint\narXiv:2203.17054 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.17054",
                        "Citation Paper Title": "Title:BEVDet4D: Exploit Temporal Cues in Multi-camera 3D Object Detection",
                        "Citation Paper Abstract": "Abstract:Single frame data contains finite information which limits the performance of the existing vision-based multi-camera 3D object detection paradigms. For fundamentally pushing the performance boundary in this area, a novel paradigm dubbed BEVDet4D is proposed to lift the scalable BEVDet paradigm from the spatial-only 3D space to the spatial-temporal 4D space. We upgrade the naive BEVDet framework with a few modifications just for fusing the feature from the previous frame with the corresponding one in the current frame. In this way, with negligible additional computing budget, we enable BEVDet4D to access the temporal cues by querying and comparing the two candidate features. Beyond this, we simplify the task of velocity prediction by removing the factors of ego-motion and time in the learning target. As a result, BEVDet4D with robust generalization performance reduces the velocity error by up to -62.9%. This makes the vision-based methods, for the first time, become comparable with those relied on LiDAR or radar in this aspect. On challenge benchmark nuScenes, we report a new record of 54.5% NDS with the high-performance configuration dubbed BEVDet4D-Base, which surpasses the previous leading method BEVDet-Base by +7.3% NDS. The source code is publicly available for further research at this https URL .",
                        "Citation Paper Authors": "Authors:Junjie Huang, Guan Huang"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "ECCV 2022 MC/T ODet/MapSeg \u2717 nuS/WOD Transformer for BEV feature\nBEVFusion ",
                    "Citation Text": "Z. Liu, H. Tang, A. Amini, X. Yang, H. Mao, D. Rus, and\nS. Han, \u201cBEVFusion: Multi-task multi-sensor fusion with\nunified bird\u2019s-eye view representation,\u201d arXiv preprint\narXiv:2205.13542 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.13542",
                        "Citation Paper Title": "Title:BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation",
                        "Citation Paper Abstract": "Abstract:Multi-sensor fusion is essential for an accurate and reliable autonomous driving system. Recent approaches are based on point-level fusion: augmenting the LiDAR point cloud with camera features. However, the camera-to-LiDAR projection throws away the semantic density of camera features, hindering the effectiveness of such methods, especially for semantic-oriented tasks (such as 3D scene segmentation). In this paper, we break this deeply-rooted convention with BEVFusion, an efficient and generic multi-task multi-sensor fusion framework. It unifies multi-modal features in the shared bird's-eye view (BEV) representation space, which nicely preserves both geometric and semantic information. To achieve this, we diagnose and lift key efficiency bottlenecks in the view transformation with optimized BEV pooling, reducing latency by more than 40x. BEVFusion is fundamentally task-agnostic and seamlessly supports different 3D perception tasks with almost no architectural changes. It establishes the new state of the art on nuScenes, achieving 1.3% higher mAP and NDS on 3D object detection and 13.6% higher mIoU on BEV map segmentation, with 1.9x lower computation cost. Code to reproduce our results is available at this https URL.",
                        "Citation Paper Authors": "Authors:Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang, Huizi Mao, Daniela Rus, Song Han"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "CVPR 2018 L ODet - KITTI Implicit voxel grids transformed to BEV\nPointPillars ",
                    "Citation Text": "A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and\nO. Beijbom, \u201cPointpillars: Fast encoders for object detec-\ntion from point clouds,\u201d in CVPR , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.05784",
                        "Citation Paper Title": "Title:PointPillars: Fast Encoders for Object Detection from Point Clouds",
                        "Citation Paper Abstract": "Abstract:Object detection in point clouds is an important aspect of many robotics applications such as autonomous driving. In this paper we consider the problem of encoding a point cloud into a format appropriate for a downstream detection pipeline. Recent literature suggests two types of encoders; fixed encoders tend to be fast but sacrifice accuracy, while encoders that are learned from data are more accurate, but slower. In this work we propose PointPillars, a novel encoder which utilizes PointNets to learn a representation of point clouds organized in vertical columns (pillars). While the encoded features can be used with any standard 2D convolutional detection architecture, we further propose a lean downstream network. Extensive experimentation shows that PointPillars outperforms previous encoders with respect to both speed and accuracy by a large margin. Despite only using lidar, our full detection pipeline significantly outperforms the state of the art, even among fusion methods, with respect to both the 3D and bird's eye view KITTI benchmarks. This detection performance is achieved while running at 62 Hz: a 2 - 4 fold runtime improvement. A faster version of our method matches the state of the art at 105 Hz. These benchmarks suggest that PointPillars is an appropriate encoding for object detection in point clouds.",
                        "Citation Paper Authors": "Authors:Alex H. Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, Oscar Beijbom"
                    }
                },
                {
                    "Sentence ID": 68,
                    "Sentence": "- -\u2713\u2713 - - - - 0.673 0.603 - - - - 0.7193\nSST ",
                    "Citation Text": "L. Fan, Z. Pang, T. Zhang, Y.-X. Wang, H. Zhao, F. Wang,\nN. Wang, and Z. Zhang, \u201cEmbracing single stride 3d\nobject detector with sparse transformer,\u201d in CVPR , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.06375",
                        "Citation Paper Title": "Title:Embracing Single Stride 3D Object Detector with Sparse Transformer",
                        "Citation Paper Abstract": "Abstract:In LiDAR-based 3D object detection for autonomous driving, the ratio of the object size to input scene size is significantly smaller compared to 2D detection cases. Overlooking this difference, many 3D detectors directly follow the common practice of 2D detectors, which downsample the feature maps even after quantizing the point clouds. In this paper, we start by rethinking how such multi-stride stereotype affects the LiDAR-based 3D object detectors. Our experiments point out that the downsampling operations bring few advantages, and lead to inevitable information loss. To remedy this issue, we propose Single-stride Sparse Transformer (SST) to maintain the original resolution from the beginning to the end of the network. Armed with transformers, our method addresses the problem of insufficient receptive field in single-stride architectures. It also cooperates well with the sparsity of point clouds and naturally avoids expensive computation. Eventually, our SST achieves state-of-the-art results on the large scale Waymo Open Dataset. It is worth mentioning that our method can achieve exciting performance (83.8 LEVEL 1 AP on validation split) on small object (pedestrian) detection due to the characteristic of single stride. Codes will be released at this https URL",
                        "Citation Paper Authors": "Authors:Lue Fan, Ziqi Pang, Tianyuan Zhang, Yu-Xiong Wang, Hang Zhao, Feng Wang, Naiyan Wang, Zhaoxiang Zhang"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": "arXiv 2022 MC ODet \u2717 nuS BEV space data augmentation\nPETR ",
                    "Citation Text": "Y. Liu, T. Wang, X. Zhang, and J. Sun, \u201cPetr: Position\nembedding transformation for multi-view 3d object de-\ntection,\u201d arXiv preprint arXiv:2203.05625 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.05625",
                        "Citation Paper Title": "Title:PETR: Position Embedding Transformation for Multi-View 3D Object Detection",
                        "Citation Paper Abstract": "Abstract:In this paper, we develop position embedding transformation (PETR) for multi-view 3D object detection. PETR encodes the position information of 3D coordinates into image features, producing the 3D position-aware features. Object query can perceive the 3D position-aware features and perform end-to-end object detection. PETR achieves state-of-the-art performance (50.4% NDS and 44.1% mAP) on standard nuScenes dataset and ranks 1st place on the benchmark. It can serve as a simple yet strong baseline for future research. Code is available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Yingfei Liu, Tiancai Wang, Xiangyu Zhang, Jian Sun"
                    }
                },
                {
                    "Sentence ID": 55,
                    "Sentence": "CVPR 2022 MC MapSeg \u2717 nuS Camera Intrinsics BEV Projection\nHDMapNet ",
                    "Citation Text": "Q. Li, Y. Wang, Y. Wang, and H. Zhao, \u201cHdmapnet: An\nonline hd map construction and evaluation framework,\u201d\ninICRA , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.06307",
                        "Citation Paper Title": "Title:HDMapNet: An Online HD Map Construction and Evaluation Framework",
                        "Citation Paper Abstract": "Abstract:Constructing HD semantic maps is a central component of autonomous driving. However, traditional pipelines require a vast amount of human efforts and resources in annotating and maintaining the semantics in the map, which limits its scalability. In this paper, we introduce the problem of HD semantic map learning, which dynamically constructs the local semantics based on onboard sensor observations. Meanwhile, we introduce a semantic map learning method, dubbed HDMapNet. HDMapNet encodes image features from surrounding cameras and/or point clouds from LiDAR, and predicts vectorized map elements in the bird's-eye view. We benchmark HDMapNet on nuScenes dataset and show that in all settings, it performs better than baseline methods. Of note, our camera-LiDAR fusion-based HDMapNet outperforms existing methods by more than 50% in all metrics. In addition, we develop semantic-level and instance-level metrics to evaluate the map learning performance. Finally, we showcase our method is capable of predicting a locally consistent map. By introducing the method and metrics, we invite the community to study this novel map learning problem.",
                        "Citation Paper Authors": "Authors:Qi Li, Yue Wang, Yilun Wang, Hang Zhao"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": ".\nMethod Venue Input Modality Task Depth Supervision Dataset Contribution\nOFT ",
                    "Citation Text": "T. Roddick, A. Kendall, and R. Cipolla, \u201cOrthographic\nfeature transform for monocular 3d object detection,\u201d in\nBritish Machine Vision Conference , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.08188",
                        "Citation Paper Title": "Title:Orthographic Feature Transform for Monocular 3D Object Detection",
                        "Citation Paper Abstract": "Abstract:3D object detection from monocular images has proven to be an enormously challenging task, with the performance of leading systems not yet achieving even 10\\% of that of LiDAR-based counterparts. One explanation for this performance gap is that existing systems are entirely at the mercy of the perspective image-based representation, in which the appearance and scale of objects varies drastically with depth and meaningful distances are difficult to infer. In this work we argue that the ability to reason about the world in 3D is an essential element of the 3D object detection task. To this end, we introduce the orthographic feature transform, which enables us to escape the image domain by mapping image-based features into an orthographic 3D space. This allows us to reason holistically about the spatial configuration of the scene in a domain where scale is consistent and distances between objects are meaningful. We apply this transformation as part of an end-to-end deep learning architecture and achieve state-of-the-art performance on the KITTI 3D object benchmark.\\footnote{We will release full source code and pretrained models upon acceptance of this manuscript for publication.",
                        "Citation Paper Authors": "Authors:Thomas Roddick, Alex Kendall, Roberto Cipolla"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "ECCV 2022 SC ODet \u2713 KITTI Motion to Depth to Voxel to BEV\nBEVDet ",
                    "Citation Text": "J. Huang, G. Huang, Z. Zhu, and D. Du, \u201cBEVDet: High-\nperformance multi-camera 3d object detection in bird-\neye-view,\u201d arXiv preprint arXiv:2112.11790 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.11790",
                        "Citation Paper Title": "Title:BEVDet: High-performance Multi-camera 3D Object Detection in Bird-Eye-View",
                        "Citation Paper Abstract": "Abstract:Autonomous driving perceives its surroundings for decision making, which is one of the most complex scenarios in visual perception. The success of paradigm innovation in solving the 2D object detection task inspires us to seek an elegant, feasible, and scalable paradigm for fundamentally pushing the performance boundary in this area. To this end, we contribute the BEVDet paradigm in this paper. BEVDet performs 3D object detection in Bird-Eye-View (BEV), where most target values are defined and route planning can be handily performed. We merely reuse existing modules to build its framework but substantially develop its performance by constructing an exclusive data augmentation strategy and upgrading the Non-Maximum Suppression strategy. In the experiment, BEVDet offers an excellent trade-off between accuracy and time-efficiency. As a fast version, BEVDet-Tiny scores 31.2% mAP and 39.2% NDS on the nuScenes val set. It is comparable with FCOS3D, but requires just 11% computational budget of 215.3 GFLOPs and runs 9.2 times faster at 15.6 FPS. Another high-precision version dubbed BEVDet-Base scores 39.3% mAP and 47.2% NDS, significantly exceeding all published results. With a comparable inference speed, it surpasses FCOS3D by a large margin of +9.8% mAP and +10.0% NDS. The source code is publicly available for further research at this https URL .",
                        "Citation Paper Authors": "Authors:Junjie Huang, Guan Huang, Zheng Zhu, Yun Ye, Dalong Du"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "CVPR 2019 L ODet - KITTI Voxelization with pillars as BEV\nCaDDN ",
                    "Citation Text": "C. Reading, A. Harakeh, J. Chae, and S. L. Waslander,\n\u201cCategorical depth distribution network for monocular\n3d object detection,\u201d in CVPR , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.01100",
                        "Citation Paper Title": "Title:Categorical Depth Distribution Network for Monocular 3D Object Detection",
                        "Citation Paper Abstract": "Abstract:Monocular 3D object detection is a key problem for autonomous vehicles, as it provides a solution with simple configuration compared to typical multi-sensor systems. The main challenge in monocular 3D detection lies in accurately predicting object depth, which must be inferred from object and scene cues due to the lack of direct range measurement. Many methods attempt to directly estimate depth to assist in 3D detection, but show limited performance as a result of depth inaccuracy. Our proposed solution, Categorical Depth Distribution Network (CaDDN), uses a predicted categorical depth distribution for each pixel to project rich contextual feature information to the appropriate depth interval in 3D space. We then use the computationally efficient bird's-eye-view projection and single-stage detector to produce the final output bounding boxes. We design CaDDN as a fully differentiable end-to-end approach for joint depth estimation and object detection. We validate our approach on the KITTI 3D object detection benchmark, where we rank 1st among published monocular methods. We also provide the first monocular 3D detection results on the newly released Waymo Open Dataset. We provide a code release for CaDDN which is made available.",
                        "Citation Paper Authors": "Authors:Cody Reading, Ali Harakeh, Julia Chae, Steven L. Waslander"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "2018 AS 103 2.5 29k 144k 144k 70k \u2020 - \u2713 - 200+\nOpenLane ",
                    "Citation Text": "L. Chen, C. Sima, Y. Li, Z. Zheng, J. Xu, X. Geng, H. Li,\nC. He, J. Shi, Y. Qiao, and J. Yan, \u201cPersFormer: 3d lane\ndetection via perspective transformer and the openlane\nbenchmark,\u201d arXiv preprint arXiv:2203.11089 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.11089",
                        "Citation Paper Title": "Title:PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark",
                        "Citation Paper Abstract": "Abstract:Methods for 3D lane detection have been recently proposed to address the issue of inaccurate lane layouts in many autonomous driving scenarios (uphill/downhill, bump, etc.). Previous work struggled in complex cases due to their simple designs of the spatial transformation between front view and bird's eye view (BEV) and the lack of a realistic dataset. Towards these issues, we present PersFormer: an end-to-end monocular 3D lane detector with a novel Transformer-based spatial feature transformation module. Our model generates BEV features by attending to related front-view local regions with camera parameters as a reference. PersFormer adopts a unified 2D/3D anchor design and an auxiliary task to detect 2D/3D lanes simultaneously, enhancing the feature consistency and sharing the benefits of multi-task learning. Moreover, we release one of the first large-scale real-world 3D lane datasets: OpenLane, with high-quality annotation and scenario diversity. OpenLane contains 200,000 frames, over 880,000 instance-level lanes, 14 lane categories, along with scene tags and the closed-in-path object annotations to encourage the development of lane detection and more industrial-related autonomous driving methods. We show that PersFormer significantly outperforms competitive baselines in the 3D lane detection task on our new OpenLane dataset as well as Apollo 3D Lane Synthetic dataset, and is also on par with state-of-the-art algorithms in the 2D task on OpenLane. The project page is available at this https URL and OpenLane dataset is provided at this https URL.",
                        "Citation Paper Authors": "Authors:Li Chen, Chonghao Sima, Yang Li, Zehan Zheng, Jiajie Xu, Xiangwei Geng, Hongyang Li, Conghui He, Jianping Shi, Yu Qiao, Junchi Yan"
                    }
                },
                {
                    "Sentence ID": 75,
                    "Sentence": "-\u2713\u2713\u2713 - - - - 0.705 0.664 - - - - -\nAutoAlign ",
                    "Citation Text": "Z. Chen, Z. Li, S. Zhang, L. Fang, Q. Jiang, F. Zhao,\nB. Zhou, and H. Zhao, \u201cAutoAlign: Pixel-instance feature\naggregation for multi-modal 3d object detection,\u201d arXiv\npreprint arXiv:2201.06493 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.06493",
                        "Citation Paper Title": "Title:AutoAlign: Pixel-Instance Feature Aggregation for Multi-Modal 3D Object Detection",
                        "Citation Paper Abstract": "Abstract:Object detection through either RGB images or the LiDAR point clouds has been extensively explored in autonomous driving. However, it remains challenging to make these two data sources complementary and beneficial to each other. In this paper, we propose \\textit{AutoAlign}, an automatic feature fusion strategy for 3D object detection. Instead of establishing deterministic correspondence with camera projection matrix, we model the mapping relationship between the image and point clouds with a learnable alignment map. This map enables our model to automate the alignment of non-homogenous features in a dynamic and data-driven manner. Specifically, a cross-attention feature alignment module is devised to adaptively aggregate \\textit{pixel-level} image features for each voxel. To enhance the semantic consistency during feature alignment, we also design a self-supervised cross-modal feature interaction module, through which the model can learn feature aggregation with \\textit{instance-level} feature guidance. Extensive experimental results show that our approach can lead to 2.3 mAP and 7.0 mAP improvements on the KITTI and nuScenes datasets, respectively. Notably, our best model reaches 70.9 NDS on the nuScenes testing leaderboard, achieving competitive performance among various state-of-the-arts.",
                        "Citation Paper Authors": "Authors:Zehui Chen, Zhenyu Li, Shiquan Zhang, Liangji Fang, Qinghong Jiang, Feng Zhao, Bolei Zhou, Hang Zhao"
                    }
                },
                {
                    "Sentence ID": 74,
                    "Sentence": "-\u2713\u2713\u2713 - - - - 0.581 0.464 - - - - -\nMVP ",
                    "Citation Text": "T. Yin, X. Zhou, and P . Kr \u00a8ahenb \u00a8uhl, \u201cMultimodal virtual\npoint 3d detection,\u201d NeurIPS , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.06881",
                        "Citation Paper Title": "Title:Multimodal Virtual Point 3D Detection",
                        "Citation Paper Abstract": "Abstract:Lidar-based sensing drives current autonomous vehicles. Despite rapid progress, current Lidar sensors still lag two decades behind traditional color cameras in terms of resolution and cost. For autonomous driving, this means that large objects close to the sensors are easily visible, but far-away or small objects comprise only one measurement or two. This is an issue, especially when these objects turn out to be driving hazards. On the other hand, these same objects are clearly visible in onboard RGB sensors. In this work, we present an approach to seamlessly fuse RGB sensors into Lidar-based 3D recognition. Our approach takes a set of 2D detections to generate dense 3D virtual points to augment an otherwise sparse 3D point cloud. These virtual points naturally integrate into any standard Lidar-based 3D detectors along with regular Lidar measurements. The resulting multi-modal detector is simple and effective. Experimental results on the large-scale nuScenes dataset show that our framework improves a strong CenterPoint baseline by a significant 6.6 mAP, and outperforms competing fusion approaches. Code and more visualizations are available at this https URL",
                        "Citation Paper Authors": "Authors:Tianwei Yin, Xingyi Zhou, Philipp Kr\u00e4henb\u00fchl"
                    }
                },
                {
                    "Sentence ID": 70,
                    "Sentence": "- -\u2713\u2713 - - - - 0.685 0.624 - - - - 0.7312\nPV-RCNN++ ",
                    "Citation Text": "S. Shi, L. Jiang, J. Deng, Z. Wang, C. Guo, J. Shi, X. Wang,\nand H. Li, \u201cPV-RCNN++: Point-voxel feature set ab-\nstraction with local vector representation for 3d object\ndetection,\u201d arXiv preprint arXiv:2102.00463 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.00463",
                        "Citation Paper Title": "Title:PV-RCNN++: Point-Voxel Feature Set Abstraction With Local Vector Representation for 3D Object Detection",
                        "Citation Paper Abstract": "Abstract:3D object detection is receiving increasing attention from both industry and academia thanks to its wide applications in various fields. In this paper, we propose Point-Voxel Region-based Convolution Neural Networks (PV-RCNNs) for 3D object detection on point clouds. First, we propose a novel 3D detector, PV-RCNN, which boosts the 3D detection performance by deeply integrating the feature learning of both point-based set abstraction and voxel-based sparse convolution through two novel steps, i.e., the voxel-to-keypoint scene encoding and the keypoint-to-grid RoI feature abstraction. Second, we propose an advanced framework, PV-RCNN++, for more efficient and accurate 3D object detection. It consists of two major improvements: sectorized proposal-centric sampling for efficiently producing more representative keypoints, and VectorPool aggregation for better aggregating local point features with much less resource consumption. With these two strategies, our PV-RCNN++ is about $3\\times$ faster than PV-RCNN, while also achieving better performance. The experiments demonstrate that our proposed PV-RCNN++ framework achieves state-of-the-art 3D detection performance on the large-scale and highly-competitive Waymo Open Dataset with 10 FPS inference speed on the detection range of 150m * 150m.",
                        "Citation Paper Authors": "Authors:Shaoshuai Shi, Li Jiang, Jiajun Deng, Zhe Wang, Chaoxu Guo, Jianping Shi, Xiaogang Wang, Hongsheng Li"
                    }
                },
                {
                    "Sentence ID": 60,
                    "Sentence": "ICCV 2019 SC LDet \u2717 OpenLane IPM Projection to BEV space\nSTSU ",
                    "Citation Text": "Y. B. Can, A. Liniger, D. P . Paudel, and L. Van Gool,\n\u201cStructured bird\u2019s-eye-view traffic scene understanding\nfrom onboard images,\u201d in ICCV , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.01997",
                        "Citation Paper Title": "Title:Structured Bird's-Eye-View Traffic Scene Understanding from Onboard Images",
                        "Citation Paper Abstract": "Abstract:Autonomous navigation requires structured representation of the road network and instance-wise identification of the other traffic agents. Since the traffic scene is defined on the ground plane, this corresponds to scene understanding in the bird's-eye-view (BEV). However, the onboard cameras of autonomous cars are customarily mounted horizontally for a better view of the surrounding, making this task very challenging. In this work, we study the problem of extracting a directed graph representing the local road network in BEV coordinates, from a single onboard camera image. Moreover, we show that the method can be extended to detect dynamic objects on the BEV plane. The semantics, locations, and orientations of the detected objects together with the road graph facilitates a comprehensive understanding of the scene. Such understanding becomes fundamental for the downstream tasks, such as path planning and navigation. We validate our approach against powerful baselines and show that our network achieves superior performance. We also demonstrate the effects of various design choices through ablation studies. Code: this https URL",
                        "Citation Paper Authors": "Authors:Yigit Baran Can, Alexander Liniger, Danda Pani Paudel, Luc Van Gool"
                    }
                },
                {
                    "Sentence ID": 58,
                    "Sentence": "ECCV 2020 MC MapSeg/Plan \u2717 nuS/Lfyt First Depth Distribution\nST-P3 ",
                    "Citation Text": "S. Hu, L. Chen, P . Wu, H. Li, J. Yan, and D. Tao,\n\u201cST-P3: End-to-end vision-based autonomous driving\nvia spatial-temporal feature learning,\u201d arXiv preprint\narXiv:2207.07601 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2207.07601",
                        "Citation Paper Title": "Title:ST-P3: End-to-end Vision-based Autonomous Driving via Spatial-Temporal Feature Learning",
                        "Citation Paper Abstract": "Abstract:Many existing autonomous driving paradigms involve a multi-stage discrete pipeline of tasks. To better predict the control signals and enhance user safety, an end-to-end approach that benefits from joint spatial-temporal feature learning is desirable. While there are some pioneering works on LiDAR-based input or implicit design, in this paper we formulate the problem in an interpretable vision-based setting. In particular, we propose a spatial-temporal feature learning scheme towards a set of more representative features for perception, prediction and planning tasks simultaneously, which is called ST-P3. Specifically, an egocentric-aligned accumulation technique is proposed to preserve geometry information in 3D space before the bird's eye view transformation for perception; a dual pathway modeling is devised to take past motion variations into account for future prediction; a temporal-based refinement unit is introduced to compensate for recognizing vision-based elements for planning. To the best of our knowledge, we are the first to systematically investigate each part of an interpretable end-to-end vision-based autonomous driving system. We benchmark our approach against previous state-of-the-arts on both open-loop nuScenes dataset as well as closed-loop CARLA simulation. The results show the effectiveness of our method. Source code, model and protocol details are made publicly available at this https URL.",
                        "Citation Paper Authors": "Authors:Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li, Junchi Yan, Dacheng Tao"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": "ITSC 2020 MC MapSeg \u2717 Synthetic Homo-graphic Projection to BEV\nFIERY ",
                    "Citation Text": "A. Hu, Z. Murez, N. Mohan, S. Dudas, J. Hawke, V . Badri-\nnarayanan, R. Cipolla, and A. Kendall, \u201cFIERY: Future\ninstance prediction in bird\u2019s-eye view from surround\nmonocular cameras,\u201d in ICCV , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.10490",
                        "Citation Paper Title": "Title:FIERY: Future Instance Prediction in Bird's-Eye View from Surround Monocular Cameras",
                        "Citation Paper Abstract": "Abstract:Driving requires interacting with road agents and predicting their future behaviour in order to navigate safely. We present FIERY: a probabilistic future prediction model in bird's-eye view from monocular cameras. Our model predicts future instance segmentation and motion of dynamic agents that can be transformed into non-parametric future trajectories. Our approach combines the perception, sensor fusion and prediction components of a traditional autonomous driving stack by estimating bird's-eye-view prediction directly from surround RGB monocular camera inputs. FIERY learns to model the inherent stochastic nature of the future solely from camera driving data in an end-to-end manner, without relying on HD maps, and predicts multimodal future trajectories. We show that our model outperforms previous prediction baselines on the NuScenes and Lyft datasets. The code and trained models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Anthony Hu, Zak Murez, Nikhil Mohan, Sof\u00eda Dudas, Jeffrey Hawke, Vijay Badrinarayanan, Roberto Cipolla, Alex Kendall"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": "arXiv 2022 MC/L ODet/MapSeg - nuS Fusion on BEV from Camera and LiDAR\nCam2BEV ",
                    "Citation Text": "L. Reiher, B. Lampe, and L. Eckstein, \u201cA sim2real deep\nlearning approach for the transformation of images from\nmultiple vehicle-mounted cameras to a semantically seg-\nmented image in bird\u2019s eye view,\u201d in IEEE 23rd Interna-\ntional Conference on Intelligent Transportation Systems , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.04078",
                        "Citation Paper Title": "Title:A Sim2Real Deep Learning Approach for the Transformation of Images from Multiple Vehicle-Mounted Cameras to a Semantically Segmented Image in Bird's Eye View",
                        "Citation Paper Abstract": "Abstract:Accurate environment perception is essential for automated driving. When using monocular cameras, the distance estimation of elements in the environment poses a major challenge. Distances can be more easily estimated when the camera perspective is transformed to a bird's eye view (BEV). For flat surfaces, Inverse Perspective Mapping (IPM) can accurately transform images to a BEV. Three-dimensional objects such as vehicles and vulnerable road users are distorted by this transformation making it difficult to estimate their position relative to the sensor. This paper describes a methodology to obtain a corrected 360\u00b0 BEV image given images from multiple vehicle-mounted cameras. The corrected BEV image is segmented into semantic classes and includes a prediction of occluded areas. The neural network approach does not rely on manually labeled data, but is trained on a synthetic dataset in such a way that it generalizes well to real-world data. By using semantically segmented images as input, we reduce the reality gap between simulated and real-world data and are able to show that our method can be successfully applied in the real world. Extensive experiments conducted on the synthetic data demonstrate the superiority of our approach compared to IPM. Source code and datasets are available at this https URL",
                        "Citation Paper Authors": "Authors:Lennart Reiher, Bastian Lampe, Lutz Eckstein"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "2022 AS \u2020 \u2020 - 211k 211k - \u2020 - \u2717 - -\nLyft L5 ",
                    "Citation Text": "J. Houston, G. Zuidhof, L. Bergamini, Y. Ye, L. Chen,\nA. Jain, S. Omari, V . Iglovikov, and P . Ondruska, \u201cOne\nthousand and one hours: Self-driving motion prediction\ndataset,\u201d in Conference on Robot Learning , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.14480",
                        "Citation Paper Title": "Title:One Thousand and One Hours: Self-driving Motion Prediction Dataset",
                        "Citation Paper Abstract": "Abstract:Motivated by the impact of large-scale datasets on ML systems we present the largest self-driving dataset for motion prediction to date, containing over 1,000 hours of data. This was collected by a fleet of 20 autonomous vehicles along a fixed route in Palo Alto, California, over a four-month period. It consists of 170,000 scenes, where each scene is 25 seconds long and captures the perception output of the self-driving system, which encodes the precise positions and motions of nearby vehicles, cyclists, and pedestrians over time. On top of this, the dataset contains a high-definition semantic map with 15,242 labelled elements and a high-definition aerial view over the area. We show that using a dataset of this size dramatically improves performance for key self-driving problems. Combined with the provided software kit, this collection forms the largest and most detailed dataset to date for the development of self-driving machine learning tasks, such as motion forecasting, motion planning and simulation. The full dataset is available at this http URL.",
                        "Citation Paper Authors": "Authors:John Houston, Guido Zuidhof, Luca Bergamini, Yawei Ye, Long Chen, Ashesh Jain, Sammy Omari, Vladimir Iglovikov, Peter Ondruska"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "2020 - 12 \u2020 6285 6285 6285 \u2020 - - \u2717 - -\nONCE ",
                    "Citation Text": "J. Mao, M. Niu, C. Jiang, X. Liang, Y. Li, C. Ye,\nW. Zhang, Z. Li, J. Yu, C. Xu et al. , \u201cOne million scenes\nfor autonomous driving: Once dataset,\u201d arXiv preprint\narXiv:2106.11037 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.11037",
                        "Citation Paper Title": "Title:One Million Scenes for Autonomous Driving: ONCE Dataset",
                        "Citation Paper Abstract": "Abstract:Current perception models in autonomous driving have become notorious for greatly relying on a mass of annotated data to cover unseen cases and address the long-tail problem. On the other hand, learning from unlabeled large-scale collected data and incrementally self-training powerful recognition models have received increasing attention and may become the solutions of next-generation industry-level powerful and robust perception models in autonomous driving. However, the research community generally suffered from data inadequacy of those essential real-world scene data, which hampers the future exploration of fully/semi/self-supervised methods for 3D perception. In this paper, we introduce the ONCE (One millioN sCenEs) dataset for 3D object detection in the autonomous driving scenario. The ONCE dataset consists of 1 million LiDAR scenes and 7 million corresponding camera images. The data is selected from 144 driving hours, which is 20x longer than the largest 3D autonomous driving dataset available (e.g. nuScenes and Waymo), and it is collected across a range of different areas, periods and weather conditions. To facilitate future research on exploiting unlabeled data for 3D detection, we additionally provide a benchmark in which we reproduce and evaluate a variety of self-supervised and semi-supervised methods on the ONCE dataset. We conduct extensive analyses on those methods and provide valuable observations on their performance related to the scale of used data. Data, code, and more information are available at this https URL.",
                        "Citation Paper Authors": "Authors:Jiageng Mao, Minzhe Niu, Chenhan Jiang, Hanxue Liang, Jingheng Chen, Xiaodan Liang, Yamin Li, Chaoqiang Ye, Wei Zhang, Zhenguo Li, Jie Yu, Hang Xu, Chunjing Xu"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "2020 NA 179 \u2020 16k 41k 14k \u2020 - 60k \u2717 - -\nKITTI-360 ",
                    "Citation Text": "Y. Liao, J. Xie, and A. Geiger, \u201cKITTI-360: A novel dataset\nand benchmarks for urban scene understanding in 2d and\n3d,\u201d arXiv preprint arXiv:2109.13410 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.13410",
                        "Citation Paper Title": "Title:KITTI-360: A Novel Dataset and Benchmarks for Urban Scene Understanding in 2D and 3D",
                        "Citation Paper Abstract": "Abstract:For the last few decades, several major subfields of artificial intelligence including computer vision, graphics, and robotics have progressed largely independently from each other. Recently, however, the community has realized that progress towards robust intelligent systems such as self-driving cars requires a concerted effort across the different fields. This motivated us to develop KITTI-360, successor of the popular KITTI dataset. KITTI-360 is a suburban driving dataset which comprises richer input modalities, comprehensive semantic instance annotations and accurate localization to facilitate research at the intersection of vision, graphics and robotics. For efficient annotation, we created a tool to label 3D scenes with bounding primitives and developed a model that transfers this information into the 2D image domain, resulting in over 150k images and 1B 3D points with coherent semantic instance annotations across 2D and 3D. Moreover, we established benchmarks and baselines for several tasks relevant to mobile perception, encompassing problems from computer vision, graphics, and robotics on the same dataset, e.g., semantic scene understanding, novel view synthesis and semantic SLAM. KITTI-360 will enable progress at the intersection of these research areas and thus contribute towards solving one of today's grand challenges: the development of fully autonomous self-driving systems.",
                        "Citation Paper Authors": "Authors:Yiyi Liao, Jun Xie, Andreas Geiger"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "2020 - \u2020 2.5 - 5k 5k 40k - - \u2717 IMU/GPS 400+\nPandaSet ",
                    "Citation Text": "P . Xiao, Z. Shao, S. Hao, Z. Zhang, X. Chai, J. Jiao,\nZ. Li, J. Wu, K. Sun, K. Jiang et al. , \u201cPandaset: Advanced\nsensor suite dataset for autonomous driving,\u201d in IEEE\nInternational Intelligent Transportation Systems Conference ,\n2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.12610",
                        "Citation Paper Title": "Title:PandaSet: Advanced Sensor Suite Dataset for Autonomous Driving",
                        "Citation Paper Abstract": "Abstract:The accelerating development of autonomous driving technology has placed greater demands on obtaining large amounts of high-quality data. Representative, labeled, real world data serves as the fuel for training deep learning networks, critical for improving self-driving perception algorithms. In this paper, we introduce PandaSet, the first dataset produced by a complete, high-precision autonomous vehicle sensor kit with a no-cost commercial license. The dataset was collected using one 360\u00b0 mechanical spinning LiDAR, one forward-facing, long-range LiDAR, and 6 cameras. The dataset contains more than 100 scenes, each of which is 8 seconds long, and provides 28 types of labels for object classification and 37 types of labels for semantic segmentation. We provide baselines for LiDAR-only 3D object detection, LiDAR-camera fusion 3D object detection and LiDAR point cloud segmentation. For more details about PandaSet and the development kit, see this https URL.",
                        "Citation Paper Authors": "Authors:Pengchuan Xiao, Zhenlei Shao, Steven Hao, Zishuo Zhang, Xiaolin Chai, Judy Jiao, Zesong Li, Jian Wu, Kai Sun, Kun Jiang, Yunlong Wang, Diange Yang"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "2019 EU 22 1.2 43k - - - - 43k \u2717 - 30+\nA2D2 ",
                    "Citation Text": "J. Geyer, Y. Kassahun, M. Mahmudi, X. Ricou, R. Durgesh,\nA. S. Chung, L. Hauswald, V . H. Pham, M. M \u00a8uhlegg,\nS. Dorn et al. , \u201cA2d2: Audi autonomous driving dataset,\u201d\narXiv preprint arXiv:2004.06320 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.06320",
                        "Citation Paper Title": "Title:A2D2: Audi Autonomous Driving Dataset",
                        "Citation Paper Abstract": "Abstract:Research in machine learning, mobile robotics, and autonomous driving is accelerated by the availability of high quality annotated data. To this end, we release the Audi Autonomous Driving Dataset (A2D2). Our dataset consists of simultaneously recorded images and 3D point clouds, together with 3D bounding boxes, semantic segmentation, instance segmentation, and data extracted from the automotive bus. Our sensor suite consists of six cameras and five LiDAR units, providing full 360 degree coverage. The recorded data is time synchronized and mutually registered. Annotations are for non-sequential frames: 41,277 frames with semantic segmentation image and point cloud labels, of which 12,497 frames also have 3D bounding box annotations for objects within the field of view of the front camera. In addition, we provide 392,556 sequential frames of unannotated sensor data for recordings in three cities in the south of Germany. These sequences contain several loops. Faces and vehicle number plates are blurred due to GDPR legislation and to preserve anonymity. A2D2 is made available under the CC BY-ND 4.0 license, permitting commercial use subject to the terms of the license. Data and further information are available at http://www.a2d2.audi.",
                        "Citation Paper Authors": "Authors:Jakob Geyer, Yohannes Kassahun, Mentar Mahmudi, Xavier Ricou, Rupesh Durgesh, Andrew S. Chung, Lorenz Hauswald, Viet Hoang Pham, Maximilian M\u00fchlegg, Sebastian Dorn, Tiffany Fernandez, Martin J\u00e4nicke, Sudesh Mirashi, Chiragkumar Savani, Martin Sturm, Oleksandr Vorobiov, Martin Oelker, Sebastian Garreis, Peter Schuberth"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "2019 NA 160 0.8 27k 83k 27k 1.1M - - \u2717 - -\nSemanticKITTI ",
                    "Citation Text": "J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke,\nC. Stachniss, and J. Gall, \u201cSemanticKITTI: A Dataset for\nSemantic Scene Understanding of LiDAR Sequences,\u201d in\nICCV , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.01416",
                        "Citation Paper Title": "Title:SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences",
                        "Citation Paper Abstract": "Abstract:Semantic scene understanding is important for various applications. In particular, self-driving cars need a fine-grained understanding of the surfaces and objects in their vicinity. Light detection and ranging (LiDAR) provides precise geometric information about the environment and is thus a part of the sensor suites of almost all self-driving cars. Despite the relevance of semantic scene understanding for this application, there is a lack of a large dataset for this task which is based on an automotive LiDAR.\nIn this paper, we introduce a large dataset to propel research on laser-based semantic segmentation. We annotated all sequences of the KITTI Vision Odometry Benchmark and provide dense point-wise annotations for the complete $360^{o}$ field-of-view of the employed automotive LiDAR. We propose three benchmark tasks based on this dataset: (i) semantic segmentation of point clouds using a single scan, (ii) semantic segmentation using multiple past scans, and (iii) semantic scene completion, which requires to anticipate the semantic scene in the future. We provide baseline experiments and show that there is a need for more sophisticated models to efficiently tackle these tasks. Our dataset opens the door for the development of more advanced methods, but also provides plentiful data to investigate new research directions.",
                        "Citation Paper Authors": "Authors:Jens Behley, Martin Garbade, Andres Milioto, Jan Quenzel, Sven Behnke, Cyrill Stachniss, Juergen Gall"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "2019 AS \u2020 55 39k 39k 39k 230k - - \u2717 - -\nH3D ",
                    "Citation Text": "A. Patil, S. Malla, H. Gang, and Y.-T. Chen, \u201cThe h3d\ndataset for full-surround 3d multi-object detection and\ntracking in crowded urban scenes,\u201d in ICRA , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.01568",
                        "Citation Paper Title": "Title:The H3D Dataset for Full-Surround 3D Multi-Object Detection and Tracking in Crowded Urban Scenes",
                        "Citation Paper Abstract": "Abstract:3D multi-object detection and tracking are crucial for traffic scene understanding. However, the community pays less attention to these areas due to the lack of a standardized benchmark dataset to advance the field. Moreover, existing datasets (e.g., KITTI) do not provide sufficient data and labels to tackle challenging scenes where highly interactive and occluded traffic participants are present. To address the issues, we present the Honda Research Institute 3D Dataset (H3D), a large-scale full-surround 3D multi-object detection and tracking dataset collected using a 3D LiDAR scanner. H3D comprises of 160 crowded and highly interactive traffic scenes with a total of 1 million labeled instances in 27,721 frames. With unique dataset size, rich annotations, and complex scenes, H3D is gathered to stimulate research on full-surround 3D multi-object detection and tracking. To effectively and efficiently annotate a large-scale 3D point cloud dataset, we propose a labeling methodology to speed up the overall annotation cycle. A standardized benchmark is created to evaluate full-surround 3D multi-object detection and tracking algorithms. 3D object detection and tracking algorithms are trained and tested on H3D. Finally, sources of errors are discussed for the development of future algorithms.",
                        "Citation Paper Authors": "Authors:Abhishek Patil, Srikanth Malla, Haiming Gang, Yi-Ting Chen"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "2022 NA 1000 6.4 - 200k 200k - 880k - \u2717 - -\nONCE-3DLanes ",
                    "Citation Text": "F. Yan, M. Nie, X. Cai, J. Han, H. Xu, Z. Yang, C. Ye, Y. Fu,\nM. B. Mi, and L. Zhang, \u201cOnce-3dlanes: Building monoc-\nular 3d lane detection,\u201d in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n2022, pp. 17 143\u201317 152.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.00301",
                        "Citation Paper Title": "Title:ONCE-3DLanes: Building Monocular 3D Lane Detection",
                        "Citation Paper Abstract": "Abstract:We present ONCE-3DLanes, a real-world autonomous driving dataset with lane layout annotation in 3D space. Conventional 2D lane detection from a monocular image yields poor performance of following planning and control tasks in autonomous driving due to the case of uneven road. Predicting the 3D lane layout is thus necessary and enables effective and safe driving. However, existing 3D lane detection datasets are either unpublished or synthesized from a simulated environment, severely hampering the development of this field. In this paper, we take steps towards addressing these issues. By exploiting the explicit relationship between point clouds and image pixels, a dataset annotation pipeline is designed to automatically generate high-quality 3D lane locations from 2D lane annotations in 211K road scenes. In addition, we present an extrinsic-free, anchor-free method, called SALAD, regressing the 3D coordinates of lanes in image view without converting the feature map into the bird's-eye view (BEV). To facilitate future research on 3D lane detection, we benchmark the dataset and provide a novel evaluation metric, performing extensive experiments of both existing approaches and our proposed method. The aim of our work is to revive the interest of 3D lane detection in a real-world scenario. We believe our work can lead to the expected and unexpected innovations in both academia and industry.",
                        "Citation Paper Authors": "Authors:Fan Yan, Ming Nie, Xinyue Cai, Jianhua Han, Hang Xu, Zhen Yang, Chaoqiang Ye, Yanwei Fu, Michael Bi Mi, Li Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.05129v2": {
            "Paper Title": "Multi-Object Navigation with dynamically learned neural implicit\n  representations",
            "Sentences": [
                {
                    "Sentence ID": 37,
                    "Sentence": "toMultiON ;PPL: Progress weighted\nBy Path Length (the official MultiON challenge metric).\nGlobal reader dataset \u2014 The Global reader rwas\ntrained on a dataset of 25ktrajectories obtained from roll-\nouts performed by a baseline agent ",
                    "Citation Text": "Pierre Marza, Laetitia Matignon, Olivier Simonin, and Chris-\ntian Wolf. Teaching agents how to map: Spatial reasoning for\nmulti-object navigation. In IROS , 2022. 3, 6, 7, 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.06011",
                        "Citation Paper Title": "Title:Teaching Agents how to Map: Spatial Reasoning for Multi-Object Navigation",
                        "Citation Paper Abstract": "Abstract:In the context of visual navigation, the capacity to map a novel environment is necessary for an agent to exploit its observation history in the considered place and efficiently reach known goals. This ability can be associated with spatial reasoning, where an agent is able to perceive spatial relationships and regularities, and discover object characteristics. Recent work introduces learnable policies parametrized by deep neural networks and trained with Reinforcement Learning (RL). In classical RL setups, the capacity to map and reason spatially is learned end-to-end, from reward alone. In this setting, we introduce supplementary supervision in the form of auxiliary tasks designed to favor the emergence of spatial perception capabilities in agents trained for a goal-reaching downstream objective. We show that learning to estimate metrics quantifying the spatial relationships between an agent at a given location and a goal to reach has a high positive impact in Multi-Object Navigation settings. Our method significantly improves the performance of different baseline agents, that either build an explicit or implicit representation of the environment, even matching the performance of incomparable oracle agents taking ground-truth maps as input. A learning-based agent from the literature trained with the proposed auxiliary losses was the winning entry to the Multi-Object Navigation Challenge, part of the CVPR 2021 Embodied AI Workshop.",
                        "Citation Paper Authors": "Authors:Pierre Marza, Laetitia Matignon, Olivier Simonin, Christian Wolf"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "Visual Navigation \u2014 is a rich problem that involves\nperception, mapping and decision making, with required\ncapacities being highly dependent on the specific task. A\nsummary of reasoning in navigation has been given in ",
                    "Citation Text": "Peter Anderson, Angel X. Chang, Devendra Singh Chaplot,\nAlexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana\nKosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva,\nand Amir Roshan Zamir. On evaluation of embodied naviga-\ntion agents. arXiv preprint , 2018. 2, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.06757",
                        "Citation Paper Title": "Title:On Evaluation of Embodied Navigation Agents",
                        "Citation Paper Abstract": "Abstract:Skillful mobile operation in three-dimensional environments is a primary topic of study in Artificial Intelligence. The past two years have seen a surge of creative work on navigation. This creative output has produced a plethora of sometimes incompatible task definitions and evaluation protocols. To coordinate ongoing and future research in this area, we have convened a working group to study empirical methodology in navigation research. The present document summarizes the consensus recommendations of this working group. We discuss different problem statements and the role of generalization, present evaluation measures, and provide standard scenarios that can be used for benchmarking.",
                        "Citation Paper Authors": "Authors:Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, Amir R. Zamir"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "as the standard protocol.\nDataset and metrics \u2014 The agent is trained on the Mat-\nterport3d ",
                    "Citation Text": "Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Hal-\nber, Matthias Niebner, Manolis Savva, Shuran Song, Andy\nZeng, and Yinda Zhang. Matterport3d: Learning from rgb-d\ndata in indoor environments. In I.C. on 3D Vision , 2018. 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.06158",
                        "Citation Paper Title": "Title:Matterport3D: Learning from RGB-D Data in Indoor Environments",
                        "Citation Paper Abstract": "Abstract:Access to large, diverse RGB-D datasets is critical for training RGB-D scene understanding algorithms. However, existing datasets still cover only a limited number of views or a restricted scale of spaces. In this paper, we introduce Matterport3D, a large-scale RGB-D dataset containing 10,800 panoramic views from 194,400 RGB-D images of 90 building-scale scenes. Annotations are provided with surface reconstructions, camera poses, and 2D and 3D semantic segmentations. The precise global alignment and comprehensive, diverse panoramic set of views over entire buildings enable a variety of supervised and self-supervised computer vision tasks, including keypoint matching, view overlap prediction, normal prediction from color, semantic segmentation, and region classification.",
                        "Citation Paper Authors": "Authors:Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Nie\u00dfner, Manolis Savva, Shuran Song, Andy Zeng, Yinda Zhang"
                    }
                },
                {
                    "Sentence ID": 55,
                    "Sentence": "or be fully\nlatent, effectively corresponding to inductive biases of the\nneural agents [ 44,5,26]. Other alternatives are topological\nmaps [ 6,12] or self-attention and transformers ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS , 2017. 1,\n2, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": ".\nImplicit representations in robotics \u2014 are a recent\nphenomenon, used to represent density ",
                    "Citation Text": "Michal Adamkiewicz, Timothy Chen, Adam Caccavale,\nRachel Gardner, Preston Culbertson, Jeannette Bohg, and\nMac Schwager. Vision-only robot navigation in a neural radi-\nance world. IEEE Robotics and Automation Letters , 2022. 2,\n3, 4, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.00168",
                        "Citation Paper Title": "Title:Vision-Only Robot Navigation in a Neural Radiance World",
                        "Citation Paper Abstract": "Abstract:Neural Radiance Fields (NeRFs) have recently emerged as a powerful paradigm for the representation of natural, complex 3D scenes. NeRFs represent continuous volumetric density and RGB values in a neural network, and generate photo-realistic images from unseen camera viewpoints through ray tracing. We propose an algorithm for navigating a robot through a 3D environment represented as a NeRF using only an on-board RGB camera for localization. We assume the NeRF for the scene has been pre-trained offline, and the robot's objective is to navigate through unoccupied space in the NeRF to reach a goal pose. We introduce a trajectory optimization algorithm that avoids collisions with high-density regions in the NeRF based on a discrete time version of differential flatness that is amenable to constraining the robot's full pose and control inputs. We also introduce an optimization based filtering method to estimate 6DoF pose and velocities for the robot in the NeRF given only an onboard RGB camera. We combine the trajectory planner with the pose filter in an online replanning loop to give a vision-based robot navigation pipeline. We present simulation results with a quadrotor robot navigating through a jungle gym environment, the inside of a church, and Stonehenge using only an RGB camera. We also demonstrate an omnidirectional ground robot navigating through the church, requiring it to reorient to fit through the narrow gap. Videos of this work can be found at this https URL .",
                        "Citation Paper Authors": "Authors:Michal Adamkiewicz, Timothy Chen, Adam Caccavale, Rachel Gardner, Preston Culbertson, Jeannette Bohg, Mac Schwager"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": ". Related to goal-oriented navigation,\nsome work targets SLAM with neural implicit representa-\ntions ",
                    "Citation Text": "Edgar Sucar, Shikun Liu, Joseph Ortiz, and Andrew J Davison.\nimap: Implicit mapping and positioning in real-time. In ICCV ,\n2021. 2, 3, 4, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.12352",
                        "Citation Paper Title": "Title:iMAP: Implicit Mapping and Positioning in Real-Time",
                        "Citation Paper Abstract": "Abstract:We show for the first time that a multilayer perceptron (MLP) can serve as the only scene representation in a real-time SLAM system for a handheld RGB-D camera. Our network is trained in live operation without prior data, building a dense, scene-specific implicit 3D model of occupancy and colour which is also immediately used for tracking.\nAchieving real-time SLAM via continual training of a neural network against a live image stream requires significant innovation. Our iMAP algorithm uses a keyframe structure and multi-processing computation flow, with dynamic information-guided pixel sampling for speed, with tracking at 10 Hz and global map updating at 2 Hz. The advantages of an implicit MLP over standard dense SLAM techniques include efficient geometry representation with automatic detail control and smooth, plausible filling-in of unobserved regions such as the back surfaces of objects.",
                        "Citation Paper Authors": "Authors:Edgar Sucar, Shikun Liu, Joseph Ortiz, Andrew J. Davison"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "generate the weights\nof a CNN from support samples in the context of few-shot\nlearning. More related to our work, ",
                    "Citation Text": "Shaowu Pan, Steven L Brunton, and J Nathan Kutz. Neu-\nral implicit flow: a mesh-agnostic dimensionality reduc-\ntion paradigm of spatio-temporal data. arXiv preprint\narXiv:2204.03216 , 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.03216",
                        "Citation Paper Title": "Title:Neural Implicit Flow: a mesh-agnostic dimensionality reduction paradigm of spatio-temporal data",
                        "Citation Paper Abstract": "Abstract:High-dimensional spatio-temporal dynamics can often be encoded in a low-dimensional subspace. Engineering applications for modeling, characterization, design, and control of such large-scale systems often rely on dimensionality reduction to make solutions computationally tractable in real-time. Common existing paradigms for dimensionality reduction include linear methods, such as the singular value decomposition (SVD), and nonlinear methods, such as variants of convolutional autoencoders (CAE). However, these encoding techniques lack the ability to efficiently represent the complexity associated with spatio-temporal data, which often requires variable geometry, non-uniform grid resolution, adaptive meshing, and/or parametric dependencies. To resolve these practical engineering challenges, we propose a general framework called Neural Implicit Flow (NIF) that enables a mesh-agnostic, low-rank representation of large-scale, parametric, spatial-temporal data. NIF consists of two modified multilayer perceptrons (MLPs): (i) ShapeNet, which isolates and represents the spatial complexity, and (ii) ParameterNet, which accounts for any other input complexity, including parametric dependencies, time, and sensor measurements. We demonstrate the utility of NIF for parametric surrogate modeling, enabling the interpretable representation and compression of complex spatio-temporal dynamics, efficient many-spatial-query tasks, and improved generalization performance for sparse reconstruction.",
                        "Citation Paper Authors": "Authors:Shaowu Pan, Steven L. Brunton, J. Nathan Kutz"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "proposes a hierarchical implicit\nrepresentation of a scene to scale to larger environments\nand obtain a more detailed reconstruction. ",
                    "Citation Text": "Chi-Ming Chung, Yang-Che Tseng, Ya-Ching Hsu, Xiang-\nQian Shi, Yun-Hung Hua, Jia-Fong Yeh, Wen-Chin Chen,\nYi-Ting Chen, and Winston H Hsu. Orbeez-slam: A real-time\nmonocular visual slam with orb features and nerf-realized\nmapping. In ICRA , 2023. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2209.13274",
                        "Citation Paper Title": "Title:Orbeez-SLAM: A Real-time Monocular Visual SLAM with ORB Features and NeRF-realized Mapping",
                        "Citation Paper Abstract": "Abstract:A spatial AI that can perform complex tasks through visual signals and cooperate with humans is highly anticipated. To achieve this, we need a visual SLAM that easily adapts to new scenes without pre-training and generates dense maps for downstream tasks in real-time. None of the previous learning-based and non-learning-based visual SLAMs satisfy all needs due to the intrinsic limitations of their components. In this work, we develop a visual SLAM named Orbeez-SLAM, which successfully collaborates with implicit neural representation and visual odometry to achieve our goals. Moreover, Orbeez-SLAM can work with the monocular camera since it only needs RGB inputs, making it widely applicable to the real world. Results show that our SLAM is up to 800x faster than the strong baseline with superior rendering outcomes. Code link: this https URL.",
                        "Citation Paper Authors": "Authors:Chi-Ming Chung, Yang-Che Tseng, Ya-Ching Hsu, Xiang-Qian Shi, Yun-Hung Hua, Jia-Fong Yeh, Wen-Chin Chen, Yi-Ting Chen, Winston H. Hsu"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": ". For\na more detailed overview of recent advances in the rapidlygrowing field, we refer the reader to ",
                    "Citation Text": "Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany,\nShiqin Yan, Numair Khan, Federico Tombari, James Tompkin,\nVincent Sitzmann, and Srinath Sridhar. Neural fields in visual\ncomputing and beyond. arXiv preprint arXiv:2111.11426 ,\n2021. 2, 3, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.11426",
                        "Citation Paper Title": "Title:Neural Fields in Visual Computing and Beyond",
                        "Citation Paper Abstract": "Abstract:Recent advances in machine learning have created increasing interest in solving visual computing problems using a class of coordinate-based neural networks that parametrize physical properties of scenes or objects across space and time. These methods, which we call neural fields, have seen successful application in the synthesis of 3D shapes and image, animation of human bodies, 3D reconstruction, and pose estimation. However, due to rapid progress in a short time, many papers exist but a comprehensive review and formulation of the problem has not yet emerged. In this report, we address this limitation by providing context, mathematical grounding, and an extensive review of literature on neural fields. This report covers research along two dimensions. In Part I, we focus on techniques in neural fields by identifying common components of neural field methods, including different representations, architectures, forward mapping, and generalization methods. In Part II, we focus on applications of neural fields to different problems in visual computing, and beyond (e.g., robotics, audio). Our review shows the breadth of topics already covered in visual computing, both historically and in current incarnations, demonstrating the improved quality, flexibility, and capability brought by neural fields methods. Finally, we present a companion website that contributes a living version of this review that can be continually updated by the community.",
                        "Citation Paper Authors": "Authors:Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Federico Tombari, James Tompkin, Vincent Sitzmann, Srinath Sridhar"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.06940v3": {
            "Paper Title": "TEAM: a parameter-free algorithm to teach collaborative robots motions\n  from user demonstrations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.12160v2": {
            "Paper Title": "PL-EVIO: Robust Monocular Event-based Visual Inertial Odometry with\n  Point and Line Features",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": "presented a feature tracker based on\nExpectation Maximisation (EM). Ref. ",
                    "Citation Text": "A. Dietsche, G. Cioffi, J. Hidalgo-Carri \u00b4o, and D. Scaramuzza, \u201cPow-\nerline tracking with event cameras,\u201d in 2021 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS) . IEEE, 2021,\npp. 6990\u20136997.\n15",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.00515",
                        "Citation Paper Title": "Title:Powerline Tracking with Event Cameras",
                        "Citation Paper Abstract": "Abstract:Autonomous inspection of powerlines with quadrotors is challenging. Flights require persistent perception to keep a close look at the lines. We propose a method that uses event cameras to robustly track powerlines. Event cameras are inherently robust to motion blur, have low latency, and high dynamic range. Such properties are advantageous for autonomous inspection of powerlines with drones, where fast motions and challenging illumination conditions are ordinary. Our method identifies lines in the stream of events by detecting planes in the spatio-temporal signal, and tracks them through time. The implementation runs onboard and is capable of detecting multiple distinct lines in real time with rates of up to $320$ thousand events per second. The performance is evaluated in real-world flights along a powerline. The tracker is able to persistently track the powerlines, with a mean lifetime of the line $10\\times$ longer than existing approaches.",
                        "Citation Paper Authors": "Authors:Alexander Dietsche, Giovanni Cioffi, Javier Hidalgo-Carrio, Davide Scaramuzza"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.12566v2": {
            "Paper Title": "Solving Continuous Control via Q-learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.08401v3": {
            "Paper Title": "Nonlinear Heterogeneous Bayesian Decentralized Data Fusion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.16721v2": {
            "Paper Title": "Where Am I Now? Dynamically Finding Optimal Sensor States to Minimize\n  Localization Uncertainty for a Perception-Denied Rover",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": "surveys methods for active mapping and\nrobot exploration, Placed et al. ",
                    "Citation Text": "Julio A. Placed, Jared Strader, Henry Carrillo, Nikolay Atanasov,\nVadim Indelman, Luca Carlone, and Jos \u00b4e A. Castellanos. A Survey\non Active Simultaneous Localization and Mapping: State of the Art\nand New Frontiers. 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2207.00254",
                        "Citation Paper Title": "Title:A Survey on Active Simultaneous Localization and Mapping: State of the Art and New Frontiers",
                        "Citation Paper Abstract": "Abstract:Active Simultaneous Localization and Mapping (SLAM) is the problem of planning and controlling the motion of a robot to build the most accurate and complete model of the surrounding environment. Since the first foundational work in active perception appeared, more than three decades ago, this field has received increasing attention across different scientific communities. This has brought about many different approaches and formulations, and makes a review of the current trends necessary and extremely valuable for both new and experienced researchers. In this work, we survey the state-of-the-art in active SLAM and take an in-depth look at the open challenges that still require attention to meet the needs of modern applications. After providing a historical perspective, we present a unified problem formulation and review the well-established modular solution scheme, which decouples the problem into three stages that identify, select, and execute potential navigation actions. We then analyze alternative approaches, including belief-space planning and deep reinforcement learning techniques, and review related work on multi-robot coordination. The manuscript concludes with a discussion of new research directions, addressing reproducible research, active spatial perception, and practical applications, among other topics.",
                        "Citation Paper Authors": "Authors:Julio A. Placed, Jared Strader, Henry Carrillo, Nikolay Atanasov, Vadim Indelman, Luca Carlone, Jos\u00e9 A. Castellanos"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.05178v3": {
            "Paper Title": "Pre-Training for Robots: Offline RL Enables Learning New Tasks from a\n  Handful of Trials",
            "Sentences": [
                {
                    "Sentence ID": 37,
                    "Sentence": "and the newly collected door\nopening data) consist of human demonstrations, as indicated\nby prior work ",
                    "Citation Text": "Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush\nNasiriany, Chen Wang, Rohun Kulkarni, Fei-Fei Li,\nSilvio Savarese, Yuke Zhu, and Roberto Mart \u00b4\u0131n-Mart \u00b4\u0131n.\nWhat matters in learning from offline human demonstra-\ntions for robot manipulation. In 5th Annual Conference\non Robot Learning , 2021. URL https://openreview.net/\nforum?id=JrsfBJtDFdI.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.03298",
                        "Citation Paper Title": "Title:What Matters in Learning from Offline Human Demonstrations for Robot Manipulation",
                        "Citation Paper Abstract": "Abstract:Imitating human demonstrations is a promising approach to endow robots with various manipulation capabilities. While recent advances have been made in imitation learning and batch (offline) reinforcement learning, a lack of open-source human datasets and reproducible learning methods make assessing the state of the field difficult. In this paper, we conduct an extensive study of six offline learning algorithms for robot manipulation on five simulated and three real-world multi-stage manipulation tasks of varying complexity, and with datasets of varying quality. Our study analyzes the most critical challenges when learning from offline human data for manipulation. Based on the study, we derive a series of lessons including the sensitivity to different algorithmic design choices, the dependence on the quality of the demonstrations, and the variability based on the stopping criteria due to the different objectives in training and evaluation. We also highlight opportunities for learning from human datasets, such as the ability to learn proficient policies on challenging, multi-stage tasks beyond the scope of current reinforcement learning methods, and the ability to easily scale to natural, real-world manipulation scenarios where only raw sensory signals are available. We have open-sourced our datasets and all algorithm implementations to facilitate future research and fair comparisons in learning from human demonstration data. Codebase, datasets, trained models, and more available at this https URL",
                        "Citation Paper Authors": "Authors:Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, Roberto Mart\u00edn-Mart\u00edn"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": ") to represent Q\u03d5, but faced\ndivergence challenges similar to prior efforts that use batch\nnormalization [4, 3] in the Q-network. Batch normalization\nlayers are known to be hard to train with TD-learning ",
                    "Citation Text": "Aditya Bhatt, Max Argus, Artemij Amiranashvili, and\nThomas Brox. CrossNorm: Normalization for Off-\nPolicy TD Reinforcement Learning. arXiv e-prints , art.\narXiv:1902.05605, February 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.05605",
                        "Citation Paper Title": "Title:CrossQ: Batch Normalization in Deep Reinforcement Learning for Greater Sample Efficiency and Simplicity",
                        "Citation Paper Abstract": "Abstract:Sample efficiency is a crucial problem in deep reinforcement learning. Recent algorithms, such as REDQ and DroQ, found a way to improve the sample efficiency by increasing the update-to-data (UTD) ratio to 20 gradient update steps on the critic per environment sample. However, this comes at the expense of a greatly increased computational cost. To reduce this computational burden, we introduce Cross$Q$: a lightweight algorithm that makes careful use of Batch Normalization and removes target networks to surpass the state-of-the-art in sample efficiency while maintaining a low UTD ratio of $1$. Notably, Cross$Q$ does not rely on advanced bias-reduction schemes used in current methods. Cross$Q$'s contributions are thus threefold: (1) state-of-the-art sample efficiency, (2) substantial reduction in computational cost compared to REDQ and DroQ, and (3) ease of implementation, requiring just a few lines of code on top of SAC.",
                        "Citation Paper Authors": "Authors:Aditya Bhatt, Daniel Palenicek, Boris Belousov, Max Argus, Artemij Amiranashvili, Thomas Brox, Jan Peters"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.11679v3": {
            "Paper Title": "Mean Shift Mask Transformer for Unseen Object Instance Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.04952v3": {
            "Paper Title": "Read the Room: Adapting a Robot's Voice to Ambient and Social Contexts",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.07773v4": {
            "Paper Title": "MEDIRL: Predicting the Visual Attention of Drivers via Maximum Entropy\n  Deep Inverse Reinforcement Learning",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "are the most\nwell-known large-scale annotated datasets in naturalistic\nand in-lab driving settings, respectively. Importantly, the\nrecently-released annotated driving attention dataset with\nin-lab settings, DADA-2000 ",
                    "Citation Text": "Jianwu Fang, Dingxin Yan, Jiahuan Qiao, Jianru Xue, He\nWang, and Sen Li. Dada-2000: Can driving accident be pre-\ndicted by driver attention analyzed by a benchmark. In 2019\nIEEE Intelligent Transportation Systems Conference (ITSC) ,\npages 4303\u20134309. IEEE, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.12634",
                        "Citation Paper Title": "Title:DADA-2000: Can Driving Accident be Predicted by Driver Attention? Analyzed by A Benchmark",
                        "Citation Paper Abstract": "Abstract:Driver attention prediction is currently becoming the focus in safe driving research community, such as the DR(eye)VE project and newly emerged Berkeley DeepDrive Attention (BDD-A) database in critical situations. In safe driving, an essential task is to predict the incoming accidents as early as possible. BDD-A was aware of this problem and collected the driver attention in laboratory because of the rarity of such scenes. Nevertheless, BDD-A focuses the critical situations which do not encounter actual accidents, and just faces the driver attention prediction task, without a close step for accident prediction. In contrast to this, we explore the view of drivers' eyes for capturing multiple kinds of accidents, and construct a more diverse and larger video benchmark than ever before with the driver attention and the driving accident annotation simultaneously (named as DADA-2000), which has 2000 video clips owning about 658,476 frames on 54 kinds of accidents. These clips are crowd-sourced and captured in various occasions (highway, urban, rural, and tunnel), weather (sunny, rainy and snowy) and light conditions (daytime and nighttime). For the driver attention representation, we collect the maps of fixations, saccade scan path and focusing time. The accidents are annotated by their categories, the accident window in clips and spatial locations of the crash-objects. Based on the analysis, we obtain a quantitative and positive answer for the question in this paper.",
                        "Citation Paper Authors": "Authors:Jianwu Fang, Dingxin Yan, Jiahuan Qiao, Jianru Xue, He Wang, Sen Li"
                    }
                },
                {
                    "Sentence ID": 62,
                    "Sentence": "which can handle raw image inputs\nand enables the model to handle the often sub-optimal and\nseemingly stochastic behaviors of drivers ",
                    "Citation Text": "Markus Wulfmeier, Peter Ondruska, and Ingmar Posner.\nMaximum entropy deep inverse reinforcement learning.\narXiv preprint arXiv:1507.04888 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1507.04888",
                        "Citation Paper Title": "Title:Maximum Entropy Deep Inverse Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:This paper presents a general framework for exploiting the representational capacity of neural networks to approximate complex, nonlinear reward functions in the context of solving the inverse reinforcement learning (IRL) problem. We show in this context that the Maximum Entropy paradigm for IRL lends itself naturally to the efficient training of deep architectures. At test time, the approach leads to a computational complexity independent of the number of demonstrations, which makes it especially well-suited for applications in life-long learning scenarios. Our approach achieves performance commensurate to the state-of-the-art on existing benchmarks while exceeding on an alternative benchmark based on highly varying reward structures. Finally, we extend the basic architecture - which is equivalent to a simplified subclass of Fully Convolutional Neural Networks (FCNNs) with width one - to include larger convolutions in order to eliminate dependency on precomputed spatial features and work on raw input representations.",
                        "Citation Paper Authors": "Authors:Markus Wulfmeier, Peter Ondruska, Ingmar Posner"
                    }
                }
            ]
        }
    }
}