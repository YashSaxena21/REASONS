{
    "Cryptography": {
        "http://arxiv.org/abs/1904.05528v2": {
            "Paper Title": "Review on DNA Cryptography",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.11993v2": {
            "Paper Title": "Secure Summation via Subset Sums: A New Primitive for Privacy-Preserving\n  Distributed Machine Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.12362v2": {
            "Paper Title": "Cumulus: Blockchain-Enabled Privacy Preserving Data Audit in Cloud",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.02621v3": {
            "Paper Title": "The Threat of Adversarial Attacks on Machine Learning in Network\n  Security -- A Survey",
            "Sentences": [
                {
                    "Sentence ID": 44,
                    "Sentence": ". However, the \ufb01rst notable discovery in\nadversarial attacks for computer vision was by Szegedy et\nal. ",
                    "Citation Text": "C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfel-\nlow, and R. Fergus, \u201cIntriguing properties of neural networks,\u201d arXiv\npreprint arXiv:1312.6199 , 2013.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1312.6199",
                        "Citation Paper Title": "Title:Intriguing properties of neural networks",
                        "Citation Paper Abstract": "Abstract:Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.\nFirst, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks.\nSecond, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
                        "Citation Paper Authors": "Authors:Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": ".\n\u000fURLNet -Learning a URL Representation with Deep\nLearning for Malicious URL Detection Le et al. ",
                    "Citation Text": "H. Le, Q. Pham, D. Sahoo, and S. C. Hoi, \u201cUrlnet: Learning a url\nrepresentation with deep learning for malicious url detection,\u201d arXiv\npreprint arXiv:1802.03162 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.03162",
                        "Citation Paper Title": "Title:URLNet: Learning a URL Representation with Deep Learning for Malicious URL Detection",
                        "Citation Paper Abstract": "Abstract:Malicious URLs host unsolicited content and are used to perpetrate cybercrimes. It is imperative to detect them in a timely manner. Traditionally, this is done through the usage of blacklists, which cannot be exhaustive, and cannot detect newly generated malicious URLs. To address this, recent years have witnessed several efforts to perform Malicious URL Detection using Machine Learning. The most popular and scalable approaches use lexical properties of the URL string by extracting Bag-of-words like features, followed by applying machine learning models such as SVMs. There are also other features designed by experts to improve the prediction performance of the model. These approaches suffer from several limitations: (i) Inability to effectively capture semantic meaning and sequential patterns in URL strings; (ii) Requiring substantial manual feature engineering; and (iii) Inability to handle unseen features and generalize to test data. To address these challenges, we propose URLNet, an end-to-end deep learning framework to learn a nonlinear URL embedding for Malicious URL Detection directly from the URL. Specifically, we apply Convolutional Neural Networks to both characters and words of the URL String to learn the URL embedding in a jointly optimized framework. This approach allows the model to capture several types of semantic information, which was not possible by the existing models. We also propose advanced word-embeddings to solve the problem of too many rare words observed in this task. We conduct extensive experiments on a large-scale dataset and show a significant performance gain over existing methods. We also conduct ablation studies to evaluate the performance of various components of URLNet.",
                        "Citation Paper Authors": "Authors:Hung Le, Quang Pham, Doyen Sahoo, Steven C.H. Hoi"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": ". Signature based IDS detects\nattacks based on the repository of attacks signatures with no\nfalse alarm ",
                    "Citation Text": "M. Almseidin, M. Alzubi, S. Kovacs, and M. Alkasassbeh, \u201cEvalua-\ntion of machine learning algorithms for intrusion detection system,\u201d\ninIntelligent Systems and Informatics (SISY), 2017 IEEE 15th\nInternational Symposium on , pp. 000277\u2013000282, IEEE, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.02330",
                        "Citation Paper Title": "Title:Evaluation of Machine Learning Algorithms for Intrusion Detection System",
                        "Citation Paper Abstract": "Abstract:Intrusion detection system (IDS) is one of the implemented solutions against harmful attacks. Furthermore, attackers always keep changing their tools and techniques. However, implementing an accepted IDS system is also a challenging task. In this paper, several experiments have been performed and evaluated to assess various machine learning classifiers based on KDD intrusion dataset. It succeeded to compute several performance metrics in order to evaluate the selected classifiers. The focus was on false negative and false positive performance metrics in order to enhance the detection rate of the intrusion detection system. The implemented experiments demonstrated that the decision table classifier achieved the lowest value of false negative while the random forest classifier has achieved the highest average accuracy rate.",
                        "Citation Paper Authors": "Authors:Mohammad Almseidin, Maen Alzubi, Szilveszter Kovacs, Mouhammd Alkasassbeh"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "in which\nvarious adversarial attacks and defenses in images, graphs\nand texts were reviewed. In the \ufb01eld of natural language\nprocessing, zhang et.al ",
                    "Citation Text": "W. E. Zhang, Q. Z. Sheng, A. Alhazmi, and C. Li, \u201cAdversarial\nattacks on deep-learning models in natural language processing: A\nsurvey,\u201d ACM Transactions on Intelligent Systems and Technology\n(TIST) , vol. 11, no. 3, pp. 1\u201341, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.06796",
                        "Citation Paper Title": "Title:Adversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey",
                        "Citation Paper Abstract": "Abstract:With the development of high computational devices, deep neural networks (DNNs), in recent years, have gained significant popularity in many Artificial Intelligence (AI) applications. However, previous efforts have shown that DNNs were vulnerable to strategically modified samples, named adversarial examples. These samples are generated with some imperceptible perturbations but can fool the DNNs to give false predictions. Inspired by the popularity of generating adversarial examples for image DNNs, research efforts on attacking DNNs for textual applications emerges in recent years. However, existing perturbation methods for images cannotbe directly applied to texts as text data is discrete. In this article, we review research works that address this difference and generatetextual adversarial examples on DNNs. We collect, select, summarize, discuss and analyze these works in a comprehensive way andcover all the related information to make the article self-contained. Finally, drawing on the reviewed literature, we provide further discussions and suggestions on this topic.",
                        "Citation Paper Authors": "Authors:Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, Chenliang Li"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1909.11261v3": {
            "Paper Title": "Practical Low Latency Proof of Work Consensus",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": ". However, how to scale the latency has\nbeen a major challenge in blockchain research for years. Prism++\nborrows an idea from Prism ",
                    "Citation Text": "Vivek Bagaria, Sreeram Kannan, David Tse, Giulia Fanti, and Pramod Viswanath.\nPrism: Deconstructing the blockchain to approach physical limits. In Proceedings\nof the 2019 ACM SIGSAC Conference on Computer and Communications Security ,\nCCS \u201919, page 585\u2013602, New York, NY, USA, 2019. Association for Computing\nMachinery.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.08092",
                        "Citation Paper Title": "Title:Deconstructing the Blockchain to Approach Physical Limits",
                        "Citation Paper Abstract": "Abstract:Transaction throughput, confirmation latency and confirmation reliability are fundamental performance measures of any blockchain system in addition to its security. In a decentralized setting, these measures are limited by two underlying physical network attributes: communication capacity and speed-of-light propagation delay. Existing systems operate far away from these physical limits. In this work we introduce Prism, a new proof-of-work blockchain protocol, which can achieve 1) security against up to 50% adversarial hashing power; 2) optimal throughput up to the capacity C of the network; 3) confirmation latency for honest transactions proportional to the propagation delay D, with confirmation error probability exponentially small in CD ; 4) eventual total ordering of all transactions. Our approach to the design of this protocol is based on deconstructing the blockchain into its basic functionalities and systematically scaling up these functionalities to approach their physical limits.",
                        "Citation Paper Authors": "Authors:Vivek Bagaria, Sreeram Kannan, David Tse, Giulia Fanti, Pramod Viswanath"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "Therearebroadlythreedifferentapproachestoscaletheperformance\nof blockchains. First, on-chain scaling aims to design consensus pro-\ntocols with inherently high throughput and low latency. Protocols\nsuch as Bitcoin-NG ",
                    "Citation Text": "Ittay Eyal, Adem Efe Gencer, Emin Gun Sirer, and Robbert Van Renesse. Bitcoin-ng:\nA scalable blockchain protocol. In 13th USENIX Symposium on Networked Systems\nDesign and Implementation (NSDI 16) , pages 45\u201359, Santa Clara, CA, March 2016.\nUSENIX Association.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1510.02037",
                        "Citation Paper Title": "Title:Bitcoin-NG: A Scalable Blockchain Protocol",
                        "Citation Paper Abstract": "Abstract:Cryptocurrencies, based on and led by Bitcoin, have shown promise as infrastructure for pseudonymous online payments, cheap remittance, trustless digital asset exchange, and smart contracts. However, Bitcoin-derived blockchain protocols have inherent scalability limits that trade-off between throughput and latency and withhold the realization of this potential.\nThis paper presents Bitcoin-NG, a new blockchain protocol designed to scale. Based on Bitcoin's blockchain protocol, Bitcoin-NG is Byzantine fault tolerant, is robust to extreme churn, and shares the same trust model obviating qualitative changes to the ecosystem.\nIn addition to Bitcoin-NG, we introduce several novel metrics of interest in quantifying the security and efficiency of Bitcoin-like blockchain protocols. We implement Bitcoin-NG and perform large-scale experiments at 15% the size of the operational Bitcoin system, using unchanged clients of both protocols. These experiments demonstrate that Bitcoin-NG scales optimally, with bandwidth limited only by the capacity of the individual nodes and latency limited only by the propagation time of the network.",
                        "Citation Paper Authors": "Authors:Ittay Eyal, Adem Efe Gencer, Emin Gun Sirer, Robbert van Renesse"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1910.13659v3": {
            "Paper Title": "Efficient Privacy-Preserving Stochastic Nonconvex Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.12820v5": {
            "Paper Title": "Empirical Differential Privacy",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.08634v3": {
            "Paper Title": "Universal Composability is Robust Compilation",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": ". We believe\nourconnectiondoesnotneedtoaccountforsourceandtargettraced i\ufb00erencesince UCdealswith\nthe samelanguageforprotocolsandfunctionalities.\nAnovelcriterionforsecurecompilation, RChasbeencomparedtoexistingsecurecompilation\nnotionssuchasfully-abstractcompilation ",
                    "Citation Text": "Carmine Abate, Matteo Busi, and Stelios Tsampas. 2021. Fully Abstract and Robust Compilation: And How to Rec-\noncile the Two, Abstractly. In Programming Languages and Systems - 19th Asian Symposium, A PLAS 2021, Chicago,\nIL, USA, October 17-18, 2021, Proceedings (Lecture Notes in Computer Science, Vol. 13008) , Hakjoo Oh (Ed.). Springer,\n83\u2013101. https://doi.org/10.1007/978-3-030-89051-3_6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.14969",
                        "Citation Paper Title": "Title:Fully Abstract and Robust Compilation and How to Reconcile the Two, Abstractly",
                        "Citation Paper Abstract": "Abstract:The most prominent formal criterion for secure compilation is full abstraction, the preservation and reflection of contextual equivalence. Recent work introduced robust compilation, defined as the preservation of robust satisfaction of hyperproperties, i.e., their satisfaction against arbitrary attackers. In this paper, we initially set out to compare these two approaches to secure compilation. To that end, we provide an exact description of the hyperproperties that are robustly satisfied by programs compiled with a fully abstract compiler, and show that they can be meaningless or trivial. We then propose a novel criterion for secure compilation formulated in the framework of Mathematical Operational Semantics (MOS), guaranteeing both full abstraction and the preservation of robust satisfaction of hyperproperties in a more sensible manner.",
                        "Citation Paper Authors": "Authors:Carmine Abate, Matteo Busi, Stelios Tsampas"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1906.01337v6": {
            "Paper Title": "SoK: Differential Privacies",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.06635v2": {
            "Paper Title": "Parsimonious Black-Box Adversarial Attacks via Efficient Combinatorial\n  Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.01030v3": {
            "Paper Title": "Correctness Verification of Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.00482v4": {
            "Paper Title": "Estimating Smooth GLM in Non-interactive Local Differential Privacy\n  Model with Public Unlabeled Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.07921v3": {
            "Paper Title": "FASHION: Functional and Attack graph Secured HybrId Optimization of\n  virtualized Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.08144v4": {
            "Paper Title": "They may look and look, yet not see: BMDs cannot be tested adequately",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.06380v2": {
            "Paper Title": "Bridging Information Security and Environmental Criminology Research to\n  Better Mitigate Cybercrime",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.10480v4": {
            "Paper Title": "FENCE: Feasible Evasion Attacks on Neural Networks in Constrained\n  Environments",
            "Sentences": [
                {
                    "Sentence ID": 46,
                    "Sentence": ".\nDefenses against evasion attacks. Standard methods to defend against adversarial evasion\nattacks in continuous domains include: adversarial training ",
                    "Citation Text": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2017. Towards deep\nlearning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06083",
                        "Citation Paper Title": "Title:Towards Deep Learning Models Resistant to Adversarial Attacks",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL.",
                        "Citation Paper Authors": "Authors:Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "generate adversarial examples for\ndenial of service attacks. Lin et al. ",
                    "Citation Text": "Zilong Lin, Yong Shi, and Zhi Xue. 2018. Idsgan: Generative adversarial networks for attack generation against\nintrusion detection. arXiv preprint arXiv:1809.02077 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.02077",
                        "Citation Paper Title": "Title:IDSGAN: Generative Adversarial Networks for Attack Generation against Intrusion Detection",
                        "Citation Paper Abstract": "Abstract:As an essential tool in security, the intrusion detection system bears the responsibility of the defense to network attacks performed by malicious traffic. Nowadays, with the help of machine learning algorithms, intrusion detection systems develop rapidly. However, the robustness of this system is questionable when it faces adversarial attacks. For the robustness of detection systems, more potential attack approaches are under research. In this paper, a framework of the generative adversarial networks, called IDSGAN, is proposed to generate the adversarial malicious traffic records aiming to attack intrusion detection systems by deceiving and evading the detection. Given that the internal structure and parameters of the detection system are unknown to attackers, the adversarial attack examples perform the black-box attacks against the detection system. IDSGAN leverages a generator to transform original malicious traffic records into adversarial malicious ones. A discriminator classifies traffic examples and dynamically learns the real-time black-box detection system. More significantly, the restricted modification mechanism is designed for the adversarial generation to preserve original attack functionalities of adversarial traffic records. The effectiveness of the model is indicated by attacking multiple algorithm-based detection models with different attack categories. The robustness is verified by changing the number of the modified features. A comparative experiment with adversarial attack baselines demonstrates the superiority of our model.",
                        "Citation Paper Authors": "Authors:Zilong Lin, Yong Shi, Zhi Xue"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "analyzing the ro-\nbustness of random forest for botnet classification; Clements et al. ",
                    "Citation Text": "Joseph Clements, Yuzhe Yang, Ankur Sharma, Hongxin Hu, and Yingjie Lao. 2019. Rallying Adversarial Techniques\nagainst Deep Learning for Network Security. arXiv preprint arXiv:1903.11688 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.11688",
                        "Citation Paper Title": "Title:Rallying Adversarial Techniques against Deep Learning for Network Security",
                        "Citation Paper Abstract": "Abstract:Recent advances in artificial intelligence and the increasing need for powerful defensive measures in the domain of network security, have led to the adoption of deep learning approaches for use in network intrusion detection systems. These methods have achieved superior performance against conventional network attacks, which enable the deployment of practical security systems to unique and dynamic sectors. Adversarial machine learning, unfortunately, has recently shown that deep learning models are inherently vulnerable to adversarial modifications on their input data. Because of this susceptibility, the deep learning models deployed to power a network defense could in fact be the weakest entry point for compromising a network system. In this paper, we show that by modifying on average as little as 1.38 of the input features, an adversary can generate malicious inputs which effectively fool a deep learning based NIDS. Therefore, when designing such systems, it is crucial to consider the performance from not only the conventional network security perspective but also the adversarial machine learning domain.",
                        "Citation Paper Authors": "Authors:Joseph Clements, Yuzhe Yang, Ankur Sharma, Hongxin Hu, Yingjie Lao"
                    }
                },
                {
                    "Sentence ID": 55,
                    "Sentence": "propose a graphical\nframework for discrete domains with guarantees of minimal adversarial cost. Recently, Pierazzi et\nal. ",
                    "Citation Text": "Fabio Pierazzi, Feargus Pendlebury, Jacopo Cortellazzi, and Lorenzo Cavallaro. 2019. Intriguing Properties of Adversarial\nML Attacks in the Problem Space. arXiv preprint arXiv:1911.02142 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.02142",
                        "Citation Paper Title": "Title:Intriguing Properties of Adversarial ML Attacks in the Problem Space",
                        "Citation Paper Abstract": "Abstract:Recent research efforts on adversarial ML have investigated problem-space attacks, focusing on the generation of real evasive objects in domains where, unlike images, there is no clear inverse mapping to the feature space (e.g., software). However, the design, comparison, and real-world implications of problem-space attacks remain underexplored. This paper makes two major contributions. First, we propose a novel formalization for adversarial ML evasion attacks in the problem-space, which includes the definition of a comprehensive set of constraints on available transformations, preserved semantics, robustness to preprocessing, and plausibility. We shed light on the relationship between feature space and problem space, and we introduce the concept of side-effect features as the byproduct of the inverse feature-mapping problem. This enables us to define and prove necessary and sufficient conditions for the existence of problem-space attacks. We further demonstrate the expressive power of our formalization by using it to describe several attacks from related literature across different domains. Second, building on our formalization, we propose a novel problem-space attack on Android malware that overcomes past limitations. Experiments on a dataset with 170K Android apps from 2017 and 2018 show the practical feasibility of evading a state-of-the-art malware classifier along with its hardened version. Our results demonstrate that \"adversarial-malware as a service\" is a realistic threat, as we automatically generate thousands of realistic and inconspicuous adversarial applications at scale, where on average it takes only a few minutes to generate an adversarial app. Our formalization of problem-space attacks paves the way to more principled research in this domain.",
                        "Citation Paper Authors": "Authors:Fabio Pierazzi, Feargus Pendlebury, Jacopo Cortellazzi, Lorenzo Cavallaro"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "construct a general black-box framework based on reinforcement learning for\nattacking static portable executable anti-malware engines. Kulynych et al. ",
                    "Citation Text": "Bogdan Kulynych, Jamie Hayes, Nikita Samarin, and Carmela Troncoso. 2018. Evading classifiers in discrete domains\nwith provable optimality guarantees. arXiv preprint arXiv:1810.10939 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.10939",
                        "Citation Paper Title": "Title:Evading classifiers in discrete domains with provable optimality guarantees",
                        "Citation Paper Abstract": "Abstract:Machine-learning models for security-critical applications such as bot, malware, or spam detection, operate in constrained discrete domains. These applications would benefit from having provable guarantees against adversarial examples. The existing literature on provable adversarial robustness of models, however, exclusively focuses on robustness to gradient-based attacks in domains such as images. These attacks model the adversarial cost, e.g., amount of distortion applied to an image, as a $p$-norm. We argue that this approach is not well-suited to model adversarial costs in constrained domains where not all examples are feasible.\nWe introduce a graphical framework that (1) generalizes existing attacks in discrete domains, (2) can accommodate complex cost functions beyond $p$-norms, including financial cost incurred when attacking a classifier, and (3) efficiently produces valid adversarial examples with guarantees of minimal adversarial cost. These guarantees directly translate into a notion of adversarial robustness that takes into account domain constraints and the adversary's capabilities. We show how our framework can be used to evaluate security by crafting adversarial examples that evade a Twitter-bot detection classifier with provably minimal number of changes; and to build privacy defenses by crafting adversarial examples that evade a privacy-invasive website-fingerprinting classifier.",
                        "Citation Paper Authors": "Authors:Bogdan Kulynych, Jamie Hayes, Nikita Samarin, Carmela Troncoso"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "propose a black-box\nattack against PDF malware classifiers that uses hill-climbing over a set of feasible transformations.\nAnderson et al. ",
                    "Citation Text": "Hyrum S Anderson, Anant Kharkar, Bobby Filar, David Evans, and Phil Roth. 2018. Learning to evade static PE machine\nlearning malware models via reinforcement learning. arXiv preprint arXiv:1801.08917 (2018).\nACM Trans. Priv. Sec., Vol. 1, No. 1, Article . Publication date: June 2022.32 Alesia Chernikova and Alina Oprea",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.08917",
                        "Citation Paper Title": "Title:Learning to Evade Static PE Machine Learning Malware Models via Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:Machine learning is a popular approach to signatureless malware detection because it can generalize to never-before-seen malware families and polymorphic strains. This has resulted in its practical use for either primary detection engines or for supplementary heuristic detection by anti-malware vendors. Recent work in adversarial machine learning has shown that deep learning models are susceptible to gradient-based attacks, whereas non-differentiable models that report a score can be attacked by genetic algorithms that aim to systematically reduce the score. We propose a more general framework based on reinforcement learning (RL) for attacking static portable executable (PE) anti-malware engines. The general framework does not require a differentiable model nor does it require the engine to produce a score. Instead, an RL agent is equipped with a set of functionality-preserving operations that it may perform on the PE file. Through a series of games played against the anti-malware engine, it learns which sequences of operations are likely to result in evading the detector for any given malware sample. This enables completely black-box attacks against static PE anti-malware, and produces functional evasive malware samples as a direct result. We show in experiments that our method can attack a gradient-boosted machine learning model with evasion rates that are substantial and appear to be strongly dependent on the dataset. We demonstrate that attacks against this model appear to also evade components of publicly hosted antivirus engines. Adversarial training results are also presented: by retraining the model on evasive ransomware samples, a subsequent attack is 33% less effective. However, there are overfitting dangers when adversarial training, which we note. We release code to allow researchers to reproduce and improve this approach.",
                        "Citation Paper Authors": "Authors:Hyrum S. Anderson, Anant Kharkar, Bobby Filar, David Evans, Phil Roth"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "propose a black-box attack based on genetic algorithms for\nmanipulating PDF files while maintaining the required format. Dang et al. ",
                    "Citation Text": "Hung Dang, Yue Huang, and Ee-Chien Chang. 2017. Evading classifiers by morphing in the dark. In Proceedings of the\n2017 ACM SIGSAC Conference on Computer and Communications Security . ACM, 119\u2013133.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.07535",
                        "Citation Paper Title": "Title:Evading Classifiers by Morphing in the Dark",
                        "Citation Paper Abstract": "Abstract:Learning-based systems have been shown to be vulnerable to evasion through adversarial data manipulation. These attacks have been studied under assumptions that the adversary has certain knowledge of either the target model internals, its training dataset or at least classification scores it assigns to input samples. In this paper, we investigate a much more constrained and realistic attack scenario wherein the target classifier is minimally exposed to the adversary, revealing on its final classification decision (e.g., reject or accept an input sample). Moreover, the adversary can only manipulate malicious samples using a blackbox morpher. That is, the adversary has to evade the target classifier by morphing malicious samples \"in the dark\". We present a scoring mechanism that can assign a real-value score which reflects evasion progress to each sample based on the limited information available. Leveraging on such scoring mechanism, we propose an evasion method -- EvadeHC -- and evaluate it against two PDF malware detectors, namely PDFRate and Hidost. The experimental evaluation demonstrates that the proposed evasion attacks are effective, attaining $100\\%$ evasion rate on the evaluation dataset. Interestingly, EvadeHC outperforms the known classifier evasion technique that operates based on classification scores output by the classifiers. Although our evaluations are conducted on PDF malware classifier, the proposed approaches are domain-agnostic and is of wider application to other learning-based systems.",
                        "Citation Paper Authors": "Authors:Hung Dang, Yue Huang, Ee-Chien Chang"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "discover regions in executables that would not affect the intended malware behavior.\nKolosnjaji et al. ",
                    "Citation Text": "Bojan Kolosnjaji, Ambra Demontis, Battista Biggio, Davide Maiorca, Giorgio Giacinto, Claudia Eckert, and Fabio Roli.\n2018. Adversarial malware binaries: Evading deep learning for malware detection in executables. In 2018 26th European\nSignal Processing Conference (EUSIPCO) . IEEE, 533\u2013537.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.04173",
                        "Citation Paper Title": "Title:Adversarial Malware Binaries: Evading Deep Learning for Malware Detection in Executables",
                        "Citation Paper Abstract": "Abstract:Machine-learning methods have already been exploited as useful tools for detecting malicious executable files. They leverage data retrieved from malware samples, such as header fields, instruction sequences, or even raw bytes, to learn models that discriminate between benign and malicious software. However, it has also been shown that machine learning and deep neural networks can be fooled by evasion attacks (also referred to as adversarial examples), i.e., small changes to the input data that cause misclassification at test time. In this work, we investigate the vulnerability of malware detection methods that use deep networks to learn from raw bytes. We propose a gradient-based attack that is capable of evading a recently-proposed deep network suited to this purpose by only changing few specific bytes at the end of each malware sample, while preserving its intrusive functionality. Promising results show that our adversarial malware binaries evade the targeted network with high probability, even though less than 1% of their bytes are modified.",
                        "Citation Paper Authors": "Authors:Bojan Kolosnjaji, Ambra Demontis, Battista Biggio, Davide Maiorca, Giorgio Giacinto, Claudia Eckert, Fabio Roli"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "add bytes to malicious binaries either at the end or in slack regions to create adversarial examples.\nKreuk ",
                    "Citation Text": "Felix Kreuk, Assi Barak, Shir Aviv-Reuven, Moran Baruch, Benny Pinkas, and Joseph Keshet. 2018. Deceiving end-to-end\ndeep learning malware detectors using adversarial examples. arXiv preprint arXiv:1802.04528 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.04528",
                        "Citation Paper Title": "Title:Deceiving End-to-End Deep Learning Malware Detectors using Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:In recent years, deep learning has shown performance breakthroughs in many applications, such as image detection, image segmentation, pose estimation, and speech recognition. However, this comes with a major concern: deep networks have been found to be vulnerable to adversarial examples. Adversarial examples are slightly modified inputs that are intentionally designed to cause a misclassification by the model. In the domains of images and speech, the modifications are so small that they are not seen or heard by humans, but nevertheless greatly affect the classification of the model.\nDeep learning models have been successfully applied to malware detection. In this domain, generating adversarial examples is not straightforward, as small modifications to the bytes of the file could lead to significant changes in its functionality and validity. We introduce a novel loss function for generating adversarial examples specifically tailored for discrete input sets, such as executable bytes. We modify malicious binaries so that they would be detected as benign, while preserving their original functionality, by injecting a small sequence of bytes (payload) in the binary file. We applied this approach to an end-to-end convolutional deep learning malware detection model and show a high rate of detection evasion. Moreover, we show that our generated payload is robust enough to be transferable within different locations of the same file and across different files, and that its entropy is low and similar to that of benign data sections.",
                        "Citation Paper Authors": "Authors:Felix Kreuk, Assi Barak, Shir Aviv-Reuven, Moran Baruch, Benny Pinkas, Joseph Keshet"
                    }
                },
                {
                    "Sentence ID": 66,
                    "Sentence": "for a\nmalware classification application in which features can be added or removed. Suciu et al. ",
                    "Citation Text": "Octavian Suciu, Scott E Coull, and Jeffrey Johns. 2018. Exploring adversarial examples in malware detection. arXiv\npreprint arXiv:1810.08280 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.08280",
                        "Citation Paper Title": "Title:Exploring Adversarial Examples in Malware Detection",
                        "Citation Paper Abstract": "Abstract:The convolutional neural network (CNN) architecture is increasingly being applied to new domains, such as malware detection, where it is able to learn malicious behavior from raw bytes extracted from executables. These architectures reach impressive performance with no feature engineering effort involved, but their robustness against active attackers is yet to be understood. Such malware detectors could face a new attack vector in the form of adversarial interference with the classification model. Existing evasion attacks intended to cause misclassification on test-time instances, which have been extensively studied for image classifiers, are not applicable because of the input semantics that prevents arbitrary changes to the binaries. This paper explores the area of adversarial examples for malware detection. By training an existing model on a production-scale dataset, we show that some previous attacks are less effective than initially reported, while simultaneously highlighting architectural weaknesses that facilitate new attack strategies for malware classification. Finally, we explore how generalizable different attack strategies are, the trade-offs when aiming to increase their effectiveness, and the transferability of single-step attacks.",
                        "Citation Paper Authors": "Authors:Octavian Suciu, Scott E. Coull, Jeffrey Johns"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": ". Research on the\nrobustness of DNNs at testing time started with the work of Biggio et al. ",
                    "Citation Text": "Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic, Pavel Laskov, Giorgio Giacinto, and Fabio\nRoli. 2013. Evasion Attacks against Machine Learning at Test Time. In Proc. Joint European Conference on Machine\nLearning and Knowledge Discovery in Databases (ECML PKDD) .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.06131",
                        "Citation Paper Title": "Title:Evasion Attacks against Machine Learning at Test Time",
                        "Citation Paper Abstract": "Abstract:In security-sensitive applications, the success of machine learning depends on a thorough vetting of their resistance to adversarial data. In one pertinent, well-motivated attack scenario, an adversary may attempt to evade a deployed system at test time by carefully manipulating attack samples. In this work, we present a simple but effective gradient-based approach that can be exploited to systematically assess the security of several, widely-used classification algorithms against evasion attacks. Following a recently proposed framework for security evaluation, we simulate attack scenarios that exhibit different risk levels for the classifier by increasing the attacker's knowledge of the system and her ability to manipulate attack samples. This gives the classifier designer a better picture of the classifier performance under evasion attacks, and allows him to perform a more informed model selection (or parameter setting). We evaluate our approach on the relevant security task of malware detection in PDF files, and show that such systems can be easily evaded. We also sketch some countermeasures suggested by our analysis.",
                        "Citation Paper Authors": "Authors:Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic, Pavel Laskov, Giorgio Giacinto, Fabio Roli"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1909.05819v2": {
            "Paper Title": "Query Obfuscation Semantic Decomposition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.02203v3": {
            "Paper Title": "Model Agnostic Defence against Backdoor Attacks in Machine Learning",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": "generalises randomised smoothing to\ndefend against backdoor attacks and shows the theoretical\nfeasibility of using randomised smoothing to certify robustness\nagainst backdoor attacks. RAB ",
                    "Citation Text": "Maurice Weber, Xiaojun Xu, Bojan Karlas, Ce Zhang, and Bo Li. RAB:\nprovable robustness against backdoor attacks. CoRR , abs/2003.08904,\n2020. URL: https://arxiv.org/abs/2003.08904, arXiv:2003.08904 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.08904",
                        "Citation Paper Title": "Title:RAB: Provable Robustness Against Backdoor Attacks",
                        "Citation Paper Abstract": "Abstract:Recent studies have shown that deep neural networks (DNNs) are vulnerable to adversarial attacks, including evasion and backdoor (poisoning) attacks. On the defense side, there have been intensive efforts on improving both empirical and provable robustness against evasion attacks; however, the provable robustness against backdoor attacks still remains largely unexplored. In this paper, we focus on certifying the machine learning model robustness against general threat models, especially backdoor attacks. We first provide a unified framework via randomized smoothing techniques and show how it can be instantiated to certify the robustness against both evasion and backdoor attacks. We then propose the first robust training process, RAB, to smooth the trained model and certify its robustness against backdoor attacks. We prove the robustness bound for machine learning models trained with RAB and prove that our robustness bound is tight. In addition, we theoretically show that it is possible to train the robust smoothed models efficiently for simple models such as K-nearest neighbor classifiers, and we propose an exact smooth-training algorithm that eliminates the need to sample from a noise distribution for such models. Empirically, we conduct comprehensive experiments for different machine learning (ML) models such as DNNs, support vector machines, and K-NN models on MNIST, CIFAR-10, and ImageNette datasets and provide the first benchmark for certified robustness against backdoor attacks. In addition, we evaluate K-NN models on a spambase tabular dataset to demonstrate the advantages of the proposed exact algorithm. Both the theoretic analysis and the comprehensive evaluation on diverse ML models and datasets shed light on further robust learning strategies against general training time attacks.",
                        "Citation Paper Authors": "Authors:Maurice Weber, Xiaojun Xu, Bojan Karla\u0161, Ce Zhang, Bo Li"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": ".\nKey Insight: Backdoor attack is triggered when a speci\ufb01c set\nof neurons in the targeted model is activated upon encountering\na backdoored image ",
                    "Citation Text": "Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning:\nDefending against backdooring attacks on deep neural networks. In\nResearch in Attacks, Intrusions, and Defenses - 21st International\nSymposium, RAID 2018, Heraklion, Crete, Greece, September 10-12,\n2018, Proceedings , pages 273\u2013294, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.12185",
                        "Citation Paper Title": "Title:Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:Deep neural networks (DNNs) provide excellent performance across a wide range of classification tasks, but their training requires high computational resources and is often outsourced to third parties. Recent work has shown that outsourced training introduces the risk that a malicious trainer will return a backdoored DNN that behaves normally on most inputs but causes targeted misclassifications or degrades the accuracy of the network when a trigger known only to the attacker is present. In this paper, we provide the first effective defenses against backdoor attacks on DNNs. We implement three backdoor attacks from prior work and use them to investigate two promising defenses, pruning and fine-tuning. We show that neither, by itself, is sufficient to defend against sophisticated attackers. We then evaluate fine-pruning, a combination of pruning and fine-tuning, and show that it successfully weakens or even eliminates the backdoors, i.e., in some cases reducing the attack success rate to 0% with only a 0.4% drop in accuracy for clean (non-triggering) inputs. Our work provides the first step toward defenses against backdoor attacks in deep neural networks.",
                        "Citation Paper Authors": "Authors:Kang Liu, Brendan Dolan-Gavitt, Siddharth Garg"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": ".\nThe goal of NEOis to defend against these classes of attacks\nin an ef\ufb01cient manner and reconstruct the backdoor trigger.\nBackdoor defences in ML: A recent work ",
                    "Citation Text": "Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures\nin backdoor attacks. In Advances in Neural Information Process-\ning Systems 31: Annual Conference on Neural Information Process-\ning Systems 2018, NeurIPS 2018, 3-8 December 2018, Montr\u00e9al,\nCanada. , pages 8011\u20138021, 2018. URL: http://papers.nips.cc/paper/\n8024-spectral-signatures-in-backdoor-attacks.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.00636",
                        "Citation Paper Title": "Title:Spectral Signatures in Backdoor Attacks",
                        "Citation Paper Abstract": "Abstract:A recent line of work has uncovered a new form of data poisoning: so-called \\emph{backdoor} attacks. These attacks are particularly dangerous because they do not affect a network's behavior on typical, benign data. Rather, the network only deviates from its expected output when triggered by a perturbation planted by an adversary.\nIn this paper, we identify a new property of all known backdoor attacks, which we call \\emph{spectral signatures}. This property allows us to utilize tools from robust statistics to thwart the attacks. We demonstrate the efficacy of these signatures in detecting and removing poisoned examples on real image sets and state of the art neural network architectures. We believe that understanding spectral signatures is a crucial first step towards designing ML systems secure against such backdoor attacks",
                        "Citation Paper Authors": "Authors:Brandon Tran, Jerry Li, Aleksander Madry"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "to estimate a DNN\u2019s decision\nboundary by calculating tight bounds on the output of a model\nfor a given range of inputs. Similarly, Reluplex ",
                    "Citation Text": "Guy Katz, Clark W. Barrett, David L. Dill, Kyle Julian, and Mykel J.\nKochenderfer. Reluplex: An ef\ufb01cient SMT solver for verifying deep\nneural networks. In Computer Aided Veri\ufb01cation - 29th International Con-\nference, CAV 2017, Heidelberg, Germany, July 24-28, 2017, Proceedings,\nPart I , pages 97\u2013117, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.01135",
                        "Citation Paper Title": "Title:Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:Deep neural networks have emerged as a widely used and effective means for tackling complex, real-world problems. However, a major obstacle in applying them to safety-critical systems is the great difficulty in providing formal guarantees about their behavior. We present a novel, scalable, and efficient technique for verifying properties of deep neural networks (or providing counter-examples). The technique is based on the simplex method, extended to handle the non-convex Rectified Linear Unit (ReLU) activation function, which is a crucial ingredient in many modern neural networks. The verification procedure tackles neural networks as a whole, without making any simplifying assumptions. We evaluated our technique on a prototype deep neural network implementation of the next-generation airborne collision avoidance system for unmanned aircraft (ACAS Xu). Results show that our technique can successfully prove properties of networks that are an order of magnitude larger than the largest networks verified using existing methods.",
                        "Citation Paper Authors": "Authors:Guy Katz, Clark Barrett, David Dill, Kyle Julian, Mykel Kochenderfer"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "takes a feature\nguided black-box approach to verify the safety of DNNs.\nDeepConcolic ",
                    "Citation Text": "Youcheng Sun, Min Wu, Wenjie Ruan, Xiaowei Huang, Marta\nKwiatkowska, and Daniel Kroening. Concolic testing for deep neu-\nral networks. In Proceedings of the 33rd ACM/IEEE International\nConference on Automated Software Engineering, ASE 2018, Montpellier,\nFrance, September 3-7, 2018 , pages 109\u2013119, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.00089",
                        "Citation Paper Title": "Title:Concolic Testing for Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:Concolic testing combines program execution and symbolic analysis to explore the execution paths of a software program. This paper presents the first concolic testing approach for Deep Neural Networks (DNNs). More specifically, we formalise coverage criteria for DNNs that have been studied in the literature, and then develop a coherent method for performing concolic testing to increase test coverage. Our experimental results show the effectiveness of the concolic testing approach in both achieving high coverage and finding adversarial examples.",
                        "Citation Paper Authors": "Authors:Youcheng Sun, Min Wu, Wenjie Ruan, Xiaowei Huang, Marta Kwiatkowska, Daniel Kroening"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "measures the test quality and formalises a set\nof testing criteria for DNNs. SafeCV ",
                    "Citation Text": "Matthew Wicker, Xiaowei Huang, and Marta Kwiatkowska. Feature-\nguided black-box safety testing of deep neural networks. In Tools\nand Algorithms for the Construction and Analysis of Systems - 24th\nInternational Conference, TACAS 2018, Held as Part of the European\nJoint Conferences on Theory and Practice of Software, ETAPS 2018,\nThessaloniki, Greece, April 14-20, 2018, Proceedings, Part I , pages\n408\u2013426, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.07859",
                        "Citation Paper Title": "Title:Feature-Guided Black-Box Safety Testing of Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:Despite the improved accuracy of deep neural networks, the discovery of adversarial examples has raised serious safety concerns. Most existing approaches for crafting adversarial examples necessitate some knowledge (architecture, parameters, etc.) of the network at hand. In this paper, we focus on image classifiers and propose a feature-guided black-box approach to test the safety of deep neural networks that requires no such knowledge. Our algorithm employs object detection techniques such as SIFT (Scale Invariant Feature Transform) to extract features from an image. These features are converted into a mutable saliency distribution, where high probability is assigned to pixels that affect the composition of the image with respect to the human visual system. We formulate the crafting of adversarial examples as a two-player turn-based stochastic game, where the first player's objective is to minimise the distance to an adversarial example by manipulating the features, and the second player can be cooperative, adversarial, or random. We show that, theoretically, the two-player game can con- verge to the optimal strategy, and that the optimal strategy represents a globally minimal adversarial image. For Lipschitz networks, we also identify conditions that provide safety guarantees that no adversarial examples exist. Using Monte Carlo tree search we gradually explore the game state space to search for adversarial examples. Our experiments show that, despite the black-box setting, manipulations guided by a perception-based saliency distribution are competitive with state-of-the-art methods that rely on white-box saliency matrices or sophisticated optimization procedures. Finally, we show how our method can be used to evaluate robustness of neural networks in safety-critical applications such as traffic sign recognition in self-driving cars.",
                        "Citation Paper Authors": "Authors:Matthew Wicker, Xiaowei Huang, Marta Kwiatkowska"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "leverages metamorphic\nrelations to discover error cases in DNNs. A recent work\nDeepGauge ",
                    "Citation Text": "Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li,\nChunyang Chen, Ting Su, Li Li, Yang Liu, Jianjun Zhao, and Yadong\nWang. Deepgauge: multi-granularity testing criteria for deep learning\nsystems. In Proceedings of the 33rd ACM/IEEE International Conference\non Automated Software Engineering, ASE 2018, Montpellier, France,\nSeptember 3-7, 2018 , pages 120\u2013131, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.07519",
                        "Citation Paper Title": "Title:DeepGauge: Multi-Granularity Testing Criteria for Deep Learning Systems",
                        "Citation Paper Abstract": "Abstract:Deep learning (DL) defines a new data-driven programming paradigm that constructs the internal system logic of a crafted neuron network through a set of training data. We have seen wide adoption of DL in many safety-critical scenarios. However, a plethora of studies have shown that the state-of-the-art DL systems suffer from various vulnerabilities which can lead to severe consequences when applied to real-world applications. Currently, the testing adequacy of a DL system is usually measured by the accuracy of test data. Considering the limitation of accessible high quality test data, good accuracy performance on test data can hardly provide confidence to the testing adequacy and generality of DL systems. Unlike traditional software systems that have clear and controllable logic and functionality, the lack of interpretability in a DL system makes system analysis and defect detection difficult, which could potentially hinder its real-world deployment. In this paper, we propose DeepGauge, a set of multi-granularity testing criteria for DL systems, which aims at rendering a multi-faceted portrayal of the testbed. The in-depth evaluation of our proposed testing criteria is demonstrated on two well-known datasets, five DL systems, and with four state-of-the-art adversarial attack techniques against DL. The potential usefulness of DeepGauge sheds light on the construction of more generic and robust DL systems.",
                        "Citation Paper Authors": "Authors:Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chunyang Chen, Ting Su, Li Li, Yang Liu, Jianjun Zhao, Yadong Wang"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": ", which uses the entropy of\nthe outputs of poisoned inputs to identify backdoored models.\nHence, such adaptive attack is not an applicable attack for\nNeuralCleanse and N EO.\nVI. R ELATED WORK\nTesting and Veri\ufb01cation of ML models: DeepXplore ",
                    "Citation Text": "Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. Deepxplore:\nAutomated whitebox testing of deep learning systems. In Proceedings of\nthe 26th Symposium on Operating Systems Principles, Shanghai, China,\nOctober 28-31, 2017 , pages 1\u201318, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.06640",
                        "Citation Paper Title": "Title:DeepXplore: Automated Whitebox Testing of Deep Learning Systems",
                        "Citation Paper Abstract": "Abstract:Deep learning (DL) systems are increasingly deployed in safety- and security-critical domains including self-driving cars and malware detection, where the correctness and predictability of a system's behavior for corner case inputs are of great importance. Existing DL testing depends heavily on manually labeled data and therefore often fails to expose erroneous behaviors for rare inputs.\nWe design, implement, and evaluate DeepXplore, the first whitebox framework for systematically testing real-world DL systems. First, we introduce neuron coverage for systematically measuring the parts of a DL system exercised by test inputs. Next, we leverage multiple DL systems with similar functionality as cross-referencing oracles to avoid manual checking. Finally, we demonstrate how finding inputs for DL systems that both trigger many differential behaviors and achieve high neuron coverage can be represented as a joint optimization problem and solved efficiently using gradient-based search techniques.\nDeepXplore efficiently finds thousands of incorrect corner case behaviors (e.g., self-driving cars crashing into guard rails and malware masquerading as benign software) in state-of-the-art DL models with thousands of neurons trained on five popular datasets including ImageNet and Udacity self-driving challenge data. For all tested DL models, on average, DeepXplore generated one test input demonstrating incorrect behavior within one second while running only on a commodity laptop. We further show that the test inputs generated by DeepXplore can also be used to retrain the corresponding DL model to improve the model's accuracy by up to 3%.",
                        "Citation Paper Authors": "Authors:Kexin Pei, Yinzhi Cao, Junfeng Yang, Suman Jana"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1905.11947v3": {
            "Paper Title": "Private Identity Testing for High-Dimensional Distributions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.02919v4": {
            "Paper Title": "An Empirical Study on the Intrinsic Privacy of SGD",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": ". This poisoning attack yields \u2018worst-case\u2019 privacy under potentially\narbitrary dataset perturbations. Nasr et al. demonstrate that the theoretical upper bounds are often loose in realistic\nadversarial setting such as a black-box API based adversary ",
                    "Citation Text": "Milad Nasr, Shuang Song, Abhradeep Thakurta, Nicolas Papernot, and Nicholas Carlini. Adversary instantiation:\nLower bounds for differentially private machine learning. arXiv preprint arXiv:2101.04535 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.04535",
                        "Citation Paper Title": "Title:Adversary Instantiation: Lower Bounds for Differentially Private Machine Learning",
                        "Citation Paper Abstract": "Abstract:Differentially private (DP) machine learning allows us to train models on private data while limiting data leakage. DP formalizes this data leakage through a cryptographic game, where an adversary must predict if a model was trained on a dataset D, or a dataset D' that differs in just one example.If observing the training algorithm does not meaningfully increase the adversary's odds of successfully guessing which dataset the model was trained on, then the algorithm is said to be differentially private. Hence, the purpose of privacy analysis is to upper bound the probability that any adversary could successfully guess which dataset the model was trained this http URL our paper, we instantiate this hypothetical adversary in order to establish lower bounds on the probability that this distinguishing game can be won. We use this adversary to evaluate the importance of the adversary capabilities allowed in the privacy analysis of DP training algorithms.For DP-SGD, the most common method for training neural networks with differential privacy, our lower bounds are tight and match the theoretical upper bound. This implies that in order to prove better upper bounds, it will be necessary to make use of additional assumptions. Fortunately, we find that our attacks are significantly weaker when additional (realistic)restrictions are put in place on the adversary's capabilities.Thus, in the practical setting common to many real-world deployments, there is a gap between our lower bounds and the upper bounds provided by the analysis: differential privacy is conservative and adversaries may not be able to leak as much information as suggested by the theoretical bound.",
                        "Citation Paper Authors": "Authors:Milad Nasr, Shuang Song, Abhradeep Thakurta, Nicolas Papernot, Nicholas Carlini"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": "showed that initialisation noise can produce very different models. Neural tangent kernels have\nalso yielded results supporting the importance of a good initialisation of generalisation error ",
                    "Citation Text": "Yaoyu Zhang, Zhi-Qin John Xu, Tao Luo, and Zheng Ma. A type of generalization error induced by initialization\nin deep neural networks. In Mathematical and Scienti\ufb01c Machine Learning , pages 144\u2013164. PMLR, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.07777",
                        "Citation Paper Title": "Title:A type of generalization error induced by initialization in deep neural networks",
                        "Citation Paper Abstract": "Abstract:How initialization and loss function affect the learning of a deep neural network (DNN), specifically its generalization error, is an important problem in practice. In this work, by exploiting the linearity of DNN training dynamics in the NTK regime \\citep{jacot2018neural,lee2019wide}, we provide an explicit and quantitative answer to this problem. Focusing on regression problem, we prove that, in the NTK regime, for any loss in a general class of functions, the DNN finds the same \\emph{global} minima---the one that is nearest to the initial value in the parameter space, or equivalently, the one that is closest to the initial DNN output in the corresponding reproducing kernel Hilbert space. Using these optimization problems, we quantify the impact of initial output and prove that a random non-zero one increases the generalization error. We further propose an antisymmetrical initialization (ASI) trick that eliminates this type of error and accelerates the training. To understand whether the above results hold in general, we also perform experiments for DNNs in the non-NTK regime, which demonstrate the effectiveness of our theoretical results and the ASI trick in a qualitative sense. Overall, our work serves as a baseline for the further investigation of the impact of initialization and loss function on the generalization of DNNs, which can potentially guide and improve the training of DNNs in practice.",
                        "Citation Paper Authors": "Authors:Yaoyu Zhang, Zhi-Qin John Xu, Tao Luo, Zheng Ma"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": ". They treat SGD as a \u2018black box\u2019 and inject noise on the \ufb01nal model weights,\nhowever they do not consider the intrinsic noise in SGD. The random sampling of SGD is implicitly used for privacy\nampli\ufb01cation ",
                    "Citation Text": "Borja Balle, Gilles Barthe, and Marco Gaboardi. Privacy ampli\ufb01cation by subsampling: Tight analyses via\ncouplings and divergences. arXiv preprint arXiv:1807.01647 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.01647",
                        "Citation Paper Title": "Title:Privacy Amplification by Subsampling: Tight Analyses via Couplings and Divergences",
                        "Citation Paper Abstract": "Abstract:Differential privacy comes equipped with multiple analytical tools for the design of private data analyses. One important tool is the so-called \"privacy amplification by subsampling\" principle, which ensures that a differentially private mechanism run on a random subsample of a population provides higher privacy guarantees than when run on the entire population. Several instances of this principle have been studied for different random subsampling methods, each with an ad-hoc analysis. In this paper we present a general method that recovers and improves prior analyses, yields lower bounds and derives new instances of privacy amplification by subsampling. Our method leverages a characterization of differential privacy as a divergence which emerged in the program verification community. Furthermore, it introduces new tools, including advanced joint convexity and privacy profiles, which might be of independent interest.",
                        "Citation Paper Authors": "Authors:Borja Balle, Gilles Barthe, Marco Gaboardi"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "3.1 Differentially-private machine learning\nOngoing work develops differentially-private variants of training algorithms, including objective perturbation ",
                    "Citation Text": "Kamalika Chaudhuri, Claire Monteleoni, and Anand D Sarwate. Differentially private empirical risk minimization.\nJournal of Machine Learning Research , 12(Mar):1069\u20131109, 2011.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:0912.0071",
                        "Citation Paper Title": "Title:Differentially Private Empirical Risk Minimization",
                        "Citation Paper Abstract": "Abstract:Privacy-preserving machine learning algorithms are crucial for the increasingly common setting in which personal data, such as medical or financial records, are analyzed. We provide general techniques to produce privacy-preserving approximations of classifiers learned via (regularized) empirical risk minimization (ERM). These algorithms are private under the $\\epsilon$-differential privacy definition due to Dwork et al. (2006). First we apply the output perturbation ideas of Dwork et al. (2006), to ERM classification. Then we propose a new method, objective perturbation, for privacy-preserving machine learning algorithm design. This method entails perturbing the objective function before optimizing over classifiers. If the loss and regularizer satisfy certain convexity and differentiability criteria, we prove theoretical results showing that our algorithms preserve privacy, and provide generalization bounds for linear and nonlinear kernels. We further present a privacy-preserving technique for tuning the parameters in general machine learning algorithms, thereby providing end-to-end privacy guarantees for the training process. We apply these results to produce privacy-preserving analogues of regularized logistic regression and support vector machines. We obtain encouraging results from evaluating their performance on real demographic and benchmark data sets. Our results show that both theoretically and empirically, objective perturbation is superior to the previous state-of-the-art, output perturbation, in managing the inherent tradeoff between privacy and learning performance.",
                        "Citation Paper Authors": "Authors:Kamalika Chaudhuri, Claire Monteleoni, Anand D. Sarwate"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1908.11568v5": {
            "Paper Title": "Pacer: Comprehensive Network Side-Channel Mitigation in the Cloud",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.09470v3": {
            "Paper Title": "Characterizing Attacks on Deep Reinforcement Learning",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": "Adversarial attacks on machine learning models . Our attacks\ndraw inspirations from previously proposed attacks. Goodfellow\net al. ",
                    "Citation Text": "Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining\nand harnessing adversarial examples. In International Conference on Learning\nRepresentations .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.6572",
                        "Citation Paper Title": "Title:Explaining and Harnessing Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "for discrete control, and Deep Determinis-\ntic Policy Gradient (DDPG) ",
                    "Citation Text": "Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez,\nYuval Tassa, David Silver, and Daan Wierstra. 2016. Continuous control with deep\nreinforcement learning. In International Conference on Learning Representations .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1509.02971",
                        "Citation Paper Title": "Title:Continuous control with deep reinforcement learning",
                        "Citation Paper Abstract": "Abstract:We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.",
                        "Citation Paper Authors": "Authors:Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "propose a black-box attack\nmethod that trains another DQN network to minimize the expected\nreturn using FGSM. Gleave et al . ",
                    "Citation Text": "Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart\nRussell. 2019. Adversarial policies: Attacking deep reinforcement learning. arXiv\npreprint arXiv:1905.10615 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.10615",
                        "Citation Paper Title": "Title:Adversarial Policies: Attacking Deep Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:Deep reinforcement learning (RL) policies are known to be vulnerable to adversarial perturbations to their observations, similar to adversarial examples for classifiers. However, an attacker is not usually able to directly modify another agent's observations. This might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, against state-of-the-art victims trained via self-play to be robust to opponents. The adversarial policies reliably win against the victims but generate seemingly random and uncoordinated behavior. We find that these policies are more successful in high-dimensional environments, and induce substantially different activations in the victim policy network than when the victim plays against a normal opponent. Videos are available at this https URL.",
                        "Citation Paper Authors": "Authors:Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, Stuart Russell"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "demonstrates a way to generate a \u201cuniversal\u201d perturbation that is\neffective on multiple inputs. Xiao et al . ",
                    "Citation Text": "Chaowei Xiao, Dawei Yang, Bo Li, Jia Deng, and Mingyan Liu. 2019. Meshadv:\nAdversarial meshes for visual recognition. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition . 6898\u20136907.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.05206",
                        "Citation Paper Title": "Title:MeshAdv: Adversarial Meshes for Visual Recognition",
                        "Citation Paper Abstract": "Abstract:Highly expressive models such as deep neural networks (DNNs) have been widely applied to various applications. However, recent studies show that DNNs are vulnerable to adversarial examples, which are carefully crafted inputs aiming to mislead the predictions. Currently, the majority of these studies have focused on perturbation added to image pixels, while such manipulation is not physically realistic. Some works have tried to overcome this limitation by attaching printable 2D patches or painting patterns onto surfaces, but can be potentially defended because 3D shape features are intact. In this paper, we propose meshAdv to generate \"adversarial 3D meshes\" from objects that have rich shape features but minimal textural variation. To manipulate the shape or texture of the objects, we make use of a differentiable renderer to compute accurate shading on the shape and propagate the gradient. Extensive experiments show that the generated 3D meshes are effective in attacking both classifiers and object detectors. We evaluate the attack under different viewpoints. In addition, we design a pipeline to perform black-box attack on a photorealistic renderer with unknown rendering parameters.",
                        "Citation Paper Authors": "Authors:Chaowei Xiao, Dawei Yang, Bo Li, Jia Deng, Mingyan Liu"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "describe additional methods based on optimization,\nwhich results in smaller perturbations. Moosavi-Dezfooli et al . ",
                    "Citation Text": "Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal\nFrossard. 2017. Universal adversarial perturbations. In Computer Vision and\nPattern Recognition (CVPR), 2017 IEEE Conference on . IEEE, 1765\u20131773.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.08401",
                        "Citation Paper Title": "Title:Universal adversarial perturbations",
                        "Citation Paper Abstract": "Abstract:Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images.",
                        "Citation Paper Authors": "Authors:Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal Frossard"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1908.11230v4": {
            "Paper Title": "Defeating Misclassification Attacks Against Transfer Learning",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": ".\nOthers propose to preprocess the inputs before they are sent\nto the classi\ufb01ers by extra layers or networks ",
                    "Citation Text": "W. Xu, D. Evans, and Y. Qi, \u201cFeature squeezing: Detecting adver-\nsarial examples in deep neural networks,\u201d in Proc. NDSS, 2018 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.01155",
                        "Citation Paper Title": "Title:Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:Although deep neural networks (DNNs) have achieved great success in many tasks, they can often be fooled by \\emph{adversarial examples} that are generated by adding small but purposeful distortions to natural examples. Previous studies to defend against adversarial examples mostly focused on refining the DNN models, but have either shown limited success or required expensive computation. We propose a new strategy, \\emph{feature squeezing}, that can be used to harden DNN models by detecting adversarial examples. Feature squeezing reduces the search space available to an adversary by coalescing samples that correspond to many different feature vectors in the original space into a single sample. By comparing a DNN model's prediction on the original input with that on squeezed inputs, feature squeezing detects adversarial examples with high accuracy and few false positives. This paper explores two feature squeezing methods: reducing the color bit depth of each pixel and spatial smoothing. These simple strategies are inexpensive and complementary to other defenses, and can be combined in a joint detection framework to achieve high detection rates against state-of-the-art attacks.",
                        "Citation Paper Authors": "Authors:Weilin Xu, David Evans, Yanjun Qi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.12257v2": {
            "Paper Title": "Performance Analysis of TLS for Quantum Robust Cryptography on a\n  Constrained Device",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.02598v3": {
            "Paper Title": "Real-time malware process detection and automated process killing",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": ". A DQN was the reinforcement algorithm\ntrialled here, though it did not perform well by comparison with the other\nmethods, a di\u000berent RL algorithm may perform better ",
                    "Citation Text": "V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Sil-\nver, and K. Kavukcuoglu, \\Asynchronous methods for deep reinforcement\nlearning,\" in International conference on machine learning , pp. 1928{1937,\nPMLR, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.01783",
                        "Citation Paper Title": "Title:Asynchronous Methods for Deep Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.",
                        "Citation Paper Authors": "Authors:Volodymyr Mnih, Adri\u00e0 Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1908.00592v4": {
            "Paper Title": "The House That Knows You: User Authentication Based on IoT Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.10905v3": {
            "Paper Title": "PixelSteganalysis: Pixel-wise Hidden Information Removal with Low Visual\n  Degradation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.02613v2": {
            "Paper Title": "Security Issues in Language-based Software Ecosystems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.11815v4": {
            "Paper Title": "Local Model Poisoning Attacks to Byzantine-Robust Federated Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.00288v2": {
            "Paper Title": "Towards end-to-end verifiable online voting: adding verifiability to\n  established voting systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.11299v3": {
            "Paper Title": "Introducing the Robot Vulnerability Database (RVD)",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": ". In robotics, security \ufb02aws such as vulnerabilities\nare of special relevance given the physical connection to\nthe world that these systems imply. As discussed in ",
                    "Citation Text": "L. A. Kirschgens, I. Z. Ugarte, E. G. Uriarte, A. M. Rosas, and\nV . M. Vilches, \u201cRobot hazards: from safety to security,\u201d arXiv preprint\narXiv:1806.06681 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.06681",
                        "Citation Paper Title": "Title:Robot hazards: from safety to security",
                        "Citation Paper Abstract": "Abstract:Robotics landscape is experiencing big changes. Robots are spreading and will soon be everywhere. Systems traditionally employed in industry are being replaced by collaborative robots, while more and more professional and consumer robots are introduced in people's daily activities. Robots are increasingly intertwined with other facets of IT and envisioned to get much more autonomy, interacting physically with humans. We claim that, following Personal Computers (PCs) and smartphones, robots are the next technological revolution and yet, robot security is being ignored by manufacturers. The present paper aims to alert about the need of dealing not only with safety but with robot security from the very beginning of the forthcoming technological era. We provide herein a document that reviews robot hazards and analyzes the consequences of not facing these issues. We advocate strongly for a security-first approach as a must to be implemented now.",
                        "Citation Paper Authors": "Authors:Laura Alzola Kirschgens, Irati Zamalloa Ugarte, Endika Gil Uriarte, Aday Mu\u00f1iz Rosas, V\u00edctor Mayoral Vilches"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1906.03563v3": {
            "Paper Title": "Adversarial Attack Generation Empowered by Min-Max Optimization",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": "introduced a min-max based adaptive attacker\u2019s objective to craft perturbation so that\nit simultaneously evades detection and causes misclassi\ufb01cation. Inspired by our work, the min-max\nformulation has also been extended to zero-order blackbox attacks ",
                    "Citation Text": "S. Liu, S. Lu, X. Chen, Y . Feng, K. Xu, A. Al-Dujaili, M. Hong, and U. Obelilly. Min-max\noptimization without gradients: Convergence and applications to adversarial ML. CoRR ,\nabs/1909.13806, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.13806",
                        "Citation Paper Title": "Title:Min-Max Optimization without Gradients: Convergence and Applications to Adversarial ML",
                        "Citation Paper Abstract": "Abstract:In this paper, we study the problem of constrained robust (min-max) optimization ina black-box setting, where the desired optimizer cannot access the gradients of the objective function but may query its values. We present a principled optimization framework, integrating a zeroth-order (ZO) gradient estimator with an alternating projected stochastic gradient descent-ascent method, where the former only requires a small number of function queries and the later needs just one-step descent/ascent update. We show that the proposed framework, referred to as ZO-Min-Max, has a sub-linear convergence rate under mild conditions and scales gracefully with problem size. From an application side, we explore a promising connection between black-box min-max optimization and black-box evasion and poisoning attacks in adversarial machine learning (ML). Our empirical evaluations on these use cases demonstrate the effectiveness of our approach and its scalability to dimensions that prohibit using recent black-box solvers.",
                        "Citation Paper Authors": "Authors:Sijia Liu, Songtao Lu, Xiangyi Chen, Yao Feng, Kaidi Xu, Abdullah Al-Dujaili, Minyi Hong, Una-May O'Reilly"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.05164v2": {
            "Paper Title": "A Reproducibility Study of \"IP Spoofing Detection in Inter-Domain\n  Traffic\"",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": "such as ( i) inac-\ncurate timing with respect to control and data plane measurements,\n(ii) disregarding of BGP withdraw messages in the public dump\nfiles, and ( iii) inaccurate modeling of transitivity in the BGP routing\ngraph ",
                    "Citation Text": "Johann Schlamp, Matthias W\u00e4hlisch, Thomas C. Schmidt, Georg Carle, and\nErnst W. Biersack. 2016. CAIR: Using Formal Languages to Study Routing, Leaking,\nand Interception in BGP . Technical Report arXiv:1605.00618. Open Archive:\narXiv.org. http://arxiv.org/abs/1605.00618",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.00618",
                        "Citation Paper Title": "Title:CAIR: Using Formal Languages to Study Routing, Leaking, and Interception in BGP",
                        "Citation Paper Abstract": "Abstract:  The Internet routing protocol BGP expresses topological reachability and policy-based decisions simultaneously in path vectors. A complete view on the Internet backbone routing is given by the collection of all valid routes, which is infeasible to obtain due to information hiding of BGP, the lack of omnipresent collection points, and data complexity. Commonly, graph-based data models are used to represent the Internet topology from a given set of BGP routing tables but fall short of explaining policy contexts. As a consequence, routing anomalies such as route leaks and interception attacks cannot be explained with graphs.\nIn this paper, we use formal languages to represent the global routing system in a rigorous model. Our CAIR framework translates BGP announcements into a finite route language that allows for the incremental construction of minimal route automata. CAIR preserves route diversity, is highly efficient, and well-suited to monitor BGP path changes in real-time. We formally derive implementable search patterns for route leaks and interception attacks. In contrast to the state-of-the-art, we can detect these incidents. In practical experiments, we analyze public BGP data over the last seven years.",
                        "Citation Paper Authors": "Authors:Johann Schlamp, Matthias W\u00e4hlisch, Thomas C. Schmidt, Georg Carle, Ernst W. Biersack"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1909.11707v2": {
            "Paper Title": "Implementation of three LWC Schemes in the WiFi 4-Way Handshake with\n  Software Defined Radio",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.10214v5": {
            "Paper Title": "Partially Encrypted Machine Learning using Functional Encryption",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": "). The simplicity of the operations they build on guarantees good ef\ufb01ciency,\nespecially for the gradient computations, and works like ",
                    "Citation Text": "Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational ef\ufb01ciency of training\nneural networks. CoRR , abs/1410.1141, 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1410.1141",
                        "Citation Paper Title": "Title:On the Computational Efficiency of Training Neural Networks",
                        "Citation Paper Abstract": "Abstract:It is well-known that neural networks are computationally hard to train. On the other hand, in practice, modern day neural networks are trained efficiently using SGD and a variety of tricks that include different activation functions (e.g. ReLU), over-specification (i.e., train networks which are larger than needed), and regularization. In this paper we revisit the computational complexity of training neural networks from a modern perspective. We provide both positive and negative results, some of them yield new provably efficient and practical algorithms for training certain types of neural networks.",
                        "Citation Paper Authors": "Authors:Roi Livni, Shai Shalev-Shwartz, Ohad Shamir"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1905.10029v2": {
            "Paper Title": "Power up! Robust Graph Convolutional Network via Graph Powering",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.00389v4": {
            "Paper Title": "Disparate Vulnerability to Membership Inference Attacks",
            "Sentences": [
                {
                    "Sentence ID": 43,
                    "Sentence": "Theory studies on MIA. Yeom et al. studied the relation of\nMIAs to over/f_itting ",
                    "Citation Text": "Samuel Yeom, Irene Giacomelli, Ma/t_t Fredrikson, and Somesh\nJha. Privacy risk in machine learning: Analyzing the connection\nto overfi/t_ting. In IEEE Computer Security Foundations Symposium,\nCSF, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.01604",
                        "Citation Paper Title": "Title:Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting",
                        "Citation Paper Abstract": "Abstract:Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role.\nThis paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.",
                        "Citation Paper Authors": "Authors:Samuel Yeom, Irene Giacomelli, Matt Fredrikson, Somesh Jha"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "showed that\nMIAs success is bounded by DP. Humphries et al. ",
                    "Citation Text": "Thomas Humphries, Ma/t_thew Rafuse, Lindsey Tulloch, Simon\nOya, Ian Goldberg, Urs Hengartner, and Florian Kerschbaum.\nDi\ufb00erentially private learning does not bound membership\ninference. arXiv preprint arXiv:2010.12112 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.12112",
                        "Citation Paper Title": "Title:Investigating Membership Inference Attacks under Data Dependencies",
                        "Citation Paper Abstract": "Abstract:Training machine learning models on privacy-sensitive data has become a popular practice, driving innovation in ever-expanding fields. This has opened the door to new attacks that can have serious privacy implications. One such attack, the Membership Inference Attack (MIA), exposes whether or not a particular data point was used to train a model. A growing body of literature uses Differentially Private (DP) training algorithms as a defence against such attacks. However, these works evaluate the defence under the restrictive assumption that all members of the training set, as well as non-members, are independent and identically distributed. This assumption does not hold for many real-world use cases in the literature. Motivated by this, we evaluate membership inference with statistical dependencies among samples and explain why DP does not provide meaningful protection (the privacy parameter $\\epsilon$ scales with the training set size $n$) in this more general case. We conduct a series of empirical evaluations with off-the-shelf MIAs using training sets built from real-world data showing different types of dependencies among samples. Our results reveal that training set dependencies can severely increase the performance of MIAs, and therefore assuming that data samples are statistically independent can significantly underestimate the performance of MIAs.",
                        "Citation Paper Authors": "Authors:Thomas Humphries, Simon Oya, Lindsey Tulloch, Matthew Rafuse, Ian Goldberg, Urs Hengartner, Florian Kerschbaum"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "which ensures\nthat distributions of model outputs between demographic\nsubgroups are close: gap\u03c6\u02c6Y\u22480. Second, equalized odds ,\nwhich ensures that true-positive rates and false-positive rates\nbetween the subgroups are close ",
                    "Citation Text": "Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity\nin supervised learning. In NIPS , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.02413",
                        "Citation Paper Title": "Title:Equality of Opportunity in Supervised Learning",
                        "Citation Paper Abstract": "Abstract:We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy.\nIn line with other studies, our notion is oblivious: it depends only on the joint statistics of the predictor, the target and the protected attribute, but not on interpretation of individualfeatures. We study the inherent limits of defining and identifying biases based on such oblivious measures, outlining what can and cannot be inferred from different oblivious tests.\nWe illustrate our notion using a case study of FICO credit scores.",
                        "Citation Paper Authors": "Authors:Moritz Hardt, Eric Price, Nathan Srebro"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": ". Similarly, for the 0-1 loss\nproperty of the model, choosing \u03c0(AS,x) =1[AS(x) =\ny(x)]gives us accuracy equality ",
                    "Citation Text": "Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and\nAaron Roth. Fairness in criminal justice risk assessments: The state\nof the art. Sociological Methods & Research , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.09207",
                        "Citation Paper Title": "Title:Fairness in Criminal Justice Risk Assessments: The State of the Art",
                        "Citation Paper Abstract": "Abstract:Objectives: Discussions of fairness in criminal justice risk assessments typically lack conceptual precision. Rhetoric too often substitutes for careful analysis. In this paper, we seek to clarify the tradeoffs between different kinds of fairness and between fairness and accuracy.\nMethods: We draw on the existing literatures in criminology, computer science and statistics to provide an integrated examination of fairness and accuracy in criminal justice risk assessments. We also provide an empirical illustration using data from arraignments.\nResults: We show that there are at least six kinds of fairness, some of which are incompatible with one another and with accuracy.\nConclusions: Except in trivial cases, it is impossible to maximize accuracy and fairness at the same time, and impossible simultaneously to satisfy all kinds of fairness. In practice, a major complication is different base rates across different legally protected groups. There is a need to consider challenging tradeoffs.",
                        "Citation Paper Authors": "Authors:Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, Aaron Roth"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": ", or the model\u2019s gradients\nas in some white-box attacks ",
                    "Citation Text": "Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive\nprivacy analysis of deep learning: Stand-alone and federated\nlearning under passive and active white-box inference a/t_tacks. In\nIEEE Symposium on Security and Privacy, S&P , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.00910",
                        "Citation Paper Title": "Title:Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning",
                        "Citation Paper Abstract": "Abstract:Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge.\nWe evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.",
                        "Citation Paper Authors": "Authors:Milad Nasr, Reza Shokri, Amir Houmansadr"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ", and Jayaraman et al. investigated their\ndependence on the prior probability that the example given\nto the adversary is a member or non-member of the training\nset ",
                    "Citation Text": "Bargav Jayaraman, Lingxiao Wang, David Evans, and /Q_uanquan\nGu. Revisiting membership inference under realistic assumptions.\nProceedings on Privacy Enhancing Technologies , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.10881",
                        "Citation Paper Title": "Title:Revisiting Membership Inference Under Realistic Assumptions",
                        "Citation Paper Abstract": "Abstract:We study membership inference in settings where some of the assumptions typically used in previous research are relaxed. First, we consider skewed priors, to cover cases such as when only a small fraction of the candidate pool targeted by the adversary are actually members and develop a PPV-based metric suitable for this setting. This setting is more realistic than the balanced prior setting typically considered by researchers. Second, we consider adversaries that select inference thresholds according to their attack goals and develop a threshold selection procedure that improves inference attacks. Since previous inference attacks fail in imbalanced prior setting, we develop a new inference attack based on the intuition that inputs corresponding to training set members will be near a local minimum in the loss function, and show that an attack that combines this with thresholds on the per-instance loss can achieve high PPV even in settings where other attacks appear to be ineffective. Code for our experiments can be found here: this https URL.",
                        "Citation Paper Authors": "Authors:Bargav Jayaraman, Lingxiao Wang, Katherine Knipmeyer, Quanquan Gu, David Evans"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1908.11435v2": {
            "Paper Title": "Improving Adversarial Robustness via Attention and Adversarial Logit\n  Pairing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.04124v5": {
            "Paper Title": "Tuning PoW with Hybrid Expenditure",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.09436v2": {
            "Paper Title": "KNG: The K-Norm Gradient Mechanism",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.06306v3": {
            "Paper Title": "On the Complexity of Anonymous Communication Through Public Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.07824v2": {
            "Paper Title": "SilentDelivery: Practical Timed-delivery of Private Information using\n  Smart Contracts",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.00830v5": {
            "Paper Title": "DAWN: Dynamic Adversarial Watermarking of Neural Networks",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "in order to make\nthe model unique. The watermark is composed of synthetically\ngenerated adversarial samples ",
                    "Citation Text": "Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and\nharnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.6572",
                        "Citation Paper Title": "Title:Explaining and Harnessing Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "consist in modifying the original\nmodel boundary using adversarial retraining ",
                    "Citation Text": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and\nAdrian Vladu. 2017. Towards deep learning models resistant to adversarial\nattacks. arXiv preprint arXiv:1706.06083 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06083",
                        "Citation Paper Title": "Title:Towards Deep Learning Models Resistant to Adversarial Attacks",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL.",
                        "Citation Paper Authors": "Authors:Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1907.08355v3": {
            "Paper Title": "Data Structures Meet Cryptography: 3SUM with Preprocessing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.04620v4": {
            "Paper Title": "Selfish Mining in Ethereum",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.11201v4": {
            "Paper Title": "Matrix Sketching for Secure Collaborative Machine Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.07082v6": {
            "Paper Title": "The Audio Auditor: User-Level Membership Inference in Internet of Things\n  Voice Services",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.12561v5": {
            "Paper Title": "Adversarial Robustness vs Model Compression, or Both?",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": ". In addition, adversarial training\nsuffers from a more signi\ufb01cant over\ufb01tting issue than the nat-\nural training ",
                    "Citation Text": "Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal\nTalwar, and Aleksander Madry. Adversarially robust gener-\nalization requires more data. In Advances in Neural Infor-\nmation Processing Systems , pages 5019\u20135031, 2018. 3, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.11285",
                        "Citation Paper Title": "Title:Adversarially Robust Generalization Requires More Data",
                        "Citation Paper Abstract": "Abstract:Machine learning models are often susceptible to adversarial perturbations of their inputs. Even small perturbations can cause state-of-the-art classifiers with high \"standard\" accuracy to produce an incorrect prediction with high confidence. To better understand this phenomenon, we study adversarially robust learning from the viewpoint of generalization. We show that already in a simple natural data model, the sample complexity of robust learning can be significantly larger than that of \"standard\" learning. This gap is information theoretic and holds irrespective of the training algorithm or the model family. We complement our theoretical results with experiments on popular image classification datasets and show that a similar gap exists here as well. We postulate that the difficulty of training robust classifiers stems, at least partially, from this inherently larger sample complexity.",
                        "Citation Paper Authors": "Authors:Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, Aleksander M\u0105dry"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1905.12418v5": {
            "Paper Title": "Expected Tight Bounds for Robust Training",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.11824v2": {
            "Paper Title": "Attacker Behaviour Profiling using Stochastic Ensemble of Hidden Markov\n  Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.10899v4": {
            "Paper Title": "Adversarial Attack and Defense on Point Sets",
            "Sentences": [
                {
                    "Sentence ID": 51,
                    "Sentence": ". Therefore, it is expected that\nthe input diversity method ",
                    "Citation Text": "C. Xie, Z. Zhang, J. Wang, Y . Zhou, Z. Ren, and A. L. Yuille.\nImproving transferability of adversarial examples with input diversity.\nInProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.06978",
                        "Citation Paper Title": "Title:Improving Transferability of Adversarial Examples with Input Diversity",
                        "Citation Paper Abstract": "Abstract:Though CNNs have achieved the state-of-the-art performance on various vision tasks, they are vulnerable to adversarial examples --- crafted by adding human-imperceptible perturbations to clean images. However, most of the existing adversarial attacks only achieve relatively low success rates under the challenging black-box setting, where the attackers have no knowledge of the model structure and parameters. To this end, we propose to improve the transferability of adversarial examples by creating diverse input patterns. Instead of only using the original images to generate adversarial examples, our method applies random transformations to the input images at each iteration. Extensive experiments on ImageNet show that the proposed attack method can generate adversarial examples that transfer much better to different networks than existing baselines. By evaluating our method against top defense solutions and official baselines from NIPS 2017 adversarial competition, the enhanced attack reaches an average success rate of 73.0%, which outperforms the top-1 attack submission in the NIPS competition by a large margin of 6.6%. We hope that our proposed attack strategy can serve as a strong benchmark baseline for evaluating the robustness of networks to adversaries and the effectiveness of different defense methods in the future. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren, Alan Yuille"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ". It is also very\nrelated to the PGD method ",
                    "Citation Text": "A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards\ndeep learning models resistant to adversarial attacks. arXiv preprint\narXiv:1706.06083 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06083",
                        "Citation Paper Title": "Title:Towards Deep Learning Models Resistant to Adversarial Attacks",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL.",
                        "Citation Paper Authors": "Authors:Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "in our experiments).\nAs will be shown later, the distributions of O0\nmare very dif-\nferent between benign samples and adversarial examples. We\ncompute certain statistics over O0\nm, to capture the difference,\nand report several metrics as ",
                    "Citation Text": "S. Liang, Y . Li, and R. Srikant. Enhancing the reliability of out-\nof-distribution image detection in neural networks. arXiv preprint\narXiv:1706.02690 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.02690",
                        "Citation Paper Title": "Title:Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks",
                        "Citation Paper Abstract": "Abstract:We consider the problem of detecting out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions between in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7% to 4.3% on the DenseNet (applied to CIFAR-10) when the true positive rate is 95%.",
                        "Citation Paper Authors": "Authors:Shiyu Liang, Yixuan Li, R. Srikant"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1907.03103v3": {
            "Paper Title": "Towards Enhancing Fault Tolerance in Neural Networks",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": ". Further, perma-\nnent fault detection by thresholding activation values have been\nconsidered ",
                    "Citation Text": "Hoang, L.-H., Hanif, M. A., and Shafiqe, M. Ft-clipact: Resilience analysis of\ndeep neural networks and improving their fault tolerance using clipped activation.\narXiv:1912.00941 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.00941",
                        "Citation Paper Title": "Title:FT-ClipAct: Resilience Analysis of Deep Neural Networks and Improving their Fault Tolerance using Clipped Activation",
                        "Citation Paper Abstract": "Abstract:Deep Neural Networks (DNNs) are widely being adopted for safety-critical applications, e.g., healthcare and autonomous driving. Inherently, they are considered to be highly error-tolerant. However, recent studies have shown that hardware faults that impact the parameters of a DNN (e.g., weights) can have drastic impacts on its classification accuracy. In this paper, we perform a comprehensive error resilience analysis of DNNs subjected to hardware faults (e.g., permanent faults) in the weight memory. The outcome of this analysis is leveraged to propose a novel error mitigation technique which squashes the high-intensity faulty activation values to alleviate their impact. We achieve this by replacing the unbounded activation functions with their clipped versions. We also present a method to systematically define the clipping values of the activation functions that result in increased resilience of the networks against faults. We evaluate our technique on the AlexNet and the VGG-16 DNNs trained for the CIFAR-10 dataset. The experimental results show that our mitigation technique significantly improves the resilience of the DNNs to faults. For example, the proposed technique offers on average 68.92% improvement in the classification accuracy of resilience-optimized VGG-16 model at 1e-5 fault rate, when compared to the base network without any fault mitigation.",
                        "Citation Paper Authors": "Authors:Le-Ha Hoang, Muhammad Abdullah Hanif, Muhammad Shafique"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1906.09613v4": {
            "Paper Title": "The Cost of a Reductions Approach to Private Fair Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.04572v3": {
            "Paper Title": "A Formal Approach to Physics-Based Attacks in Cyber-Physical Systems\n  (Extended Version)",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.08736v4": {
            "Paper Title": "ER-AE: Differentially Private Text Generation for Authorship\n  Anonymization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.11477v4": {
            "Paper Title": "Quantum Lazy Sampling and Game-Playing Proofs for Quantum\n  Indifferentiability",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.09420v2": {
            "Paper Title": "Elliptical Perturbations for Differential Privacy",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.12069v2": {
            "Paper Title": "SpoC: Spoofing Camera Fingerprints",
            "Sentences": [
                {
                    "Sentence ID": 46,
                    "Sentence": "the adversarial examples are gen-\nerated from scratch, with no further constraint. Other pa-\npers instead are more relevant to our scenario and use a\ngenerative approach to slightly modify an existing image\n[24, 46, 59, 10, 16]. In ",
                    "Citation Text": "Omid Poursaeed, Isay Katsman, Bicheng Gao, and Serge\nBelongie. Generative Adversarial Perturbations. In IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR) , pages 4422\u20134431, 2018. 3, 7, 10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.02328",
                        "Citation Paper Title": "Title:Generative Adversarial Perturbations",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose novel generative models for creating adversarial examples, slightly perturbed images resembling natural images but maliciously crafted to fool pre-trained models. We present trainable deep neural networks for transforming images to adversarial perturbations. Our proposed models can produce image-agnostic and image-dependent perturbations for both targeted and non-targeted attacks. We also demonstrate that similar architectures can achieve impressive results in fooling classification and semantic segmentation models, obviating the need for hand-crafting attack methods for each task. Using extensive experiments on challenging high-resolution datasets such as ImageNet and Cityscapes, we show that our perturbations achieve high fooling rates with small perturbation norms. Moreover, our attacks are considerably faster than current iterative methods at inference time.",
                        "Citation Paper Authors": "Authors:Omid Poursaeed, Isay Katsman, Bicheng Gao, Serge Belongie"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": ", Translation-Invariant\nMomentum Iterative Fast Gradient Sign Method (TI-MI-\nFGSM) ",
                    "Citation Text": "Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu.\nEvading defenses to transferable adversarial examples by\ntranslation-invariant attacks. In IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR) , 2019. 7, 10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.02884",
                        "Citation Paper Title": "Title:Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks",
                        "Citation Paper Abstract": "Abstract:Deep neural networks are vulnerable to adversarial examples, which can mislead classifiers by adding imperceptible perturbations. An intriguing property of adversarial examples is their good transferability, making black-box attacks feasible in real-world applications. Due to the threat of adversarial attacks, many methods have been proposed to improve the robustness. Several state-of-the-art defenses are shown to be robust against transferable adversarial examples. In this paper, we propose a translation-invariant attack method to generate more transferable adversarial examples against the defense models. By optimizing a perturbation over an ensemble of translated images, the generated adversarial example is less sensitive to the white-box model being attacked and has better transferability. To improve the efficiency of attacks, we further show that our method can be implemented by convolving the gradient at the untranslated image with a pre-defined kernel. Our method is generally applicable to any gradient-based attack method. Extensive experiments on the ImageNet dataset validate the effectiveness of the proposed method. Our best attack fools eight state-of-the-art defenses at an 82% success rate on average based only on the transferability, demonstrating the insecurity of the current defense techniques.",
                        "Citation Paper Authors": "Authors:Yinpeng Dong, Tianyu Pang, Hang Su, Jun Zhu"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": ", which proved to be an ef-\nfective tool for GAN image detection and deepfakes video\n[38, 48, 55]. We also include PatchForensics that is a\nfully-convolutional patch-based classi\ufb01er proposed in ",
                    "Citation Text": "Lucy Chai, David Bau, Ser-Nam Lim, and Phillip Isola.\nWhat makes fake images detectable? understanding prop-\nerties that generalize. In European Conference on Computer\nVision (ECCV) , pages 103\u2013120, 2020. 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.10588",
                        "Citation Paper Title": "Title:What makes fake images detectable? Understanding properties that generalize",
                        "Citation Paper Abstract": "Abstract:The quality of image generation and manipulation is reaching impressive levels, making it increasingly difficult for a human to distinguish between what is real and what is fake. However, deep networks can still pick up on the subtle artifacts in these doctored images. We seek to understand what properties of fake images make them detectable and identify what generalizes across different model architectures, datasets, and variations in training. We use a patch-based classifier with limited receptive fields to visualize which regions of fake images are more easily detectable. We further show a technique to exaggerate these detectable properties and demonstrate that, even when the image generator is adversarially finetuned against a fake image classifier, it is still imperfect and leaves detectable artifacts in certain image patches. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Lucy Chai, David Bau, Ser-Nam Lim, Phillip Isola"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "it has been investigated the robustness\nof such classi\ufb01ers to adversarial attacks both in a white-box\nand in a black-box scenario. Experiments show that im-\nperceptible perturbations can cause misclassi\ufb01cation in both\nscenarios. A different perspective is pursued in ",
                    "Citation Text": "Jo \u02dcao C. Neves, Ruben Tolosana, Ruben Vera-Rodriguez,\nVasco Lopes, Hugo Proenc \u00b8a, and Julian Fierrez. Ganprintr:\nImproved fakes and evaluation of the state of the art in face\nmanipulation detection. IEEE Journal of Selected Topics in\nSignal Processing , 14(5):1038\u20131048, 2020. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.05351",
                        "Citation Paper Title": "Title:GANprintR: Improved Fakes and Evaluation of the State of the Art in Face Manipulation Detection",
                        "Citation Paper Abstract": "Abstract:The availability of large-scale facial databases, together with the remarkable progresses of deep learning technologies, in particular Generative Adversarial Networks (GANs), have led to the generation of extremely realistic fake facial content, raising obvious concerns about the potential for misuse. Such concerns have fostered the research on manipulation detection methods that, contrary to humans, have already achieved astonishing results in various scenarios. In this study, we focus on the synthesis of entire facial images, which is a specific type of facial manipulation. The main contributions of this study are four-fold: i) a novel strategy to remove GAN \"fingerprints\" from synthetic fake images based on autoencoders is described, in order to spoof facial manipulation detection systems while keeping the visual quality of the resulting images; ii) an in-depth analysis of the recent literature in facial manipulation detection; iii) a complete experimental assessment of this type of facial manipulation, considering the state-of-the-art fake detection systems (based on holistic deep networks, steganalysis, and local artifacts), remarking how challenging is this task in unconstrained scenarios; and finally iv) we announce a novel public database, named iFakeFaceDB, yielding from the application of our proposed GAN-fingerprint Removal approach (GANprintR) to already very realistic synthetic fake images.\nThe results obtained in our empirical evaluation show that additional efforts are required to develop robust facial manipulation detection systems against unseen conditions and spoof techniques, such as the one proposed in this study.",
                        "Citation Paper Authors": "Authors:Jo\u00e3o C. Neves, Ruben Tolosana, Ruben Vera-Rodriguez, Vasco Lopes, Hugo Proen\u00e7a, Julian Fierrez"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "Adversarial attacks to camera identi\ufb01cation. Adversar-\nial attacks are conceived to fool a classi\ufb01er by adding im-\nperceptible perturbations ",
                    "Citation Text": "Ian Goodfellow, Jonathon Shlens, and Christian Szegedy.\nExplaining and harnessing adversarial examples. In Inter-\nnational Conference on Learning Representations , 2015. 2,\n3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.6572",
                        "Citation Paper Title": "Title:Explaining and Harnessing Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1902.07285v6": {
            "Paper Title": "Towards a Robust Deep Neural Network in Texts: A Survey",
            "Sentences": [
                {
                    "Sentence ID": 206,
                    "Sentence": "refers to a technique to naturally\nsynthesize human imperceptible fake images and editing\nimages via arti\ufb01cial intelligence (AI), especially through\nGANs. In response to this emerging challenge, researchers\nhave constructed various deepfake detectors ",
                    "Citation Text": "D. Afchar, V . Nozick, J. Yamagishi, and I. Echizen, \u201cMesonet: a\ncompact facial video forgery detection network,\u201d in Proceedings\nof the 2018 IEEE International Workshop on Information Forensics\nand Security (WIFS) . IEEE, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.00888",
                        "Citation Paper Title": "Title:MesoNet: a Compact Facial Video Forgery Detection Network",
                        "Citation Paper Abstract": "Abstract:This paper presents a method to automatically and efficiently detect face tampering in videos, and particularly focuses on two recent techniques used to generate hyper-realistic forged videos: Deepfake and Face2Face. Traditional image forensics techniques are usually not well suited to videos due to the compression that strongly degrades the data. Thus, this paper follows a deep learning approach and presents two networks, both with a low number of layers to focus on the mesoscopic properties of images. We evaluate those fast networks on both an existing dataset and a dataset we have constituted from online videos. The tests demonstrate a very successful detection rate with more than 98% for Deepfake and 95% for Face2Face.",
                        "Citation Paper Authors": "Authors:Darius Afchar, Vincent Nozick, Junichi Yamagishi, Isao Echizen"
                    }
                },
                {
                    "Sentence ID": 205,
                    "Sentence": ", or weakening\nthe transferability of adversarial examples.17\nAlongside, the combination of deepfake and adversarial\nexamples (also called AdvDeepfakes) is a worthy research\ndirection. Deepfake ",
                    "Citation Text": "R. Wang, F. Juefei-Xu, L. Ma, X. Xie, Y. Huang, J. Wang, and\nY. Liu, \u201cFakespotter: A simple yet robust baseline for spotting\nai-synthesized fake faces,\u201d in Proceedings of the 29th International\nJoint Conference on Arti\ufb01cial Intelligence (IJCAI 2020) , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.06122",
                        "Citation Paper Title": "Title:FakeSpotter: A Simple yet Robust Baseline for Spotting AI-Synthesized Fake Faces",
                        "Citation Paper Abstract": "Abstract:In recent years, generative adversarial networks (GANs) and its variants have achieved unprecedented success in image synthesis. They are widely adopted in synthesizing facial images which brings potential security concerns to humans as the fakes spread and fuel the misinformation. However, robust detectors of these AI-synthesized fake faces are still in their infancy and are not ready to fully tackle this emerging challenge. In this work, we propose a novel approach, named FakeSpotter, based on monitoring neuron behaviors to spot AI-synthesized fake faces. The studies on neuron coverage and interactions have successfully shown that they can be served as testing criteria for deep learning systems, especially under the settings of being exposed to adversarial attacks. Here, we conjecture that monitoring neuron behavior can also serve as an asset in detecting fake faces since layer-by-layer neuron activation patterns may capture more subtle features that are important for the fake detector. Experimental results on detecting four types of fake faces synthesized with the state-of-the-art GANs and evading four perturbation attacks show the effectiveness and robustness of our approach.",
                        "Citation Paper Authors": "Authors:Run Wang, Felix Juefei-Xu, Lei Ma, Xiaofei Xie, Yihao Huang, Jian Wang, Yang Liu"
                    }
                },
                {
                    "Sentence ID": 169,
                    "Sentence": "that directly constructs the training\ndataset with all the adversarial examples and legitimate\nones. Yet, researchers have demonstrated that traditional\nadversarial training is weak against iterative attacks and\nproposed iterative training methods in the image domain ",
                    "Citation Text": "A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu,\n\u201cTowards deep learning models resistant to adversarial attacks,\u201d\ninProceedings of the International Conference on Learning Represen-\ntations , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06083",
                        "Citation Paper Title": "Title:Towards Deep Learning Models Resistant to Adversarial Attacks",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL.",
                        "Citation Paper Authors": "Authors:Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "generated adver-\nsarial examples to discover the limitations of their machine\nreading comprehension model. In different scenarios ",
                    "Citation Text": "X. Yuan, P . He, Q. Zhu, and X. Li, \u201cAdversarial examples: Attacks\nand defenses for deep learning,\u201d IEEE Transactions on Neural\nNetworks and learning systems , pp. 1\u201320, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.07107",
                        "Citation Paper Title": "Title:Adversarial Examples: Attacks and Defenses for Deep Learning",
                        "Citation Paper Abstract": "Abstract:With rapid progress and significant successes in a wide spectrum of applications, deep learning is being applied in many safety-critical environments. However, deep neural networks have been recently found vulnerable to well-designed input samples, called adversarial examples. Adversarial examples are imperceptible to human but can easily fool deep neural networks in the testing/deploying stage. The vulnerability to adversarial examples becomes one of the major risks for applying deep neural networks in safety-critical environments. Therefore, attacks and defenses on adversarial examples draw great attention. In this paper, we review recent findings on adversarial examples for deep neural networks, summarize the methods for generating adversarial examples, and propose a taxonomy of these methods. Under the taxonomy, applications for adversarial examples are investigated. We further elaborate on countermeasures for adversarial examples and explore the challenges and the potential solutions.",
                        "Citation Paper Authors": "Authors:Xiaoyong Yuan, Pan He, Qile Zhu, Xiaolin Li"
                    }
                },
                {
                    "Sentence ID": 189,
                    "Sentence": "presented activation clustering to\ntrack the maximally activated neurons. Similarly, Dalvi et al. ",
                    "Citation Text": "F. Dalvi, N. Durrani, H. Sajjad, Y. Belinkov, D. A. Bau, and J. Glass,\n\u201cWhat is one grain of sand in the desert? analyzing individual\nneurons in deep nlp models,\u201d in Proceedings of the 33th AAAI\nConference on Arti\ufb01cial Intelligence (AAAI) , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.09355",
                        "Citation Paper Title": "Title:What Is One Grain of Sand in the Desert? Analyzing Individual Neurons in Deep NLP Models",
                        "Citation Paper Abstract": "Abstract:Despite the remarkable evolution of deep neural networks in natural language processing (NLP), their interpretability remains a challenge. Previous work largely focused on what these models learn at the representation level. We break this analysis down further and study individual dimensions (neurons) in the vector representation learned by end-to-end neural models in NLP tasks. We propose two methods: Linguistic Correlation Analysis, based on a supervised method to extract the most relevant neurons with respect to an extrinsic task, and Cross-model Correlation Analysis, an unsupervised method to extract salient neurons w.r.t. the model itself. We evaluate the effectiveness of our techniques by ablating the identified neurons and reevaluating the network's performance for two tasks: neural machine translation (NMT) and neural language modeling (NLM). We further present a comprehensive analysis of neurons with the aim to address the following questions: i) how localized or distributed are different linguistic properties in the models? ii) are certain neurons exclusive to some properties and not others? iii) is the information more or less distributed in NMT vs. NLM? and iv) how important are the neurons identified through the linguistic correlation method to the overall task? Our code is publicly available as part of the NeuroX toolkit (Dalvi et al. 2019).",
                        "Citation Paper Authors": "Authors:Fahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan Belinkov, Anthony Bau, James Glass"
                    }
                },
                {
                    "Sentence ID": 97,
                    "Sentence": "conducted a char-level black-box at-\ntack to explore the vulnerability of three different neural ma-\nchine translation models ",
                    "Citation Text": "X. Zhang, J. Zhao, and Y. LeCun, \u201cCharacter-level convolutional\nnetworks for text classi\ufb01cation,\u201d in Proceedings of the Conference on\nNeural Information Processing Systems , 2015, p. 649\u2013657.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1509.01626",
                        "Citation Paper Title": "Title:Character-level Convolutional Networks for Text Classification",
                        "Citation Paper Abstract": "Abstract:This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.",
                        "Citation Paper Authors": "Authors:Xiang Zhang, Junbo Zhao, Yann LeCun"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "in the image domain, attackers\ngenerate adversarial examples by calculating the gradient\nof text vectors in a model.\nAs far as we know, Papernot et al. ",
                    "Citation Text": "N. Papernot, P . McDaniel, A. Swami, and R. Harang, \u201cCrafting\nadversarial input sequences for recurrent neural networks,\u201d in\nProceedings of the IEEE Military Communications Conference , 2016,\np. 49\u201354.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1604.08275",
                        "Citation Paper Title": "Title:Crafting Adversarial Input Sequences for Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:Machine learning models are frequently used to solve complex security problems, as well as to make decisions in sensitive situations like guiding autonomous vehicles or predicting financial market behaviors. Previous efforts have shown that numerous machine learning models were vulnerable to adversarial manipulations of their inputs taking the form of adversarial samples. Such inputs are crafted by adding carefully selected perturbations to legitimate inputs so as to force the machine learning model to misbehave, for instance by outputting a wrong class if the machine learning task of interest is classification. In fact, to the best of our knowledge, all previous work on adversarial samples crafting for neural network considered models used to solve classification tasks, most frequently in computer vision applications. In this paper, we contribute to the field of adversarial machine learning by investigating adversarial input sequences for recurrent neural networks processing sequential data. We show that the classes of algorithms introduced previously to craft adversarial samples misclassified by feed-forward neural networks can be adapted to recurrent neural networks. In a experiment, we show that adversaries can craft adversarial sequences misleading both categorical and sequential recurrent neural networks.",
                        "Citation Paper Authors": "Authors:Nicolas Papernot, Patrick McDaniel, Ananthram Swami, Richard Harang"
                    }
                },
                {
                    "Sentence ID": 159,
                    "Sentence": "and limitations of the access to target models. In\naddition, gradient masking ",
                    "Citation Text": "A. Athalye, N. Carlini, and D. Wagner, \u201cObfuscated gradients\ngive a false sense of security: Circumventing defenses to adver-\nsarial examples,\u201d in arXiv preprint arXiv: 1802.00420 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.00420",
                        "Citation Paper Title": "Title:Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimization-based attacks, we find defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining non-certified white-box-secure defenses at ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers.",
                        "Citation Paper Authors": "Authors:Anish Athalye, Nicholas Carlini, David Wagner"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "also support the linear hypothesis contributing\nto the vulnerability of DNNs. However, Sabour et al. ",
                    "Citation Text": "S. Sabour, Y. Cao, F. Faghri1, and D. J. Fleet, \u201cAdversarial manip-\nulation of deep representations,\u201d in Proceedings of the International\nConference on Learning Representations (ICLR) , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.05122",
                        "Citation Paper Title": "Title:Adversarial Manipulation of Deep Representations",
                        "Citation Paper Abstract": "Abstract:We show that the representation of an image in a deep neural network (DNN) can be manipulated to mimic those of other natural images, with only minor, imperceptible perturbations to the original image. Previous methods for generating adversarial images focused on image perturbations designed to produce erroneous class labels, while we concentrate on the internal layers of DNN representations. In this way our new class of adversarial images differs qualitatively from others. While the adversary is perceptually similar to one image, its internal representation appears remarkably similar to a different image, one from a different class, bearing little if any apparent similarity to the input; they appear generic and consistent with the space of natural images. This phenomenon raises questions about DNN representations, as well as the properties of natural images themselves.",
                        "Citation Paper Authors": "Authors:Sara Sabour, Yanshuai Cao, Fartash Faghri, David J. Fleet"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1908.05584v6": {
            "Paper Title": "Quantum preprocessing for information-theoretic security in two-party\n  computation",
            "Sentences": [
                {
                    "Sentence ID": 41,
                    "Sentence": "in\nthe check-based way.\nP(A\bB=abja;b) =1\n2(1 +E); (14)\nwith 0\u0014E\u00141. According to an argument in ",
                    "Citation Text": "Ll. Masanes, A. Acin, and N. Gisin. General properties\nof nonsignaling theories. Phys. Rev. A , 73:012112, Jan\n2006.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:quant-ph/0508016",
                        "Citation Paper Title": "Title:General properties of Nonsignaling Theories",
                        "Citation Paper Abstract": "Abstract:  This article identifies a series of properties common to all theories that do not allow for superluminal signaling and predict the violation of Bell inequalities. Intrinsic randomness, uncertainty due to the incompatibility of two observables, monogamy of correlations, impossibility of perfect cloning, privacy of correlations, bounds in the shareability of some states; all these phenomena are solely a consequence of the no-signaling principle and nonlocality. In particular, it is shown that for any distribution, the properties of (i) nonlocal, (ii) no arbitrarily shareable and (iii) positive secrecy content are equivalent.",
                        "Citation Paper Authors": "Authors:Ll. Masanes, A. Acin, N. Gisin"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "Yingkai Ouyang, Si-Hui Tan, Liming Zhao, and Joseph F.\nFitzsimons. Computing on quantum shared secrets.\nPhys. Rev. A , 96:052333, Nov 2017. ",
                    "Citation Text": "Yingkai Ouyang, Si-Hui Tan, Joseph Fitzsimons, and Pe-\nter P. Rohde. Homomorphic encryption of linear optics\nquantum computation on almost arbitrary states of light\nwith asymptotically perfect security. http://arxiv.org/\nabs/1902.10972 , Feb 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.10972",
                        "Citation Paper Title": "Title:Homomorphic encryption of linear optics quantum computation on almost arbitrary states of light with asymptotically perfect security",
                        "Citation Paper Abstract": "Abstract:Future quantum computers are likely to be expensive and affordable outright by few, motivating client/server models for outsourced computation. However, the applications for quantum computing will often involve sensitive data, and the client would like to keep her data secret, both from eavesdroppers and the server itself. Homomorphic encryption is an approach for encrypted, outsourced quantum computation, where the client's data remains secret, even during execution of the computation. We present a scheme for the homomorphic encryption of arbitrary quantum states of light with no more than a fixed number of photons, under the evolution of both passive and adaptive linear optics, the latter of which is universal for quantum computation. The scheme uses random coherent displacements in phase-space to obfuscate client data. In the limit of large coherent displacements, the protocol exhibits asymptotically perfect information-theoretic secrecy. The experimental requirements are modest, and easily implementable using present-day technology.",
                        "Citation Paper Authors": "Authors:Yingkai Ouyang, Si-Hui Tan, Joseph Fitzsimons, Peter P. Rohde"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "K. Fisher, A. Broadbent, L.K. Shalm, Z. Yan, J. Lavoie,\nR. Prevedel, T. Jennewein, and K.J. Resch. Quantum\ncomputing on encrypted data. Nat. Commun. , 5:3074,\n2014. ",
                    "Citation Text": "Yingkai Ouyang, Si-Hui Tan, Liming Zhao, and Joseph F.\nFitzsimons. Computing on quantum shared secrets.\nPhys. Rev. A , 96:052333, Nov 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.03689",
                        "Citation Paper Title": "Title:Computing on quantum shared secrets",
                        "Citation Paper Abstract": "Abstract:A (k,n)-threshold secret-sharing scheme allows for a string to be split into n shares in such a way that any subset of at least k shares suffices to recover the secret string, but such that any subset of at most k-1 shares contains no information about the secret. Quantum secret-sharing schemes extend this idea to the sharing of quantum states. Here we propose a method of performing computation on quantum shared secrets. We introduce a (n,n)-quantum secret sharing scheme together with a set of protocols that allow quantum circuits to be evaluated on the shared secret without the need to decode the secret. We consider a multipartite setting, with each participant holding a share of the secret. We show that if there exists at least one honest participant, no group of dishonest participants can recover any information about the shared secret, independent of their deviations from the protocol.",
                        "Citation Paper Authors": "Authors:Yingkai Ouyang, Si-Hui Tan, Liming Zhao, Joseph F. Fitzsimons"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "Andrew Childs. Secure assisted quantum computation.\nQuantum Information and Computation , 5(6):456, 2005. ",
                    "Citation Text": "K. Fisher, A. Broadbent, L.K. Shalm, Z. Yan, J. Lavoie,\nR. Prevedel, T. Jennewein, and K.J. Resch. Quantum\ncomputing on encrypted data. Nat. Commun. , 5:3074,\n2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1309.2586",
                        "Citation Paper Title": "Title:Quantum computing on encrypted data",
                        "Citation Paper Abstract": "Abstract:The ability to perform computations on encrypted data is a powerful tool for protecting privacy. Recently, protocols to achieve this on classical computing systems have been found. Here we present an efficient solution to the quantum analogue of this problem that enables arbitrary quantum computations to be carried out on encrypted quantum data. We prove that an untrusted server can implement a universal set of quantum gates on encrypted quantum bits (qubits) without learning any information about the inputs, while the client, knowing the decryption key, can easily decrypt the results of the computation. We experimentally demonstrate, using single photons and linear optics, the encryption and decryption scheme on a set of gates sufficient for arbitrary quantum computations. Because our protocol requires few extra resources compared to other schemes it can be easily incorporated into the design of future quantum servers. These results will play a key role in enabling the development of secure distributed quantum systems.",
                        "Citation Paper Authors": "Authors:K. Fisher, A. Broadbent, L.K. Shalm, Z. Yan, J. Lavoie, R. Prevedel, T. Jennewein, K.J. Resch"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "M. Newman. Further Limitations on Information-\nTheoretically Secure Quantum Homomorphic Encryp-\ntion. http://arxiv.org/abs/1809.08719 , September\n2018. ",
                    "Citation Text": "Si-Hui Tan, Yingkai Ouyang, and Peter P. Rohde. Prac-\ntical somewhat-secure quantum somewhat-homomorphic\nencryption with coherent states. Phys. Rev. A , 97:042308,\nApr 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.03968",
                        "Citation Paper Title": "Title:Practical quantum somewhat-homomorphic encryption with coherent states",
                        "Citation Paper Abstract": "Abstract:We present a scheme for implementing homomorphic encryption on coherent states encoded using phase-shift keys. The encryption operations require only rotations in phase space, which commute with computations in the codespace performed via passive linear optics, and with generalized non-linear phase operations that are polynomials of the photon-number operator in the codespace. This encoding scheme can thus be applied to any computation with coherent state inputs, and the computation proceeds via a combination of passive linear optics and generalized non-linear phase operations. An example of such a computation is matrix multiplication, whereby a vector representing coherent state amplitudes is multiplied by a matrix representing a linear optics network, yielding a new vector of coherent state amplitudes. By finding an orthogonal partitioning of the support of our encoded states, we quantify the security of our scheme via the indistinguishability of the encrypted codewords. Whilst we focus on coherent state encodings, we expect that this phase-key encoding technique could apply to any continuous-variable computation scheme where the phase-shift operator commutes with the computation.",
                        "Citation Paper Authors": "Authors:Si-Hui Tan, Yingkai Ouyang, Peter P. Rohde"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "S.-H. Tan, J. A. Kettlewell, Y. Ouyang, L. Chen, and\nJ. F. Fitzsimons. A quantum approach to homomorphic\nencryption. Sci. Rep. , 6:33467, 2016. ",
                    "Citation Text": "Y. Ouyang, S.-H. Tan, and J. Fitzsimons. Quantum ho-\nmomorphic encryption from quantum codes. Phys. Rev.\nA, 98:042334, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1508.00938",
                        "Citation Paper Title": "Title:Quantum homomorphic encryption from quantum codes",
                        "Citation Paper Abstract": "Abstract:The recent discovery of fully-homomorphic classical encryption schemes has had a dramatic effect on the direction of modern cryptography. Such schemes, however, implicitly rely on the assumptions that solving certain computation problems are intractable. Here we present a quantum encryption scheme which is homomorphic for arbitrary classical and quantum circuits which have at most some constant number of non-Clifford gates. Unlike classical schemes, the security of the scheme we present is information theoretic and hence independent of the computational power of an adversary.",
                        "Citation Paper Authors": "Authors:Yingkai Ouyang, Si-Hui Tan, Joseph Fitzsimons"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "Li Yu, Carlos A. P\u0013 erez-Delgado, and Joseph F. Fitzsi-\nmons. Limitations on information-theoretically-secure\nquantum homomorphic encryption. Phys. Rev. A ,\n90:050303(R), Nov 2014. ",
                    "Citation Text": "S.-H. Tan, J. A. Kettlewell, Y. Ouyang, L. Chen, and\nJ. F. Fitzsimons. A quantum approach to homomorphic\nencryption. Sci. Rep. , 6:33467, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1411.5254",
                        "Citation Paper Title": "Title:A quantum approach to homomorphic encryption",
                        "Citation Paper Abstract": "Abstract:Encryption schemes often derive their power from the properties of the underlying algebra on the symbols used. Inspired by group theoretic tools, we use the centralizer of a subgroup of operations to present a private-key quantum homomorphic encryption scheme that enables a broad class of quantum computation on encrypted data. A particular instance of our encoding hides up to a constant fraction of the information encrypted. This fraction can be made arbitrarily close to unity with overhead scaling only polynomially in the message length. This highlights the potential of our protocol to hide a non-trivial amount of information, and is suggestive of a large class of encodings that might yield better security.",
                        "Citation Paper Authors": "Authors:Si-Hui Tan, Joshua A. Kettlewell, Yingkai Ouyang, Lin Chen, Joseph F. Fitzsimons"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "Min Liang. Symmetric quantum fully homomorphic en-\ncryption with perfect security. Quantum Inf. Process. ,\n12:3675{3687, 2013. ",
                    "Citation Text": "Li Yu, Carlos A. P\u0013 erez-Delgado, and Joseph F. Fitzsi-\nmons. Limitations on information-theoretically-secure\nquantum homomorphic encryption. Phys. Rev. A ,\n90:050303(R), Nov 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.2456",
                        "Citation Paper Title": "Title:Limitations on information theoretically secure quantum homomorphic encryption",
                        "Citation Paper Abstract": "Abstract:Homomorphic encryption is a form of encryption which allows computation to be carried out on the encrypted data without the need for decryption. The success of quantum approaches to related tasks in a delegated computation setting has raised the question of whether quantum mechanics may be used to achieve information theoretically secure fully homomorphic encryption. Here we show, via an information localisation argument, that deterministic fully homomorphic encryption necessarily incurs exponential overhead if perfect security is required.",
                        "Citation Paper Authors": "Authors:Li Yu, Carlos A. Perez-Delgado, Joseph F. Fitzsimons"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "Donald Beaver. One-time tables for two-party compu-\ntation. In Wen-Lian Hsu and Ming-Yang Kao, editors,\nComputing and Combinatorics , pages 361{370, Berlin,\nHeidelberg, 1998. Springer Berlin Heidelberg. ",
                    "Citation Text": "Peter P. Rohde, Joseph F. Fitzsimons, and Alexei\nGilchrist. Quantum walks with encrypted data. Phys.\nRev. Lett. , 109:150501, 2012.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1204.3370",
                        "Citation Paper Title": "Title:Quantum walks with encrypted data",
                        "Citation Paper Abstract": "Abstract:In the setting of networked computation, data security can be a significant concern. Here we consider the problem of allowing a server to remotely manipulate client supplied data, in such a way that both the information obtained by the client about the server's operation and the information obtained by the server about the client's data are significantly limited. We present a protocol for achieving such functionality in two closely related models of restricted quantum computation -- the Boson sampling and quantum walk models. Due to the limited technological requirements of the Boson scattering model, small scale implementations of this technique are feasible with present-day technology.",
                        "Citation Paper Authors": "Authors:Peter P. Rohde, Joseph F. Fitzsimons, Alexei Gilchrist"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1902.09033v2": {
            "Paper Title": "Expect More from the Networking: DDoS Mitigation by FITT in Named Data\n  Networking",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.05490v3": {
            "Paper Title": "Non-malleability for quantum public-key encryption",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.07783v4": {
            "Paper Title": "Ephemeral Astroturfing Attacks: The Case of Fake Twitter Trends",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": ". Colors indicate the communities obtained by the\nLouvain method ",
                    "Citation Text": "V . D. Blondel, J.-L. Guillaume, R. Lambiotte, and E. Lefebvre,\n\u201cFast unfolding of communities in large networks,\u201d Journal of\nstatistical mechanics: theory and experiment , 2008.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:0803.0476",
                        "Citation Paper Title": "Title:Fast unfolding of communities in large networks",
                        "Citation Paper Abstract": "Abstract:  We propose a simple method to extract the community structure of large networks. Our method is a heuristic method that is based on modularity optimization. It is shown to outperform all other known community detection method in terms of computation time. Moreover, the quality of the communities detected is very good, as measured by the so-called modularity. This is shown first by identifying language communities in a Belgian mobile phone network of 2.6 million customers and by analyzing a web graph of 118 million nodes and more than one billion links. The accuracy of our algorithm is also verified on ad-hoc modular networks. .",
                        "Citation Paper Authors": "Authors:Vincent D. Blondel, Jean-Loup Guillaume, Renaud Lambiotte, Etienne Lefebvre"
                    }
                },
                {
                    "Sentence ID": 82,
                    "Sentence": "works on a snapshot of a\npro\ufb01le and not on real-time activity, so it cannot detect the\nbot-like activity of accounts analyzed in this study since\nsuch activity is deleted quickly. Recently, Varol et al. ",
                    "Citation Text": "K.-C. Yang, O. Varol, C. A. Davis, E. Ferrara, A. Flammini,\nand F. Menczer, \u201cArming the public with arti\ufb01cial intelligence to\ncounter social bots,\u201d Human Behavior and Emerging Technologies ,\n2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.00912",
                        "Citation Paper Title": "Title:Arming the public with artificial intelligence to counter social bots",
                        "Citation Paper Abstract": "Abstract:The increased relevance of social media in our daily life has been accompanied by efforts to manipulate online conversations and opinions. Deceptive social bots -- automated or semi-automated accounts designed to impersonate humans -- have been successfully exploited for these kinds of abuse. Researchers have responded by developing AI tools to arm the public in the fight against social bots. Here we review the literature on different types of bots, their impact, and detection methods. We use the case study of Botometer, a popular bot detection tool developed at Indiana University, to illustrate how people interact with AI countermeasures. A user experience survey suggests that bot detection has become an integral part of the social media experience for many users. However, barriers in interpreting the output of AI tools can lead to fundamental misunderstandings. The arms race between machine learning methods to develop sophisticated bots and effective countermeasures makes it necessary to update the training data and features of detection tools. We again use the Botometer case to illustrate both algorithmic and interpretability improvements of bot scores, designed to meet user expectations. We conclude by discussing how future AI developments may affect the fight between malicious bots and the public.",
                        "Citation Paper Authors": "Authors:Kai-Cheng Yang, Onur Varol, Clayton A. Davis, Emilio Ferrara, Alessandro Flammini, Filippo Menczer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.04977v3": {
            "Paper Title": "Advances and Open Problems in Federated Learning",
            "Sentences": [
                {
                    "Sentence ID": 462,
                    "Sentence": ".\nThe values communicated can nevertheless, in general, reveal information about the underlying data.\nHow much, and whether this is acceptable, is likely going to be application and con\ufb01guration speci\ufb01c. A\nvariation of split learning called NoPeek SplitNN ",
                    "Citation Text": "Praneeth Vepakomma, Otkrist Singh, Abhishek Gupta, and Ramesh Raskar. Nopeek: Information leakage\nreduction to share activations in distributed deep learning. arXiv preprint arXiv:2008.09161 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.09161",
                        "Citation Paper Title": "Title:NoPeek: Information leakage reduction to share activations in distributed deep learning",
                        "Citation Paper Abstract": "Abstract:For distributed machine learning with sensitive data, we demonstrate how minimizing distance correlation between raw data and intermediary representations reduces leakage of sensitive raw data patterns across client communications while maintaining model accuracy. Leakage (measured using distance correlation between input and intermediate representations) is the risk associated with the invertibility of raw data from intermediary representations. This can prevent client entities that hold sensitive data from using distributed deep learning services. We demonstrate that our method is resilient to such reconstruction attacks and is based on reduction of distance correlation between raw data and learned representations during training and inference with image datasets. We prevent such reconstruction of raw data while maintaining information required to sustain good classification accuracies.",
                        "Citation Paper Authors": "Authors:Praneeth Vepakomma, Abhishek Singh, Otkrist Gupta, Ramesh Raskar"
                    }
                },
                {
                    "Sentence ID": 421,
                    "Sentence": ".\nIn several settings, the overall communication requirements of split learning and federated learning\nwere compared in ",
                    "Citation Text": "Abhishek Singh, Praneeth Vepakomma, Otkrist Gupta, and Ramesh Raskar. Detailed comparison of communi-\ncation ef\ufb01ciency of split learning and federated learning. arXiv preprint arXiv:1909.09145 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.09145",
                        "Citation Paper Title": "Title:Detailed comparison of communication efficiency of split learning and federated learning",
                        "Citation Paper Abstract": "Abstract:We compare communication efficiencies of two compelling distributed machine learning approaches of split learning and federated learning. We show useful settings under which each method outperforms the other in terms of communication efficiency. We consider various practical scenarios of distributed learning setup and juxtapose the two methods under various real-life scenarios. We consider settings of small and large number of clients as well as small models (1M - 6M parameters), large models (10M - 200M parameters) and very large models (1 Billion-100 Billion parameters). We show that increasing number of clients or increasing model size favors split learning setup over the federated while increasing the number of data samples while keeping the number of clients or model size low makes federated learning more communication efficient.",
                        "Citation Paper Authors": "Authors:Abhishek Singh, Praneeth Vepakomma, Otkrist Gupta, Ramesh Raskar"
                    }
                },
                {
                    "Sentence ID": 316,
                    "Sentence": ". Local updates similar to Federated Av-\neraging (see Section 3.2) has been proposed to address the communication challenges of feature-partitioned\nsystems ",
                    "Citation Text": "Yang Liu, Yan Kang, Xinwei Zhang, Liping Li, Yong Cheng, Tianjian Chen, Mingyi Hong, and Qiang Yang.\nA communication ef\ufb01cient vertical federated learning framework. CoRR , abs/1912.11187, 2019. URL http:\n//arxiv.org/abs/1912.11187 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.11187",
                        "Citation Paper Title": "Title:A Communication Efficient Collaborative Learning Framework for Distributed Features",
                        "Citation Paper Abstract": "Abstract:We introduce a collaborative learning framework allowing multiple parties having different sets of attributes about the same user to jointly build models without exposing their raw data or model parameters. In particular, we propose a Federated Stochastic Block Coordinate Descent (FedBCD) algorithm, in which each party conducts multiple local updates before each communication to effectively reduce the number of communication rounds among parties, a principal bottleneck for collaborative learning problems. We analyze theoretically the impact of the number of local updates and show that when the batch size, sample size, and the local iterations are selected appropriately, within $T$ iterations, the algorithm performs $\\mathcal{O}(\\sqrt{T})$ communication rounds and achieves some $\\mathcal{O}(1/\\sqrt{T})$ accuracy (measured by the average of the gradient norm squared). The approach is supported by our empirical evaluations on a variety of tasks and datasets, demonstrating advantages over stochastic gradient descent (SGD) approaches.",
                        "Citation Paper Authors": "Authors:Yang Liu, Yan Kang, Xinwei Zhang, Liping Li, Yong Cheng, Tianjian Chen, Mingyi Hong, Qiang Yang"
                    }
                },
                {
                    "Sentence ID": 498,
                    "Sentence": ", for the case of deep learning and SGD this remains\nan open question. When the network graph is complete but messages have a \ufb01xed probability to be dropped,\nYu et al. ",
                    "Citation Text": "Chen Yu, Hanlin Tang, Cedric Renggli, Simon Kassing, Ankit Singla, Dan Alistarh, Ce Zhang, and Ji Liu.\nDistributed learning over unreliable networks. arXiv preprint arXiv:1810.07766 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.07766",
                        "Citation Paper Title": "Title:Distributed Learning over Unreliable Networks",
                        "Citation Paper Abstract": "Abstract:Most of today's distributed machine learning systems assume {\\em reliable networks}: whenever two machines exchange information (e.g., gradients or models), the network should guarantee the delivery of the message. At the same time, recent work exhibits the impressive tolerance of machine learning algorithms to errors or noise arising from relaxed communication or synchronization. In this paper, we connect these two trends, and consider the following question: {\\em Can we design machine learning systems that are tolerant to network unreliability during training?} With this motivation, we focus on a theoretical problem of independent interest---given a standard distributed parameter server architecture, if every communication between the worker and the server has a non-zero probability $p$ of being dropped, does there exist an algorithm that still converges, and at what speed? The technical contribution of this paper is a novel theoretical analysis proving that distributed learning over unreliable network can achieve comparable convergence rate to centralized or distributed learning over reliable networks. Further, we prove that the influence of the packet drop rate diminishes with the growth of the number of \\textcolor{black}{parameter servers}. We map this theoretical result onto a real-world scenario, training deep neural networks over an unreliable network layer, and conduct network simulation to validate the system improvement by allowing the networks to be unreliable.",
                        "Citation Paper Authors": "Authors:Chen Yu, Hanlin Tang, Cedric Renggli, Simon Kassing, Ankit Singla, Dan Alistarh, Ce Zhang, Ji Liu"
                    }
                },
                {
                    "Sentence ID": 305,
                    "Sentence": ". Furthermore, the server may even become a bottleneck when the number of clients is very large, as\ndemonstrated by Lian et al. ",
                    "Citation Text": "Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can Decentralized Algorithms\nOutperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent. In\nNIPS , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.09056",
                        "Citation Paper Title": "Title:Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent",
                        "Citation Paper Abstract": "Abstract:Most distributed machine learning systems nowadays, including TensorFlow and CNTK, are built in a centralized fashion. One bottleneck of centralized algorithms lies on high communication cost on the central node. Motivated by this, we ask, can decentralized algorithms be faster than its centralized counterpart?\nAlthough decentralized PSGD (D-PSGD) algorithms have been studied by the control community, existing analysis and theory do not show any advantage over centralized PSGD (C-PSGD) algorithms, simply assuming the application scenario where only the decentralized network is available. In this paper, we study a D-PSGD algorithm and provide the first theoretical analysis that indicates a regime in which decentralized algorithms might outperform centralized algorithms for distributed stochastic gradient descent. This is because D-PSGD has comparable total computational complexities to C-PSGD but requires much less communication cost on the busiest node. We further conduct an empirical study to validate our theoretical analysis across multiple frameworks (CNTK and Torch), different network configurations, and computation platforms up to 112 GPUs. On network configurations with low bandwidth or high latency, D-PSGD can be up to one order of magnitude faster than its well-optimized centralized counterparts.",
                        "Citation Paper Authors": "Authors:Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, Ji Liu"
                    }
                },
                {
                    "Sentence ID": 298,
                    "Sentence": ", and Snips has explored cross-device FL for hotword detection ",
                    "Citation Text": "David Leroy, Alice Coucke, Thibaut Lavril, Thibault Gisselbrecht, and Joseph Dureau. Federated learning for\nkeyword spotting. arXiv preprint arXiv:1810.05512 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.05512",
                        "Citation Paper Title": "Title:Federated Learning for Keyword Spotting",
                        "Citation Paper Abstract": "Abstract:We propose a practical approach based on federated learning to solve out-of-domain issues with continuously running embedded speech-based models such as wake word detectors. We conduct an extensive empirical study of the federated averaging algorithm for the \"Hey Snips\" wake word based on a crowdsourced dataset that mimics a federation of wake word users. We empirically demonstrate that using an adaptive averaging strategy inspired from Adam in place of standard weighted model averaging highly reduces the number of communication rounds required to reach our target performance. The associated upstream communication costs per user are estimated at 8 MB, which is a reasonable in the context of smart home voice assistants. Additionally, the dataset used for these experiments is being open sourced with the aim of fostering further transparent research in the application of federated learning to speech data.",
                        "Citation Paper Authors": "Authors:David Leroy, Alice Coucke, Thibaut Lavril, Thibault Gisselbrecht, Joseph Dureau"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1905.05725v2": {
            "Paper Title": "Store-to-Leak Forwarding: Leaking Data on Meltdown-resistant CPUs\n  (Updated and Extended Version)",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.11118v2": {
            "Paper Title": "Detecting stuffing of a user's credentials at her own accounts",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.09716v2": {
            "Paper Title": "Too Quiet in the Library: An Empirical Study of Security Updates in\n  Android Apps' Native Code",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.01837v3": {
            "Paper Title": "DeepObfusCode: Source Code Obfuscation Through Sequence-to-Sequence\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.03461v6": {
            "Paper Title": "Fine Grained Dataflow Tracking with Proximal Gradients",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.01149v5": {
            "Paper Title": "Cost-Aware Robust Tree Ensembles for Security Applications",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "are the \ufb01rst to train cost-sensitive robustness with\nregard to classi\ufb01cation output error costs, since some errors\nhave more catastrophic consequences than others ",
                    "Citation Text": "T. Dreossi, S. Jha, and S. A. Seshia. Semantic adversarial deep\nlearning. arXiv preprint arXiv:1804.07045 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.07045",
                        "Citation Paper Title": "Title:Semantic Adversarial Deep Learning",
                        "Citation Paper Abstract": "Abstract:Fueled by massive amounts of data, models produced by machine-learning (ML) algorithms, especially deep neural networks, are being used in diverse domains where trustworthiness is a concern, including automotive systems, finance, health care, natural language processing, and malware detection. Of particular concern is the use of ML algorithms in cyber-physical systems (CPS), such as self-driving cars and aviation, where an adversary can cause serious consequences. However, existing approaches to generating adversarial examples and devising robust ML algorithms mostly ignore the semantics and context of the overall system containing the ML component. For example, in an autonomous vehicle using deep learning for perception, not every adversarial example for the neural network might lead to a harmful consequence. Moreover, one may want to prioritize the search for adversarial examples towards those that significantly modify the desired semantics of the overall system. Along the same lines, existing algorithms for constructing robust ML algorithms ignore the specification of the overall system. In this paper, we argue that the semantics and specification of the overall system has a crucial role to play in this line of research. We present preliminary research results that support this claim.",
                        "Citation Paper Authors": "Authors:Tommaso Dreossi, Somesh Jha, Sanjit A. Seshia"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "has been\ndemonstrated to work on ensemble trees. The attack mini-\nmizes the distance between a benign example and the decision\nboundary, using a zeroth order optimization algorithm with\nthe randomized gradient-free method. Papernot et al. ",
                    "Citation Text": "N. Papernot, P. McDaniel, and I. Goodfellow. Transferability\nin machine learning: from phenomena to black-box attacks\nusing adversarial samples. arXiv preprint arXiv:1605.07277 ,\n2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.07277",
                        "Citation Paper Title": "Title:Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples",
                        "Citation Paper Abstract": "Abstract:Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that affect one model often affect another model, even if the two models have different architectures or were trained on different training sets, so long as both models were trained to perform the same task. An attacker may therefore train their own substitute model, craft adversarial examples against the substitute, and transfer them to a victim model, with very little information about the victim. Recent work has further developed a technique that uses the victim model as an oracle to label a synthetic training set for the substitute, so the attacker need not even collect a training set to mount the attack. We extend these recent techniques using reservoir sampling to greatly enhance the efficiency of the training procedure for the substitute model. We introduce new transferability attacks between previously unexplored (substitute, victim) pairs of machine learning model classes, most notably SVMs and decision trees. We demonstrate our attacks on two commercial machine learning classification systems from Amazon (96.19% misclassification rate) and Google (88.94%) using only 800 queries of the victim model, thereby showing that existing machine learning approaches are in general vulnerable to systematic black-box attacks regardless of their structure.",
                        "Citation Paper Authors": "Authors:Nicolas Papernot, Patrick McDaniel, Ian Goodfellow"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "or boosting [21, 22,\n50] to grow the decision trees. Random forest and gradient\n1Oblique trees which use multiple feature values in a predicate is rarely\nused in an ensemble due to high construction costs ",
                    "Citation Text": "M. Norouzi, M. Collins, M. A. Johnson, D. J. Fleet, and\nP. Kohli. Ef\ufb01cient non-greedy optimization of decision trees.\nInAdvances in neural information processing systems , pages\n1729\u20131737, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.04056",
                        "Citation Paper Title": "Title:Efficient non-greedy optimization of decision trees",
                        "Citation Paper Abstract": "Abstract:Decision trees and randomized forests are widely used in computer vision and machine learning. Standard algorithms for decision tree induction optimize the split functions one node at a time according to some splitting criteria. This greedy procedure often leads to suboptimal trees. In this paper, we present an algorithm for optimizing the split functions at all levels of the tree jointly with the leaf parameters, based on a global objective. We show that the problem of finding optimal linear-combination (oblique) splits for decision trees is related to structured prediction with latent variables, and we formulate a convex-concave upper bound on the tree's empirical loss. The run-time of computing the gradient of the proposed surrogate objective with respect to each training exemplar is quadratic in the the tree depth, and thus training deep trees is feasible. The use of stochastic gradient descent for optimization enables effective training with large datasets. Experiments on several classification benchmarks demonstrate that the resulting non-greedy decision trees outperform greedy decision tree baselines.",
                        "Citation Paper Authors": "Authors:Mohammad Norouzi, Maxwell D. Collins, Matthew Johnson, David J. Fleet, Pushmeet Kohli"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.05721v2": {
            "Paper Title": "Using Deep Learning to Solve Computer Security Challenges: A Survey",
            "Sentences": [
                {
                    "Sentence ID": 53,
                    "Sentence": "regarded input \ufb01les directly as\nbyte sequences and fed them into the neural network model. Mohit Rajpal et al. ",
                    "Citation Text": "Mohit Rajpal, William Blum, and Rishabh Singh. Not All Bytes are Equal: Neural Byte\nSieve for Fuzzing. arXiv preprint arXiv:1711.04596, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.04596",
                        "Citation Paper Title": "Title:Not all bytes are equal: Neural byte sieve for fuzzing",
                        "Citation Paper Abstract": "Abstract:Fuzzing is a popular dynamic program analysis technique used to find vulnerabilities in complex software. Fuzzing involves presenting a target program with crafted malicious input designed to cause crashes, buffer overflows, memory errors, and exceptions. Crafting malicious inputs in an efficient manner is a difficult open problem and often the best approach to generating such inputs is through applying uniform random mutations to pre-existing valid inputs (seed files). We present a learning technique that uses neural networks to learn patterns in the input files from past fuzzing explorations to guide future fuzzing explorations. In particular, the neural models learn a function to predict good (and bad) locations in input files to perform fuzzing mutations based on the past mutations and corresponding code coverage information. We implement several neural models including LSTMs and sequence-to-sequence models that can encode variable length input files. We incorporate our models in the state-of-the-art AFL (American Fuzzy Lop) fuzzer and show significant improvements in terms of code coverage, unique code paths, and crashes for various input formats including ELF, PNG, PDF, and XML.",
                        "Citation Paper Authors": "Authors:Mohit Rajpal, William Blum, Rishabh Singh"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "uses LSTM to predict node failures that occur in super computing systems from HPC logs.\nAndy Brown et al. ",
                    "Citation Text": "Andy Brown, Aaron Tuor, Brian Hutchinson, and Nicole Nichols. Recurrent Neu-\nral Network Attention Mechanisms for Interpretable System Log Anomaly Detection.\nInProceedings oftheFirst Workshop onMachine Learning forComputing Systems,\nMLCS\u201918, pages 1:1\u20131:8, New York, NY , USA, 2018. ACM.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.04967",
                        "Citation Paper Title": "Title:Recurrent Neural Network Attention Mechanisms for Interpretable System Log Anomaly Detection",
                        "Citation Paper Abstract": "Abstract:Deep learning has recently demonstrated state-of-the art performance on key tasks related to the maintenance of computer systems, such as intrusion detection, denial of service attack detection, hardware and software system failures, and malware detection. In these contexts, model interpretability is vital for administrator and analyst to trust and act on the automated analysis of machine learning models. Deep learning methods have been criticized as black box oracles which allow limited insight into decision factors. In this work we seek to \"bridge the gap\" between the impressive performance of deep learning models and the need for interpretable model introspection. To this end we present recurrent neural network (RNN) language models augmented with attention for anomaly detection in system logs. Our methods are generally applicable to any computer system and logging source.\nBy incorporating attention variants into our RNN language models we create opportunities for model introspection and analysis without sacrificing state-of-the art performance.\nWe demonstrate model performance and illustrate model interpretability on an intrusion detection task using the Los Alamos National Laboratory (LANL) cyber security dataset, reporting upward of 0.99 area under the receiver operator characteristic curve despite being trained only on a single day's worth of data.",
                        "Citation Paper Authors": "Authors:Andy Brown, Aaron Tuor, Brian Hutchinson, Nicole Nichols"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1902.00641v2": {
            "Paper Title": "CodedPrivateML: A Fast and Privacy-Preserving Framework for Distributed\n  Machine Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.02945v2": {
            "Paper Title": "V-Gas: Generating High Gas Consumption Inputs to Avoid Out-of-Gas\n  Vulnerability",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": "classi\ufb01es and identi\ufb01es gas-focused vulnerabilities, and present\na static program analysis technique to automatically detect gas-\nfocused vulnerabilities with very high con\ufb01dence. EVMFuzz ",
                    "Citation Text": "Ying Fu, Meng Ren, Fuchen Ma, Yu Jiang, Heyuan Shi, and Jiaguang\nSun. Evmfuzz: Differential fuzz testing of ethereum virtual machine.\narXiv preprint arXiv:1903.08483 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.08483",
                        "Citation Paper Title": "Title:EVMFuzz: Differential Fuzz Testing of Ethereum Virtual Machine",
                        "Citation Paper Abstract": "Abstract:Ethereum Virtual Machine (EVM) is the run-time environment for smart contracts and its vulnerabilities may lead to serious problems to the Ethereum ecology. With lots of techniques being developed for the validation of smart contracts, the security problems of EVM have not been well-studied. In this paper, we propose EVMFuzz, aiming to detect vulnerabilities of EVMs with differential fuzz testing. The core idea of EVMFuzz is to continuously generate seed contracts for different EVMs' execution, so as to find as many inconsistencies among execution results as possible, eventually discover vulnerabilities with output cross-referencing. First, we present the evaluation metric for the internal inconsistency indicator, such as the opcode sequence executed and gas used. Then, we construct seed contracts via a set of predefined mutators and employ dynamic priority scheduling algorithm to guide seed contracts selection and maximize the inconsistency. Finally, we leverage different EVMs as crossreferencing oracles to avoid manual checking of the execution output. For evaluation, we conducted large-scale mutation on 36,295 real-world smart contracts and generated 253,153 smart contracts. Among them, 66.2% showed differential performance, including 1,596 variant contracts triggered inconsistent output among EVMs. Accompanied by manual root cause analysis, we found 5 previously unknown security bugs in four widely used EVMs, and all had been included in Common Vulnerabilities and Exposures (CVE) database.",
                        "Citation Paper Authors": "Authors:Ying Fu, Meng Ren, Fuchen Ma, Yu Jiang, Heyuan Shi, Jiaguang Sun"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "presents\na security analysis framework for Ethereum smart contracts.\nAnother recent work ",
                    "Citation Text": "Ilya Grishchenko, Matteo Maffei, and Clara Schneidewind. A semantic\nframework for the security analysis of ethereum smart contracts. In\nInternational Conference on Principles of Security and Trust , pages\n243\u2013269. Springer, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.08660",
                        "Citation Paper Title": "Title:A Semantic Framework for the Security Analysis of Ethereum smart contracts",
                        "Citation Paper Abstract": "Abstract:Smart contracts are programs running on cryptocurrency (e.g., Ethereum) blockchains, whose popularity stem from the possibility to perform financial transactions, such as payments and auctions, in a distributed environment without need for any trusted third party. Given their financial nature, bugs or vulnerabilities in these programs may lead to catastrophic consequences, as witnessed by recent attacks. Unfortunately, programming smart contracts is a delicate task that requires strong expertise: Ethereum smart contracts are written in Solidity, a dedicated language resembling JavaScript, and shipped over the blockchain in the EVM bytecode format. In order to rigorously verify the security of smart contracts, it is of paramount importance to formalize their semantics as well as the security properties of interest, in particular at the level of the bytecode being executed.\nIn this paper, we present the first complete small-step semantics of EVM bytecode, which we formalize in the F* proof assistant, obtaining executable code that we successfully validate against the official Ethereum test suite. Furthermore, we formally define for the first time a number of central security properties for smart contracts, such as call integrity, atomicity, and independence from miner controlled parameters. This formalization relies on a combination of hyper- and safety properties. Along this work, we identified various mistakes and imprecisions in existing semantics and verification tools for Ethereum smart contracts, thereby demonstrating once more the importance of rigorous semantic foundations for the design of security verification techniques.",
                        "Citation Paper Authors": "Authors:Ilya Grishchenko, Matteo Maffei, Clara Schneidewind"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "de\ufb01nes some oracles to detect vulnerabilities in smart con-\ntracts such as dangerous delegate bugs. Vandal ",
                    "Citation Text": "Lexi Brent, Anton Jurisevic, Michael Kong, Eric Liu, Francois Gauthier,\nVincent Gramoli, Ralph Holz, and Bernhard Scholz. Vandal: A\nscalable security analysis framework for smart contracts. arXiv preprint\narXiv:1809.03981 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.03981",
                        "Citation Paper Title": "Title:Vandal: A Scalable Security Analysis Framework for Smart Contracts",
                        "Citation Paper Abstract": "Abstract:The rise of modern blockchains has facilitated the emergence of smart contracts: autonomous programs that live and run on the blockchain. Smart contracts have seen a rapid climb to prominence, with applications predicted in law, business, commerce, and governance.\nSmart contracts are commonly written in a high-level language such as Ethereum's Solidity, and translated to compact low-level bytecode for deployment on the blockchain. Once deployed, the bytecode is autonomously executed, usually by a %Turing-complete virtual machine. As with all programs, smart contracts can be highly vulnerable to malicious attacks due to deficient programming methodologies, languages, and toolchains, including buggy compilers. At the same time, smart contracts are also high-value targets, often commanding large amounts of cryptocurrency. Hence, developers and auditors need security frameworks capable of analysing low-level bytecode to detect potential security vulnerabilities.\nIn this paper, we present Vandal: a security analysis framework for Ethereum smart contracts. Vandal consists of an analysis pipeline that converts low-level Ethereum Virtual Machine (EVM) bytecode to semantic logic relations. Users of the framework can express security analyses in a declarative fashion: a security analysis is expressed in a logic specification written in the \\souffle language. We conduct a large-scale empirical study for a set of common smart contract security vulnerabilities, and show the effectiveness and efficiency of Vandal. Vandal is both fast and robust, successfully analysing over 95\\% of all 141k unique contracts with an average runtime of 4.15 seconds; outperforming the current state of the art tools---Oyente, EthIR, Mythril, and Rattle---under equivalent conditions.",
                        "Citation Paper Authors": "Authors:Lexi Brent, Anton Jurisevic, Michael Kong, Eric Liu, Francois Gauthier, Vincent Gramoli, Ralph Holz, Bernhard Scholz"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1905.12774v3": {
            "Paper Title": "Quantifying the Privacy Risks of Learning High-Dimensional Graphical\n  Models",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": "learn a Bayesian network in a di\ufb00erentially privat e way and then use a noisy version of\nit to generate synthetic data. The authors in ",
                    "Citation Text": "V. Bindschaedler, R. Shokri, and C. A. Gunter. Plausible d eniability for privacy-preserving\ndata synthesis. Proceedings of the VLDB Endowment , 10(5):481\u2013492, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.07975",
                        "Citation Paper Title": "Title:Plausible Deniability for Privacy-Preserving Data Synthesis",
                        "Citation Paper Abstract": "Abstract:Releasing full data records is one of the most challenging problems in data privacy. On the one hand, many of the popular techniques such as data de-identification are problematic because of their dependence on the background knowledge of adversaries. On the other hand, rigorous methods such as the exponential mechanism for differential privacy are often computationally impractical to use for releasing high dimensional data or cannot preserve high utility of original data due to their extensive data perturbation.\nThis paper presents a criterion called plausible deniability that provides a formal privacy guarantee, notably for releasing sensitive datasets: an output record can be released only if a certain amount of input records are indistinguishable, up to a privacy parameter. This notion does not depend on the background knowledge of an adversary. Also, it can efficiently be checked by privacy tests. We present mechanisms to generate synthetic datasets with similar statistical properties to the input data and the same format. We study this technique both theoretically and experimentally. A key theoretical result shows that, with proper randomization, the plausible deniability mechanism generates differentially private synthetic data. We demonstrate the efficiency of this generative technique on a large dataset; it is shown to preserve the utility of original data with respect to various statistical analysis and machine learning measures.",
                        "Citation Paper Authors": "Authors:Vincent Bindschaedler, Reza Shokri, Carl A. Gunter"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "and in\na collaborative learning setting [17, 18]. These works prov ide empirical analysis of tracing attack\non complex models. A theoretical formulation of Bayes-optim al attack for membership inference\nagainst neural networks was given in ",
                    "Citation Text": "A. Sablayrolles, M. Douze, Y. Ollivier, C. Schmid, and H . J\u00e9gou. White-box vs black-box:\nBayes optimal strategies for membership inference. arXiv preprint arXiv:1908.11229 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.11229",
                        "Citation Paper Title": "Title:White-box vs Black-box: Bayes Optimal Strategies for Membership Inference",
                        "Citation Paper Abstract": "Abstract:Membership inference determines, given a sample and trained parameters of a machine learning model, whether the sample was part of the training set. In this paper, we derive the optimal strategy for membership inference with a few assumptions on the distribution of the parameters. We show that optimal attacks only depend on the loss function, and thus black-box attacks are as good as white-box attacks. As the optimal strategy is not tractable, we provide approximations of it leading to several inference methods, and show that existing membership inference methods are coarser approximations of this optimal strategy. Our membership attacks outperform the state of the art in various settings, ranging from a simple logistic regression to more complex architectures and datasets, such as ResNet-101 and Imagenet.",
                        "Citation Paper Authors": "Authors:Alexandre Sablayrolles, Matthijs Douze, Yann Ollivier, Cordelia Schmid, Herv\u00e9 J\u00e9gou"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "but relax certain assumptions on the knowledge and po wer of adversary. Similar attacks\nwere performed against aggregate location data ",
                    "Citation Text": "A. Pyrgelis, C. Troncoso, and E. De Cristofaro. Knock kn ock, who\u2019s there? membership\ninference on aggregate location data. arXiv preprint arXiv:1708.06145 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.06145",
                        "Citation Paper Title": "Title:Knock Knock, Who's There? Membership Inference on Aggregate Location Data",
                        "Citation Paper Abstract": "Abstract:Aggregate location data is often used to support smart services and applications, e.g., generating live traffic maps or predicting visits to businesses. In this paper, we present the first study on the feasibility of membership inference attacks on aggregate location time-series. We introduce a game-based definition of the adversarial task, and cast it as a classification problem where machine learning can be used to distinguish whether or not a target user is part of the aggregates.\nWe empirically evaluate the power of these attacks on both raw and differentially private aggregates using two mobility datasets. We find that membership inference is a serious privacy threat, and show how its effectiveness depends on the adversary's prior knowledge, the characteristics of the underlying location data, as well as the number of users and the timeframe on which aggregation is performed. Although differentially private mechanisms can indeed reduce the extent of the attacks, they also yield a significant loss in utility. Moreover, a strategic adversary mimicking the behavior of the defense mechanism can greatly limit the protection they provide. Overall, our work presents a novel methodology geared to evaluate membership inference on aggregate location data in real-world settings and can be used by providers to assess the quality of privacy protection before data release or by regulators to detect violations.",
                        "Citation Paper Authors": "Authors:Apostolos Pyrgelis, Carmela Troncoso, Emiliano De Cristofaro"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "constructs shadow models that mimic the behavior of the target\nmodel. The attack is treated as a binary classi\ufb01cation probl em and the decision rule is a machine\nlearning model trained on data from the shadow models. Salem et al. ",
                    "Citation Text": "A. Salem, Y. Zhang, M. Humbert, P. Berrang, M. Fritz, and M . Backes. Ml-leaks: Model\nand data independent membership inference attacks and defe nses on machine learning models.\narXiv preprint arXiv:1806.01246 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.01246",
                        "Citation Paper Title": "Title:ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models",
                        "Citation Paper Abstract": "Abstract:Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications.\nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains.\nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model.",
                        "Citation Paper Authors": "Authors:Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, Michael Backes"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.12322v3": {
            "Paper Title": "Crypto-Oriented Neural Architecture Design",
            "Sentences": [
                {
                    "Sentence ID": 54,
                    "Sentence": "further improved this approach\nby introducing the inverted residual with linear bottleneck\nblock. Shuf\ufb02eNetV1 ",
                    "Citation Text": "Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.\nShuf\ufb02enet: An extremely ef\ufb01cient convolutional neural net-\nwork for mobile devices. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition , pages\n6848\u20136856, 2018. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.01083",
                        "Citation Paper Title": "Title:ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices",
                        "Citation Paper Abstract": "Abstract:We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures, e.g. lower top-1 error (absolute 7.8%) than recent MobileNet on ImageNet classification task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves ~13x actual speedup over AlexNet while maintaining comparable accuracy.",
                        "Citation Paper Authors": "Authors:Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, Jian Sun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1906.04411v2": {
            "Paper Title": "Evolutionary Trigger Set Generation for DNN Black-Box Watermarking",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": ". Those methods again focus\non individual samples and are essentially solving a differ-\nent problem than ours. It is worth noting that the work from\nMoosavi-Dezfooli et al. aims at creating universal adversarial\nperturbations ",
                    "Citation Text": "S. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and\nP. Frossard, \u201cUniversal adversarial perturbations,\u201d in\n2017 IEEE Conference on Computer Vision and Pattern\nRecognition , pp. 86\u201394, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.08401",
                        "Citation Paper Title": "Title:Universal adversarial perturbations",
                        "Citation Paper Abstract": "Abstract:Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images.",
                        "Citation Paper Authors": "Authors:Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal Frossard"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1910.14268v4": {
            "Paper Title": "RIGA: Covert and Robust White-Box Watermarking of Deep Neural Networks",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": "discussion in Section 2).\nA black-box watermark can be extracted by only querying the model\n(black-box access). A white-box watermark needs access to the model\nand its parameters in order to extract the watermark. Recent studies ",
                    "Citation Text": "Masoumeh Shafieinejad, Jiaqi Wang, Nils Lukas, and Florian Kerschbaum.\n2019. On the Robustness of the Backdoor-based Watermarking in Deep Neural\nNetworks. CoRR abs/1906.07745 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.07745",
                        "Citation Paper Title": "Title:On the Robustness of the Backdoor-based Watermarking in Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:Obtaining the state of the art performance of deep learning models imposes a high cost to model generators, due to the tedious data preparation and the substantial processing requirements. To protect the model from unauthorized re-distribution, watermarking approaches have been introduced in the past couple of years. We investigate the robustness and reliability of state-of-the-art deep neural network watermarking schemes. We focus on backdoor-based watermarking and propose two -- a black-box and a white-box -- attacks that remove the watermark. Our black-box attack steals the model and removes the watermark with minimum requirements; it just relies on public unlabeled data and a black-box access to the classification label. It does not need classification confidences or access to the model's sensitive information such as the training data set, the trigger set or the model parameters. The white-box attack, proposes an efficient watermark removal when the parameters of the marked model are available; our white-box attack does not require access to the labeled data or the trigger set and improves the runtime of the black-box attack up to seventeen times. We as well prove the security inadequacy of the backdoor-based watermarking in keeping the watermark undetectable by proposing an attack that detects whether a model contains a watermark. Our attacks show that a recipient of a marked model can remove a backdoor-based watermark with significantly less effort than training a new model and some other techniques are needed to protect against re-distribution by a motivated attacker.",
                        "Citation Paper Authors": "Authors:Masoumeh Shafieinejad, Jiaqi Wang, Nils Lukas, Xinda Li, Florian Kerschbaum"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "use adversarial examples to generate wa-\ntermarks. Szyller et al. ",
                    "Citation Text": "Sebastian Szyller, Buse Gul Atli, Samuel Marchal, and N. Asokan. 2019. DAWN:\nDynamic Adversarial Watermarking of Neural Networks. CoRR abs/1906.00830\n(2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.00830",
                        "Citation Paper Title": "Title:DAWN: Dynamic Adversarial Watermarking of Neural Networks",
                        "Citation Paper Abstract": "Abstract:Training machine learning (ML) models is expensive in terms of computational power, amounts of labeled data and human expertise. Thus, ML models constitute intellectual property (IP) and business value for their owners. Embedding digital watermarks during model training allows a model owner to later identify their models in case of theft or misuse. However, model functionality can also be stolen via model extraction, where an adversary trains a surrogate model using results returned from a prediction API of the original model. Recent work has shown that model extraction is a realistic threat. Existing watermarking schemes are ineffective against IP theft via model extraction since it is the adversary who trains the surrogate model. In this paper, we introduce DAWN (Dynamic Adversarial Watermarking of Neural Networks), the first approach to use watermarking to deter model extraction IP theft. Unlike prior watermarking schemes, DAWN does not impose changes to the training process but it operates at the prediction API of the protected model, by dynamically changing the responses for a small subset of queries (e.g., <0.5%) from API clients. This set is a watermark that will be embedded in case a client uses its queries to train a surrogate model. We show that DAWN is resilient against two state-of-the-art model extraction attacks, effectively watermarking all extracted surrogate models, allowing model owners to reliably demonstrate ownership (with confidence $>1- 2^{-64}$), incurring negligible loss of prediction accuracy (0.03-0.5%).",
                        "Citation Paper Authors": "Authors:Sebastian Szyller, Buse Gul Atli, Samuel Marchal, N. Asokan"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": ". Attacks with stronger assumptions [ 2,6,6,20,30] have\nlater confirmed this result.\nThere are other types of black-box algorithms. Chen et al. ",
                    "Citation Text": "Huili Chen, Bita Darvish Rouhani, and Farinaz Koushanfar. 2019. BlackMarks:\nBlackbox Multibit Watermarking for Deep Neural Networks. CoRR abs/1904.00344\n(2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.00344",
                        "Citation Paper Title": "Title:BlackMarks: Blackbox Multibit Watermarking for Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:Deep Neural Networks have created a paradigm shift in our ability to comprehend raw data in various important fields ranging from computer vision and natural language processing to intelligence warfare and healthcare. While DNNs are increasingly deployed either in a white-box setting where the model internal is publicly known, or a black-box setting where only the model outputs are known, a practical concern is protecting the models against Intellectual Property (IP) infringement. We propose BlackMarks, the first end-to-end multi-bit watermarking framework that is applicable in the black-box scenario. BlackMarks takes the pre-trained unmarked model and the owner's binary signature as inputs and outputs the corresponding marked model with a set of watermark keys. To do so, BlackMarks first designs a model-dependent encoding scheme that maps all possible classes in the task to bit '0' and bit '1' by clustering the output activations into two groups. Given the owner's watermark signature (a binary string), a set of key image and label pairs are designed using targeted adversarial attacks. The watermark (WM) is then embedded in the prediction behavior of the target DNN by fine-tuning the model with generated WM key set. To extract the WM, the remote model is queried by the WM key images and the owner's signature is decoded from the corresponding predictions according to the designed encoding scheme. We perform a comprehensive evaluation of BlackMarks's performance on MNIST, CIFAR10, ImageNet datasets and corroborate its effectiveness and robustness. BlackMarks preserves the functionality of the original DNN and incurs negligible WM embedding runtime overhead as low as 2.054%.",
                        "Citation Paper Authors": "Authors:Huili Chen, Bita Darvish Rouhani, Farinaz Koushanfar"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.05184v2": {
            "Paper Title": "CHEETAH: An Ultra-Fast, Approximation-Free, and Privacy-Preserved Neural\n  Network Framework based on Joint Obscure Linear and Nonlinear Computations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.04374v2": {
            "Paper Title": "Fast and Robust Distributed Learning in High Dimension",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.07310v2": {
            "Paper Title": "Incentivized Blockchain-based Social Media Platforms: A Case Study of\n  Steemit",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": "Munmun De Choudhury and Sushovan De. Mental health discourse on reddit:\nSelf-disclosure, social support, and anonymity. In ICWSM , 2014. ",
                    "Citation Text": "Emilio Ferrara, Onur Varol, Clayton Davis, Filippo Menczer, and Alessandro\nFlammini. The rise of social bots. Communications of the ACM , 59(7):96\u2013104,\n2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1407.5225",
                        "Citation Paper Title": "Title:The Rise of Social Bots",
                        "Citation Paper Abstract": "Abstract:The Turing test aimed to recognize the behavior of a human from that of a computer algorithm. Such challenge is more relevant than ever in today's social media context, where limited attention and technology constrain the expressive power of humans, while incentives abound to develop software agents mimicking humans. These social bots interact, often unnoticed, with real people in social media ecosystems, but their abundance is uncertain. While many bots are benign, one can design harmful bots with the goals of persuading, smearing, or deceiving. Here we discuss the characteristics of modern, sophisticated social bots, and how their presence can endanger online ecosystems and our society. We then review current efforts to detect social bots on Twitter. Features related to content, network, sentiment, and temporal patterns of activity are imitated by bots but at the same time can help discriminate synthetic behaviors from human ones, yielding signatures of engineered social tampering.",
                        "Citation Paper Authors": "Authors:Emilio Ferrara, Onur Varol, Clayton Davis, Filippo Menczer, Alessandro Flammini"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1906.00230v6": {
            "Paper Title": "Improving VAEs' Robustness to Adversarial Attack",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.09703v3": {
            "Paper Title": "Privacy-Preserving Smart Parking System Using Blockchain and Private\n  Information Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.00783v2": {
            "Paper Title": "A Stealthy Hardware Trojan Exploiting the Architectural Vulnerability of\n  Deep Learning Architectures: Input Interception Attack (IIA)",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.12640v2": {
            "Paper Title": "Copy Move Source-Target Disambiguation through Multi-Branch CNNs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.00888v4": {
            "Paper Title": "Deep Neural Network Fingerprinting by Conferrable Adversarial Examples",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.10615v3": {
            "Paper Title": "Adversarial Policies: Attacking Deep Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.11299v3": {
            "Paper Title": "Quick survey of graph-based fraud detection methods",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.13229v5": {
            "Paper Title": "Private Hypothesis Selection",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.01783v3": {
            "Paper Title": "Oracle Efficient Private Non-Convex Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.10958v3": {
            "Paper Title": "Malware Classification using Deep Learning based Feature Extraction and\n  Wrapper based Feature Selection Technique",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.12318v2": {
            "Paper Title": "More (or Less) Economic Limits of the Blockchain",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.12165v4": {
            "Paper Title": "Adversarial Defense via Local Flatness Regularization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.00268v2": {
            "Paper Title": "Detecting Covert Cryptomining using HPC",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.03817v3": {
            "Paper Title": "Machine Unlearning",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": "diverges in an unbounded way unless the\nnumber of queries made is small, which is not the case for\nthe deep neural networks we experiment with.\n4. Decremental Learning: Ginart et al. ",
                    "Citation Text": "A. Ginart, M. Y . Guan, G. Valiant, and J. Zou, \u201cMaking AI forget you:\nData deletion in machine learning,\u201d CoRR , vol. abs/1907.05012, 2019.\n[Online]. Available: http://arxiv.org/abs/1907.05012",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.05012",
                        "Citation Paper Title": "Title:Making AI Forget You: Data Deletion in Machine Learning",
                        "Citation Paper Abstract": "Abstract:Intense recent discussions have focused on how to provide individuals with control over when their data can and cannot be used --- the EU's Right To Be Forgotten regulation is an example of this effort. In this paper we initiate a framework studying what to do when it is no longer permissible to deploy models derivative from specific user data. In particular, we formulate the problem of efficiently deleting individual data points from trained machine learning models. For many standard ML models, the only way to completely remove an individual's data is to retrain the whole model from scratch on the remaining data, which is often not computationally practical. We investigate algorithmic principles that enable efficient data deletion in ML. For the specific setting of k-means clustering, we propose two provably efficient deletion algorithms which achieve an average of over 100X improvement in deletion efficiency across 6 datasets, while producing clusters of comparable statistical quality to a canonical k-means++ baseline.",
                        "Citation Paper Authors": "Authors:Antonio Ginart, Melody Y. Guan, Gregory Valiant, James Zou"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": ",\"-\ndifferential privacy offers probabilistic guarantees about the\nprivacy of individual records in a database. In our case, \"\nbounds the changes in model parameters that may be induced\nby any single training point. While several efforts ",
                    "Citation Text": "M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov,\nK. Talwar, and L. Zhang, \u201cDeep learning with differential privacy,\u201d in\nProceedings of the 2016 ACM SIGSAC Conference on Computer and\nCommunications Security . ACM, 2016, pp. 308\u2013318.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.00133",
                        "Citation Paper Title": "Title:Deep Learning with Differential Privacy",
                        "Citation Paper Abstract": "Abstract:Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.",
                        "Citation Paper Authors": "Authors:Mart\u00edn Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, Li Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2001.00071v4": {
            "Paper Title": "privGAN: Protecting GANs from membership inference attacks at low cost",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.07426v2": {
            "Paper Title": "TAAL: Tampering Attack on Any Key-based Logic Locked Circuits",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.00686v2": {
            "Paper Title": "Demon in the Variant: Statistical Analysis of DNNs for Robust Backdoor\n  Contamination Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.03250v2": {
            "Paper Title": "Differentially Private Synthetic Mixed-Type Data Generation For\n  Unsupervised Learning",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "is used to learn the\ndistribution in the latent space of encoded data of the given label-class. The main workhorse, DM-EM\nalgorithm, is designed and analyzed for Gaussian mixture models and more general factor analysis mod-\nels. ",
                    "Citation Text": "Qingrong Chen, Chong Xiang, Minhui Xue, Bo Li, Nikita Borisov, Dali Kaarfar, and Haojin Zhu.\nDi\u000berentially private data generative models. arXiv preprint 1812.02274, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.02274",
                        "Citation Paper Title": "Title:Differentially Private Data Generative Models",
                        "Citation Paper Abstract": "Abstract:Deep neural networks (DNNs) have recently been widely adopted in various applications, and such success is largely due to a combination of algorithmic breakthroughs, computation resource improvements, and access to a large amount of data. However, the large-scale data collections required for deep learning often contain sensitive information, therefore raising many privacy concerns. Prior research has shown several successful attacks in inferring sensitive training data information, such as model inversion, membership inference, and generative adversarial networks (GAN) based leakage attacks against collaborative deep learning. In this paper, to enable learning efficiency as well as to generate data with privacy guarantees and high utility, we propose a differentially private autoencoder-based generative model (DP-AuGM) and a differentially private variational autoencoder-based generative model (DP-VaeGM). We evaluate the robustness of two proposed models. We show that DP-AuGM can effectively defend against the model inversion, membership inference, and GAN-based attacks. We also show that DP-VaeGM is robust against the membership inference attack. We conjecture that the key to defend against the model inversion and GAN-based attacks is not due to differential privacy but the perturbation of training data. Finally, we demonstrate that both DP-AuGM and DP-VaeGM can be easily integrated with real-world machine learning applications, such as machine learning as a service and federated learning, which are otherwise threatened by the membership inference attack and the GAN-based attack, respectively.",
                        "Citation Paper Authors": "Authors:Qingrong Chen, Chong Xiang, Minhui Xue, Bo Li, Nikita Borisov, Dali Kaarfar, Haojin Zhu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1901.02094v3": {
            "Paper Title": "Differentially Private ADMM for Distributed Medical Machine Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.02046v2": {
            "Paper Title": "Data Poisoning Attacks to Local Differential Privacy Protocols",
            "Sentences": [
                {
                    "Sentence ID": 48,
                    "Sentence": ", our\nattacks are applicable and the security-privacy trade-off still\nholds. When there is no local encoding or perturbation step in\nthe SMPC-based DP protocols like ",
                    "Citation Text": "Amrita Roy Chowdhury, Chenghong Wang, Xi He, Ash-\nwin Machanavajjhala, and Somesh Jha. Crypt e: Crypto-\nassisted differential privacy on untrusted servers. In\nSIGMOD , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.07756",
                        "Citation Paper Title": "Title:Crypt$\u03b5$: Crypto-Assisted Differential Privacy on Untrusted Servers",
                        "Citation Paper Abstract": "Abstract:Differential privacy (DP) has steadily become the de-facto standard for achieving privacy in data analysis, which is typically implemented either in the \"central\" or \"local\" model. The local model has been more popular for commercial deployments as it does not require a trusted data collector. This increased privacy, however, comes at a cost of utility and algorithmic expressibility as compared to the central model.\nIn this work, we propose, Crypt$\\epsilon$, a system and programming framework that (1) achieves the accuracy guarantees and algorithmic expressibility of the central model (2) without any trusted data collector like in the local model. Crypt$\\epsilon$ achieves the \"best of both worlds\" by employing two non-colluding untrusted servers that run DP programs on encrypted data from the data owners. Although straightforward implementations of DP programs using secure computation tools can achieve the above goal theoretically, in practice they are beset with many challenges such as poor performance and tricky security proofs. To this end, Crypt$\\epsilon$ allows data analysts to author logical DP programs that are automatically translated to secure protocols that work on encrypted data. These protocols ensure that the untrusted servers learn nothing more than the noisy outputs, thereby guaranteeing DP (for computationally bounded adversaries) for all Crypt$\\epsilon$ programs. Crypt$\\epsilon$ supports a rich class of DP programs that can be expressed via a small set of transformation and measurement operators followed by arbitrary post-processing. Further, we propose performance optimizations leveraging the fact that the output is noisy. We demonstrate Crypt$\\epsilon$'s feasibility for practical DP analysis with extensive empirical evaluations on real datasets.",
                        "Citation Paper Authors": "Authors:Amrita Roy Chowdhury, Chenghong Wang, Xi He, Ashwin Machanavajjhala, Somesh Jha"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "apply shuf\ufb02ing to the\nusers\u2019 perturbed vectors such that a better DP guarantee can\nbe derived. Since they still encode and perturb each user\u2019s\ndata, our attacks are applicable. When SMPC-based proto-\ncols have local encoding and perturbation steps like ",
                    "Citation Text": "Peter Kairouz, Sewoong Oh, and Pramod Viswanath.\nSecure multi-party differential privacy. In NeurIPS ,\n2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1311.0776",
                        "Citation Paper Title": "Title:The Composition Theorem for Differential Privacy",
                        "Citation Paper Abstract": "Abstract:Sequential querying of differentially private mechanisms degrades the overall privacy level. In this paper, we answer the fundamental question of characterizing the level of overall privacy degradation as a function of the number of queries and the privacy levels maintained by each privatization mechanism. Our solution is complete: we prove an upper bound on the overall privacy level and construct a sequence of privatization mechanisms that achieves this bound. The key innovation is the introduction of an operational interpretation of differential privacy (involving hypothesis testing) and the use of new data processing inequalities. Our result improves over the state-of-the-art, and has immediate applications in several problems studied in the literature including differentially private multi-party computation.",
                        "Citation Paper Authors": "Authors:Peter Kairouz, Sewoong Oh, Pramod Viswanath"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "). However, such methods generally\nsacri\ufb01ce computational ef\ufb01ciency.\n7 Discussion\nApplicability to shuf\ufb02ing-based and SMPC-based proto-\ncols: Shuf\ufb02ing-based protocols ",
                    "Citation Text": "\u00dalfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth\nRaghunathan, Kunal Talwar, and Abhradeep Thakurta.\nAmpli\ufb01cation by shuf\ufb02ing: From local to central differ-\nential privacy via anonymity. In SODA , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.12469",
                        "Citation Paper Title": "Title:Amplification by Shuffling: From Local to Central Differential Privacy via Anonymity",
                        "Citation Paper Abstract": "Abstract:Sensitive statistics are often collected across sets of users, with repeated collection of reports done over time. For example, trends in users' private preferences or software usage may be monitored via such reports. We study the collection of such statistics in the local differential privacy (LDP) model, and describe an algorithm whose privacy cost is polylogarithmic in the number of changes to a user's value.\nMore fundamentally---by building on anonymity of the users' reports---we also demonstrate how the privacy cost of our LDP algorithm can actually be much lower when viewed in the central model of differential privacy. We show, via a new and general privacy amplification technique, that any permutation-invariant algorithm satisfying $\\varepsilon$-local differential privacy will satisfy $(O(\\varepsilon \\sqrt{\\log(1/\\delta)/n}), \\delta)$-central differential privacy. By this, we explain how the high noise and $\\sqrt{n}$ overhead of LDP protocols is a consequence of them being significantly more private in the central model. As a practical corollary, our results imply that several LDP-based industrial deployments may have much lower privacy cost than their advertised $\\varepsilon$ would indicate---at least if reports are anonymized.",
                        "Citation Paper Authors": "Authors:\u00dalfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, Abhradeep Thakurta"
                    }
                },
                {
                    "Sentence ID": 50,
                    "Sentence": "studied data poisoning attacks to regression\nmodels. Shafahi et al. ",
                    "Citation Text": "Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian\nSuciu, Christoph Studer, Tudor Dumitras, and Tom Gold-\nstein. Poison frogs! targeted clean-label poisoning at-\ntacks on neural networks. In NeurIPS , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.00792",
                        "Citation Paper Title": "Title:Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks",
                        "Citation Paper Abstract": "Abstract:Data poisoning is an attack on machine learning models wherein the attacker adds examples to the training set to manipulate the behavior of the model at test time. This paper explores poisoning attacks on neural nets. The proposed attacks use \"clean-labels\"; they don't require the attacker to have any control over the labeling of training data. They are also targeted; they control the behavior of the classifier on a $\\textit{specific}$ test instance without degrading overall classifier performance. For example, an attacker could add a seemingly innocuous image (that is properly labeled) to a training set for a face recognition engine, and control the identity of a chosen person at test time. Because the attacker does not need to control the labeling function, poisons could be entered into the training set simply by leaving them on the web and waiting for them to be scraped by a data collection bot.\nWe present an optimization-based method for crafting poisons, and show that just one single poison image can control classifier behavior when transfer learning is used. For full end-to-end training, we present a \"watermarking\" strategy that makes poisoning reliable using multiple ($\\approx$50) poisoned training instances. We demonstrate our method by generating poisoned frog images from the CIFAR dataset and using them to manipulate image classifiers.",
                        "Citation Paper Authors": "Authors:Ali Shafahi, W. Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, Tom Goldstein"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "investigated data\npoisoning attacks against Support Vector Machines. Jagiel-\nski et al. ",
                    "Citation Text": "Matthew Jagielski, Alina Oprea, Battista Biggio, Chang\nLiu, Cristina Nita-Rotaru, and Bo Li. Manipulating ma-\nchine learning: Poisoning attacks and countermeasures\nfor regression learning. In S&P , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.00308",
                        "Citation Paper Title": "Title:Manipulating Machine Learning: Poisoning Attacks and Countermeasures for Regression Learning",
                        "Citation Paper Abstract": "Abstract:As machine learning becomes widely used for automated decisions, attackers have strong incentives to manipulate the results and models generated by machine learning algorithms. In this paper, we perform the first systematic study of poisoning attacks and their countermeasures for linear regression models. In poisoning attacks, attackers deliberately influence the training data to manipulate the results of a predictive model. We propose a theoretically-grounded optimization framework specifically designed for linear regression and demonstrate its effectiveness on a range of datasets and models. We also introduce a fast statistical attack that requires limited knowledge of the training process. Finally, we design a new principled defense method that is highly resilient against all poisoning attacks. We provide formal guarantees about its convergence and an upper bound on the effect of poisoning attacks when the defense is deployed. We evaluate extensively our attacks and defenses on three realistic datasets from health care, loan assessment, and real estate domains.",
                        "Citation Paper Authors": "Authors:Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina Nita-Rotaru, Bo Li"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1908.07701v2": {
            "Paper Title": "A Novel Privacy-Preserving Deep Learning Scheme without Using\n  Cryptography Component",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01226v3": {
            "Paper Title": "Piracy Resistant Watermarks for Deep Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.03020v2": {
            "Paper Title": "On the security relevance of weights in deep learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.00650v6": {
            "Paper Title": "Differential privacy with partial knowledge",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.01551v5": {
            "Paper Title": "Imperio: Robust Over-the-Air Adversarial Examples for Automatic Speech\n  Recognition Systems",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": ". Almost all previous works on attacks\nagainst ASR systems did not focus on real-world attacks [ 7,40] or\nwere only successful for simulated over-the-air attacks ",
                    "Citation Text": "Yao Qin, Nicholas Carlini, Ian Goodfellow, Garrison Cottrell, and Colin Raffel.\n2019. Imperceptible, Robust, and Targeted Adversarial Examples for Automatic\nSpeech Recognition. In arXiv preprint arXiv:1903.10346 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.10346",
                        "Citation Paper Title": "Title:Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition",
                        "Citation Paper Abstract": "Abstract:Adversarial examples are inputs to machine learning models designed by an adversary to cause an incorrect output. So far, adversarial examples have been studied most extensively in the image domain. In this domain, adversarial examples can be constructed by imperceptibly modifying images to cause misclassification, and are practical in the physical world. In contrast, current targeted adversarial examples applied to speech recognition systems have neither of these properties: humans can easily identify the adversarial perturbations, and they are not effective when played over-the-air. This paper makes advances on both of these fronts. First, we develop effectively imperceptible audio adversarial examples (verified through a human study) by leveraging the psychoacoustic principle of auditory masking, while retaining 100% targeted success rate on arbitrary full-sentence targets. Next, we make progress towards physical-world over-the-air audio adversarial examples by constructing perturbations which remain effective even after applying realistic simulated environmental distortions.",
                        "Citation Paper Authors": "Authors:Yao Qin, Nicholas Carlini, Ian Goodfellow, Garrison Cottrell, Colin Raffel"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "present a similar approach in their paper.\nSch\u00f6nherr et al. ",
                    "Citation Text": "Lea Sch\u00f6nherr, Katharina Kohls, Steffen Zeiler, Thorsten Holz, and Dorothea\nKolossa. 2019. Adversarial Attacks Against Automatic Speech Recognition Sys-\ntems via Psychoacoustic Hiding. In Network and Distributed System Security\nSymposium (NDSS) .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.05665",
                        "Citation Paper Title": "Title:Adversarial Attacks Against Automatic Speech Recognition Systems via Psychoacoustic Hiding",
                        "Citation Paper Abstract": "Abstract:Voice interfaces are becoming accepted widely as input methods for a diverse set of devices. This development is driven by rapid improvements in automatic speech recognition (ASR), which now performs on par with human listening in many tasks. These improvements base on an ongoing evolution of DNNs as the computational core of ASR. However, recent research results show that DNNs are vulnerable to adversarial perturbations, which allow attackers to force the transcription into a malicious output.\nIn this paper, we introduce a new type of adversarial examples based on psychoacoustic hiding. Our attack exploits the characteristics of DNN-based ASR systems, where we extend the original analysis procedure by an additional backpropagation step. We use this backpropagation to learn the degrees of freedom for the adversarial perturbation of the input signal, i.e., we apply a psychoacoustic model and manipulate the acoustic signal below the thresholds of human perception. To further minimize the perceptibility of the perturbations, we use forced alignment to find the best fitting temporal alignment between the original audio sample and the malicious target transcription. These extensions allow us to embed an arbitrary audio input with a malicious voice command that is then transcribed by the ASR system, with the audio signal remaining barely distinguishable from the original signal. In an experimental evaluation, we attack the state-of-the-art speech recognition system Kaldi and determine the best performing parameter and analysis setup for different types of input. Our results show that we are successful in up to 98% of cases with a computational effort of fewer than two minutes for a ten-second audio file. Based on user studies, we found that none of our target transcriptions were audible to human listeners, who still understand the original speech content with unchanged accuracy.",
                        "Citation Paper Authors": "Authors:Lea Sch\u00f6nherr, Katharina Kohls, Steffen Zeiler, Thorsten Holz, Dorothea Kolossa"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "proposed\na black-box attack based on evolutionary optimization, and also\nTaori et al. ",
                    "Citation Text": "Rohan Taori, Amog Kamsetty, Brenton Chu, and Nikita Vemuri. 2018. Targeted\nadversarial examples for black box audio systems. arXiv preprint arXiv:1805.07820\n(2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.07820",
                        "Citation Paper Title": "Title:Targeted Adversarial Examples for Black Box Audio Systems",
                        "Citation Paper Abstract": "Abstract:The application of deep recurrent networks to audio transcription has led to impressive gains in automatic speech recognition (ASR) systems. Many have demonstrated that small adversarial perturbations can fool deep neural networks into incorrectly predicting a specified target with high confidence. Current work on fooling ASR systems have focused on white-box attacks, in which the model architecture and parameters are known. In this paper, we adopt a black-box approach to adversarial generation, combining the approaches of both genetic algorithms and gradient estimation to solve the task. We achieve a 89.25% targeted attack similarity after 3000 generations while maintaining 94.6% audio file similarity.",
                        "Citation Paper Authors": "Authors:Rohan Taori, Amog Kamsetty, Brenton Chu, Nikita Vemuri"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "published a work to\nobfuscate Amazon\u2019s Alexa wake word via specifically crafted music.\nHowever, their approach was not successful at creating targeted\nadversarial examples that work over the air.\nAlzantot et al. ",
                    "Citation Text": "Moustafa Alzantot, Bharathan Balaji, and Mani Srivastava. 2018. Did you hear\nthat? Adversarial examples against automatic speech recognition. arXiv preprint\narXiv:1801.00554 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.00554",
                        "Citation Paper Title": "Title:Did you hear that? Adversarial Examples Against Automatic Speech Recognition",
                        "Citation Paper Abstract": "Abstract:Speech is a common and effective way of communication between humans, and modern consumer devices such as smartphones and home hubs are equipped with deep learning based accurate automatic speech recognition to enable natural interaction between humans and machines. Recently, researchers have demonstrated powerful attacks against machine learning models that can fool them to produceincorrect results. However, nearly all previous research in adversarial attacks has focused on image recognition and object detection models. In this short paper, we present a first of its kind demonstration of adversarial attacks against speech classification model. Our algorithm performs targeted attacks with 87% success by adding small background noise without having to know the underlying model parameter and architecture. Our attack only changes the least significant bits of a subset of audio clip samples, and the noise does not change 89% the human listener's perception of the audio clip as evaluated in our human study.",
                        "Citation Paper Authors": "Authors:Moustafa Alzantot, Bharathan Balaji, Mani Srivastava"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.04226v7": {
            "Paper Title": "Privacy-Preserving Multiple Tensor Factorization for Synthesizing\n  Large-Scale Location Traces with Cluster-Specific Features",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": ". This is called trimming , and is e\u000bective for ma-\ntrix completion ",
                    "Citation Text": "R. H. Keshavan, A. Montanari, and S. Oh. Matrix\ncompletion from noisy entries. In Proceedings of the\n22nd Conference on Neural Information Processing\nSystems (NIPS'09) , pages 952{960, 2009.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:0906.2027",
                        "Citation Paper Title": "Title:Matrix Completion from Noisy Entries",
                        "Citation Paper Abstract": "Abstract:Given a matrix M of low-rank, we consider the problem of reconstructing it from noisy observations of a small, random subset of its entries. The problem arises in a variety of applications, from collaborative filtering (the `Netflix problem') to structure-from-motion and positioning. We study a low complexity algorithm introduced by Keshavan et al.(2009), based on a combination of spectral techniques and manifold optimization, that we call here OptSpace. We prove performance guarantees that are order-optimal in a number of circumstances.",
                        "Citation Paper Authors": "Authors:Raghunandan H. Keshavan, Andrea Montanari, Sewoong Oh"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1904.00733v2": {
            "Paper Title": "Blockchain And The Future of the Internet: A Comprehensive Review",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.06895v2": {
            "Paper Title": "Deep Poisoning: Towards Robust Image Data Sharing against Visual\n  Disclosure",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": "for\nimage reconstruction under both conventional privacy pre-\nserving and the introduced image data sharing for collabo-\nration. The comparison experiment is conducted on CelebA\ndataset ",
                    "Citation Text": "Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.\nDeep learning face attributes in the wild. In Proceedings\nof International Conference on Computer Vision , December\n2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1411.7766",
                        "Citation Paper Title": "Title:Deep Learning Face Attributes in the Wild",
                        "Citation Paper Abstract": "Abstract:Predicting face attributes in the wild is challenging due to complex face variations. We propose a novel deep learning framework for attribute prediction in the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointly with attribute tags, but pre-trained differently. LNet is pre-trained by massive general object categories for face localization, while ANet is pre-trained by massive face identities for attribute prediction. This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation.\n(1) It shows how the performances of face localization (LNet) and attribute prediction (ANet) can be improved by different pre-training strategies.\n(2) It reveals that although the filters of LNet are fine-tuned only with image-level attribute tags, their response maps over entire images have strong indication of face locations. This fact enables training LNet for face localization with only image-level annotations, but without face bounding boxes or landmarks, which are required by all attribute recognition works.\n(3) It also demonstrates that the high-level hidden neurons of ANet automatically discover semantic concepts after pre-training with massive face identities, and such concepts are significantly enriched after fine-tuning with attribute tags. Each attribute can be well explained with a sparse linear combination of these concepts.",
                        "Citation Paper Authors": "Authors:Ziwei Liu, Ping Luo, Xiaogang Wang, Xiaoou Tang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.07497v4": {
            "Paper Title": "BDoS: Blockchain Denial of Service",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.02664v2": {
            "Paper Title": "Data Encoding for Byzantine-Resilient Distributed Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.12903v5": {
            "Paper Title": "IPGuard: Protecting Intellectual Property of Deep Neural Networks via\n  Fingerprinting the Classification Boundary",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": "as our target classifier, which is an improved version\nof ResNet20 for CIFAR-100.\nImageNet: ImageNet ",
                    "Citation Text": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,\nZhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C.\nBerg, and Li Fei-Fei. 2015. ImageNet Large Scale Visual Recognition Challenge.\nInternational Journal of Computer Vision (IJCV) 115, 3 (2015), 211\u2013252. https:\n//doi.org/10.1007/s11263-015-0816-y",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.0575",
                        "Citation Paper Title": "Title:ImageNet Large Scale Visual Recognition Challenge",
                        "Citation Paper Abstract": "Abstract:The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions.\nThis paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.",
                        "Citation Paper Authors": "Authors:Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, Li Fei-Fei"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1902.06626v5": {
            "Paper Title": "Mockingbird: Defending Against Deep-Learning-Based Website\n  Fingerprinting Attacks with Adversarial Traces",
            "Sentences": [
                {
                    "Sentence ID": 52,
                    "Sentence": "can be used to train new, moreef\ufb01cient student models with minimal reductions in accuracy.\nApplication of these techniques may achieve speedups in the\nrange of 100-300% for our detector model.\nFinally, recent research by Chen et al. ",
                    "Citation Text": "B. Chen, T. Medini, J. Farwell, S. Gobriel, C. Tai, and A. Shrivastava,\n\u201cSLIDE: In defense of smart algorithms over hardware acceleration for\nlarge-scale deep learning systems,\u201d arXiv preprint:1903.03129 , 2019.\nAPPENDIX A\nPERFORMANCE OF BASE C&W M ETHOD\nWe evaluate the ef\ufb01cacy of the adversarial traces generated\nby the Carlini and Wagner",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.03129",
                        "Citation Paper Title": "Title:SLIDE : In Defense of Smart Algorithms over Hardware Acceleration for Large-Scale Deep Learning Systems",
                        "Citation Paper Abstract": "Abstract:Deep Learning (DL) algorithms are the central focus of modern machine learning systems. As data volumes keep growing, it has become customary to train large neural networks with hundreds of millions of parameters to maintain enough capacity to memorize these volumes and obtain state-of-the-art accuracy. To get around the costly computations associated with large models and data, the community is increasingly investing in specialized hardware for model training. However, specialized hardware is expensive and hard to generalize to a multitude of tasks. The progress on the algorithmic front has failed to demonstrate a direct advantage over powerful hardware such as NVIDIA-V100 GPUs. This paper provides an exception. We propose SLIDE (Sub-LInear Deep learning Engine) that uniquely blends smart randomized algorithms, with multi-core parallelism and workload optimization. Using just a CPU, SLIDE drastically reduces the computations during both training and inference outperforming an optimized implementation of Tensorflow (TF) on the best available GPU. Our evaluations on industry-scale recommendation datasets, with large fully connected architectures, show that training with SLIDE on a 44 core CPU is more than 3.5 times (1 hour vs. 3.5 hours) faster than the same network trained using TF on Tesla V100 at any given accuracy level. On the same CPU hardware, SLIDE is over 10x faster than TF. We provide codes and scripts for reproducibility.",
                        "Citation Paper Authors": "Authors:Beidi Chen, Tharun Medini, James Farwell, Sameh Gobriel, Charlie Tai, Anshumali Shrivastava"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1905.11814v3": {
            "Paper Title": "Shredder: Learning Noise Distributions to Protect Inference Privacy",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.08842v2": {
            "Paper Title": "T-TER: Defeating A2 Trojans with Targeted Tamper-Evident Routing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.12562v2": {
            "Paper Title": "Towards Security Threats of Deep Learning Systems: A Survey",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": "used deep contractive\nnetwork with contractive autoencoders and denoising\nautoencoders, which can remove amounts of adversarial\nnoise. Akhtar et al. ",
                    "Citation Text": "N. Akhtar, J. Liu, and A. S. Mian. Defense against univers al\nadversarial perturbations. CoRR , abs/1711.05929, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.05929",
                        "Citation Paper Title": "Title:Defense against Universal Adversarial Perturbations",
                        "Citation Paper Abstract": "Abstract:Recent advances in Deep Learning show the existence of image-agnostic quasi-imperceptible perturbations that when applied to `any' image can fool a state-of-the-art network classifier to change its prediction about the image label. These `Universal Adversarial Perturbations' pose a serious threat to the success of Deep Learning in practice. We present the first dedicated framework to effectively defend the networks against such perturbations. Our approach learns a Perturbation Rectifying Network (PRN) as `pre-input' layers to a targeted model, such that the targeted model needs no modification. The PRN is learned from real and synthetic image-agnostic perturbations, where an efficient method to compute the latter is also proposed. A perturbation detector is separately trained on the Discrete Cosine Transform of the input-output difference of the PRN. A query image is first passed through the PRN and verified by the detector. If a perturbation is detected, the output of the PRN is used for label prediction instead of the actual image. A rigorous evaluation shows that our framework can defend the network classifiers against unseen adversarial perturbations in the real-world scenarios with up to 97.5% success rate. The PRN also generalizes well in the sense that training for one targeted network defends another network with a comparable success rate.",
                        "Citation Paper Authors": "Authors:Naveed Akhtar, Jian Liu, Ajmal Mian"
                    }
                },
                {
                    "Sentence ID": 255,
                    "Sentence": "proposed feature squeezing methods, including reducing\nthe depth of color bit on each pixel and spatial smoothing.\nYang et al. ",
                    "Citation Text": "Y. Yang, G. Zhang, Z. Xu, and D. Katabi. Me-net: Towards effec-\ntive adversarial robustness with matrix estimation. In Proceedings\nof the 36th International Conference on Machine Learning, I CML 2019,\n9-15 June 2019, Long Beach, California, USA , pages 7025\u20137034.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.11971",
                        "Citation Paper Title": "Title:ME-Net: Towards Effective Adversarial Robustness with Matrix Estimation",
                        "Citation Paper Abstract": "Abstract:Deep neural networks are vulnerable to adversarial attacks. The literature is rich with algorithms that can easily craft successful adversarial examples. In contrast, the performance of defense techniques still lags behind. This paper proposes ME-Net, a defense method that leverages matrix estimation (ME). In ME-Net, images are preprocessed using two steps: first pixels are randomly dropped from the image; then, the image is reconstructed using ME. We show that this process destroys the adversarial structure of the noise, while re-enforcing the global structure in the original image. Since humans typically rely on such global structures in classifying images, the process makes the network mode compatible with human perception. We conduct comprehensive experiments on prevailing benchmarks such as MNIST, CIFAR-10, SVHN, and Tiny-ImageNet. Comparing ME-Net with state-of-the-art defense mechanisms shows that ME-Net consistently outperforms prior techniques, improving robustness against both black-box and white-box attacks.",
                        "Citation Paper Authors": "Authors:Yuzhe Yang, Guo Zhang, Dina Katabi, Zhi Xu"
                    }
                },
                {
                    "Sentence ID": 253,
                    "Sentence": "used\nbounded ReLU activation function for hedging forward\npropagation of adversarial perturbation. Xu et al. ",
                    "Citation Text": "W. Xu, D. Evans, and Y. Qi. Feature squeezing: Detectin g\nadversarial examples in deep neural networks. In 25th Annual\nNetwork and Distributed System Security Symposium, NDSS 20 18,\nSan Diego, California, USA, February 18-21, 2018 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.01155",
                        "Citation Paper Title": "Title:Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:Although deep neural networks (DNNs) have achieved great success in many tasks, they can often be fooled by \\emph{adversarial examples} that are generated by adding small but purposeful distortions to natural examples. Previous studies to defend against adversarial examples mostly focused on refining the DNN models, but have either shown limited success or required expensive computation. We propose a new strategy, \\emph{feature squeezing}, that can be used to harden DNN models by detecting adversarial examples. Feature squeezing reduces the search space available to an adversary by coalescing samples that correspond to many different feature vectors in the original space into a single sample. By comparing a DNN model's prediction on the original input with that on squeezed inputs, feature squeezing detects adversarial examples with high accuracy and few false positives. This paper explores two feature squeezing methods: reducing the color bit depth of each pixel and spatial smoothing. These simple strategies are inexpensive and complementary to other defenses, and can be combined in a joint detection framework to achieve high detection rates against state-of-the-art attacks.",
                        "Citation Paper Authors": "Authors:Weilin Xu, David Evans, Yanjun Qi"
                    }
                },
                {
                    "Sentence ID": 129,
                    "Sentence": "propose\nfeature distillation, a JPEG-based defensive compression\nframework to rectify AEs.\n\u2022Data preprocessing . Liang et al. ",
                    "Citation Text": "B. Liang, H. Li, M. Su, X. Li, W. Shi, and X. Wang. Detecting\nadversarial image examples in deep networks with adaptive\nnoise reduction. 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.08378",
                        "Citation Paper Title": "Title:Detecting Adversarial Image Examples in Deep Networks with Adaptive Noise Reduction",
                        "Citation Paper Abstract": "Abstract:Recently, many studies have demonstrated deep neural network (DNN) classifiers can be fooled by the adversarial example, which is crafted via introducing some perturbations into an original sample. Accordingly, some powerful defense techniques were proposed. However, existing defense techniques often require modifying the target model or depend on the prior knowledge of attacks. In this paper, we propose a straightforward method for detecting adversarial image examples, which can be directly deployed into unmodified off-the-shelf DNN models. We consider the perturbation to images as a kind of noise and introduce two classic image processing techniques, scalar quantization and smoothing spatial filter, to reduce its effect. The image entropy is employed as a metric to implement an adaptive noise reduction for different kinds of images. Consequently, the adversarial example can be effectively detected by comparing the classification results of a given sample and its denoised version, without referring to any prior knowledge of attacks. More than 20,000 adversarial examples against some state-of-the-art DNN models are used to evaluate the proposed method, which are crafted with different attack techniques. The experiments show that our detection method can achieve a high overall F1 score of 96.39% and certainly raises the bar for defense-aware attacks.",
                        "Citation Paper Authors": "Authors:Bin Liang, Hongcheng Li, Miaoqiang Su, Xirong Li, Wenchang Shi, Xiaofeng Wang"
                    }
                },
                {
                    "Sentence ID": 143,
                    "Sentence": "also\nused the knowledge extracted in distillation to reduce the\nmagnitude of network gradient. Liu et al. ",
                    "Citation Text": "Z. Liu, Q. Liu, T. Liu, N. Xu, X. Lin, Y. Wang, and W. Wen. F eature\ndistillation: Dnn-oriented JPEG compression against adve rsarial\nexamples. In IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 201 9,\npages 860\u2013868.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.05787",
                        "Citation Paper Title": "Title:Feature Distillation: DNN-Oriented JPEG Compression Against Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:Image compression-based approaches for defending against the adversarial-example attacks, which threaten the safety use of deep neural networks (DNN), have been investigated recently. However, prior works mainly rely on directly tuning parameters like compression rate, to blindly reduce image features, thereby lacking guarantee on both defense efficiency (i.e. accuracy of polluted images) and classification accuracy of benign images, after applying defense methods. To overcome these limitations, we propose a JPEG-based defensive compression framework, namely \"feature distillation\", to effectively rectify adversarial examples without impacting classification accuracy on benign data. Our framework significantly escalates the defense efficiency with marginal accuracy reduction using a two-step method: First, we maximize malicious features filtering of adversarial input perturbations by developing defensive quantization in frequency domain of JPEG compression or decompression, guided by a semi-analytical method; Second, we suppress the distortions of benign features to restore classification accuracy through a DNN-oriented quantization refine process. Our experimental results show that proposed \"feature distillation\" can significantly surpass the latest input-transformation based mitigations such as Quilting and TV Minimization in three aspects, including defense efficiency (improve classification accuracy from $\\sim20\\%$ to $\\sim90\\%$ on adversarial examples), accuracy of benign images after defense ($\\le1\\%$ accuracy degradation), and processing time per image ($\\sim259\\times$ Speedup). Moreover, our solution can also provide the best defense efficiency ($\\sim60\\%$ accuracy) against the recent adaptive attack with least accuracy reduction ($\\sim1\\%$) on benign images when compared with other input-transformation based defense methods.",
                        "Citation Paper Authors": "Authors:Zihao Liu, Qi Liu, Tao Liu, Nuo Xu, Xue Lin, Yanzhi Wang, Wujie Wen"
                    }
                },
                {
                    "Sentence ID": 178,
                    "Sentence": "proposed Defensive Dis-\ntillation, which could successfully mitigate AEs con-\nstructed by FGSM and JSMA. Papernot et al. ",
                    "Citation Text": "N. Papernot, P . D. McDaniel, X. Wu, S. Jha, and A. Swami.\nDistillation as a defense to adversarial perturbations aga inst deep\nneural networks. In IEEE Symposium on Security and Privacy, SP\n2016, San Jose, CA, USA, May 22-26, 2016 , pages 582\u2013597, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.04508",
                        "Citation Paper Title": "Title:Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content filters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95% to less than 0.5% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 10^30. We also find that distillation increases the average minimum number of features that need to be modified to create adversarial samples by about 800% on one of the DNNs we tested.",
                        "Citation Paper Authors": "Authors:Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, Ananthram Swami"
                    }
                },
                {
                    "Sentence ID": 149,
                    "Sentence": "trained a separate\nlightweight distribution classi\ufb01er to recognize differen t\nfeatures of transformed images.\n\u2022Gradient regularization/masking . This method hides gradi-\nents or reduces the sensitivity of models. Madry et al. ",
                    "Citation Text": "A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vlad u.\nTowards deep learning models resistant to adversarial atta cks.\nCoRR , abs/1706.06083, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06083",
                        "Citation Paper Title": "Title:Towards Deep Learning Models Resistant to Adversarial Attacks",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL.",
                        "Citation Paper Authors": "Authors:Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu"
                    }
                },
                {
                    "Sentence ID": 115,
                    "Sentence": "used thermometer code\nand one-hot code discretization to increase the robustness\nof network to AEs. Kou et al. ",
                    "Citation Text": "C. Kou, H. K. Lee, E.-C. Chang, and T. K. Ng. Enhancing\ntransformation-based defenses against adversarial attac ks with\na distribution classi\ufb01er. In International Conference on Learning\nRepresentations , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.00258",
                        "Citation Paper Title": "Title:Enhancing Transformation-based Defenses using a Distribution Classifier",
                        "Citation Paper Abstract": "Abstract:Adversarial attacks on convolutional neural networks (CNN) have gained significant attention and there have been active research efforts on defense mechanisms. Stochastic input transformation methods have been proposed, where the idea is to recover the image from adversarial attack by random transformation, and to take the majority vote as consensus among the random samples. However, the transformation improves the accuracy on adversarial images at the expense of the accuracy on clean images. While it is intuitive that the accuracy on clean images would deteriorate, the exact mechanism in which how this occurs is unclear. In this paper, we study the distribution of softmax induced by stochastic transformations. We observe that with random transformations on the clean images, although the mass of the softmax distribution could shift to the wrong class, the resulting distribution of softmax could be used to correct the prediction. Furthermore, on the adversarial counterparts, with the image transformation, the resulting shapes of the distribution of softmax are similar to the distributions from the clean images. With these observations, we propose a method to improve existing transformation-based defenses. We train a separate lightweight distribution classifier to recognize distinct features in the distributions of softmax outputs of transformed images. Our empirical studies show that our distribution classifier, by training on distributions obtained from clean images only, outperforms majority voting for both clean and adversarial images. Our method is generic and can be integrated with existing transformation-based defenses.",
                        "Citation Paper Authors": "Authors:Connie Kou, Hwee Kuan Lee, Ee-Chien Chang, Teck Khim Ng"
                    }
                },
                {
                    "Sentence ID": 231,
                    "Sentence": "considered that AEs are more sensitive to\ncertain image transformation operations, such as rotation\nand shifting, than normal images. Wang et al. ",
                    "Citation Text": "J. Wang, J. Sun, P . Zhang, and X. Wang. Detecting adversa rial\nsamples for deep neural networks through mutation testing.\nCoRR , abs/1805.05010, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.05793",
                        "Citation Paper Title": "Title:Adversarial Sample Detection for Deep Neural Network through Model Mutation Testing",
                        "Citation Paper Abstract": "Abstract:Deep neural networks (DNN) have been shown to be useful in a wide range of applications. However, they are also known to be vulnerable to adversarial samples. By transforming a normal sample with some carefully crafted human imperceptible perturbations, even highly accurate DNN make wrong decisions. Multiple defense mechanisms have been proposed which aim to hinder the generation of such adversarial samples. However, a recent work show that most of them are ineffective. In this work, we propose an alternative approach to detect adversarial samples at runtime. Our main observation is that adversarial samples are much more sensitive than normal samples if we impose random mutations on the DNN. We thus first propose a measure of `sensitivity' and show empirically that normal samples and adversarial samples have distinguishable sensitivity. We then integrate statistical hypothesis testing and model mutation testing to check whether an input sample is likely to be normal or adversarial at runtime by measuring its sensitivity. We evaluated our approach on the MNIST and CIFAR10 datasets. The results show that our approach detects adversarial samples generated by state-of-the-art attacking methods efficiently and accurately.",
                        "Citation Paper Authors": "Authors:Jingyi Wang, Guoliang Dong, Jun Sun, Xinyu Wang, Peixin Zhang"
                    }
                },
                {
                    "Sentence ID": 247,
                    "Sentence": "explored model-agnostic defenses on image-classi\ufb01cation systems by im-\nage transformations. Xie et al. ",
                    "Citation Text": "C. Xie, J. Wang, Z. Zhang, Z. Ren, and A. L. Yuille. Mitig ating ad-\nversarial effects through randomization. CoRR , abs/1711.01991,\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.01991",
                        "Citation Paper Title": "Title:Mitigating Adversarial Effects Through Randomization",
                        "Citation Paper Abstract": "Abstract:Convolutional neural networks have demonstrated high accuracy on various tasks in recent years. However, they are extremely vulnerable to adversarial examples. For example, imperceptible perturbations added to clean images can cause convolutional neural networks to fail. In this paper, we propose to utilize randomization at inference time to mitigate adversarial effects. Specifically, we use two randomization operations: random resizing, which resizes the input images to a random size, and random padding, which pads zeros around the input images in a random manner. Extensive experiments demonstrate that the proposed randomization method is very effective at defending against both single-step and iterative attacks. Our method provides the following advantages: 1) no additional training or fine-tuning, 2) very few additional computations, 3) compatible with other adversarial defense methods. By combining the proposed randomization method with an adversarially trained model, it achieves a normalized score of 0.924 (ranked No.2 among 107 defense teams) in the NIPS 2017 adversarial examples defense challenge, which is far better than using adversarial training alone with a normalized score of 0.773 (ranked No.56). The code is public available at this https URL.",
                        "Citation Paper Authors": "Authors:Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, Alan Yuille"
                    }
                },
                {
                    "Sentence ID": 78,
                    "Sentence": "found that AEs\nmainly lay in the low probability regions of the training\nregions. So they puri\ufb01ed an AE by moving it back towards\nthe distribution adaptively. Guo et al. ",
                    "Citation Text": "C. Guo, M. Rana, M. Ciss\u00b4 e, and L. van der Maaten. Coun-\ntering adversarial images using input transformations. CoRR ,\nabs/1711.00117, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.00117",
                        "Citation Paper Title": "Title:Countering Adversarial Images using Input Transformations",
                        "Citation Paper Abstract": "Abstract:This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods",
                        "Citation Paper Authors": "Authors:Chuan Guo, Mayank Rana, Moustapha Cisse, Laurens van der Maaten"
                    }
                },
                {
                    "Sentence ID": 208,
                    "Sentence": "added a large and diverse\nclass of background images into datasets.\n\u2022Transformation . Transforming inputs can defend adversar-\nial attack to a large extent. Song et al. ",
                    "Citation Text": "Y. Song, T. Kim, S. Nowozin, S. Ermon, and N. Kushman. Pixel de-\nfend: Leveraging generative models to understand and defen d\nagainst adversarial examples. CoRR , abs/1710.10766, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.10766",
                        "Citation Paper Title": "Title:PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:Adversarial perturbations of normal images are usually imperceptible to humans, but they can seriously confuse state-of-the-art machine learning models. What makes them so special in the eyes of image classifiers? In this paper, we show empirically that adversarial examples mainly lie in the low probability regions of the training distribution, regardless of attack types and targeted models. Using statistical hypothesis testing, we find that modern neural density models are surprisingly good at detecting imperceptible image perturbations. Based on this discovery, we devised PixelDefend, a new approach that purifies a maliciously perturbed image by moving it back towards the distribution seen in the training data. The purified image is then run through an unmodified classifier, making our method agnostic to both the classifier and the attacking method. As a result, PixelDefend can be used to protect already deployed models and be combined with other model-specific defenses. Experiments show that our method greatly improves resilience across a wide variety of state-of-the-art attacking methods, increasing accuracy on the strongest attack from 63% to 84% for Fashion MNIST and from 32% to 70% for CIFAR-10.",
                        "Citation Paper Authors": "Authors:Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, Nate Kushman"
                    }
                },
                {
                    "Sentence ID": 171,
                    "Sentence": "develop DNNs using region-based classi\ufb01cation instead\nof point-based. They predicted labels through randomly\nselecting several points from the hypercube centered at\nthe testing sample. In ",
                    "Citation Text": "T. Pang, C. Du, and J. Zhu. Robust deep learning via re-\nverse cross-entropy training and thresholding test. CoRR ,\nabs/1706.00633, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.00633",
                        "Citation Paper Title": "Title:Towards Robust Detection of Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:Although the recent progress is substantial, deep learning methods can be vulnerable to the maliciously generated adversarial examples. In this paper, we present a novel training procedure and a thresholding test strategy, towards robust detection of adversarial examples. In training, we propose to minimize the reverse cross-entropy (RCE), which encourages a deep network to learn latent representations that better distinguish adversarial examples from normal ones. In testing, we propose to use a thresholding strategy as the detector to filter out adversarial examples for reliable predictions. Our method is simple to implement using standard algorithms, with little extra training cost compared to the common cross-entropy minimization. We apply our method to defend various attacking methods on the widely used MNIST and CIFAR-10 datasets, and achieve significant improvements on robust predictions under all the threat models in the adversarial setting.",
                        "Citation Paper Authors": "Authors:Tianyu Pang, Chao Du, Yinpeng Dong, Jun Zhu"
                    }
                },
                {
                    "Sentence ID": 238,
                    "Sentence": "generate adversarial images for training by feature scat-\ntering in the latent space. Wong et al. ",
                    "Citation Text": "E. Wong, L. Rice, and J. Z. Kolter. Fast is better than fr ee:\nRevisiting adversarial training. In International Conference on\nLearning Representations , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2001.03994",
                        "Citation Paper Title": "Title:Fast is better than free: Revisiting adversarial training",
                        "Citation Paper Abstract": "Abstract:Adversarial training, a method for learning robust deep networks, is typically assumed to be more expensive than traditional training due to the necessity of constructing adversarial examples via a first-order method like projected gradient decent (PGD). In this paper, we make the surprising discovery that it is possible to train empirically robust models using a much weaker and cheaper adversary, an approach that was previously believed to be ineffective, rendering the method no more costly than standard training in practice. Specifically, we show that adversarial training with the fast gradient sign method (FGSM), when combined with random initialization, is as effective as PGD-based training but has significantly lower cost. Furthermore we show that FGSM adversarial training can be further accelerated by using standard techniques for efficient training of deep networks, allowing us to learn a robust CIFAR10 classifier with 45% robust accuracy to PGD attacks with $\\epsilon=8/255$ in 6 minutes, and a robust ImageNet classifier with 43% robust accuracy at $\\epsilon=2/255$ in 12 hours, in comparison to past work based on \"free\" adversarial training which took 10 and 50 hours to reach the same respective thresholds. Finally, we identify a failure mode referred to as \"catastrophic overfitting\" which may have caused previous attempts to use FGSM adversarial training to fail. All code for reproducing the experiments in this paper as well as pretrained model weights are at this https URL.",
                        "Citation Paper Authors": "Authors:Eric Wong, Leslie Rice, J. Zico Kolter"
                    }
                },
                {
                    "Sentence ID": 259,
                    "Sentence": "introduce adversarial noise to the output em-\nbedding layer while training neural language models.\nYeet al. ",
                    "Citation Text": "S. Ye, X. Lin, K. Xu, S. Liu, H. Cheng, J. Lambrechts, H. Zha ng,\nA. Zhou, K. Ma, and Y. Wang. Adversarial robustness vs. model\ncompression, or both? In 2019 IEEE/CVF International Conference\non Computer Vision, ICCV 2019, Seoul, Korea (South), Octobe r 27 -\nNovember 2, 2019 , pages 111\u2013120.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.12561",
                        "Citation Paper Title": "Title:Adversarial Robustness vs Model Compression, or Both?",
                        "Citation Paper Abstract": "Abstract:It is well known that deep neural networks (DNNs) are vulnerable to adversarial attacks, which are implemented by adding crafted perturbations onto benign examples. Min-max robust optimization based adversarial training can provide a notion of security against adversarial attacks. However, adversarial robustness requires a significantly larger capacity of the network than that for the natural training with only benign examples. This paper proposes a framework of concurrent adversarial training and weight pruning that enables model compression while still preserving the adversarial robustness and essentially tackles the dilemma of adversarial training. Furthermore, this work studies two hypotheses about weight pruning in the conventional setting and finds that weight pruning is essential for reducing the network model size in the adversarial setting, training a small model from scratch even with inherited initialization from the large model cannot achieve both adversarial robustness and high standard accuracy. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Shaokai Ye, Kaidi Xu, Sijia Liu, Jan-Henrik Lambrechts, Huan Zhang, Aojun Zhou, Kaisheng Ma, Yanzhi Wang, Xue Lin"
                    }
                },
                {
                    "Sentence ID": 93,
                    "Sentence": "utilizes the adversarial space to solve the min -\nmax game between attackers and defenders.\n\u2022Adversarial training . This method selects AEs as part of\nthe training dataset to make trained model learn charac-\nteristics of AEs ",
                    "Citation Text": "R. Huang, B. Xu, D. Schuurmans, and C. Szepesv\u00b4 ari. Learni ng\nwith a strong adversary. CoRR , abs/1511.03034, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.03034",
                        "Citation Paper Title": "Title:Learning with a Strong Adversary",
                        "Citation Paper Abstract": "Abstract:The robustness of neural networks to intended perturbations has recently attracted significant attention. In this paper, we propose a new method, \\emph{learning with a strong adversary}, that learns robust classifiers from supervised data. The proposed method takes finding adversarial examples as an intermediate step. A new and simple way of finding adversarial examples is presented and experimentally shown to be efficient. Experimental results demonstrate that resulting learning method greatly improves the robustness of the classification models produced.",
                        "Citation Paper Authors": "Authors:Ruitong Huang, Bing Xu, Dale Schuurmans, Csaba Szepesvari"
                    }
                },
                {
                    "Sentence ID": 237,
                    "Sentence": "proves that adversarial vulnerabili ty\nof networks increases as gradients, and gradients grow\nas the input image dimension. PROVEN ",
                    "Citation Text": "L. Weng, P . Chen, L. M. Nguyen, M. S. Squillante, A. Boopat hy,\nI. V . Oseledets, and L. Daniel. PROVEN: verifying robustnes s of\nneural networks with a probabilistic approach. In Proceedings of\nthe 36th International Conference on Machine Learning, ICM L 2019,\n9-15 June 2019, Long Beach, California, USA , pages 6727\u20136736.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.08329",
                        "Citation Paper Title": "Title:PROVEN: Certifying Robustness of Neural Networks with a Probabilistic Approach",
                        "Citation Paper Abstract": "Abstract:With deep neural networks providing state-of-the-art machine learning models for numerous machine learning tasks, quantifying the robustness of these models has become an important area of research. However, most of the research literature merely focuses on the \\textit{worst-case} setting where the input of the neural network is perturbed with noises that are constrained within an $\\ell_p$ ball; and several algorithms have been proposed to compute certified lower bounds of minimum adversarial distortion based on such worst-case analysis. In this paper, we address these limitations and extend the approach to a \\textit{probabilistic} setting where the additive noises can follow a given distributional characterization. We propose a novel probabilistic framework PROVEN to PRObabilistically VErify Neural networks with statistical guarantees -- i.e., PROVEN certifies the probability that the classifier's top-1 prediction cannot be altered under any constrained $\\ell_p$ norm perturbation to a given input. Importantly, we show that it is possible to derive closed-form probabilistic certificates based on current state-of-the-art neural network robustness verification frameworks. Hence, the probabilistic certificates provided by PROVEN come naturally and with almost no overhead when obtaining the worst-case certified lower bounds from existing methods such as Fast-Lin, CROWN and CNN-Cert. Experiments on small and large MNIST and CIFAR neural network models demonstrate our probabilistic approach can achieve up to around $75\\%$ improvement in the robustness certification with at least a $99.99\\%$ confidence compared with the worst-case robustness certificate delivered by CROWN.",
                        "Citation Paper Authors": "Authors:Tsui-Wei Weng, Pin-Yu Chen, Lam M. Nguyen, Mark S. Squillante, Ivan Oseledets, Luca Daniel"
                    }
                },
                {
                    "Sentence ID": 204,
                    "Sentence": "aim to seek certi\ufb01ed\nadversary-free regions around data points as large as pos-\nsible. Research ",
                    "Citation Text": "C. Simon-Gabriel, Y. Ollivier, L. Bottou, B. Sch\u00a8 olkopf , and\nD. Lopez-Paz. First-order adversarial vulnerability of ne ural net-works and input dimension. In Proceedings of the 36th International\nConference on Machine Learning, ICML 2019, 9-15 June 2019, L ong\nBeach, California, USA , pages 5809\u20135817.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.01421",
                        "Citation Paper Title": "Title:First-order Adversarial Vulnerability of Neural Networks and Input Dimension",
                        "Citation Paper Abstract": "Abstract:Over the past few years, neural networks were proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. Surprisingly, vulnerability does not depend on network topology: for many standard network architectures, we prove that at initialization, the $\\ell_1$-norm of these gradients grows as the square root of the input dimension, leaving the networks increasingly vulnerable with growing image size. We empirically show that this dimension dependence persists after either usual or robust training, but gets attenuated with higher regularization.",
                        "Citation Paper Authors": "Authors:Carl-Johann Simon-Gabriel, Yann Ollivier, L\u00e9on Bottou, Bernhard Sch\u00f6lkopf, David Lopez-Paz"
                    }
                },
                {
                    "Sentence ID": 185,
                    "Sentence": "can eliminate outliers in the uploaded\ngradient and obtain a gradient close to the true gradient in\ndistributed learning. Qiao et al. ",
                    "Citation Text": "X. Qiao, Y. Yang, and H. Li. Defending neural backdoors\nvia generative distribution modeling. In Advances in Neural\nInformation Processing Systems 32: Annual Conference on Ne ural\nInformation Processing Systems 2019, NeurIPS 2019, 8-14 De cember\n2019, Vancouver, BC, Canada , pages 14004\u201314013.IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, OCTORBER 2020 26",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.04749",
                        "Citation Paper Title": "Title:Defending Neural Backdoors via Generative Distribution Modeling",
                        "Citation Paper Abstract": "Abstract:Neural backdoor attack is emerging as a severe security threat to deep learning, while the capability of existing defense methods is limited, especially for complex backdoor triggers. In the work, we explore the space formed by the pixel values of all possible backdoor triggers. An original trigger used by an attacker to build the backdoored model represents only a point in the space. It then will be generalized into a distribution of valid triggers, all of which can influence the backdoored model. Thus, previous methods that model only one point of the trigger distribution is not sufficient. Getting the entire trigger distribution, e.g., via generative modeling, is a key to effective defense. However, existing generative modeling techniques for image generation are not applicable to the backdoor scenario as the trigger distribution is completely unknown. In this work, we propose max-entropy staircase approximator (MESA), an algorithm for high-dimensional sampling-free generative modeling and use it to recover the trigger distribution. We also develop a defense technique to remove the triggers from the backdoored model. Our experiments on Cifar10/100 dataset demonstrate the effectiveness of MESA in modeling the trigger distribution and the robustness of the proposed defense method.",
                        "Citation Paper Authors": "Authors:Ximing Qiao, Yukun Yang, Hai Li"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "described the context of creation or modi\ufb01cation\nof data points to enhance trustworthiness and depend-\nability of the data. Chakarov et al. ",
                    "Citation Text": "A. Chakarov, A. V . Nori, S. K. Rajamani, S. Sen, and D. Vi-\njaykeerthy. Debugging machine learning tasks. CoRR ,\nabs/1603.07292, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.07292",
                        "Citation Paper Title": "Title:Debugging Machine Learning Tasks",
                        "Citation Paper Abstract": "Abstract:Unlike traditional programs (such as operating systems or word processors) which have large amounts of code, machine learning tasks use programs with relatively small amounts of code (written in machine learning libraries), but voluminous amounts of data. Just like developers of traditional programs debug errors in their code, developers of machine learning tasks debug and fix errors in their data. However, algorithms and tools for debugging and fixing errors in data are less common, when compared to their counterparts for detecting and fixing errors in code. In this paper, we consider classification tasks where errors in training data lead to misclassifications in test points, and propose an automated method to find the root causes of such misclassifications. Our root cause analysis is based on Pearl's theory of causation, and uses Pearl's PS (Probability of Sufficiency) as a scoring metric. Our implementation, Psi, encodes the computation of PS as a probabilistic program, and uses recent work on probabilistic programs and transformations on probabilistic programs (along with gray-box models of machine learning algorithms) to efficiently compute PS. Psi is able to identify root causes of data errors in interesting data sets.",
                        "Citation Paper Authors": "Authors:Aleksandar Chakarov, Aditya Nori, Sriram Rajamani, Shayak Sen, Deepak Vijaykeerthy"
                    }
                },
                {
                    "Sentence ID": 104,
                    "Sentence": "broke down large, complex deep models to enable scal-\nable and privacy-preserving analytics by removing sensi-\ntive information with a feature extractor. MemGuard ",
                    "Citation Text": "J. Jia, A. Salem, M. Backes, Y. Zhang, and N. Z. Gong. Memg uard:\nDefending against black-box membership inference attacks via\nadversarial examples. In Proceedings of the 2019 ACM SIGSAC\nConference on Computer and Communications Security, CCS 20 19,\nLondon, UK, November 11-15, 2019 , pages 259\u2013274.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.10594",
                        "Citation Paper Title": "Title:MemGuard: Defending against Black-Box Membership Inference Attacks via Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:In a membership inference attack, an attacker aims to infer whether a data sample is in a target classifier's training dataset or not. Specifically, given a black-box access to the target classifier, the attacker trains a binary classifier, which takes a data sample's confidence score vector predicted by the target classifier as an input and predicts the data sample to be a member or non-member of the target classifier's training dataset. Membership inference attacks pose severe privacy and security threats to the training dataset. Most existing defenses leverage differential privacy when training the target classifier or regularize the training process of the target classifier. These defenses suffer from two key limitations: 1) they do not have formal utility-loss guarantees of the confidence score vectors, and 2) they achieve suboptimal privacy-utility tradeoffs.\nIn this work, we propose MemGuard, the first defense with formal utility-loss guarantees against black-box membership inference attacks. Instead of tampering the training process of the target classifier, MemGuard adds noise to each confidence score vector predicted by the target classifier. Our key observation is that attacker uses a classifier to predict member or non-member and classifier is vulnerable to adversarial examples. Based on the observation, we propose to add a carefully crafted noise vector to a confidence score vector to turn it into an adversarial example that misleads the attacker's classifier. Our experimental results on three datasets show that MemGuard can effectively defend against membership inference attacks and achieve better privacy-utility tradeoffs than existing defenses. Our work is the first one to show that adversarial examples can be used as defensive mechanisms to defend against membership inference attacks.",
                        "Citation Paper Authors": "Authors:Jinyuan Jia, Ahmed Salem, Michael Backes, Yang Zhang, Neil Zhenqiang Gong"
                    }
                },
                {
                    "Sentence ID": 215,
                    "Sentence": "is a cooperative learning system that allows\nmultiple parties to train a linear model without revealing\ndata. DCOP ",
                    "Citation Text": "T. Tassa, T. Grinshpoun, and A. Yanai. A privacy preser ving\ncollusion secure DCOP algorithm. In Proceedings of the Twenty-\nEighth International Joint Conference on Arti\ufb01cial Intell igence, IJCAI\n2019, Macao, China, August 10-16, 2019 , pages 4774\u20134780.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.09013",
                        "Citation Paper Title": "Title:A Privacy Preserving Collusion Secure DCOP Algorithm",
                        "Citation Paper Abstract": "Abstract:In recent years, several studies proposed privacy-preserving algorithms for solving Distributed Constraint Optimization Problems (DCOPs). All of those studies assumed that agents do not collude. In this study we propose the first privacy-preserving DCOP algorithm that is immune to coalitions, under the assumption of honest majority. Our algorithm -- PC-SyncBB -- is based on the classical Branch and Bound DCOP algorithm. It offers constraint, topology and decision privacy. We evaluate its performance on different benchmarks, problem sizes, and constraint densities. We show that achieving security against coalitions is feasible. As all existing privacy-preserving DCOP algorithms base their security on assuming solitary conduct of the agents, we view this study as an essential first step towards lifting this potentially harmful assumption in all those algorithms.",
                        "Citation Paper Authors": "Authors:Tamir Tassa, Tal Grinshpoun, Avishay Yanai"
                    }
                },
                {
                    "Sentence ID": 275,
                    "Sentence": ". As such,\nthe training data cannot be easily inferred by attackers\nresiding at either computing servers or the client side.\nHelen ",
                    "Citation Text": "W. Zheng, R. A. Popa, J. E. Gonzalez, and I. Stoica. Helen :\nMaliciously secure coopetitive learning for linear models . In2019\nIEEE Symposium on Security and Privacy, SP 2019, San Francis co,\nCA, USA, May 19-23, 2019 , pages 724\u2013738.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.07212",
                        "Citation Paper Title": "Title:Helen: Maliciously Secure Coopetitive Learning for Linear Models",
                        "Citation Paper Abstract": "Abstract:Many organizations wish to collaboratively train machine learning models on their combined datasets for a common benefit (e.g., better medical research, or fraud detection). However, they often cannot share their plaintext datasets due to privacy concerns and/or business competition. In this paper, we design and build Helen, a system that allows multiple parties to train a linear model without revealing their data, a setting we call coopetitive learning. Compared to prior secure training systems, Helen protects against a much stronger adversary who is malicious and can compromise m-1 out of m parties. Our evaluation shows that Helen can achieve up to five orders of magnitude of performance improvement when compared to training using an existing state-of-the-art secure multi-party computation framework.",
                        "Citation Paper Authors": "Authors:Wenting Zheng, Raluca Ada Popa, Joseph E. Gonzalez, Ion Stoica"
                    }
                },
                {
                    "Sentence ID": 180,
                    "Sentence": "and proved to be effective in privacy\npreservation in database. In model privacy preserving, DP\nstrategy can be applied to model parameters ",
                    "Citation Text": "N. Phan, M. N. Vu, Y. Liu, R. Jin, D. Dou, X. Wu, and M. T.\nThai. Heterogeneous gaussian mechanism: Preserving diffe r-\nential privacy in deep learning with provable robustness. I n\nProceedings of the Twenty-Eighth International Joint Conf erence on\nArti\ufb01cial Intelligence, IJCAI 2019, Macao, China, August 1 0-16, 2019 ,\npages 4753\u20134759.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.01444",
                        "Citation Paper Title": "Title:Heterogeneous Gaussian Mechanism: Preserving Differential Privacy in Deep Learning with Provable Robustness",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose a novel Heterogeneous Gaussian Mechanism (HGM) to preserve differential privacy in deep neural networks, with provable robustness against adversarial examples. We first relax the constraint of the privacy budget in the traditional Gaussian Mechanism from (0, 1] to (0, \\infty), with a new bound of the noise scale to preserve differential privacy. The noise in our mechanism can be arbitrarily redistributed, offering a distinctive ability to address the trade-off between model utility and privacy loss. To derive provable robustness, our HGM is applied to inject Gaussian noise into the first hidden layer. Then, a tighter robustness bound is proposed. Theoretical analysis and thorough evaluations show that our mechanism notably improves the robustness of differentially private deep neural networks, compared with baseline approaches, under a variety of model attacks.",
                        "Citation Paper Authors": "Authors:NhatHai Phan, Minh Vu, Yang Liu, Ruoming Jin, Dejing Dou, Xintao Wu, My T. Thai"
                    }
                },
                {
                    "Sentence ID": 168,
                    "Sentence": "detected attack based on sudden changes in\nthe distribution of samples submitted by a given customer.\nOrekondy et al. ",
                    "Citation Text": "T. Orekondy, B. Schiele, and M. Fritz. Prediction poiso ning:\nTowards defenses against dnn model stealing attacks. In Inter-\nnational Conference on Learning Representations , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.10908",
                        "Citation Paper Title": "Title:Prediction Poisoning: Towards Defenses Against DNN Model Stealing Attacks",
                        "Citation Paper Abstract": "Abstract:High-performance Deep Neural Networks (DNNs) are increasingly deployed in many real-world applications e.g., cloud prediction APIs. Recent advances in model functionality stealing attacks via black-box access (i.e., inputs in, predictions out) threaten the business model of such applications, which require a lot of time, money, and effort to develop. Existing defenses take a passive role against stealing attacks, such as by truncating predicted information. We find such passive defenses ineffective against DNN stealing attacks. In this paper, we propose the first defense which actively perturbs predictions targeted at poisoning the training objective of the attacker. We find our defense effective across a wide range of challenging datasets and DNN model stealing attacks, and additionally outperforms existing defenses. Our defense is the first that can withstand highly accurate model stealing attacks for tens of thousands of queries, amplifying the attacker's error rate up to a factor of 85$\\times$ with minimal impact on the utility for benign users.",
                        "Citation Paper Authors": "Authors:Tribhuvanesh Orekondy, Bernt Schiele, Mario Fritz"
                    }
                },
                {
                    "Sentence ID": 111,
                    "Sentence": ". On the other hand, detecting and\nprevent abnormal queries can also resolve this attack. Ke-\nsarwani et al. ",
                    "Citation Text": "M. Kesarwani, B. Mukhoty, V . Arya, and S. Mehta. Model\nextraction warning in mlaas paradigm. CoRR , abs/1711.07221,\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.07221",
                        "Citation Paper Title": "Title:Model Extraction Warning in MLaaS Paradigm",
                        "Citation Paper Abstract": "Abstract:Cloud vendors are increasingly offering machine learning services as part of their platform and services portfolios. These services enable the deployment of machine learning models on the cloud that are offered on a pay-per-query basis to application developers and end users. However recent work has shown that the hosted models are susceptible to extraction attacks. Adversaries may launch queries to steal the model and compromise future query payments or privacy of the training data. In this work, we present a cloud-based extraction monitor that can quantify the extraction status of models by observing the query and response streams of both individual and colluding adversarial users. We present a novel technique that uses information gain to measure the model learning rate by users with increasing number of queries. Additionally, we present an alternate technique that maintains intelligent query summaries to measure the learning rate relative to the coverage of the input feature space in the presence of collusion. Both these approaches have low computational overhead and can easily be offered as services to model owners to warn them of possible extraction attacks from adversaries. We present performance results for these approaches for decision tree models deployed on BigML MLaaS platform, using open source datasets and different adversarial attack strategies.",
                        "Citation Paper Authors": "Authors:Manish Kesarwani, Bhaskar Mukhoty, Vijay Arya, Sameep Mehta"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "also proves that neural network-based classi\ufb01e rs\nare vulnerable to rotations and translations. In a recent pa -\nper, Bhattad et al. ",
                    "Citation Text": "A. Bhattad, M. J. Chong, K. Liang, B. Li, and D. A. Forsyth .\nUnrestricted adversarial examples via semantic manipulat ion. In\nInternational Conference on Learning Representations , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.06347",
                        "Citation Paper Title": "Title:Unrestricted Adversarial Examples via Semantic Manipulation",
                        "Citation Paper Abstract": "Abstract:Machine learning models, especially deep neural networks (DNNs), have been shown to be vulnerable against adversarial examples which are carefully crafted samples with a small magnitude of the perturbation. Such adversarial perturbations are usually restricted by bounding their $\\mathcal{L}_p$ norm such that they are imperceptible, and thus many current defenses can exploit this property to reduce their adversarial impact. In this paper, we instead introduce \"unrestricted\" perturbations that manipulate semantically meaningful image-based visual descriptors - color and texture - in order to generate effective and photorealistic adversarial examples. We show that these semantically aware perturbations are effective against JPEG compression, feature squeezing and adversarially trained model. We also show that the proposed methods can effectively be applied to both image classification and image captioning tasks on complex datasets such as ImageNet and MSCOCO. In addition, we conduct comprehensive user studies to show that our generated semantic adversarial examples are photorealistic to humans despite large magnitude perturbations when compared to other attacks.",
                        "Citation Paper Authors": "Authors:Anand Bhattad, Min Jin Chong, Kaizhao Liang, Bo Li, D. A. Forsyth"
                    }
                },
                {
                    "Sentence ID": 62,
                    "Sentence": ". That is,\nperturbations with large Lpvalues may also look similar\nto humans, such as translations and rotations of images,\nand small Lpperturbations do not mean imperceptible. Re-\nsearch ",
                    "Citation Text": "L. Engstrom, B. Tran, D. Tsipras, L. Schmidt, and A. Madry .\nExploring the landscape of spatial robustness. In Proceedings of\nthe 36th International Conference on Machine Learning, ICM L 2019,\n9-15 June 2019, Long Beach, California, USA , pages 1802\u20131811.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.02779",
                        "Citation Paper Title": "Title:Exploring the Landscape of Spatial Robustness",
                        "Citation Paper Abstract": "Abstract:The study of adversarial robustness has so far largely focused on perturbations bound in p-norms. However, state-of-the-art models turn out to be also vulnerable to other, more natural classes of perturbations such as translations and rotations. In this work, we thoroughly investigate the vulnerability of neural network--based classifiers to rotations and translations. While data augmentation offers relatively small robustness, we use ideas from robust optimization and test-time input aggregation to significantly improve robustness. Finally we find that, in contrast to the p-norm case, first-order methods cannot reliably find worst-case perturbations. This highlights spatial robustness as a fundamentally different setting requiring additional study. Code available at this https URL and this https URL.",
                        "Citation Paper Authors": "Authors:Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, Aleksander Madry"
                    }
                },
                {
                    "Sentence ID": 114,
                    "Sentence": "demonstrate that adversarial attack\npolicies are also effective in reinforcement learning. Kos et\nal. ",
                    "Citation Text": "J. Kos, I. Fischer, and D. Song. Adversarial examples fo r genera-\ntive models. In IEEE Security and Privacy Workshops, SP Workshops,\nSan Francisco, CA, USA, May 24, 2018 , pages 36\u201342.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.06832",
                        "Citation Paper Title": "Title:Adversarial examples for generative models",
                        "Citation Paper Abstract": "Abstract:We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present three classes of attacks on the VAE and VAE-GAN architectures and demonstrate them against networks trained on MNIST, SVHN and CelebA. Our first attack leverages classification-based adversaries by attaching a classifier to the trained encoder of the target generative model, which can then be used to indirectly manipulate the latent representation. Our second attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our third attack moves beyond relying on classification or the standard loss for the gradient and directly optimizes against differences in source and target latent representations. We also motivate why an attacker might be interested in deploying such techniques against a target generative network.",
                        "Citation Paper Authors": "Authors:Jernej Kos, Ian Fischer, Dawn Song"
                    }
                },
                {
                    "Sentence ID": 94,
                    "Sentence": "prove that tree-\nbased models are also vulnerable to AEs. Huang et al. ",
                    "Citation Text": "S. H. Huang, N. Papernot, I. J. Goodfellow, Y. Duan, and\nP . Abbeel. Adversarial attacks on neural network policies. CoRR ,\nabs/1702.02284, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.02284",
                        "Citation Paper Title": "Title:Adversarial Attacks on Neural Network Policies",
                        "Citation Paper Abstract": "Abstract:Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at this http URL.",
                        "Citation Paper Authors": "Authors:Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, Pieter Abbeel"
                    }
                },
                {
                    "Sentence ID": 116,
                    "Sentence": "DNN L1 DNN Yes Black No No malware\nKreuk et al. ",
                    "Citation Text": "F. Kreuk, A. Barak, S. Aviv-Reuven, M. Baruch, B. Pinkas , and\nJ. Keshet. Deceiving end-to-end deep learning malware dete ctors\nusing adversarial examples. 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.04528",
                        "Citation Paper Title": "Title:Deceiving End-to-End Deep Learning Malware Detectors using Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:In recent years, deep learning has shown performance breakthroughs in many applications, such as image detection, image segmentation, pose estimation, and speech recognition. However, this comes with a major concern: deep networks have been found to be vulnerable to adversarial examples. Adversarial examples are slightly modified inputs that are intentionally designed to cause a misclassification by the model. In the domains of images and speech, the modifications are so small that they are not seen or heard by humans, but nevertheless greatly affect the classification of the model.\nDeep learning models have been successfully applied to malware detection. In this domain, generating adversarial examples is not straightforward, as small modifications to the bytes of the file could lead to significant changes in its functionality and validity. We introduce a novel loss function for generating adversarial examples specifically tailored for discrete input sets, such as executable bytes. We modify malicious binaries so that they would be detected as benign, while preserving their original functionality, by injecting a small sequence of bytes (payload) in the binary file. We applied this approach to an end-to-end convolutional deep learning malware detection model and show a high rate of detection evasion. Moreover, we show that our generated payload is robust enough to be transferable within different locations of the same file and across different files, and that its entropy is low and similar to that of benign data sections.",
                        "Citation Paper Authors": "Authors:Felix Kreuk, Assi Barak, Shir Aviv-Reuven, Moran Baruch, Benny Pinkas, Joseph Keshet"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "proposed meth-\nods to generate adversarial malware examples in black-\nbox to attack detection models. Dujaili et al. ",
                    "Citation Text": "A. Al-Dujaili, A. Huang, E. Hemberg, and U. O\u2019Reilly. Ad versar-\nial deep learning for robust detection of binary encoded mal ware.\nInIEEE Security and Privacy Workshops, San Francisco, CA, USA ,\npages 76\u201382, May 24, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.02950",
                        "Citation Paper Title": "Title:Adversarial Deep Learning for Robust Detection of Binary Encoded Malware",
                        "Citation Paper Abstract": "Abstract:Malware is constantly adapting in order to avoid detection. Model based malware detectors, such as SVM and neural networks, are vulnerable to so-called adversarial examples which are modest changes to detectable malware that allows the resulting malware to evade detection. Continuous-valued methods that are robust to adversarial examples of images have been developed using saddle-point optimization formulations. We are inspired by them to develop similar methods for the discrete, e.g. binary, domain which characterizes the features of malware. A specific extra challenge of malware is that the adversarial examples must be generated in a way that preserves their malicious functionality. We introduce methods capable of generating functionally preserved adversarial malware examples in the binary domain. Using the saddle-point formulation, we incorporate the adversarial examples into the training of models that are robust to them. We evaluate the effectiveness of the methods and others in the literature on a set of Portable Execution~(PE) files. Comparison prompts our introduction of an online measure computed during training to assess general expectation of robustness.",
                        "Citation Paper Authors": "Authors:Abdullah Al-Dujaili, Alex Huang, Erik Hemberg, Una-May O'Reilly"
                    }
                },
                {
                    "Sentence ID": 177,
                    "Sentence": "- - A3C,TRPO,DQN L1,L2,L\u221e RL No Both No No image\nPapernot et al. ",
                    "Citation Text": "N. Papernot, P . D. McDaniel, A. Swami, and R. E. Harang. C raft-\ning adversarial input sequences for recurrent neural netwo rks.\nIn2016 IEEE Military Communications Conference, MILCOM 2016 ,\nBaltimore, MD, USA, November 1-3, 2016 , pages 49\u201354, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1604.08275",
                        "Citation Paper Title": "Title:Crafting Adversarial Input Sequences for Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:Machine learning models are frequently used to solve complex security problems, as well as to make decisions in sensitive situations like guiding autonomous vehicles or predicting financial market behaviors. Previous efforts have shown that numerous machine learning models were vulnerable to adversarial manipulations of their inputs taking the form of adversarial samples. Such inputs are crafted by adding carefully selected perturbations to legitimate inputs so as to force the machine learning model to misbehave, for instance by outputting a wrong class if the machine learning task of interest is classification. In fact, to the best of our knowledge, all previous work on adversarial samples crafting for neural network considered models used to solve classification tasks, most frequently in computer vision applications. In this paper, we contribute to the field of adversarial machine learning by investigating adversarial input sequences for recurrent neural networks processing sequential data. We show that the classes of algorithms introduced previously to craft adversarial samples misclassified by feed-forward neural networks can be adapted to recurrent neural networks. In a experiment, we show that adversaries can craft adversarial sequences misleading both categorical and sequential recurrent neural networks.",
                        "Citation Paper Authors": "Authors:Nicolas Papernot, Patrick McDaniel, Ananthram Swami, Richard Harang"
                    }
                },
                {
                    "Sentence ID": 98,
                    "Sentence": "use a technique based on local\nsearch to construct the numerical approximation of network\ngradients, and then constructed perturbations in an image.\nMoreover, Ilyas et al. ",
                    "Citation Text": "A. Ilyas, L. Engstrom, A. Athalye, and J. Lin. Query-ef\ufb01 cient\nblack-box adversarial examples. CoRR , abs/1712.07113, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.08598",
                        "Citation Paper Title": "Title:Black-box Adversarial Attacks with Limited Queries and Information",
                        "Citation Paper Abstract": "Abstract:Current neural network-based classifiers are susceptible to adversarial examples even in the black-box setting, where the attacker only has query access to the model. In practice, the threat model for real-world systems is often more restrictive than the typical black-box model where the adversary can observe the full output of the network on arbitrarily many chosen inputs. We define three realistic threat models that more accurately characterize many real-world classifiers: the query-limited setting, the partial-information setting, and the label-only setting. We develop new attacks that fool classifiers under these more restrictive threat models, where previous methods would be impractical or ineffective. We demonstrate that our methods are effective against an ImageNet classifier under our proposed threat models. We also demonstrate a targeted black-box attack against a commercial classifier, overcoming the challenges of limited query access, partial information, and other practical issues to break the Google Cloud Vision API.",
                        "Citation Paper Authors": "Authors:Andrew Ilyas, Logan Engstrom, Anish Athalye, Jessy Lin"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "89.2% MNIST CNN(3Conv+1FC) L2 CNN No White Yes Yes image\nAthalye et al. ",
                    "Citation Text": "A. Athalye, L. Engstrom, A. Ilyas, and K. Kwok. Synthesiz ing\nrobust adversarial examples. In Proceedings of the 35th Interna-\ntional Conference on Machine Learning (ICML), Stockholmsm \u00a8 assan,\nStockholm, Sweden , pages 284\u2013293, July 10-15, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.07397",
                        "Citation Paper Title": "Title:Synthesizing Robust Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:Standard methods for generating adversarial examples for neural networks do not consistently fool neural network classifiers in the physical world due to a combination of viewpoint shifts, camera noise, and other natural transformations, limiting their relevance to real-world systems. We demonstrate the existence of robust 3D adversarial objects, and we present the first algorithm for synthesizing examples that are adversarial over a chosen distribution of transformations. We synthesize two-dimensional adversarial images that are robust to noise, distortion, and affine transformation. We apply our algorithm to complex three-dimensional objects, using 3D-printing to manufacture the first physical adversarial objects. Our results demonstrate the existence of 3D adversarial objects in the physical world.",
                        "Citation Paper Authors": "Authors:Anish Athalye, Logan Engstrom, Andrew Ilyas, Kevin Kwok"
                    }
                },
                {
                    "Sentence ID": 77,
                    "Sentence": "100% CIFAR-10 ResNet L0,L2,L\u221e CNN No White Yes No image\nGuo et al. ",
                    "Citation Text": "C. Guo, J. S. Frank, and K. Q. Weinberger. Low frequency\nadversarial perturbation. CoRR , abs/1809.08758, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.08758",
                        "Citation Paper Title": "Title:Low Frequency Adversarial Perturbation",
                        "Citation Paper Abstract": "Abstract:Adversarial images aim to change a target model's decision by minimally perturbing a target image. In the black-box setting, the absence of gradient information often renders this search problem costly in terms of query complexity. In this paper we propose to restrict the search for adversarial images to a low frequency domain. This approach is readily compatible with many existing black-box attack frameworks and consistently reduces their query cost by 2 to 4 times. Further, we can circumvent image transformation defenses even when both the model and the defense strategy are unknown. Finally, we demonstrate the efficacy of this technique by fooling the Google Cloud Vision platform with an unprecedented low number of model queries.",
                        "Citation Paper Authors": "Authors:Chuan Guo, Jared S. Frank, Kilian Q. Weinberger"
                    }
                },
                {
                    "Sentence ID": 175,
                    "Sentence": "approximate the nearest\nclassi\ufb01cation boundary by Taylor expansion. Instead of per -\nturbing a whole image, JSMA ",
                    "Citation Text": "N. Papernot, P . D. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik,\nand A. Swami. The limitations of deep learning in adversarial\nsettings. In IEEE European Symposium on Security and Privacy,\nEuroS&P 2016, Saarbr\u00a8 ucken, Germany, March 21-24, 2016 , pages\n372\u2013387, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.07528",
                        "Citation Paper Title": "Title:The Limitations of Deep Learning in Adversarial Settings",
                        "Citation Paper Abstract": "Abstract:Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97% adversarial success rate while only modifying on average 4.02% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.",
                        "Citation Paper Authors": "Authors:Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik, Ananthram Swami"
                    }
                },
                {
                    "Sentence ID": 74,
                    "Sentence": "\ufb01rst proposed an optimization\nfunction to \ufb01nd AEs and solved it with L-BFGS. FGSM ",
                    "Citation Text": "I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining an d\nharnessing adversarial examples. CoRR , abs/1412.6572, 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.6572",
                        "Citation Paper Title": "Title:Explaining and Harnessing Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy"
                    }
                },
                {
                    "Sentence ID": 251,
                    "Sentence": "use integrated gradient technology to\ndeal with discrete graph connections and discrete features .\nIt can accurately determine the effect of changing selected\nfeatures or edges. For handling discrete graph data, Xu et\nal. ",
                    "Citation Text": "K. Xu, H. Chen, S. Liu, P . Chen, T. Weng, M. Hong, and X. Lin .\nTopology attack and defense for graph neural networks: An\noptimization perspective. In Proceedings of the Twenty-Eighth\nInternational Joint Conference on Arti\ufb01cial Intelligence , IJCAI 2019,\nMacao, China, August 10-16, 2019 , pages 3961\u20133967.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.04214",
                        "Citation Paper Title": "Title:Topology Attack and Defense for Graph Neural Networks: An Optimization Perspective",
                        "Citation Paper Abstract": "Abstract:Graph neural networks (GNNs) which apply the deep neural networks to graph data have achieved significant performance for the task of semi-supervised node classification. However, only few work has addressed the adversarial robustness of GNNs. In this paper, we first present a novel gradient-based attack method that facilitates the difficulty of tackling discrete graph data. When comparing to current adversarial attacks on GNNs, the results show that by only perturbing a small number of edge perturbations, including addition and deletion, our optimization-based attack can lead to a noticeable decrease in classification performance. Moreover, leveraging our gradient-based attack, we propose the first optimization-based adversarial training for GNNs. Our method yields higher robustness against both different gradient based and greedy attack methods without sacrificing classification accuracy on original graph.",
                        "Citation Paper Authors": "Authors:Kaidi Xu, Hongge Chen, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Mingyi Hong, Xue Lin"
                    }
                },
                {
                    "Sentence ID": 127,
                    "Sentence": "propose a real-time adversarial\nattack approach, in which attackers can only observe past\ndata points and add perturbations to the remaining data\npoints of the input. Li et al. ",
                    "Citation Text": "S. Li, A. Neupane, S. Paul, C. Song, S. V . Krishnamurthy, A. K .\nRoy-Chowdhury, and A. Swami. Stealthy adversarial pertur-\nbations against real-time video classi\ufb01cation systems. In 26th\nAnnual Network and Distributed System Security Symposium, NDSS\n2019, San Diego, California, USA, February 24-27, 2019 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.00458",
                        "Citation Paper Title": "Title:Adversarial Perturbations Against Real-Time Video Classification Systems",
                        "Citation Paper Abstract": "Abstract:Recent research has demonstrated the brittleness of machine learning systems to adversarial perturbations. However, the studies have been mostly limited to perturbations on images and more generally, classification that does not deal with temporally varying inputs. In this paper we ask \"Are adversarial perturbations possible in real-time video classification systems and if so, what properties must they satisfy?\" Such systems find application in surveillance applications, smart vehicles, and smart elderly care and thus, misclassification could be particularly harmful (e.g., a mishap at an elderly care facility may be missed). We show that accounting for temporal structure is key to generating adversarial examples in such systems. We exploit recent advances in generative adversarial network (GAN) architectures to account for temporal correlations and generate adversarial samples that can cause misclassification rates of over 80% for targeted activities. More importantly, the samples also leave other activities largely unaffected making them extremely stealthy. Finally, we also surprisingly find that in many scenarios, the same perturbation can be applied to every frame in a video clip that makes the adversary's ability to achieve misclassification relatively easy.",
                        "Citation Paper Authors": "Authors:Shasha Li, Ajaya Neupane, Sujoy Paul, Chengyu Song, Srikanth V. Krishnamurthy, Amit K. Roy Chowdhury, Ananthram Swami"
                    }
                },
                {
                    "Sentence ID": 236,
                    "Sentence": "propose hiding attack and appearing attack\nto produce practical AEs. Their attacks can attack real-\nworld object detectors in both long and short distance. Wei\net al. ",
                    "Citation Text": "X. Wei, S. Liang, N. Chen, and X. Cao. Transferable adver sarial\nattacks for image and video object detection. In Proceedings of the\nTwenty-Eighth International Joint Conference on Arti\ufb01cia l Intelligence,\nIJCAI 2019, Macao, China, August 10-16, 2019 , pages 954\u2013960.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.12641",
                        "Citation Paper Title": "Title:Transferable Adversarial Attacks for Image and Video Object Detection",
                        "Citation Paper Abstract": "Abstract:Adversarial examples have been demonstrated to threaten many computer vision tasks including object detection. However, the existing attacking methods for object detection have two limitations: poor transferability, which denotes that the generated adversarial examples have low success rate to attack other kinds of detection methods, and high computation cost, which means that they need more time to generate an adversarial image, and therefore are difficult to deal with the video data. To address these issues, we utilize a generative mechanism to obtain the adversarial image and video. In this way, the processing time is reduced. To enhance the transferability, we destroy the feature maps extracted from the feature network, which usually constitutes the basis of object detectors. The proposed method is based on the Generative Adversarial Network (GAN) framework, where we combine the high-level class loss and low-level feature loss to jointly train the adversarial example generator. A series of experiments conducted on PASCAL VOC and ImageNet VID datasets show that our method can efficiently generate image and video adversarial examples, and more importantly, these adversarial examples have better transferability, and thus, are able to simultaneously attack two kinds of representative object detection models: proposal based models like Faster-RCNN, and regression based models like SSD.",
                        "Citation Paper Authors": "Authors:Xingxing Wei, Siyuan Liang, Ning Chen, Xiaochun Cao"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": "present a\nword replacement method based on sememe, and a search\nalgorithm based on particle swarm optimization.\nNeural machine translation (NMT) models in NLP\nalso suffer from the vulnerability to adversarial perturba -\ntions ",
                    "Citation Text": "Y. Cheng, L. Jiang, and W. Macherey. Robust neural machi ne\ntranslation with doubly adversarial inputs. In Proceedings of the\n57th Conference of the Association for Computational Lingu istics, ACL\n2019, Florence, Italy, July 28- August 2, 2019 , pages 4324\u20134333.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.02443",
                        "Citation Paper Title": "Title:Robust Neural Machine Translation with Doubly Adversarial Inputs",
                        "Citation Paper Abstract": "Abstract:Neural machine translation (NMT) often suffers from the vulnerability to noisy perturbations in the input. We propose an approach to improving the robustness of NMT models, which consists of two parts: (1) attack the translation model with adversarial source examples; (2) defend the translation model with adversarial target inputs to improve its robustness against the adversarial source inputs.For the generation of adversarial inputs, we propose a gradient-based method to craft adversarial examples informed by the translation loss over the clean inputs.Experimental results on Chinese-English and English-German translation tasks demonstrate that our approach achieves significant improvements ($2.8$ and $1.6$ BLEU points) over Transformer on standard clean benchmarks as well as exhibiting higher robustness on noisy data.",
                        "Citation Paper Authors": "Authors:Yong Cheng, Lu Jiang, Wolfgang Macherey"
                    }
                },
                {
                    "Sentence ID": 123,
                    "Sentence": "generate\nadversarial text sequences in black-box settings. They ado pt\ndifferent score functions to better mutate words and mini-\nmize edit distance between the original and modi\ufb01ed texts.\nTextBugger ",
                    "Citation Text": "J. Li, S. Ji, T. Du, B. Li, and T. Wang. Textbugger: Genera ting\nadversarial text against real-world applications. In 26th Annual\nNetwork and Distributed System Security Symposium, NDSS 20 19,\nSan Diego, California, USA, February 24-27, 2019 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.05271",
                        "Citation Paper Title": "Title:TextBugger: Generating Adversarial Text Against Real-world Applications",
                        "Citation Paper Abstract": "Abstract:Deep Learning-based Text Understanding (DLTU) is the backbone technique behind various applications, including question answering, machine translation, and text classification. Despite its tremendous popularity, the security vulnerabilities of DLTU are still largely unknown, which is highly concerning given its increasing use in security-sensitive applications such as sentiment analysis and toxic content detection. In this paper, we show that DLTU is inherently vulnerable to adversarial text attacks, in which maliciously crafted texts trigger target DLTU systems and services to misbehave. Specifically, we present TextBugger, a general attack framework for generating adversarial texts. In contrast to prior works, TextBugger differs in significant ways: (i) effective -- it outperforms state-of-the-art attacks in terms of attack success rate; (ii) evasive -- it preserves the utility of benign text, with 94.9\\% of the adversarial text correctly recognized by human readers; and (iii) efficient -- it generates adversarial text with computational complexity sub-linear to the text length. We empirically evaluate TextBugger on a set of real-world DLTU systems and services used for sentiment analysis and toxic content detection, demonstrating its effectiveness, evasiveness, and efficiency. For instance, TextBugger achieves 100\\% success rate on the IMDB dataset based on Amazon AWS Comprehend within 4.61 seconds and preserves 97\\% semantic similarity. We further discuss possible defense mechanisms to mitigate such attack and the adversary's potential countermeasures, which leads to promising directions for further research.",
                        "Citation Paper Authors": "Authors:Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, Ting Wang"
                    }
                },
                {
                    "Sentence ID": 186,
                    "Sentence": "could convert any given\nwaveform into any desired target phrase through adding\nsmall perturbations on speech-to-text neural networks. Qi n\net al. ",
                    "Citation Text": "Y. Qin, N. Carlini, G. W. Cottrell, I. J. Goodfellow, an d C. Raffel.\nImperceptible, robust, and targeted adversarial examples for au-\ntomatic speech recognition. In Proceedings of the 36th International\nConference on Machine Learning, ICML 2019, 9-15 June 2019, L ong\nBeach, California, USA , pages 5231\u20135240.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.10346",
                        "Citation Paper Title": "Title:Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition",
                        "Citation Paper Abstract": "Abstract:Adversarial examples are inputs to machine learning models designed by an adversary to cause an incorrect output. So far, adversarial examples have been studied most extensively in the image domain. In this domain, adversarial examples can be constructed by imperceptibly modifying images to cause misclassification, and are practical in the physical world. In contrast, current targeted adversarial examples applied to speech recognition systems have neither of these properties: humans can easily identify the adversarial perturbations, and they are not effective when played over-the-air. This paper makes advances on both of these fronts. First, we develop effectively imperceptible audio adversarial examples (verified through a human study) by leveraging the psychoacoustic principle of auditory masking, while retaining 100% targeted success rate on arbitrary full-sentence targets. Next, we make progress towards physical-world over-the-air audio adversarial examples by constructing perturbations which remain effective even after applying realistic simulated environmental distortions.",
                        "Citation Paper Authors": "Authors:Yao Qin, Nicholas Carlini, Ian Goodfellow, Garrison Cottrell, Colin Raffel"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": "propose a\ndecision-based attack method by constraining perturbatio ns\nin low-frequency subspace with small queries. Cheng et\nal. ",
                    "Citation Text": "S. Cheng, Y. Dong, T. Pang, H. Su, and J. Zhu. Improving blac k-\nbox adversarial attacks with a transfer-based prior. In Advances\nin Neural Information Processing Systems 32: Annual Confer ence on\nNeural Information Processing Systems 2019, NeurIPS 2019, 8-14\nDecember 2019, Vancouver, BC, Canada , pages 10932\u201310942.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.06919",
                        "Citation Paper Title": "Title:Improving Black-box Adversarial Attacks with a Transfer-based Prior",
                        "Citation Paper Abstract": "Abstract:We consider the black-box adversarial setting, where the adversary has to generate adversarial perturbations without access to the target models to compute gradients. Previous methods tried to approximate the gradient either by using a transfer gradient of a surrogate white-box model, or based on the query feedback. However, these methods often suffer from low attack success rates or poor query efficiency since it is non-trivial to estimate the gradient in a high-dimensional space with limited information. To address these problems, we propose a prior-guided random gradient-free (P-RGF) method to improve black-box adversarial attacks, which takes the advantage of a transfer-based prior and the query information simultaneously. The transfer-based prior given by the gradient of a surrogate model is appropriately integrated into our algorithm by an optimal coefficient derived by a theoretical analysis. Extensive experiments demonstrate that our method requires much fewer queries to attack black-box models with higher success rates compared with the alternative state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Shuyu Cheng, Yinpeng Dong, Tianyu Pang, Hang Su, Jun Zhu"
                    }
                },
                {
                    "Sentence ID": 58,
                    "Sentence": "pro-\npose a method based on an alternative training algorithm\nusing synthetic data generation in black-box settings.\nThis step needs that AEs have high transferability from\nthe substitute model to the target model ",
                    "Citation Text": "Y. Dong, T. Pang, H. Su, and J. Zhu. Evading defenses to tra ns-\nferable adversarial examples by translation-invariant at tacks. In\nIEEE Conference on Computer Vision and Pattern Recognition , CVPR\n2019, Long Beach, CA, USA, June 16-20, 2019 , pages 4312\u20134321.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.02884",
                        "Citation Paper Title": "Title:Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks",
                        "Citation Paper Abstract": "Abstract:Deep neural networks are vulnerable to adversarial examples, which can mislead classifiers by adding imperceptible perturbations. An intriguing property of adversarial examples is their good transferability, making black-box attacks feasible in real-world applications. Due to the threat of adversarial attacks, many methods have been proposed to improve the robustness. Several state-of-the-art defenses are shown to be robust against transferable adversarial examples. In this paper, we propose a translation-invariant attack method to generate more transferable adversarial examples against the defense models. By optimizing a perturbation over an ensemble of translated images, the generated adversarial example is less sensitive to the white-box model being attacked and has better transferability. To improve the efficiency of attacks, we further show that our method can be implemented by convolving the gradient at the untranslated image with a pre-defined kernel. Our method is generally applicable to any gradient-based attack method. Extensive experiments on the ImageNet dataset validate the effectiveness of the proposed method. Our best attack fools eight state-of-the-art defenses at an 82% success rate on average based only on the transferability, demonstrating the insecurity of the current defense techniques.",
                        "Citation Paper Authors": "Authors:Yinpeng Dong, Tianyu Pang, Hang Su, Jun Zhu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1906.07902v3": {
            "Paper Title": "Trade-offs and Guarantees of Adversarial Representation Learning for\n  Information Obfuscation",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": "6). Principal Component Analysis (PCA) 7). Normal Training\n(NORM-TRAIN), 8) Local Differential Privacy (LDP) with Laplacian mechanism, 9). Differentially\nPrivate SGD (DPSGD) ",
                    "Citation Text": "Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar,\nand Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC\nConference on Computer and Communications Security , pages 308\u2013318. ACM, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.00133",
                        "Citation Paper Title": "Title:Deep Learning with Differential Privacy",
                        "Citation Paper Abstract": "Abstract:Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.",
                        "Citation Paper Authors": "Authors:Mart\u00edn Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, Li Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1908.06062v3": {
            "Paper Title": "Adversarial shape perturbations on 3D point clouds",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.09058v2": {
            "Paper Title": "Fine-grained Synthesis of Unrestricted Adversarial Examples",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.11503v3": {
            "Paper Title": "Body Shape Privacy in Images: Understanding Privacy and Preventing\n  Automatic Shape Extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.05972v3": {
            "Paper Title": "Motion Sensor-based Privacy Attack on Smartphones",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.07452v3": {
            "Paper Title": "BAFFLE : Blockchain Based Aggregator Free Federated Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.06710v5": {
            "Paper Title": "Smart Contract Vulnerabilities: Vulnerable Does Not Imply Exploited",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.10242v5": {
            "Paper Title": "PACStack: an Authenticated Call Stack",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ", or using a dedicated register for\nshadow stack addressing ",
                    "Citation Text": "Nathan Burow, Xingping Zhang, and Mathias Payer.\nSoK: Shining light on shadow stacks. In Proc. IEEE\nS&P \u201919 , pages 985\u2013999, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.03165",
                        "Citation Paper Title": "Title:Shining Light On Shadow Stacks",
                        "Citation Paper Abstract": "Abstract:Control-Flow Hijacking attacks are the dominant attack vector against C/C++ programs. Control-Flow Integrity (CFI) solutions mitigate these attacks on the forward edge,i.e., indirect calls through function pointers and virtual calls. Protecting the backward edge is left to stack canaries, which are easily bypassed through information leaks. Shadow Stacks are a fully precise mechanism for protecting backwards edges, and should be deployed with CFI mitigations. We present a comprehensive analysis of all possible shadow stack mechanisms along three axes: performance, compatibility, and security. For performance comparisons we use SPEC CPU2006, while security and compatibility are qualitatively analyzed. Based on our study, we renew calls for a shadow stack design that leverages a dedicated register, resulting in low performance overhead, and minimal memory overhead, but sacrifices compatibility. We present case studies of our implementation of such a design, Shadesmar, on Phoronix and Apache to demonstrate the feasibility of dedicating a general purpose register to a security monitor on modern architectures, and the deployability of Shadesmar. Our comprehensive analysis, including detailed case studies for our novel design, allows compiler designers and practitioners to select the correct shadow stack design for different usage scenarios.",
                        "Citation Paper Authors": "Authors:Nathan Burow, Xinping Zhang, Mathias Payer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1903.01711v3": {
            "Paper Title": "Profitable Double-Spending Attacks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.12704v3": {
            "Paper Title": "Comparative Study of Differentially Private Synthetic Data Algorithms\n  from the NIST PSCR Differential Privacy Synthetic Data Challenge",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.12366v3": {
            "Paper Title": "Thieves on Sesame Street! Model Extraction of BERT-based APIs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.10666v2": {
            "Paper Title": "ARM Pointer Authentication based Forward-Edge and Backward-Edge Control\n  Flow Integrity for Kernels",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.05243v3": {
            "Paper Title": "Crucial and Redundant Shares and Compartments in Secret Sharing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.07433v4": {
            "Paper Title": "KRNC: New Foundations for Permissionless Byzantine Consensus and Global\n  Monetary Stability",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.03137v4": {
            "Paper Title": "Detecting AI Trojans Using Meta Neural Analysis",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": ". In\ncontrast, the objective of Trojan attacks is to embed backdoors\nwhile not degrading the model\u2019s prediction accuracy on clean\ninputs.\nProperty Inference. Property inference attacks ",
                    "Citation Text": "Giuseppe Ateniese, Luigi V Mancini, Angelo Spognardi, Antonio Vil-\nlani, Domenico Vitali, and Giovanni Felici. Hacking smart machines\nwith smarter ones: How to extract meaningful data from machine\nlearning classi\ufb01ers. International Journal of Security and Networks ,\n10(3):137\u2013150, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1306.4447",
                        "Citation Paper Title": "Title:Hacking Smart Machines with Smarter Ones: How to Extract Meaningful Data from Machine Learning Classifiers",
                        "Citation Paper Abstract": "Abstract:Machine Learning (ML) algorithms are used to train computers to perform a variety of complex tasks and improve with experience. Computers learn how to recognize patterns, make unintended decisions, or react to a dynamic environment. Certain trained machines may be more effective than others because they are based on more suitable ML algorithms or because they were trained through superior training sets. Although ML algorithms are known and publicly released, training sets may not be reasonably ascertainable and, indeed, may be guarded as trade secrets. While much research has been performed about the privacy of the elements of training sets, in this paper we focus our attention on ML classifiers and on the statistical information that can be unconsciously or maliciously revealed from them. We show that it is possible to infer unexpected but useful information from ML classifiers. In particular, we build a novel meta-classifier and train it to hack other classifiers, obtaining meaningful information about their training sets. This kind of information leakage can be exploited, for example, by a vendor to build more effective classifiers or to simply acquire trade secrets from a competitor's apparatus, potentially violating its intellectual property rights.",
                        "Citation Paper Authors": "Authors:Giuseppe Ateniese, Giovanni Felici, Luigi V. Mancini, Angelo Spognardi, Antonio Villani, Domenico Vitali"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": ". We discussed the differences of these\ndetection levels in Section IX.\nTrojan Attack Defense/Mitigation. To the best of our knowl-\nedge, there are few evaluated defenses against Trojan at-\ntacks ",
                    "Citation Text": "Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning:\nDefending against backdooring attacks on deep neural networks. In In-\nternational Symposium on Research in Attacks, Intrusions, and Defenses ,\npages 273\u2013294. Springer, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.12185",
                        "Citation Paper Title": "Title:Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:Deep neural networks (DNNs) provide excellent performance across a wide range of classification tasks, but their training requires high computational resources and is often outsourced to third parties. Recent work has shown that outsourced training introduces the risk that a malicious trainer will return a backdoored DNN that behaves normally on most inputs but causes targeted misclassifications or degrades the accuracy of the network when a trigger known only to the attacker is present. In this paper, we provide the first effective defenses against backdoor attacks on DNNs. We implement three backdoor attacks from prior work and use them to investigate two promising defenses, pruning and fine-tuning. We show that neither, by itself, is sufficient to defend against sophisticated attackers. We then evaluate fine-pruning, a combination of pruning and fine-tuning, and show that it successfully weakens or even eliminates the backdoors, i.e., in some cases reducing the attack success rate to 0% with only a 0.4% drop in accuracy for clean (non-triggering) inputs. Our work provides the first step toward defenses against backdoor attacks in deep neural networks.",
                        "Citation Paper Authors": "Authors:Kang Liu, Brendan Dolan-Gavitt, Siddharth Garg"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "considers Trojaning a publicly available model using training\ndata generated via reverse engineering. Bagdasaryan et al. ",
                    "Citation Text": "Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and\nVitaly Shmatikov. How to backdoor federated learning. arXiv preprint\narXiv:1807.00459 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.00459",
                        "Citation Paper Title": "Title:How To Backdoor Federated Learning",
                        "Citation Paper Abstract": "Abstract:Federated learning enables thousands of participants to construct a deep learning model without sharing their private training data with each other. For example, multiple smartphones can jointly train a next-word predictor for keyboards without revealing what individual users type. We demonstrate that any participant in federated learning can introduce hidden backdoor functionality into the joint global model, e.g., to ensure that an image classifier assigns an attacker-chosen label to images with certain features, or that a word predictor completes certain sentences with an attacker-chosen word.\nWe design and evaluate a new model-poisoning methodology based on model replacement. An attacker selected in a single round of federated learning can cause the global model to immediately reach 100% accuracy on the backdoor task. We evaluate the attack under different assumptions for the standard federated-learning tasks and show that it greatly outperforms data poisoning. Our generic constrain-and-scale technique also evades anomaly detection-based defenses by incorporating the evasion into the attacker's loss function during training.",
                        "Citation Paper Authors": "Authors:Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, Vitaly Shmatikov"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1902.09587v3": {
            "Paper Title": "A Unified Access Control Model for Calibration Traceability in\n  Safety-Critical IoT",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.07672v2": {
            "Paper Title": "Secure Quantum Extraction Protocols",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.09215v2": {
            "Paper Title": "Express: Lowering the Cost of Metadata-hiding Communication with\n  Cryptographic Privacy",
            "Sentences": [
                {
                    "Sentence ID": 43,
                    "Sentence": "and outsourced the\nshu\ufb04ing to a smaller set of servers [52,61]. Most recently,\nmixingtechniqueshavebeenextendedtosupportlargenumbers\nof users in Atom ",
                    "Citation Text": "Albert Kwon, Henry Corrigan-Gibbs, Srinivas Devadas, and\nBryanFord. Atom:Horizontallyscalingstronganonymity. In\nSOSP, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.07841",
                        "Citation Paper Title": "Title:Atom: Horizontally Scaling Strong Anonymity",
                        "Citation Paper Abstract": "Abstract:Atom is an anonymous messaging system that protects against traffic-analysis attacks. Unlike many prior systems, each Atom server touches only a small fraction of the total messages routed through the network. As a result, the system's capacity scales near-linearly with the number of servers. At the same time, each Atom user benefits from \"best possible\" anonymity: a user is anonymous among all honest users of the system, against an active adversary who controls the entire network, a portion of the system's servers, and any number of malicious users. The architectural ideas behind Atom have been known in theory, but putting them into practice requires new techniques for (1) avoiding the reliance on heavy general-purpose multi-party computation protocols, (2) defeating active attacks by malicious servers at minimal performance cost, and (3) handling server failure and churn.\nAtom is most suitable for sending a large number of short messages, as in a microblogging application or a high-security communication bootstrapping (\"dialing\") for private messaging systems. We show that, on a heterogeneous network of 1,024 servers, Atom can transit a million Tweet-length messages in 28 minutes. This is over 23x faster than prior systems with similar privacy guarantees.",
                        "Citation Paper Authors": "Authors:Albert Kwon, Henry Corrigan-Gibbs, Srinivas Devadas, Bryan Ford"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.07144v2": {
            "Paper Title": "Are cookie banners indeed compliant with the law? Deciphering EU legal\n  requirements on consent and technical means to verify compliance of cookie\n  banners",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.10541v2": {
            "Paper Title": "PAC learning with stable and private predictions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.04014v2": {
            "Paper Title": "Interaction is necessary for distributed learning with privacy or\n  communication constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.14028v2": {
            "Paper Title": "SoK: Chasing Accuracy and Privacy, and Catching Both in Differentially\n  Private Histogram Publication",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.13055v2": {
            "Paper Title": "Corpus Distillation for Effective Fuzzing: A Comparative Evaluation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.09822v5": {
            "Paper Title": "Scalable Differential Privacy with Certified Robustness in Adversarial\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.04078v3": {
            "Paper Title": "Cost-Effective Data Feeds to Blockchains via Workload-Adaptive Data\n  Replication",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": "is a provable-secure data feed service built\non off-chain trusted hardware that connects TLS-certified\nwebsites to blockchains. The data-feed storage in TownCrier\nis always off-chain and it does not address the dynamic data\nreplication as in GRuB.\nGasper ",
                    "Citation Text": "T. Chen, X. Li, X. Luo, and X. Zhang. Under-optimized smart con-\ntracts devour your money. In IEEE 24th International Conference\nonSoftware Analysis, Evolution and Reengineering, SANER 2017,\nKlagenfurt, Austria, February 20-24, 2017, pages 442\u2013446, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.03994",
                        "Citation Paper Title": "Title:Under-Optimized Smart Contracts Devour Your Money",
                        "Citation Paper Abstract": "Abstract:Smart contracts are full-fledged programs that run on blockchains (e.g., Ethereum, one of the most popular blockchains). In Ethereum, gas (in Ether, a cryptographic currency like Bitcoin) is the execution fee compensating the computing resources of miners for running smart contracts. However, we find that under-optimized smart contracts cost more gas than necessary, and therefore the creators or users will be overcharged. In this work, we conduct the first investigation on Solidity, the recommended compiler, and reveal that it fails to optimize gas-costly programming patterns. In particular, we identify 7 gas-costly patterns and group them to 2 categories. Then, we propose and develop GASPER, a new tool for automatically locating gas-costly patterns by analyzing smart contracts' bytecodes. The preliminary results on discovering 3 representative patterns from 4,240 real smart contracts show that 93.5%, 90.1% and 80% contracts suffer from these 3 patterns, respectively.",
                        "Citation Paper Authors": "Authors:Ting Chen, Xiaoqi Li, Xiapu Luo, Xiaosong Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1901.10618v2": {
            "Paper Title": "Persuasion-based Robust Sensor Design Against Attackers with Unknown\n  Control Objectives",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.11478v2": {
            "Paper Title": "Towards Distributed Privacy-Preserving Prediction",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": ". The most recent work\nintroduced ampli\ufb01cation by shuf\ufb02ing to lower the privacy cost\nof LDP algorithm when viewed in the central model of DP ",
                    "Citation Text": "\u00b4Ulfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth Raghunathan,\nKunal Talwar, and Abhradeep Thakurta, \u201cAmpli\ufb01cation by shuf\ufb02ing:\nFrom local to central differential privacy via anonymity,\u201d in Proceedings\nof the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms .\nSIAM, 2019, pp. 2468\u20132479.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.12469",
                        "Citation Paper Title": "Title:Amplification by Shuffling: From Local to Central Differential Privacy via Anonymity",
                        "Citation Paper Abstract": "Abstract:Sensitive statistics are often collected across sets of users, with repeated collection of reports done over time. For example, trends in users' private preferences or software usage may be monitored via such reports. We study the collection of such statistics in the local differential privacy (LDP) model, and describe an algorithm whose privacy cost is polylogarithmic in the number of changes to a user's value.\nMore fundamentally---by building on anonymity of the users' reports---we also demonstrate how the privacy cost of our LDP algorithm can actually be much lower when viewed in the central model of differential privacy. We show, via a new and general privacy amplification technique, that any permutation-invariant algorithm satisfying $\\varepsilon$-local differential privacy will satisfy $(O(\\varepsilon \\sqrt{\\log(1/\\delta)/n}), \\delta)$-central differential privacy. By this, we explain how the high noise and $\\sqrt{n}$ overhead of LDP protocols is a consequence of them being significantly more private in the central model. As a practical corollary, our results imply that several LDP-based industrial deployments may have much lower privacy cost than their advertised $\\varepsilon$ would indicate---at least if reports are anonymized.",
                        "Citation Paper Authors": "Authors:\u00dalfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, Abhradeep Thakurta"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1901.11218v4": {
            "Paper Title": "Replay Attacks and Defenses Against Cross-shard Consensus in Sharded\n  Distributed Ledgers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.10499v2": {
            "Paper Title": "Improving Frequency Estimation under Local Differential Privacy",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": "A lower bound for distribution estimation is given in\nProposition 6 of ",
                    "Citation Text": "John C. Duchi, Michael I. Jordan, and Mar-\ntin J. Wainwright. \u201cLocal privacy, data process-\ning inequalities, and statistical minimax rates\u201d. In:\narXiv:1302.3203 (2013). Preprint.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1302.3203",
                        "Citation Paper Title": "Title:Local Privacy, Data Processing Inequalities, and Statistical Minimax Rates",
                        "Citation Paper Abstract": "Abstract:Working under a model of privacy in which data remains private even from the statistician, we study the tradeoff between privacy guarantees and the utility of the resulting statistical estimators. We prove bounds on information-theoretic quantities, including mutual information and Kullback-Leibler divergence, that depend on the privacy guarantees. When combined with standard minimax techniques, including the Le Cam, Fano, and Assouad methods, these inequalities allow for a precise characterization of statistical rates under local privacy constraints. We provide a treatment of several canonical families of problems: mean estimation, parameter estimation in fixed-design regression, multinomial probability estimation, and nonparametric density estimation. For all of these families, we provide lower and upper bounds that match up to constant factors, and exhibit new (optimal) privacy-preserving mechanisms and computationally efficient estimators that achieve the bounds.",
                        "Citation Paper Authors": "Authors:John C. Duchi, Michael I. Jordan, Martin J. Wainwright"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.12982v3": {
            "Paper Title": "Scalable Online Vetting of Android Apps for Measuring Declared SDK\n  Versions and Their Consistency with API Calls",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.02742v3": {
            "Paper Title": "Invisible Backdoor Attacks on Deep Neural Networks via Steganography and\n  Regularization",
            "Sentences": [
                {
                    "Sentence ID": 51,
                    "Sentence": "incurs high computational\ncosts. As prior work has demonstrated [3, 12, 46] that \ufb01ne-\npruning ",
                    "Citation Text": "Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg.\nFine-pruning: Defending against backdooring attacks on\ndeep neural networks. In International Symposium on Re-\nsearch in Attacks, Intrusions, and Defenses , pages 273\u2013294.\nSpringer, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.12185",
                        "Citation Paper Title": "Title:Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:Deep neural networks (DNNs) provide excellent performance across a wide range of classification tasks, but their training requires high computational resources and is often outsourced to third parties. Recent work has shown that outsourced training introduces the risk that a malicious trainer will return a backdoored DNN that behaves normally on most inputs but causes targeted misclassifications or degrades the accuracy of the network when a trigger known only to the attacker is present. In this paper, we provide the first effective defenses against backdoor attacks on DNNs. We implement three backdoor attacks from prior work and use them to investigate two promising defenses, pruning and fine-tuning. We show that neither, by itself, is sufficient to defend against sophisticated attackers. We then evaluate fine-pruning, a combination of pruning and fine-tuning, and show that it successfully weakens or even eliminates the backdoors, i.e., in some cases reducing the attack success rate to 0% with only a 0.4% drop in accuracy for clean (non-triggering) inputs. Our work provides the first step toward defenses against backdoor attacks in deep neural networks.",
                        "Citation Paper Authors": "Authors:Kang Liu, Brendan Dolan-Gavitt, Siddharth Garg"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "and it is laborious for humans\nto closely examine such an enormous dataset.\nRun-Time. The backdoor detection can also work on the\nclassi\ufb01er during run-time. In STRIP ",
                    "Citation Text": "Yansong Gao, Chang Xu, Derui Wang, Shiping Chen,\nDamith Chinthana Ranasinghe, and Surya Nepal. STRIP:\nA defence against trojan attacks on deep neural networks.\nCoRR , abs/1902.06531, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.06531",
                        "Citation Paper Title": "Title:STRIP: A Defence Against Trojan Attacks on Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:A recent trojan attack on deep neural network (DNN) models is one insidious variant of data poisoning attacks. Trojan attacks exploit an effective backdoor created in a DNN model by leveraging the difficulty in interpretability of the learned model to misclassify any inputs signed with the attacker's chosen trojan trigger. Since the trojan trigger is a secret guarded and exploited by the attacker, detecting such trojan inputs is a challenge, especially at run-time when models are in active operation. This work builds STRong Intentional Perturbation (STRIP) based run-time trojan attack detection system and focuses on vision system. We intentionally perturb the incoming input, for instance by superimposing various image patterns, and observe the randomness of predicted classes for perturbed inputs from a given deployed model---malicious or benign. A low entropy in predicted classes violates the input-dependence property of a benign model and implies the presence of a malicious input---a characteristic of a trojaned input. The high efficacy of our method is validated through case studies on three popular and contrasting datasets: MNIST, CIFAR10 and GTSRB. We achieve an overall false acceptance rate (FAR) of less than 1%, given a preset false rejection rate (FRR) of 1%, for different types of triggers. Using CIFAR10 and GTSRB, we have empirically achieved result of 0% for both FRR and FAR. We have also evaluated STRIP robustness against a number of trojan attack variants and adaptive attacks.",
                        "Citation Paper Authors": "Authors:Yansong Gao, Chang Xu, Derui Wang, Shiping Chen, Damith C.Ranasinghe, Surya Nepal"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.02038v3": {
            "Paper Title": "Using Name Confusion to Enhance Security",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.10836v4": {
            "Paper Title": "Random CapsNet Forest Model for Imbalanced Malware Type Classification\n  Task",
            "Sentences": [
                {
                    "Sentence ID": 61,
                    "Sentence": "uses a weighted loss function to handle\nthe imbalanced class distribution problem in the Malimg dataset; and also\nuses the transfer learning ",
                    "Citation Text": "Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How\ntransferable are features in deep neural networks? In Advances in\nneural information processing systems , pages 3320 \u20133328 ,2014 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1411.1792",
                        "Citation Paper Title": "Title:How transferable are features in deep neural networks?",
                        "Citation Paper Abstract": "Abstract:Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.",
                        "Citation Paper Authors": "Authors:Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "use CapsNet for object\nsegmentation. Traditional CNN structures, on the other hand, are used\ninGenerative Adversarial Networks, GANs. CapsNet is very useful to\nmake GANs better by removing the weakest point of these CNNs ",
                    "Citation Text": "Ayush Jaiswal, Wael AbdAlmageed, Yue Wu, and Premkumar\nNatarajan. Capsulegan: Generative adversarial capsule network.\nInProceedings of the European Conference on Computer Vision (ECCV) ,\npages 0\u20130,2018 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.06167",
                        "Citation Paper Title": "Title:CapsuleGAN: Generative Adversarial Capsule Network",
                        "Citation Paper Abstract": "Abstract:We present Generative Adversarial Capsule Network (CapsuleGAN), a framework that uses capsule networks (CapsNets) instead of the standard convolutional neural networks (CNNs) as discriminators within the generative adversarial network (GAN) setting, while modeling image data. We provide guidelines for designing CapsNet discriminators and the updated GAN objective function, which incorporates the CapsNet margin loss, for training CapsuleGAN models. We show that CapsuleGAN outperforms convolutional-GAN at modeling image data distribution on MNIST and CIFAR-10 datasets, evaluated on the generative adversarial metric and at semi-supervised image classification.",
                        "Citation Paper Authors": "Authors:Ayush Jaiswal, Wael AbdAlmageed, Yue Wu, Premkumar Natarajan"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": ", CapsNet have many applications\nin the literature. For instance, Afshar et al. ",
                    "Citation Text": "Parnian Afshar, Arash Mohammadi, and Konstantinos N Plataniotis.\nBrain tumor type classi\ufb01cation via capsule networks. In 2018 25 th\nIEEE International Conference on Image Processing (ICIP) , pages 3129 \u2013\n3133 . IEEE, 2018 .\n25",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.00597",
                        "Citation Paper Title": "Title:Capsule Networks for Brain Tumor Classification based on MRI Images and Course Tumor Boundaries",
                        "Citation Paper Abstract": "Abstract:According to official statistics, cancer is considered as the second leading cause of human fatalities. Among different types of cancer, brain tumor is seen as one of the deadliest forms due to its aggressive nature, heterogeneous characteristics, and low relative survival rate. Determining the type of brain tumor has significant impact on the treatment choice and patient's survival. Human-centered diagnosis is typically error-prone and unreliable resulting in a recent surge of interest to automatize this process using convolutional neural networks (CNNs). CNNs, however, fail to fully utilize spatial relations, which is particularly harmful for tumor classification, as the relation between the tumor and its surrounding tissue is a critical indicator of the tumor's type. In our recent work, we have incorporated newly developed CapsNets to overcome this shortcoming. CapsNets are, however, highly sensitive to the miscellaneous image background. The paper addresses this gap. The main contribution is to equip CapsNet with access to the tumor surrounding tissues, without distracting it from the main target. A modified CapsNet architecture is, therefore, proposed for brain tumor classification, which takes the tumor coarse boundaries as extra inputs within its pipeline to increase the CapsNet's focus. The proposed approach noticeably outperforms its counterparts.",
                        "Citation Paper Authors": "Authors:Parnian Afshar, Konstantinos N. Plataniotis, Arash Mohammadi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.11972v2": {
            "Paper Title": "Adversarial Deep Reinforcement Learning based Adaptive Moving Target\n  Defense",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.03373v2": {
            "Paper Title": "Privacy-Preserving Classification with Secret Vector Machines",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": ".\nKeeping the data. A recently proposed solution for training ma-\nchine learning models on distributed data is called federated learning\n(FL) ",
                    "Citation Text": "Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise\nAg\u00fcera y Arcas. 2017. Communication-Efficient Learning of Deep Networks\nfrom Decentralized Data. In Proceedings of the 20th International Conference on\nArtificial Intelligence and Statistics (Proceedings of Machine Learning Research) .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.05629",
                        "Citation Paper Title": "Title:Communication-Efficient Learning of Deep Networks from Decentralized Data",
                        "Citation Paper Abstract": "Abstract:Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning.\nWe present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.",
                        "Citation Paper Authors": "Authors:H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, Blaise Ag\u00fcera y Arcas"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": ", and there is also research on using binary\nweights in neural networks ",
                    "Citation Text": "Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. 2015. BinaryCon-\nnect: Training deep neural networks with binary weights during propagations.\nInAdvances in neural information processing systems . 3123\u20133131.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.00363",
                        "Citation Paper Title": "Title:BinaryConnect: Training Deep Neural Networks with binary weights during propagations",
                        "Citation Paper Abstract": "Abstract:Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models. In the past, GPUs enabled these breakthroughs because of their greater computational speed. In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL). Binary weights, i.e., weights which are constrained to only two possible values (e.g. -1 or 1), would bring great benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and power-hungry components of the digital implementation of neural networks. We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated. Like other dropout schemes, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.",
                        "Citation Paper Authors": "Authors:Matthieu Courbariaux, Yoshua Bengio, Jean-Pierre David"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": ", the reported increase in misclassification rate as compared to\nnon-private training can be larger than 0.2 for an SVM. The same\nholds for combining LDP with threshold homomorphic encryption ",
                    "Citation Text": "Stacey Truex, Nathalie Baracaldo, Ali Anwar, Thomas Steinke, Heiko Ludwig, Rui\nZhang, and Yi Zhou. 2019. A Hybrid Approach to Privacy-Preserving Federated\nLearning. In Proceedings of the 12th ACM Workshop on Artificial Intelligence and\nSecurity . 1\u201311.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.03224",
                        "Citation Paper Title": "Title:A Hybrid Approach to Privacy-Preserving Federated Learning",
                        "Citation Paper Abstract": "Abstract:Federated learning facilitates the collaborative training of models without the sharing of raw data. However, recent attacks demonstrate that simply maintaining data locality during training processes does not provide sufficient privacy guarantees. Rather, we need a federated learning system capable of preventing inference over both the messages exchanged during training and the final trained model while ensuring the resulting model also has acceptable predictive accuracy. Existing federated learning approaches either use secure multiparty computation (SMC) which is vulnerable to inference or differential privacy which can lead to low accuracy given a large number of parties with relatively small amounts of data each. In this paper, we present an alternative approach that utilizes both differential privacy and SMC to balance these trade-offs. Combining differential privacy with secure multiparty computation enables us to reduce the growth of noise injection as the number of parties increases without sacrificing privacy while maintaining a pre-defined rate of trust. Our system is therefore a scalable approach that protects against inference threats and produces models with high accuracy. Additionally, our system can be used to train a variety of machine learning models, which we validate with experimental results on 3 different machine learning algorithms. Our experiments demonstrate that our approach out-performs state of the art solutions.",
                        "Citation Paper Authors": "Authors:Stacey Truex, Nathalie Baracaldo, Ali Anwar, Thomas Steinke, Heiko Ludwig, Rui Zhang, Yi Zhou"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1901.09697v5": {
            "Paper Title": "Bayesian Differential Privacy for Machine Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.10498v3": {
            "Paper Title": "Detection of Backdoors in Trained Classifiers Without Access to the\n  Training Set",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": ", such back-\ndoor patterns are experimentally shown to be evasive to several existing backdoor defenses (before/during-\ntraining) that are very successful against human-perceptible backdoor patterns ",
                    "Citation Text": "B. Chen, W. Carvalho, N. Baracaldo, H. Ludwig, B. Edwards, T. Lee, I. Molloy, and\nB. Srivastava. Detecting Backdoor Attacks on Deep Neural Networks by Activation\n40Clustering. http://arxiv.org/abs/1811.03728, Nov. 8, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.03728",
                        "Citation Paper Title": "Title:Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering",
                        "Citation Paper Abstract": "Abstract:While machine learning (ML) models are being increasingly trusted to make decisions in different and varying areas, the safety of systems using such models has become an increasing concern. In particular, ML models are often trained on data from potentially untrustworthy sources, providing adversaries with the opportunity to manipulate them by inserting carefully crafted samples into the training set. Recent work has shown that this type of attack, called a poisoning attack, allows adversaries to insert backdoors or trojans into the model, enabling malicious behavior with simple external backdoor triggers at inference time and only a blackbox perspective of the model itself. Detecting this type of attack is challenging because the unexpected behavior occurs only when a backdoor trigger, which is known only to the adversary, is present. Model users, either direct users of training data or users of pre-trained model from a catalog, may not guarantee the safe operation of their ML-based system. In this paper, we propose a novel approach to backdoor detection and removal for neural networks. Through extensive experimental results, we demonstrate its effectiveness for neural networks classifying text and images. To the best of our knowledge, this is the first methodology capable of detecting poisonous data crafted to insert backdoors and repairing the model that does not require a verified and trusted dataset.",
                        "Citation Paper Authors": "Authors:Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian Molloy, Biplav Srivastava"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": ".\n6The clipping operation is an alternative to imposing strict box constraints on the backdoor pattern ",
                    "Citation Text": "C. Szegedy, W. Zaremba, I Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus.\nIntriguing properties of neural networks. In Proc. ICLR , 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1312.6199",
                        "Citation Paper Title": "Title:Intriguing properties of neural networks",
                        "Citation Paper Abstract": "Abstract:Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.\nFirst, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks.\nSecond, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
                        "Citation Paper Authors": "Authors:Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.00868v2": {
            "Paper Title": "InSpectre: Breaking and Fixing Microarchitectural Vulnerabilities by\n  Formal Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.09062v3": {
            "Paper Title": "Adversarial Reinforcement Learning under Partial Observability in\n  Autonomous Computer Network Defence",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": "use previous images to predict future input and detect adver-\nsarial examples. Havens et al. ",
                    "Citation Text": "A. J. Havens, Z. Jiang, and S. Sarkar, \u201cOnline robust policy learning\nin the presence of unknown adversaries,\u201d CoRR , vol. abs/1807.06064,\n2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.06064",
                        "Citation Paper Title": "Title:Online Robust Policy Learning in the Presence of Unknown Adversaries",
                        "Citation Paper Abstract": "Abstract:The growing prospect of deep reinforcement learning (DRL) being used in cyber-physical systems has raised concerns around safety and robustness of autonomous agents. Recent work on generating adversarial attacks have shown that it is computationally feasible for a bad actor to fool a DRL policy into behaving sub optimally. Although certain adversarial attacks with specific attack models have been addressed, most studies are only interested in off-line optimization in the data space (e.g., example fitting, distillation). This paper introduces a Meta-Learned Advantage Hierarchy (MLAH) framework that is attack model-agnostic and more suited to reinforcement learning, via handling the attacks in the decision space (as opposed to data space) and directly mitigating learned bias introduced by the adversary. In MLAH, we learn separate sub-policies (nominal and adversarial) in an online manner, as guided by a supervisory master agent that detects the presence of the adversary by leveraging the advantage function for the sub-policies. We demonstrate that the proposed algorithm enables policy learning with significantly lower bias as compared to the state-of-the-art policy learning approaches even in the presence of heavy state information attacks. We present algorithm analysis and simulation results using popular OpenAI Gym environments.",
                        "Citation Paper Authors": "Authors:Aaron J. Havens, Zhanhong Jiang, Soumik Sarkar"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "highlight the observation that\nmodi\ufb01cations imperceptible to humans can cause deep neural\nnetworks to misclassify, and they design the Fast Gradient\nSign Method ",
                    "Citation Text": "I. J. Goodfellow, J. Shlens, and C. Szegedy, \u201cExplaining and Harnessing\nAdversarial Examples,\u201d arXiv:1412.6572 , 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.6572",
                        "Citation Paper Title": "Title:Explaining and Harnessing Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1909.09836v2": {
            "Paper Title": "Optimal query complexity for private sequential learning against\n  eavesdropping",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.03829v5": {
            "Paper Title": "Amora: Black-box Adversarial Morphing Attack",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "propose AdvFaces that learns to generate minimal pertur-\nbations in the salient facial regions via GAN. Dong et al. ",
                    "Citation Text": "Yinpeng Dong, Hang Su, Baoyuan Wu, Zhifeng Li, Wei Liu, Tong Zhang, and\nJun Zhu. 2019. Efficient Decision-based Black-box Adversarial Attacks on Face\nRecognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition . 7714\u20137722.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.04433",
                        "Citation Paper Title": "Title:Efficient Decision-based Black-box Adversarial Attacks on Face Recognition",
                        "Citation Paper Abstract": "Abstract:Face recognition has obtained remarkable progress in recent years due to the great improvement of deep convolutional neural networks (CNNs). However, deep CNNs are vulnerable to adversarial examples, which can cause fateful consequences in real-world face recognition applications with security-sensitive purposes. Adversarial attacks are widely studied as they can identify the vulnerability of the models before they are deployed. In this paper, we evaluate the robustness of state-of-the-art face recognition models in the decision-based black-box attack setting, where the attackers have no access to the model parameters and gradients, but can only acquire hard-label predictions by sending queries to the target model. This attack setting is more practical in real-world face recognition systems. To improve the efficiency of previous methods, we propose an evolutionary attack algorithm, which can model the local geometries of the search directions and reduce the dimension of the search space. Extensive experiments demonstrate the effectiveness of the proposed method that induces a minimum perturbation to an input face image with fewer queries. We also apply the proposed method to attack a real-world face recognition system successfully.",
                        "Citation Paper Authors": "Authors:Yinpeng Dong, Hang Su, Baoyuan Wu, Zhifeng Li, Wei Liu, Tong Zhang, Jun Zhu"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "propose a GAN, which adds a conditional variational autoen-\ncoder and attention modules, for generating fake faces [ 22,52]. Deb\net al. ",
                    "Citation Text": "Debayan Deb, Jianbang Zhang, and Anil K Jain. 2019. Advfaces: Adversarial face\nsynthesis. arXiv preprint arXiv:1908.05008 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.05008",
                        "Citation Paper Title": "Title:AdvFaces: Adversarial Face Synthesis",
                        "Citation Paper Abstract": "Abstract:Face recognition systems have been shown to be vulnerable to adversarial examples resulting from adding small perturbations to probe images. Such adversarial images can lead state-of-the-art face recognition systems to falsely reject a genuine subject (obfuscation attack) or falsely match to an impostor (impersonation attack). Current approaches to crafting adversarial face images lack perceptual quality and take an unreasonable amount of time to generate them. We propose, AdvFaces, an automated adversarial face synthesis method that learns to generate minimal perturbations in the salient facial regions via Generative Adversarial Networks. Once AdvFaces is trained, it can automatically generate imperceptible perturbations that can evade state-of-the-art face matchers with attack success rates as high as 97.22% and 24.30% for obfuscation and impersonation attacks, respectively.",
                        "Citation Paper Authors": "Authors:Debayan Deb, Jianbang Zhang, Anil K. Jain"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": "also generate adver-\nsarial attacks by solving the optimization constraints based on a\ngenerator network. These techniques are white-box attack, which\ncan be unrealistic in real-world applications. Additionally, some\nGAN-based attacking techniques have been proposed. Song et al. ",
                    "Citation Text": "Qing Song, Yingqi Wu, and Lu Yang. 2018. Attacks on State-of-the-Art Face\nRecognition using Attentional Adversarial Attack Generative Network. arXiv\npreprint arXiv:1811.12026 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.12026",
                        "Citation Paper Title": "Title:Attacks on State-of-the-Art Face Recognition using Attentional Adversarial Attack Generative Network",
                        "Citation Paper Abstract": "Abstract:With the broad use of face recognition, its weakness gradually emerges that it is able to be attacked. So, it is important to study how face recognition networks are subject to attacks. In this paper, we focus on a novel way to do attacks against face recognition network that misleads the network to identify someone as the target person not misclassify inconspicuously. Simultaneously, for this purpose, we introduce a specific attentional adversarial attack generative network to generate fake face images. For capturing the semantic information of the target person, this work adds a conditional variational autoencoder and attention modules to learn the instance-level correspondences between faces. Unlike traditional two-player GAN, this work introduces face recognition networks as the third player to participate in the competition between generator and discriminator which allows the attacker to impersonate the target person better. The generated faces which are hard to arouse the notice of onlookers can evade recognition by state-of-the-art networks and most of them are recognized as the target person.",
                        "Citation Paper Authors": "Authors:Qing Song, Yingqi Wu, Lu Yang"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "utilize the predic-\ntion score to estimate the gradients of target model. They applied\nzeroth-order optimization and stochastic coordinate descent along\nwith various tricks to decrease sample complexity and improve its\nefficiency. Ilyas et al. ",
                    "Citation Text": "Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. 2018. Black-\nbox adversarial attacks with limited queries and information. arXiv preprint\narXiv:1804.08598 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.08598",
                        "Citation Paper Title": "Title:Black-box Adversarial Attacks with Limited Queries and Information",
                        "Citation Paper Abstract": "Abstract:Current neural network-based classifiers are susceptible to adversarial examples even in the black-box setting, where the attacker only has query access to the model. In practice, the threat model for real-world systems is often more restrictive than the typical black-box model where the adversary can observe the full output of the network on arbitrarily many chosen inputs. We define three realistic threat models that more accurately characterize many real-world classifiers: the query-limited setting, the partial-information setting, and the label-only setting. We develop new attacks that fool classifiers under these more restrictive threat models, where previous methods would be impractical or ineffective. We demonstrate that our methods are effective against an ImageNet classifier under our proposed threat models. We also demonstrate a targeted black-box attack against a commercial classifier, overcoming the challenges of limited query access, partial information, and other practical issues to break the Google Cloud Vision API.",
                        "Citation Paper Authors": "Authors:Andrew Ilyas, Logan Engstrom, Anish Athalye, Jessy Lin"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "propose a local-search method to\napproximate the network gradients, which was then used to select a\nsmall fraction of pixels to perturb. Chen et al. ",
                    "Citation Text": "Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. 2017.\nZoo: Zeroth order optimization based black-box attacks to deep neural networks\nwithout training substitute models. In Proceedings of the 10th ACM Workshop on\nArtificial Intelligence and Security . ACM, 15\u201326.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.03999",
                        "Citation Paper Title": "Title:ZOO: Zeroth Order Optimization based Black-box Attacks to Deep Neural Networks without Training Substitute Models",
                        "Citation Paper Abstract": "Abstract:Deep neural networks (DNNs) are one of the most prominent technologies of our time, as they achieve state-of-the-art performance in many machine learning tasks, including but not limited to image classification, text mining, and speech processing. However, recent research on DNNs has indicated ever-increasing concern on the robustness to adversarial examples, especially for security-critical tasks such as traffic sign identification for autonomous driving. Studies have unveiled the vulnerability of a well-trained DNN by demonstrating the ability of generating barely noticeable (to both human and machines) adversarial images that lead to misclassification. Furthermore, researchers have shown that these adversarial images are highly transferable by simply training and attacking a substitute model built upon the target model, known as a black-box attack to DNNs.\nSimilar to the setting of training substitute models, in this paper we propose an effective black-box attack that also only has access to the input (images) and the output (confidence scores) of a targeted DNN. However, different from leveraging attack transferability from substitute models, we propose zeroth order optimization (ZOO) based attacks to directly estimate the gradients of the targeted DNN for generating adversarial examples. We use zeroth order stochastic coordinate descent along with dimension reduction, hierarchical attack and importance sampling techniques to efficiently attack black-box models. By exploiting zeroth order optimization, improved attacks to the targeted DNN can be accomplished, sparing the need for training substitute models and avoiding the loss in attack transferability. Experimental results on MNIST, CIFAR10 and ImageNet show that the proposed ZOO attack is as effective as the state-of-the-art white-box attack and significantly outperforms existing black-box attacks via substitute models.",
                        "Citation Paper Authors": "Authors:Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, Cho-Jui Hsieh"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "Adversarial Noise Attacks: The FR systems to be tackled in this\nwork are all deep learning based ones. Studies have demonstrated\nthat deep neural networks are vulnerable to adversarial examples\nthat are widely found in image ",
                    "Citation Text": "Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and\nharnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.6572",
                        "Citation Paper Title": "Title:Explaining and Harnessing Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.04222v3": {
            "Paper Title": "Implementing the Exponential Mechanism with Base-2 Differential Privacy",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.04255v3": {
            "Paper Title": "Communication-efficient Certificate Revocation Management for Advanced\n  Metering Infrastructure and IoT",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.01798v2": {
            "Paper Title": "SquirRL: Automating Attack Analysis on Blockchain Incentive Mechanisms\n  with Deep Reinforcement Learning",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": ") uses\nMDP solving to compute optimal sel\ufb01sh mining strategies.These exact solutions are less robust to unexpected, real-time\nchanges in honest hashpower than our RL-based approach.\nAn enhanced model with two sel\ufb01sh agents and one honest\nagent is considered in ",
                    "Citation Text": "Qianlan Bai, Xinyan Zhou, Xing Wang, Yuedong Xu, Xin Wang, and\nQingsheng Kong. A deep dive into blockchain sel\ufb01sh mining. arXiv\npreprint arXiv:1811.08263 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.08263",
                        "Citation Paper Title": "Title:A Deep Dive into Blockchain Selfish Mining",
                        "Citation Paper Abstract": "Abstract:This paper studies a fundamental problem regarding the security of blockchain on how the existence of multiple misbehaving pools influences the profitability of selfish mining. Each selfish miner maintains a private chain and makes it public opportunistically for the purpose of acquiring more rewards incommensurate to his Hashrate. We establish a novel Markov chain model to characterize all the state transitions of public and private chains. The minimum requirement of Hashrate together with the minimum delay of being profitable is derived in close-form. The former reduces to 21.48% with the symmetric selfish miners, while their competition with asymmetric Hashrates puts forward a higher requirement of the profitable threshold. The profitable delay increases with the decrease of the Hashrate of selfish miners, making the mining pools more cautious on performing selfish mining.",
                        "Citation Paper Authors": "Authors:Qianlan Bai, Xinyan Zhou, Xing Wang, Yuedong Xu, Xin Wang, Qingsheng Kong"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": ", and we found\nit to be more stable than the alternatives in the multi-agent\nsetting.\nG. Implementation\nWe used OpenAI Gym ",
                    "Citation Text": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider,\nJohn Schulman, Jie Tang, and Wojciech Zaremba. OpenAI gym, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.01540",
                        "Citation Paper Title": "Title:OpenAI Gym",
                        "Citation Paper Abstract": "Abstract:OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.",
                        "Citation Paper Authors": "Authors:Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1902.04111v5": {
            "Paper Title": "Statistical Model Checking for Hyperproperties",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.07138v6": {
            "Paper Title": "Who started this rumor? Quantifying the natural differential privacy\n  guarantees of gossip protocols",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.08551v3": {
            "Paper Title": "Ring Learning With Errors: A crossroads between postquantum\n  cryptography, machine learning and number theory",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.11515v3": {
            "Paper Title": "Improving Utility and Security of the Shuffler-based Differential\n  Privacy",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": "considered\nan intriguing removal-based LDP de\ufb01nition and work in the\nshuf\ufb02er model. Besides estimating histograms, the problem of\n12estimating the sum of numerical values are also extensively\ninvestigated ",
                    "Citation Text": "B. Ghazi, P. Manurangsi, R. Pagh, and A. Velingker, \u201cPri-\nvate aggregation from fewer anonymous messages,\u201d arXiv preprint\narXiv:1909.11073 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.11073",
                        "Citation Paper Title": "Title:Private Aggregation from Fewer Anonymous Messages",
                        "Citation Paper Abstract": "Abstract:Consider the setup where $n$ parties are each given a number $x_i \\in \\mathbb{F}_q$ and the goal is to compute the sum $\\sum_i x_i$ in a secure fashion and with as little communication as possible. We study this problem in the anonymized model of Ishai et al. (FOCS 2006) where each party may broadcast anonymous messages on an insecure channel.\nWe present a new analysis of the one-round \"split and mix\" protocol of Ishai et al. In order to achieve the same security parameter, our analysis reduces the required number of messages by a $\\Theta(\\log n)$ multiplicative factor. We complement our positive result with lower bounds showing that the dependence of the number of messages on the domain size, the number of parties, and the security parameter is essentially tight.\nUsing a reduction of Balle et al. (2019), our improved analysis of the protocol of Ishai et al. yields, in the same model, an $\\left(\\varepsilon, \\delta\\right)$-differentially private protocol for aggregation that, for any constant $\\varepsilon > 0$ and any $\\delta = \\frac{1}{\\mathrm{poly}(n)}$, incurs only a constant error and requires only a constant number of messages per party. Previously, such a protocol was known only for $\\Omega(\\log n)$ messages per party.",
                        "Citation Paper Authors": "Authors:Badih Ghazi, Pasin Manurangsi, Rasmus Pagh, Ameya Velingker"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.03076v3": {
            "Paper Title": "TeleHammer: A Formal Model of Implicit Rowhammer",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "proposed a\nsingle-sided hammering by randomly picking multiple ad-\ndresses and just hammering them with the hope that such\naddresses are in different rows within the same bank.\nOne-location hammering: one-location hammering ",
                    "Citation Text": "Daniel Gruss, Moritz Lipp, Michael Schwarz, Daniel Genkin, Jonas\nJuf\ufb01nger, Sioli O\u2019Connell, Wolfgang Schoechl, and Yuval Yarom.\nAnother \ufb02ip in the wall of rowhammer defenses. arXiv preprint\narXiv:1710.00551 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.00551",
                        "Citation Paper Title": "Title:Another Flip in the Wall of Rowhammer Defenses",
                        "Citation Paper Abstract": "Abstract:The Rowhammer bug allows unauthorized modification of bits in DRAM cells from unprivileged software, enabling powerful privilege-escalation attacks. Sophisticated Rowhammer countermeasures have been presented, aiming at mitigating the Rowhammer bug or its exploitation. However, the state of the art provides insufficient insight on the completeness of these defenses. In this paper, we present novel Rowhammer attack and exploitation primitives, showing that even a combination of all defenses is ineffective. Our new attack technique, one-location hammering, breaks previous assumptions on requirements for triggering the Rowhammer bug, i.e., we do not hammer multiple DRAM rows but only keep one DRAM row constantly open. Our new exploitation technique, opcode flipping, bypasses recent isolation mechanisms by flipping bits in a predictable and targeted way in userspace binaries. We replace conspicuous and memory-exhausting spraying and grooming techniques with a novel reliable technique called memory waylaying. Memory waylaying exploits system-level optimizations and a side channel to coax the operating system into placing target pages at attacker-chosen physical locations. Finally, we abuse Intel SGX to hide the attack entirely from the user and the operating system, making any inspection or detection of the attack infeasible. Our Rowhammer enclave can be used for coordinated denial-of-service attacks in the cloud and for privilege escalation on personal computers. We demonstrate that our attacks evade all previously proposed countermeasures for commodity systems.",
                        "Citation Paper Authors": "Authors:Daniel Gruss, Moritz Lipp, Michael Schwarz, Daniel Genkin, Jonas Juffinger, Sioli O'Connell, Wolfgang Schoechl, Yuval Yarom"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1909.07917v2": {
            "Paper Title": "Toward Efficient Evaluation of Logic Encryption Schemes: Models and\n  Metrics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.00049v3": {
            "Paper Title": "Square Attack: a query-efficient black-box adversarial attack via random\n  search",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.01430v2": {
            "Paper Title": "When Attackers Meet AI: Learning-empowered Attacks in Cooperative\n  Spectrum Sensing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.00038v2": {
            "Paper Title": "Context-Aware Local Differential Privacy",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": ", we prove this lower bound in Section 5.2.\n5.1 Achievability using a variant of HR\nWe propose an algorithm based on Hadamard response ",
                    "Citation Text": "J. Acharya, Z. Sun, and H. Zhang, \\Hadamard response: Estimating distributions privately,\ne\u000eciently, and with little communication,\" in Proceedings of Machine Learning Research , ser.\nProceedings of Machine Learning Research, K. Chaudhuri and M. Sugiyama, Eds., vol. 89.\nPMLR, 16{18 Apr 2019, pp. 1120{1129.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.04705",
                        "Citation Paper Title": "Title:Hadamard Response: Estimating Distributions Privately, Efficiently, and with Little Communication",
                        "Citation Paper Abstract": "Abstract:We study the problem of estimating $k$-ary distributions under $\\varepsilon$-local differential privacy. $n$ samples are distributed across users who send privatized versions of their sample to a central server. All previously known sample optimal algorithms require linear (in $k$) communication from each user in the high privacy regime $(\\varepsilon=O(1))$, and run in time that grows as $n\\cdot k$, which can be prohibitive for large domain size $k$.\nWe propose Hadamard Response (HR}, a local privatization scheme that requires no shared randomness and is symmetric with respect to the users. Our scheme has order optimal sample complexity for all $\\varepsilon$, a communication of at most $\\log k+2$ bits per user, and nearly linear running time of $\\tilde{O}(n + k)$.\nOur encoding and decoding are based on Hadamard matrices, and are simple to implement. The statistical performance relies on the coding theoretic aspects of Hadamard matrices, ie, the large Hamming distance between the rows. An efficient implementation of the algorithm using the Fast Walsh-Hadamard transform gives the computational gains.\nWe compare our approach with Randomized Response (RR), RAPPOR, and subset-selection mechanisms (SS), both theoretically, and experimentally. For $k=10000$, our algorithm runs about 100x faster than SS, and RAPPOR.",
                        "Citation Paper Authors": "Authors:Jayadev Acharya, Ziteng Sun, Huanyu Zhang"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": ",\nwhich also considered distribution estimation and proposed an algorithm based on RAPPOR ",
                    "Citation Text": "\u0013U. Erlingsson, V. Pihur, and A. Korolova, \\Rappor: Randomized aggregatable privacy-\npreserving ordinal response,\" in Proceedings of the 2014 ACM SIGSAC conference on computer\nand communications security . ACM, 2014, pp. 1054{1067.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1407.6981",
                        "Citation Paper Title": "Title:RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response",
                        "Citation Paper Abstract": "Abstract:Randomized Aggregatable Privacy-Preserving Ordinal Response, or RAPPOR, is a technology for crowdsourcing statistics from end-user client software, anonymously, with strong privacy guarantees. In short, RAPPORs allow the forest of client data to be studied, without permitting the possibility of looking at individual trees. By applying randomized response in a novel manner, RAPPOR provides the mechanisms for such collection as well as for efficient, high-utility analysis of the collected data. In particular, RAPPOR permits statistics to be collected on the population of client-side strings with strong privacy guarantees for each client, and without linkability of their reports. This paper describes and motivates RAPPOR, details its differential-privacy and utility guarantees, discusses its practical deployment and properties in the face of different attack models, and, finally, gives results of its application to both synthetic and real-world data.",
                        "Citation Paper Authors": "Authors:\u00dalfar Erlingsson, Vasyl Pihur, Aleksandra Korolova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1902.03083v2": {
            "Paper Title": "Hide and Speak: Towards Deep Neural Networks for Speech Steganography",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "0.0047 27.99 0.066 1.05\nOurs 0.0016 27.86 0.028 8.16\nOurs + Adv. 0.0016 31.18 0.033 6.00\nreconstruction losses using \u0015c= 3,\u0015m= 1. Each compo-\nnent in our model is implemented as a Gated Convolutional\nNeural Network as proposed by ",
                    "Citation Text": "Yann N Dauphin, Angela Fan, Michael Auli, and David Grang-\nier, \u201cLanguage modeling with gated convolutional networks,\u201d in\nProceedings of the 34th International Conference on Machine\nLearning-Volume 70 . JMLR. org, 2017, pp. 933\u2013941.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.08083",
                        "Citation Paper Title": "Title:Language Modeling with Gated Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:The pre-dominant approach to language modeling to date is based on recurrent neural networks. Their success on this task is often linked to their ability to capture unbounded context. In this paper we develop a finite context approach through stacked convolutions, which can be more efficient since they allow parallelization over sequential tokens. We propose a novel simplified gating mechanism that outperforms Oord et al (2016) and investigate the impact of key architectural decisions. The proposed approach achieves state-of-the-art on the WikiText-103 benchmark, even though it features long-term dependencies, as well as competitive results on the Google Billion Words benchmark. Our model reduces the latency to score a sentence by an order of magnitude compared to a recurrent baseline. To our knowledge, this is the first time a non-recurrent approach is competitive with strong recurrent models on these large scale language tasks.",
                        "Citation Paper Authors": "Authors:Yann N. Dauphin, Angela Fan, Michael Auli, David Grangier"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1909.10057v2": {
            "Paper Title": "Secured Traffic Monitoring in VANET",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.06919v3": {
            "Paper Title": "Improving Black-box Adversarial Attacks with a Transfer-based Prior",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": ". The experimental settings are the same\nwith those of attacking the normal models in Sec. 4.2. In our method, we use a smoothed version of\nthe transfer gradient ",
                    "Citation Text": "Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Evading defenses to transferable adversarial examples\nby translation-invariant attacks. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.02884",
                        "Citation Paper Title": "Title:Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks",
                        "Citation Paper Abstract": "Abstract:Deep neural networks are vulnerable to adversarial examples, which can mislead classifiers by adding imperceptible perturbations. An intriguing property of adversarial examples is their good transferability, making black-box attacks feasible in real-world applications. Due to the threat of adversarial attacks, many methods have been proposed to improve the robustness. Several state-of-the-art defenses are shown to be robust against transferable adversarial examples. In this paper, we propose a translation-invariant attack method to generate more transferable adversarial examples against the defense models. By optimizing a perturbation over an ensemble of translated images, the generated adversarial example is less sensitive to the white-box model being attacked and has better transferability. To improve the efficiency of attacks, we further show that our method can be implemented by convolving the gradient at the untranslated image with a pre-defined kernel. Our method is generally applicable to any gradient-based attack method. Extensive experiments on the ImageNet dataset validate the effectiveness of the proposed method. Our best attack fools eight state-of-the-art defenses at an 82% success rate on average based only on the transferability, demonstrating the insecurity of the current defense techniques.",
                        "Citation Paper Authors": "Authors:Yinpeng Dong, Tianyu Pang, Hang Su, Jun Zhu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1908.10258v2": {
            "Paper Title": "Infochain: A Decentralized, Trustless and Transparent Oracle on\n  Blockchain",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.06441v3": {
            "Paper Title": "Building Scalable Decentralized Payment Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.14667v2": {
            "Paper Title": "Making an Invisibility Cloak: Real World Adversarial Attacks on Object\n  Detectors",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.11607v3": {
            "Paper Title": "Deep Learning with Gaussian Differential Privacy",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": "There are continued e\u000borts to understand how privacy degrades under composition. Developments\nalong this line include the basic composition theorem and the advanced composition theorem [22,\n24]. In a pioneering work, ",
                    "Citation Text": "P. Kairouz, S. Oh, and P. Viswanath. The composition theorem for di\u000berential privacy. IEEE Trans-\nactions on Information Theory , 63(6):4037{4049, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1311.0776",
                        "Citation Paper Title": "Title:The Composition Theorem for Differential Privacy",
                        "Citation Paper Abstract": "Abstract:Sequential querying of differentially private mechanisms degrades the overall privacy level. In this paper, we answer the fundamental question of characterizing the level of overall privacy degradation as a function of the number of queries and the privacy levels maintained by each privatization mechanism. Our solution is complete: we prove an upper bound on the overall privacy level and construct a sequence of privatization mechanisms that achieves this bound. The key innovation is the introduction of an operational interpretation of differential privacy (involving hypothesis testing) and the use of new data processing inequalities. Our result improves over the state-of-the-art, and has immediate applications in several problems studied in the literature including differentially private multi-party computation.",
                        "Citation Paper Authors": "Authors:Peter Kairouz, Sewoong Oh, Pramod Viswanath"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1901.04879v3": {
            "Paper Title": "The Smart$^2$ Speaker Blocker: An Open-Source Privacy Filter for\n  Connected Home Speakers",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": ". Research into Apple\u2019s\nimplementation of differential privacy in macOS and iOS has\nshown a signi\ufb01cant lack of transparency in the implementation\namong other shortcomings ",
                    "Citation Text": "J. Tang, A. Korolova, X. Bai, X. Wang, and X. Wang, \u201cPrivacy loss in\nApple\u2019s implementation of differential privacy on macOS 10.12,\u201d arXiv\npreprint arXiv:1709.02753 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.02753",
                        "Citation Paper Title": "Title:Privacy Loss in Apple's Implementation of Differential Privacy on MacOS 10.12",
                        "Citation Paper Abstract": "Abstract:In June 2016, Apple announced that it will deploy differential privacy for some user data collection in order to ensure privacy of user data, even from Apple. The details of Apple's approach remained sparse. Although several patents have since appeared hinting at the algorithms that may be used to achieve differential privacy, they did not include a precise explanation of the approach taken to privacy parameter choice. Such choice and the overall approach to privacy budget use and management are key questions for understanding the privacy protections provided by any deployment of differential privacy.\nIn this work, through a combination of experiments, static and dynamic code analysis of macOS Sierra (Version 10.12) implementation, we shed light on the choices Apple made for privacy budget management. We discover and describe Apple's set-up for differentially private data processing, including the overall data pipeline, the parameters used for differentially private perturbation of each piece of data, and the frequency with which such data is sent to Apple's servers.\nWe find that although Apple's deployment ensures that the (differential) privacy loss per each datum submitted to its servers is $1$ or $2$, the overall privacy loss permitted by the system is significantly higher, as high as $16$ per day for the four initially announced applications of Emojis, New words, Deeplinks and Lookup Hints. Furthermore, Apple renews the privacy budget available every day, which leads to a possible privacy loss of 16 times the number of days since user opt-in to differentially private data collection for those four applications.\nWe advocate that in order to claim the full benefits of differentially private data collection, Apple must give full transparency of its implementation, enable user choice in areas related to privacy loss, and set meaningful defaults on the privacy loss permitted.",
                        "Citation Paper Authors": "Authors:Jun Tang, Aleksandra Korolova, Xiaolong Bai, Xueqiang Wang, Xiaofeng Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1909.10926v3": {
            "Paper Title": "ABC: Proof-of-Stake without Consensus",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": "providesanasynchronouspermissione d\nsystembyrelyingonadvancedcryptographictechniqueswit hfull\nconsensus.Again, themaindi\ufb00erences fromABCarethatthes ys-\ntemis permissioned,muchmoreinvolved, andreliant onrand om-\nization.\nTheauthorsof ",
                    "Citation Text": "Rachid Guerraoui, Jovan Komatovic, and Dragos-Adrian S eredinschi. 2020.\nDynamic Byzantine Reliable Broadcast [Technical Report]. arXiv preprint\narXiv:2001.06271 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2001.06271",
                        "Citation Paper Title": "Title:Dynamic Byzantine Reliable Broadcast [Technical Report]",
                        "Citation Paper Abstract": "Abstract:Reliable broadcast is a communication primitive guaranteeing, intuitively, that all processes in a distributed system deliver the same set of messages. The reason why this primitive is appealing is twofold: (i) we can implement it deterministically in a completely asynchronous environment, unlike stronger primitives like consensus and total-order broadcast, and yet (ii) reliable broadcast is powerful enough to implement important applications like payment systems.\nThe problem we tackle in this paper is that of dynamic reliable broadcast, i.e., enabling processes to join or leave the system. This property is desirable for long-lived applications (aiming to be highly available), yet has been precluded in previous asynchronous reliable broadcast protocols. We study this property in a general adversarial (i.e., Byzantine) environment.\nWe introduce the first specification of a dynamic Byzantine reliable broadcast (DBRB) primitive that is amenable to an asynchronous implementation. We then present an algorithm implementing this specification in an asynchronous network. Our DBRB algorithm ensures that if any correct process in the system broadcasts a message, then every correct process delivers that message unless it leaves the system. Moreover, if a correct process delivers a message, then every correct process that has not expressed its will to leave the system delivers that message. We assume that more than $2/3$ of processes in the system are correct at all times, which is tight in our context.\nWe also show that if only one process in the system can fail---and it can fail only by crashing---then it is impossible to implement a stronger primitive, ensuring that if any correct process in the system broadcasts or delivers a message, then every correct process in the system delivers that message---including those that leave.",
                        "Citation Paper Authors": "Authors:Rachid Guerraoui, Jovan Komatovic, Petr Kuznetsov, Yvonne-Anne Pignolet, Dragos-Adrian Seredinschi, Andrei Tonkikh"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "proposes a permissioned\ntransactionsystem thatdoesnotrelyonconsensus.Inthisd esign,\nastaticsetofvalidatorsisdesignatedtocon\ufb01rmtransacti ons.Our\nconcepts of Section 7 (parallization, fees, etc.) do work in the per-\nmissioned settingas well,and couldbeappliedtothis work.\nTheauthorsof ",
                    "Citation Text": "Rachid Guerraoui, Petr Kuznetsov, Matteo Monti, Matej P avlovi\u010d, and Dragos-\nAdrian Seredinschi. 2019. The Consensus Numberof a Cryptoc urrency.In Pro-\nceedings of the 2019 ACM Symposium on Principles of Distribut ed Computing .\nACM,307\u2013316.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.05574",
                        "Citation Paper Title": "Title:The Consensus Number of a Cryptocurrency (Extended Version)",
                        "Citation Paper Abstract": "Abstract:Many blockchain-based algorithms, such as Bitcoin, implement a decentralized asset transfer system, often referred to as a cryptocurrency. As stated in the original paper by Nakamoto, at the heart of these systems lies the problem of preventing double-spending; this is usually solved by achieving consensus on the order of transfers among the participants. In this paper, we treat the asset transfer problem as a concurrent object and determine its consensus number, showing that consensus is, in fact, not necessary to prevent double-spending. We first consider the problem as defined by Nakamoto, where only a single process---the account owner---can withdraw from each account. Safety and liveness need to be ensured for correct account owners, whereas misbehaving account owners might be unable to perform transfers. We show that the consensus number of an asset transfer object is $1$. We then consider a more general $k$-shared asset transfer object where up to $k$ processes can atomically withdraw from the same account, and show that this object has consensus number $k$. We establish our results in the context of shared memory with benign faults, allowing us to properly understand the level of difficulty of the asset transfer problem. We also translate these results in the message passing setting with Byzantine players, a model that is more relevant in practice. In this model, we describe an asynchronous Byzantine fault-tolerant asset transfer implementation that is both simpler and more efficient than state-of-the-art consensus-based solutions. Our results are applicable to both the permissioned (private) and permissionless (public) setting, as normally their differentiation is hidden by the abstractions on top of which our algorithms are based.",
                        "Citation Paper Authors": "Authors:Rachid Guerraoui, Petr Kuznetsov, Matteo Monti, Matej Pavlovic, Dragos-Adrian Seredinschi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1907.02143v15": {
            "Paper Title": "Key Event Receipt Infrastructure (KERI)",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.00922v3": {
            "Paper Title": "Type-based Declassification for Free",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.05077v4": {
            "Paper Title": "Fog Computing Systems: State of the Art, Research Issues and Future\n  Trends, with a Focus on Resilience",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.01452v3": {
            "Paper Title": "Pan-Private Uniformity Testing",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": "showed that noninteractive \u03b5-locally private\nuniformity testing has sample complexity \u0398/parenleftbigk\n\u03b12\u03b52/parenrightbig\n. Acharya et al. ",
                    "Citation Text": "Jayadev Acharya, Cl\u00b4 ement L Canonne, Yanjun Han, Ziteng Su n, and Himanshu Tyagi.\nDomain compression and its application to randomness-optimal distr ibuted goodness-of-\n\ufb01t.arXiv preprint arXiv:1907.08743 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.08743",
                        "Citation Paper Title": "Title:Domain Compression and its Application to Randomness-Optimal Distributed Goodness-of-Fit",
                        "Citation Paper Abstract": "Abstract:We study goodness-of-fit of discrete distributions in the distributed setting, where samples are divided between multiple users who can only release a limited amount of information about their samples due to various information constraints. Recently, a subset of the authors showed that having access to a common random seed (i.e., shared randomness) leads to a significant reduction in the sample complexity of this problem. In this work, we provide a complete understanding of the interplay between the amount of shared randomness available, the stringency of information constraints, and the sample complexity of the testing problem by characterizing a tight trade-off between these three parameters. We provide a general distributed goodness-of-fit protocol that as a function of the amount of shared randomness interpolates smoothly between the private- and public-coin sample complexities. We complement our upper bound with a general framework to prove lower bounds on the sample complexity of this testing problems under limited shared randomness. Finally, we instantiate our bounds for the two archetypal information constraints of communication and local privacy, and show that our sample complexity bounds are optimal as a function of all the parameters of the problem, including the amount of shared randomness.\nA key component of our upper bounds is a new primitive of domain compression, a tool that allows us to map distributions to a much smaller domain size while preserving their pairwise distances, using a limited amount of randomness.",
                        "Citation Paper Authors": "Authors:Jayadev Acharya, Cl\u00e9ment L. Canonne, Yanjun Han, Ziteng Sun, Himanshu Tyagi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.05021v3": {
            "Paper Title": "Design and Interpretation of Universal Adversarial Patches in Face\n  Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06122v3": {
            "Paper Title": "FakeSpotter: A Simple yet Robust Baseline for Spotting AI-Synthesized\n  Fake Faces",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.07275v2": {
            "Paper Title": "PrivacyGuard: Enforcing Private Data Usage Control with Blockchain and\n  Attested Off-chain Contract Execution",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.00888v3": {
            "Paper Title": "Zipper Stack: Shadow Stacks Without Shadow",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.09432v3": {
            "Paper Title": "A Cryptoeconomic Traffic Analysis of Bitcoin's Lightning Network",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": ". Single-\nintermediary payments do not provide privacy, although they have higher utility. Tang et al. asserts that a\nPCN either operates in a low-privacy or a low-utility regime ",
                    "Citation Text": "Weizhao Tang, Weina Wang, Giulia Fanti, and Sewoong Oh. Privacy-utility tradeo\u000bs in routing cryp-\ntocurrency over payment channel networks, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.02717",
                        "Citation Paper Title": "Title:Privacy-Utility Tradeoffs in Routing Cryptocurrency over Payment Channel Networks",
                        "Citation Paper Abstract": "Abstract:Payment channel networks (PCNs) are viewed as one of the most promising scalability solutions for cryptocurrencies today. Roughly, PCNs are networks where each node represents a user and each directed, weighted edge represents funds escrowed on a blockchain; these funds can be transacted only between the endpoints of the edge. Users efficiently transmit funds from node A to B by relaying them over a path connecting A to B, as long as each edge in the path contains enough balance (escrowed funds) to support the transaction. Whenever a transaction succeeds, the edge weights are updated accordingly. However, in deployed PCNs, channel balances (i.e., edge weights) are not revealed to users for privacy reasons; users know only the initial weights at time 0. Hence, when routing transactions, users first guess a path, then check if it supports the transaction. This guess-and-check process dramatically reduces the success rate of transactions. At the other extreme, knowing full channel balances can give substantial improvements in success rate at the expense of privacy. In this work, we study whether a network can reveal noisy channel balances to trade off privacy for utility. We show fundamental limits on such a tradeoff, and propose noise mechanisms that achieve the fundamental limit for a general class of graph topologies. Our results suggest that in practice, PCNs should operate either in the low-privacy or low-utility regime; it is not possible to get large gains in utility by giving up a little privacy, or large gains in privacy by sacrificing a little utility.",
                        "Citation Paper Authors": "Authors:Weizhao Tang, Weina Wang, Giulia Fanti, Sewoong Oh"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "to generate and analyze Barab\u0013 asi-Albert graphs as underlying networks. CLoTH ",
                    "Citation Text": "Marco Conoscenti, Antonio Vetr\u0012 o, Juan De Martin, and Federico Spini. The cloth simulator for htlc\npayment networks with introductory lightning network performance results. Information , 9(9):223, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.09940",
                        "Citation Paper Title": "Title:CLoTH: a Simulator for HTLC Payment Networks",
                        "Citation Paper Abstract": "Abstract:The Lightning Network (LN) is one of the most promising off-chain scaling solutions for Bitcoin, as it enables off-chain payments which are not subject to the well-known blockchain scalability limit. In this work, we introduce CLoTH, a simulator for HTLC payment networks, of which LN is the best working example. It simulates input-defined payments on an input-defined HTLC network and produces performance measures in terms of payment-related statistics, such as time to complete payments and probability of payment failure. CLoTH helps to predict issues that might arise in the development of an HTLC payment network, and to estimate the effects of an optimisation before deploying it. In upcoming works we'll publish the results of CLoTH simulations.",
                        "Citation Paper Authors": "Authors:Marco Conoscenti, Antonio Vetr\u00f2, Juan Carlos De Martin, Federico Spini, Fabio Castaldo, Sebastiano Scr\u00f2fina"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "is the only one\nthat has pointers to publicly available resources. Their simulator only considers single bidirectional channels\nor a star topology, and its main goal is to analyze channel opening costs and depletion. This simulator is\nextended in ",
                    "Citation Text": "Felix Engelmann, Henning Kopp, Frank Kargl, Florian Glaser, and Christof Weinhardt. Towards an\neconomic analysis of routing in payment channel networks. In Proceedings of the 1st Workshop on\nScalable and Resilient Infrastructures for Distributed Ledgers , page 2. ACM, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.02597",
                        "Citation Paper Title": "Title:Towards an Economic Analysis of Routing in Payment Channel Networks",
                        "Citation Paper Abstract": "Abstract:Payment channel networks are supposed to overcome technical scalability limitations of blockchain infrastructure by employing a special overlay network with fast payment confirmation and only sporadic settlement of netted transactions on the blockchain. However, they introduce economic routing constraints that limit decentralized scalability and are currently not well understood. In this paper, we model the economic incentives for participants in payment channel networks. We provide the first formal model of payment channel economics and analyze how the cheapest path can be found. Additionally, our simulation assesses the long-term evolution of a payment channel network. We find that even for small routing fees, sometimes it is cheaper to settle the transaction directly on the blockchain.",
                        "Citation Paper Authors": "Authors:Felix Engelmann, Florian Glaser, Henning Kopp, Frank Kargl, Christof Weinhardt"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": ". In a similar\ngame-theoretic work, the e\u000bect of routing fees was analyzed ",
                    "Citation Text": "Georgia Avarikioti, Gerrit Janssen, Yuyi Wang, and Roger Wattenhofer. Payment network design with\nfees. In Data Privacy Management, Cryptocurrencies and Blockchain Technology , pages 76{84. Springer,\n2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.07585",
                        "Citation Paper Title": "Title:Payment Network Design with Fees",
                        "Citation Paper Abstract": "Abstract:Payment channels are the most prominent solution to the blockchain scalability problem. We introduce the problem of network design with fees for payment channels from the perspective of a Payment Service Provider (PSP). Given a set of transactions, we examine the optimal graph structure and fee assignment to maximize the PSP's profit. A customer prefers to route transactions through the PSP's network if the cheapest path from sender to receiver is financially interesting, i.e., if the path costs less than the blockchain fee. When the graph structure is a tree, and the PSP facilitates all transactions, the problem can be formulated as a linear program. For a path graph, we present a polynomial time algorithm to assign optimal fees. We also show that the star network, where the center is an additional node acting as an intermediary, is a near-optimal solution to the network design problem.",
                        "Citation Paper Authors": "Authors:Georgia Avarikioti, Gerrit Janssen, Yuyi Wang, Roger Wattenhofer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.04842v3": {
            "Paper Title": "Developing Non-Stochastic Privacy-Preserving Policies Using\n  Agglomerative Clustering",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.08788v2": {
            "Paper Title": "Binsec/Rel: Efficient Relational Symbolic Execution for Constant-Time at\n  Binary-Level",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.07140v3": {
            "Paper Title": "Algebraic aspects of solving Ring-LWE, including ring-based improvements\n  in the Blum-Kalai-Wasserman algorithm",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.06912v2": {
            "Paper Title": "Public Ledger for Sensitive Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.01711v3": {
            "Paper Title": "Blockchain-Powered Collaboration in Heterogeneous Swarms of Robots",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.00300v4": {
            "Paper Title": "VisualPhishNet: Zero-Day Phishing Website Detection by Visual Similarity",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.04198v4": {
            "Paper Title": "Preech: A System for Privacy-Preserving Speech Transcription",
            "Sentences": [
                {
                    "Sentence ID": 41,
                    "Sentence": "In this section, we provide a summary of the related work.Privacy by Design: One class of approaches redesigns the\nspeech recognition pipeline to be private by design. For exam-\nple, Srivastava et al. proposes an encoder-decoder architecture\nfor speech recognition ",
                    "Citation Text": "B. M. L. Srivastava, A. Bellet, M. Tommasi, and E. Vin-\ncent. Privacy-Preserving Adversarial Representation\nLearning in ASR: Reality or Illusion? In INTER-\nSPEECH 2019 , Graz, Austria, Sept. 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.04913",
                        "Citation Paper Title": "Title:Privacy-Preserving Adversarial Representation Learning in ASR: Reality or Illusion?",
                        "Citation Paper Abstract": "Abstract:Automatic speech recognition (ASR) is a key technology in many services and applications. This typically requires user devices to send their speech data to the cloud for ASR decoding. As the speech signal carries a lot of information about the speaker, this raises serious privacy concerns. As a solution, an encoder may reside on each user device which performs local computations to anonymize the representation. In this paper, we focus on the protection of speaker identity and study the extent to which users can be recognized based on the encoded representation of their speech as obtained by a deep encoder-decoder architecture trained for ASR. Through speaker identification and verification experiments on the Librispeech corpus with open and closed sets of speakers, we show that the representations obtained from a standard architecture still carry a lot of information about speaker identity. We then propose to use adversarial training to learn representations that perform well in ASR while hiding speaker identity. Our results demonstrate that adversarial training dramatically reduces the closed-set classification accuracy, but this does not translate into increased open-set verification error hence into increased protection of the speaker identity in practice. We suggest several possible reasons behind this negative result.",
                        "Citation Paper Authors": "Authors:Brij Mohan Lal Srivastava, Aur\u00e9lien Bellet, Marc Tommasi, Emmanuel Vincent"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1906.07927v4": {
            "Paper Title": "SemanticAdv: Generating Adversarial Examples via Attribute-conditional\n  Image Editing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.04355v4": {
            "Paper Title": "Adversarial Neural Pruning with Latent Vulnerability Suppression",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.02928v3": {
            "Paper Title": "Software Ethology: An Accurate, Resilient, and Cross-Architecture Binary\n  Analysis Framework",
            "Sentences": [
                {
                    "Sentence ID": 66,
                    "Sentence": ", employ\na deep neural network to extract features from functions and\nthe binary call graph. These features are then used to create a\ndistance metric for determining binary similarity. Xu et al. ",
                    "Citation Text": "X. Xu, C. Liu, Q. Feng, H. Yin, L. Song, and D. Song, \u201cNeural network-\nbased graph embedding for cross-platform binary code similarity detec-\ntion,\u201d in Proceedings of the 2017 ACM SIGSAC Conference on Computer\nand Communications Security . ACM, 2017, pp. 363\u2013376.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.06525",
                        "Citation Paper Title": "Title:Neural Network-based Graph Embedding for Cross-Platform Binary Code Similarity Detection",
                        "Citation Paper Abstract": "Abstract:The problem of cross-platform binary code similarity detection aims at detecting whether two binary functions coming from different platforms are similar or not. It has many security applications, including plagiarism detection, malware detection, vulnerability search, etc. Existing approaches rely on approximate graph matching algorithms, which are inevitably slow and sometimes inaccurate, and hard to adapt to a new task. To address these issues, in this work, we propose a novel neural network-based approach to compute the embedding, i.e., a numeric vector, based on the control flow graph of each binary function, then the similarity detection can be done efficiently by measuring the distance between the embeddings for two functions. We implement a prototype called Gemini. Our extensive evaluation shows that Gemini outperforms the state-of-the-art approaches by large margins with respect to similarity detection accuracy. Further, Gemini can speed up prior art's embedding generation time by 3 to 4 orders of magnitude and reduce the required training time from more than 1 week down to 30 minutes to 10 hours. Our real world case studies demonstrate that Gemini can identify significantly more vulnerable firmware images than the state-of-the-art, i.e., Genius. Our research showcases a successful application of deep learning on computer security problems.",
                        "Citation Paper Authors": "Authors:Xiaojun Xu, Chang Liu, Qian Feng, Heng Yin, Le Song, Dawn Song"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1910.06259v4": {
            "Paper Title": "Confidence-Calibrated Adversarial Training: Generalizing to Unseen\n  Attacks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.04486v3": {
            "Paper Title": "Private Rank Aggregation under Local Differential Privacy",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.12157v3": {
            "Paper Title": "Silhouette: Efficient Protected Shadow Stacks for Embedded Systems",
            "Sentences": [
                {
                    "Sentence ID": 71,
                    "Sentence": "architecture but not on ARMv7-M, to provide coarse-\ngrained CFI and a protected shadow stack. CFI CaRE\u2019s perfor-\nmance overhead on CoreMark is 513%. SCFP ",
                    "Citation Text": "Mario Werner, Thomas Unterluggauer, David Schaffenrath,\nand Stefan Mangard. Sponge-based control-\ufb02ow protection\nfor IoT devices. In Proceedings of the 2018 IEEE European\nSymposium on Security and Privacy , EuroSP \u201918, pages 214\u2013\n226, London, United Kingdom, 2018. IEEE Computer Society.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.06691",
                        "Citation Paper Title": "Title:Sponge-Based Control-Flow Protection for IoT Devices",
                        "Citation Paper Abstract": "Abstract:Embedded devices in the Internet of Things (IoT) face a wide variety of security challenges. For example, software attackers perform code injection and code-reuse attacks on their remote interfaces, and physical access to IoT devices allows to tamper with code in memory, steal confidential Intellectual Property (IP), or mount fault attacks to manipulate a CPU's control flow.\nIn this work, we present Sponge-based Control Flow Protection (SCFP). SCFP is a stateful, sponge-based scheme to ensure the confidentiality of software IP and its authentic execution on IoT devices. At compile time, SCFP encrypts and authenticates software with instruction-level granularity. During execution, an SCFP hardware extension between the CPU's fetch and decode stage continuously decrypts and authenticates instructions. Sponge-based authenticated encryption in SCFP yields fine-grained control-flow integrity and thus prevents code-reuse, code-injection, and fault attacks on the code and the control flow. In addition, SCFP withstands any modification of software in memory. For evaluation, we extended a RISC-V core with SCFP and fabricated a real System on Chip (SoC). The average overhead in code size and execution time of SCFP on this design is 19.8% and 9.1%, respectively, and thus meets the requirements of embedded IoT devices.",
                        "Citation Paper Authors": "Authors:Mario Werner, Thomas Unterluggauer, David Schaffenrath, Stefan Mangard"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": ", which Section 8.6\ndiscusses, there are several other control-\ufb02ow hijacking de-\nfenses for embedded devices. CFI CaRE ",
                    "Citation Text": "Thomas Nyman, Jan-Erik Ekberg, Lucas Davi, and N. Asokan.\nCFI CaRE: Hardware-supported call and return enforcement\nfor commercial microcontrollers. In Proceedings of the 20th\nInternational Symposium on Research in Attacks, Intrusions,\nand Defenses , RAID \u201917, pages 259\u2013284, Atlanta, GA, 2017.\nSpringer-Verlag.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.05715",
                        "Citation Paper Title": "Title:CFI CaRE: Hardware-supported Call and Return Enforcement for Commercial Microcontrollers",
                        "Citation Paper Abstract": "Abstract:With the increasing scale of deployment of Internet of Things (IoT), concerns about IoT security have become more urgent. In particular, memory corruption attacks play a predominant role as they allow remote compromise of IoT devices. Control-flow integrity (CFI) is a promising and generic defense technique against these attacks. However, given the nature of IoT deployments, existing protection mechanisms for traditional computing environments (including CFI) need to be adapted to the IoT setting. In this paper, we describe the challenges of enabling CFI on microcontroller (MCU) based IoT devices. We then present CaRE, the first interrupt-aware CFI scheme for low-end MCUs. CaRE uses a novel way of protecting the CFI metadata by leveraging TrustZone-M security extensions introduced in the ARMv8-M architecture. Its binary instrumentation approach preserves the memory layout of the target MCU software, allowing pre-built bare-metal binary code to be protected by CaRE. We describe our implementation on a Cortex-M Prototyping System and demonstrate that CaRE is secure while imposing acceptable performance and memory impact.",
                        "Citation Paper Authors": "Authors:Thomas Nyman, Jan-Erik Ekberg, Lucas Davi, N. Asokan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1907.04156v2": {
            "Paper Title": "Private key encryption and recovery in blockchain",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "Selecting appropriate biometric data to create key pairs is another issue\nthat must be considered. Researchers have investigated several biometric\nfeatures in biometric-based cryptographic key generations ",
                    "Citation Text": "Jagadeesan, A. and Duraiswamy, K. (2010). Secured cryptographic key\ngeneration from multimodal biometrics: feature level fusion of \fngerprint\nand iris. arXiv preprint arXiv:1003.1458 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1002.2527",
                        "Citation Paper Title": "Title:Secured Cryptographic Key Generation From Multimodal Biometrics Feature Level Fusion Of Fingerprint And Iris",
                        "Citation Paper Abstract": "Abstract:  Human users have a tough time remembering long cryptographic keys. Hence, researchers, for so long, have been examining ways to utilize biometric features of the user instead of a memorable password or passphrase, in an effort to generate strong and repeatable cryptographic keys. Our objective is to incorporate the volatility of the users biometric features into the generated key, so as to make the key unguessable to an attacker lacking significant knowledge of the users biometrics. We go one step further trying to incorporate multiple biometric modalities into cryptographic key generation so as to provide better security. In this article, we propose an efficient approach based on multimodal biometrics (Iris and fingerprint) for generation of secure cryptographic key. The proposed approach is composed of three modules namely, 1) Feature extraction, 2) Multimodal biometric template generation and 3) Cryptographic key generation. Initially, the features, minutiae points and texture properties are extracted from the fingerprint and iris images respectively. Subsequently, the extracted features are fused together at the feature level to construct the multibiometric template. Finally, a 256bit secure cryptographic key is generated from the multibiometric template. For experimentation, we have employed the fingerprint images obtained from publicly available sources and the iris images from CASIA Iris Database. The experimental results demonstrate the effectiveness of the proposed approach.",
                        "Citation Paper Authors": "Authors:A. Jagadeesan, K. Duraiswamy"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1906.11798v2": {
            "Paper Title": "Stolen Memories: Leveraging Model Memorization for Calibrated White-Box\n  Membership Inference",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": ", and in fact, our empirical results (Section 5) show\nthat we can successfully attack even models with negligible\ngeneralization error; however, dropout has been shown not\nonly to reduce over\ufb01tting, but to strengthen privacy guarantees\nin neural networks ",
                    "Citation Text": "Prateek Jain, Vivek Kulkarni, Abhradeep Thakurta, and\nOliver Williams. To drop or not to drop: Robustness, consis-\ntency and differential privacy properties of dropout. CoRR ,\nabs/1503.02031, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1503.02031",
                        "Citation Paper Title": "Title:To Drop or Not to Drop: Robustness, Consistency and Differential Privacy Properties of Dropout",
                        "Citation Paper Abstract": "Abstract:Training deep belief networks (DBNs) requires optimizing a non-convex function with an extremely large number of parameters. Naturally, existing gradient descent (GD) based methods are prone to arbitrarily poor local minima. In this paper, we rigorously show that such local minima can be avoided (upto an approximation error) by using the dropout technique, a widely used heuristic in this domain. In particular, we show that by randomly dropping a few nodes of a one-hidden layer neural network, the training objective function, up to a certain approximation error, decreases by a multiplicative factor.\nOn the flip side, we show that for training convex empirical risk minimizers (ERM), dropout in fact acts as a \"stabilizer\" or regularizer. That is, a simple dropout based GD method for convex ERMs is stable in the face of arbitrary changes to any one of the training points. Using the above assertion, we show that dropout provides fast rates for generalization error in learning (convex) generalized linear models (GLM). Moreover, using the above mentioned stability properties of dropout, we design dropout based differentially private algorithms for solving ERMs. The learned GLM thus, preserves privacy of each of the individual training points while providing accurate predictions for new test points. Finally, we empirically validate our stability assertions for dropout in the context of convex ERMs and show that surprisingly, dropout significantly outperforms (in terms of prediction accuracy) the L2 regularization based methods for several benchmark datasets.",
                        "Citation Paper Authors": "Authors:Prateek Jain, Vivek Kulkarni, Abhradeep Thakurta, Oliver Williams"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1910.03180v3": {
            "Paper Title": "Supersingular Curves With Small Non-integer Endomorphisms",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.00981v2": {
            "Paper Title": "Proving Data-Poisoning Robustness in Decision Trees",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.09791v2": {
            "Paper Title": "Towards a Blockchain based digital identity verification, record\n  attestation and record sharing system",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.03080v3": {
            "Paper Title": "A Privacy-preserving Method to Optimize Distributed Resource Allocation",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": ". Other techniques could\nalso be considered such as consensus-based aggregation algorithms ",
                    "Citation Text": "J. He, L. Cai, P. Cheng, J. Pan, and L. Shi ,Consensus-based data-privacy preserving data\naggregation , IEEE Trans. Autom. Control, (2019), pp. 1\u20131.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.06381",
                        "Citation Paper Title": "Title:Consensus-based Privacy-preserving Data Aggregation",
                        "Citation Paper Abstract": "Abstract:Privacy-preserving data aggregation in ad hoc networks is a challenging problem, considering the distributed communication and control requirement, dynamic network topology, unreliable communication links, etc. Different from the widely used cryptographic approaches, in this paper, we address this challenging problem by exploiting the distributed consensus technique. We first propose a secure consensus-based data aggregation (SCDA) algorithm that guarantees an accurate sum aggregation while preserving the privacy of sensitive data. Then, we prove that the proposed algorithm converges accurately and is $(\\epsilon, \\sigma)$-data-privacy, and the mathematical relationship between $\\epsilon$ and $\\sigma$ is provided. Extensive simulations have shown that the proposed algorithm has high accuracy and low complexity, and they are robust against network dynamics.",
                        "Citation Paper Authors": "Authors:Jianping He, Lin Cai, Peng Cheng, Jianping Pan, Ling Shi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1907.07159v2": {
            "Paper Title": "Security Smells in Ansible and Chef Scripts: A Replication Study",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": "surveyed practitioners to investigate which\nfactors influence usage of IaC tools. Rahman et al. ",
                    "Citation Text": "Akond Rahman, Rezvan Mahdavi-Hezaveh, and Laurie Williams. 2018. A systematic mapping study of infrastructure as code research. Information\nand Software Technology (2018). https://doi.org/10.1016/j.infsof.2018.12.004",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.04872",
                        "Citation Paper Title": "Title:Where Are The Gaps? A Systematic Mapping Study of Infrastructure as Code Research",
                        "Citation Paper Abstract": "Abstract:Context:Infrastructure as code (IaC) is the practice to automatically configure system dependencies and to provision local and remote instances. Practitioners consider IaC as a fundamental pillar to implement DevOps practices, which helps them to rapidly deliver software and services to end-users. Information technology (IT) organizations, such as Github, Mozilla, Facebook, Google and Netflix have adopted IaC. A systematic mapping study on existing IaC research can help researchers to identify potential research areas related to IaC, for example, the areas of defects and security flaws that may occur in IaC scripts. Objective: The objective of this paper is to help researchers identify research areas related to infrastructure as code (IaC) by conducting a systematic mapping study of IaC-related research. Methodology: We conduct our research study by searching six scholar databases. We collect a set of 33,887 publications by using seven search strings. By systematically applying inclusion and exclusion criteria, we identify 31 publications related to IaC. We identify topics addressed in these publications by applying qualitative analysis. Results: We identify four topics studied in IaC-related publications: (i) framework/tool for infrastructure as code; (ii) use of infrastructure as code; (iii) empirical study related to infrastructure as code; and (iv) testing in infrastructure as code. According to our analysis, 52% of the studied 31 publications propose a framework or tool to implement the practice of IaC or extend the functionality of an existing IaC tool. Conclusion: As defects and security flaws can have serious consequences for the deployment and development environments in DevOps, along with other topics, we observe the need for research studies that will study defects and security flaws for IaC.",
                        "Citation Paper Authors": "Authors:Akond Rahman, Rezvan Mahdavi-Hezaveh, Laurie Williams"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1909.11592v2": {
            "Paper Title": "Mining user interaction patterns in the darkweb to predict enterprise\n  cyber incidents",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.09465v5": {
            "Paper Title": "Managing Recurrent Virtual Network Updates in Multi-Tenant Datacenters:\n  A System Perspective",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.13245v2": {
            "Paper Title": "EnclaveDom: Privilege Separation for Large-TCB Applications in Trusted\n  Execution Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.13441v2": {
            "Paper Title": "Lattice PUF: A Strong Physical Unclonable Function Provably Secure\n  against Machine Learning Attacks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.02785v2": {
            "Paper Title": "BUZz: BUffer Zones for defending adversarial examples in image\n  classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.03034v3": {
            "Paper Title": "Methodologies for Quantifying (Re-)randomization Security and Timing\n  under JIT-ROP",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.00870v2": {
            "Paper Title": "MadNet: Using a MAD Optimization for Defending Against Adversarial\n  Attacks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.10367v2": {
            "Paper Title": "Dispel: Byzantine SMR with Distributed Pipelining",
            "Sentences": [
                {
                    "Sentence ID": 65,
                    "Sentence": ", again due to the network bottle-\nneck at the leader (cf. \u00a72.1). HotStuff ",
                    "Citation Text": "Maofan Yin, Dahlia Malkhi, Michael K. Reiter, Guy Golan-Gueta, and\nIttai Abraham. HotStuff: BFT consensus with linearity and respon-\nsiveness. In Proceedings of the 2019 ACM Symposium on Principles of\nDistributed Computing , pages 347\u2013356, 2019.\n15",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.05069",
                        "Citation Paper Title": "Title:HotStuff: BFT Consensus in the Lens of Blockchain",
                        "Citation Paper Abstract": "Abstract:We present HotStuff, a leader-based Byzantine fault-tolerant replication protocol for the partially synchronous model. Once network communication becomes synchronous, HotStuff enables a correct leader to drive the protocol to consensus at the pace of actual (vs. maximum) network delay--a property called responsiveness--and with communication complexity that is linear in the number of replicas. To our knowledge, HotStuff is the first partially synchronous BFT replication protocol exhibiting these combined properties. HotStuff is built around a novel framework that forms a bridge between classical BFT foundations and blockchains. It allows the expression of other known protocols (DLS, PBFT, Tendermint, Casper), and ours, in a common framework.\nOur deployment of HotStuff over a network with over 100 replicas achieves throughput and latency comparable to that of BFT-SMaRt, while enjoying linear communication footprint during leader failover (vs. quadratic with BFT-SMaRt).",
                        "Citation Paper Authors": "Authors:Maofan Yin, Dahlia Malkhi, Michael K. Reiter, Guy Golan Gueta, Ittai Abraham"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "proposed a Byzantine leaderless consensus\nwhose communication complexity is exponential.\nRecently, Crain et al. ",
                    "Citation Text": "Tyler Crain, Vincent Gramoli, Mikel Larrea, and Michel Raynal.\nDBFT: Efficient leaderless Byzantine consensus and its applications to\nblockchains. In Proceedings of the 17th IEEE International Symposium\non Network Computing and Applications (NCA\u201918) , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.03068",
                        "Citation Paper Title": "Title:DBFT: Efficient Byzantine Consensus with a Weak Coordinator and its Application to Consortium Blockchains",
                        "Citation Paper Abstract": "Abstract:This paper introduces a deterministic Byzantine consensus algorithm that relies on a new weak coordinator. As opposed to previous algorithms that cannot terminate in the presence of a faulty or slow coordinator, our algorithm can terminate even when its coordinator is faulty, hence the name weak coordinator. The key idea is to allow processes to complete asynchronous rounds as soon as they receive a threshold of messages, instead of having to wait for a message from a coordinator that may be slow.\nThe resulting algorithm assumes partial synchrony, is resilience optimal, time optimal and does not need signatures. Our presentation is didactic: we first present a simple safe binary Byzantine consensus algorithm, modify it to ensure termination, and finally present an optimized reduction from multivalue consensus to binary consensus that may terminate in 4 message delays. To evaluate our algorithm, we deployed it on 100 machines distributed in 5 datacenters across different continents and compared its performance against the randomized solution from Mostefaoui, Moumem and Raynal [PODC14] that terminates in O(1) rounds in expectation. Our algorithm always outperforms the latter even in the presence of Byzantine behaviors. Our algorithm has a subsecond average latency in most of our geo-distributed experiments, even when attacked by a well-engineered coalition of Byzantine processes.",
                        "Citation Paper Authors": "Authors:Tyler Crain, Vincent Gramoli, Mikel Larrea, Michel Raynal"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "bypasses the leader whose failure might im-\npact performance by exploiting one leader per command\nissued. If these commands do not conflict, they are commit-\nted concurrently. Atlas ",
                    "Citation Text": "Vitor Enes, Carlos Baquero, Tuanir Fran\u00e7a Rezende, Alexey Gotsman,\nMatthieu Perrin, and Pierre Sutra. State-machine replication for planet-\nscale systems. In Proceedings of the Fifteenth European Conference on\nComputer Systems , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.11789",
                        "Citation Paper Title": "Title:State-Machine Replication for Planet-Scale Systems (Extended Version)",
                        "Citation Paper Abstract": "Abstract:Online applications now routinely replicate their data at multiple sites around the world. In this paper we present Atlas, the first state-machine replication protocol tailored for such planet-scale systems. Atlas does not rely on a distinguished leader, so clients enjoy the same quality of service independently of their geographical locations. Furthermore, client-perceived latency improves as we add sites closer to clients. To achieve this, Atlas minimizes the size of its quorums using an observation that concurrent data center failures are rare. It also processes a high percentage of accesses in a single round trip, even when these conflict. We experimentally demonstrate that Atlas consistently outperforms state-of-the-art protocols in planet-scale scenarios. In particular, Atlas is up to two times faster than Flexible Paxos with identical failure assumptions, and more than doubles the performance of Egalitarian Paxos in the YCSB benchmark.",
                        "Citation Paper Authors": "Authors:Vitor Enes, Carlos Baquero, Tuanir Fran\u00e7a Rezende, Alexey Gotsman, Matthieu Perrin, Pierre Sutra"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": ". It was used for the\nordering service of Hyperledger Fabric ",
                    "Citation Text": "Elli Androulaki, Artem Barger, Vita Bortnikov, Christian Cachin,\nKonstantinos Christidis, Angelo De Caro, David Enyeart, Christo-\npher Ferris, Gennady Laventman, Yacov Manevich, Srinivasan Mu-\nralidharan, Chet Murthy, Binh Nguyen, Manish Sethi, Gari Singh,\nKeith Smith, Alessandro Sorniotti, Chrysoula Stathakopoulou, Marko\nVukoli\u0107, Sharon Weed Cocco, and Jason Yellick. Hyperledger fab-\nric: A distributed operating system for permissioned blockchains. In\nProceedings of the Thirteenth EuroSys Conference , EuroSys \u201918, pages\n30:1\u201330:15, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.10228",
                        "Citation Paper Title": "Title:Hyperledger Fabric: A Distributed Operating System for Permissioned Blockchains",
                        "Citation Paper Abstract": "Abstract:Fabric is a modular and extensible open-source system for deploying and operating permissioned blockchains and one of the Hyperledger projects hosted by the Linux Foundation (this http URL).\nFabric is the first truly extensible blockchain system for running distributed applications. It supports modular consensus protocols, which allows the system to be tailored to particular use cases and trust models. Fabric is also the first blockchain system that runs distributed applications written in standard, general-purpose programming languages, without systemic dependency on a native cryptocurrency. This stands in sharp contrast to existing blockchain platforms that require \"smart-contracts\" to be written in domain-specific languages or rely on a cryptocurrency. Fabric realizes the permissioned model using a portable notion of membership, which may be integrated with industry-standard identity management. To support such flexibility, Fabric introduces an entirely novel blockchain design and revamps the way blockchains cope with non-determinism, resource exhaustion, and performance attacks.\nThis paper describes Fabric, its architecture, the rationale behind various design decisions, its most prominent implementation aspects, as well as its distributed application programming model. We further evaluate Fabric by implementing and benchmarking a Bitcoin-inspired digital currency. We show that Fabric achieves end-to-end throughput of more than 3500 transactions per second in certain popular deployment configurations, with sub-second latency, scaling well to over 100 peers.",
                        "Citation Paper Authors": "Authors:Elli Androulaki, Artem Barger, Vita Bortnikov, Christian Cachin, Konstantinos Christidis, Angelo De Caro, David Enyeart, Christopher Ferris, Gennady Laventman, Yacov Manevich, Srinivasan Muralidharan, Chet Murthy, Binh Nguyen, Manish Sethi, Gari Singh, Keith Smith, Alessandro Sorniotti, Chrysoula Stathakopoulou, Marko Vukoli\u0107, Sharon Weed Cocco, Jason Yellick"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1903.11508v2": {
            "Paper Title": "Text Processing Like Humans Do: Visually Attacking and Shielding NLP\n  Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.04833v2": {
            "Paper Title": "Learning and Planning in the Feature Deception Problem",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.06148v2": {
            "Paper Title": "FiFTy: Large-scale File Fragment Type Identification using Neural\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.13409v2": {
            "Paper Title": "Bypassing Backdoor Detection Algorithms in Deep Learning",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": ".\nBesides backdoor attacks, there have been many stud-\nies on adversarial machine learning where the discontin-\nuous input-output mappings of models are exploited to\ngenerate adversarial examples ",
                    "Citation Text": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna,\nDumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing\nproperties of neural networks. arXiv preprint arXiv:1312.6199 ,\n2013.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1312.6199",
                        "Citation Paper Title": "Title:Intriguing properties of neural networks",
                        "Citation Paper Abstract": "Abstract:Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.\nFirst, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks.\nSecond, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
                        "Citation Paper Authors": "Authors:Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": ".\nIn response to the backdoor attacks that have been\ndevised, there have been several papers that aim to remove\nbackdoor behavior, besides the ones mentioned in this\npaper. Liu et al., 2018 ",
                    "Citation Text": "Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-\npruning: Defending against backdooring attacks on deep neural\nnetworks. In International Symposium on Research in Attacks,\nIntrusions, and Defenses , pages 273\u2013294. Springer, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.12185",
                        "Citation Paper Title": "Title:Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:Deep neural networks (DNNs) provide excellent performance across a wide range of classification tasks, but their training requires high computational resources and is often outsourced to third parties. Recent work has shown that outsourced training introduces the risk that a malicious trainer will return a backdoored DNN that behaves normally on most inputs but causes targeted misclassifications or degrades the accuracy of the network when a trigger known only to the attacker is present. In this paper, we provide the first effective defenses against backdoor attacks on DNNs. We implement three backdoor attacks from prior work and use them to investigate two promising defenses, pruning and fine-tuning. We show that neither, by itself, is sufficient to defend against sophisticated attackers. We then evaluate fine-pruning, a combination of pruning and fine-tuning, and show that it successfully weakens or even eliminates the backdoors, i.e., in some cases reducing the attack success rate to 0% with only a 0.4% drop in accuracy for clean (non-triggering) inputs. Our work provides the first step toward defenses against backdoor attacks in deep neural networks.",
                        "Citation Paper Authors": "Authors:Kang Liu, Brendan Dolan-Gavitt, Siddharth Garg"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": ". The guarantees that these robust algorithms\nprovide, however, are shown to be insuf\ufb01cient ",
                    "Citation Text": "El Mahdi El Mhamdi, Rachid Guerraoui, and S \u00b4ebastien Rouault.\nThe hidden vulnerability of distributed learning in byzantium.\narXiv preprint arXiv:1802.07927 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.07927",
                        "Citation Paper Title": "Title:The Hidden Vulnerability of Distributed Learning in Byzantium",
                        "Citation Paper Abstract": "Abstract:While machine learning is going through an era of celebrated success, concerns have been raised about the vulnerability of its backbone: stochastic gradient descent (SGD). Recent approaches have been proposed to ensure the robustness of distributed SGD against adversarial (Byzantine) workers sending poisoned gradients during the training phase. Some of these approaches have been proven Byzantine-resilient: they ensure the convergence of SGD despite the presence of a minority of adversarial workers.\nWe show in this paper that convergence is not enough. In high dimension $d \\gg 1$, an adver\\-sary can build on the loss function's non-convexity to make SGD converge to ineffective models. More precisely, we bring to light that existing Byzantine-resilient schemes leave a margin of poisoning of $\\Omega\\left(f(d)\\right)$, where $f(d)$ increases at least like $\\sqrt{d~}$. Based on this leeway, we build a simple attack, and experimentally show its strong to utmost effectivity on CIFAR-10 and MNIST.\nWe introduce Bulyan, and prove it significantly reduces the attackers leeway to a narrow $O( \\frac{1}{\\sqrt{d~}})$ bound. We empirically show that Bulyan does not suffer the fragility of existing aggregation rules and, at a reasonable cost in terms of required batch size, achieves convergence as if only non-Byzantine gradients had been used to update the model.",
                        "Citation Paper Authors": "Authors:El Mahdi El Mhamdi, Rachid Guerraoui, S\u00e9bastien Rouault"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1909.11245v4": {
            "Paper Title": "On Locally Decodable Codes in Resource Bounded Channels",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.10186v2": {
            "Paper Title": "KRATOS: Multi-User Multi-Device-Aware Access Control System for the\n  Smart Home",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.12860v2": {
            "Paper Title": "Clash of the Trackers: Measuring the Evolution of the Online Tracking\n  Ecosystem",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ". Instead,\nour approach is simpler and relies on automated crawls\nwithout personas, which captures much of the cookie\nsynchronization, but not all. Thus, it measures a lower\nbound of such data \ufb02ows, as would be expected for real\nusers ",
                    "Citation Text": "P. Papadopoulos, N. Kourtellis, and E. Markatos, \u201cCook ie synchroniza-\ntion: Everything you always wanted to know but were afraid to ask,\u201d in\nThe World Wide Web Conference . ACM, 2019, pp. 1432\u20131442.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.10505",
                        "Citation Paper Title": "Title:Cookie Synchronization: Everything You Always Wanted to Know But Were Afraid to Ask",
                        "Citation Paper Abstract": "Abstract:User data is the primary input of digital advertising, fueling the free Internet as we know it. As a result, web companies invest a lot in elaborate tracking mechanisms to acquire user data that can sell to data markets and advertisers. However, with same-origin policy, and cookies as a primary identification mechanism on the web, each tracker knows the same user with a different ID. To mitigate this, Cookie Synchronization (CSync) came to the rescue, facilitating an information sharing channel between third parties that may or not have direct access to the website the user visits. In the background, with CSync, they merge user data they own, but also reconstruct a user's browsing history, bypassing the same origin policy. In this paper, we perform a first to our knowledge in-depth study of CSync in the wild, using a year-long weblog from 850 real mobile users. Through our study, we aim to understand the characteristics of the CSync protocol and the impact it has on web users' privacy. For this, we design and implement CONRAD, a holistic mechanism to detect CSync events at real time, and the privacy loss on the user side, even when the synced IDs are obfuscated. Using CONRAD, we find that 97% of the regular web users are exposed to CSync: most of them within the first week of their browsing, and the median userID gets leaked, on average, to 3.5 different domains. Finally, we see that CSync increases the number of domains that track the user by a factor of 6.75.",
                        "Citation Paper Authors": "Authors:Panagiotis Papadopoulos, Nicolas Kourtellis, Evangelos P. Markatos"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1901.05324v3": {
            "Paper Title": "A wireless secure key distribution system with no couriers: a\n  One-Time-Pad Revival",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.08774v2": {
            "Paper Title": "Implement Liquid Democracy on Ethereum: A Fast Algorithm for Realtime\n  Self-tally Voting System",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.12754v2": {
            "Paper Title": "Code based Cryptography: Classic McEliece",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.05429v3": {
            "Paper Title": "Extraction of Complex DNN Models: Real Threat or Boogeyman?",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.11328v4": {
            "Paper Title": "Assessing differentially private deep learning with Membership Inference",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "is a benchmark suite for performance measurements related to machine learning. Hay et al. ",
                    "Citation Text": "M. Hay, A. Machanavajjhala, G. Miklau, Y . Chen, and D. Zhang. Principled evaluation of differentially private\nalgorithms using dpbench. In Proc. of Conference on Management of Data (SIGMOD) , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.04817",
                        "Citation Paper Title": "Title:Principled Evaluation of Differentially Private Algorithms using DPBench",
                        "Citation Paper Abstract": "Abstract:Differential privacy has become the dominant standard in the research community for strong privacy protection. There has been a flood of research into query answering algorithms that meet this standard. Algorithms are becoming increasingly complex, and in particular, the performance of many emerging algorithms is {\\em data dependent}, meaning the distribution of the noise added to query answers may change depending on the input data. Theoretical analysis typically only considers the worst case, making empirical study of average case performance increasingly important.\nIn this paper we propose a set of evaluation principles which we argue are essential for sound evaluation. Based on these principles we propose DPBench, a novel evaluation framework for standardized evaluation of privacy algorithms. We then apply our benchmark to evaluate algorithms for answering 1- and 2-dimensional range queries. The result is a thorough empirical study of 15 published algorithms on a total of 27 datasets that offers new insights into algorithm behavior---in particular the influence of dataset scale and shape---and a more complete characterization of the state of the art. Our methodology is able to resolve inconsistencies in prior empirical studies and place algorithm performance in context through comparison to simple baselines. Finally, we pose open research questions which we hope will guide future algorithm design.",
                        "Citation Paper Authors": "Authors:Michael Hay, Ashwin Machanavajjhala, Gerome Miklau, Yan Chen, Dan Zhang"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": ", a (composed) local randomizer. By Equation (1)randomized response yields\n\u000f= ln(3) LDP for a one-time collection of values from binary domains (e.g., fyes;nog) with two fair coins ",
                    "Citation Text": "U. Erlingsson, V . Pihur, and A. Korolova. RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal\nResponse. In Proc. of Conference on Computer and Communications Security (CCS) , 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1407.6981",
                        "Citation Paper Title": "Title:RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response",
                        "Citation Paper Abstract": "Abstract:Randomized Aggregatable Privacy-Preserving Ordinal Response, or RAPPOR, is a technology for crowdsourcing statistics from end-user client software, anonymously, with strong privacy guarantees. In short, RAPPORs allow the forest of client data to be studied, without permitting the possibility of looking at individual trees. By applying randomized response in a novel manner, RAPPOR provides the mechanisms for such collection as well as for efficient, high-utility analysis of the collected data. In particular, RAPPOR permits statistics to be collected on the population of client-side strings with strong privacy guarantees for each client, and without linkability of their reports. This paper describes and motivates RAPPOR, details its differential-privacy and utility guarantees, discusses its practical deployment and properties in the face of different attack models, and, finally, gives results of its application to both synthetic and real-world data.",
                        "Citation Paper Authors": "Authors:\u00dalfar Erlingsson, Vasyl Pihur, Aleksandra Korolova"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ". LDP is the standard choice when the\nserver which evaluates a function f(D)is untrusted. We adapt the de\ufb01nitions of Kasiviswanathan et al. ",
                    "Citation Text": "S. P. Kasiviswanathan, H. K. Lee, K. Nissim, S. Raskhodnikova, and A. Smith. What can we learn privately?\nSIAM Journal on Computing , 2008.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:0803.0924",
                        "Citation Paper Title": "Title:What Can We Learn Privately?",
                        "Citation Paper Abstract": "Abstract:  Learning problems form an important category of computational tasks that generalizes many of the computations researchers apply to large real-life data sets. We ask: what concept classes can be learned privately, namely, by an algorithm whose output does not depend too heavily on any one input or specific training example? More precisely, we investigate learning algorithms that satisfy differential privacy, a notion that provides strong confidentiality guarantees in contexts where aggregate information is released about a database containing sensitive information about individuals. We demonstrate that, ignoring computational constraints, it is possible to privately agnostically learn any concept class using a sample size approximately logarithmic in the cardinality of the concept class. Therefore, almost anything learnable is learnable privately: specifically, if a concept class is learnable by a (non-private) algorithm with polynomial sample complexity and output size, then it can be learned privately using a polynomial number of samples. We also present a computationally efficient private PAC learner for the class of parity functions. Local (or randomized response) algorithms are a practical class of private algorithms that have received extensive investigation. We provide a precise characterization of local private learning algorithms. We show that a concept class is learnable by a local algorithm if and only if it is learnable in the statistical query (SQ) model. Finally, we present a separation between the power of interactive and noninteractive local learning algorithms.",
                        "Citation Paper Authors": "Authors:Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya Raskhodnikova, Adam Smith"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1909.04841v2": {
            "Paper Title": "Packet Chasing: Spying on Network Packets over a Cache Side-Channel",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": "architectures.\nIn addition to P RIME +PROBE , multiple other\nvariants of cache attacks are also proposed ",
                    "Citation Text": "D. Gruss, C. Maurice, K. Wagner, and S. Mangard, \u201cFlush+\ufb02ush: A fast\nand stealthy cache attack,\u201d in 13th International Conference on Detection\nof Intrusions and Malware, and Vulnerability Assessment (DIMVA) ,\n2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.04594",
                        "Citation Paper Title": "Title:Flush+Flush: A Fast and Stealthy Cache Attack",
                        "Citation Paper Abstract": "Abstract:Research on cache attacks has shown that CPU caches leak significant information. Proposed detection mechanisms assume that all cache attacks cause more cache hits and cache misses than benign applications and use hardware performance counters for detection.\nIn this article, we show that this assumption does not hold by developing a novel attack technique: the Flush+Flush attack. The Flush+Flush attack only relies on the execution time of the flush instruction, which depends on whether data is cached or not. Flush+Flush does not make any memory accesses, contrary to any other cache attack. Thus, it causes no cache misses at all and the number of cache hits is reduced to a minimum due to the constant cache flushes. Therefore, Flush+Flush attacks are stealthy, i.e., the spy process cannot be detected based on cache hits and misses, or state-of-the-art detection mechanisms. The Flush+Flush attack runs in a higher frequency and thus is faster than any existing cache attack. With 496 KB/s in a cross-core covert channel it is 6.7 times faster than any previously published cache covert channel.",
                        "Citation Paper Authors": "Authors:Daniel Gruss, Cl\u00e9mentine Maurice, Klaus Wagner, Stefan Mangard"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1907.08116v2": {
            "Paper Title": "Communication and Consensus Co-Design for Distributed, Low-Latency and\n  Reliable Wireless Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.03895v2": {
            "Paper Title": "BrokenStrokes: On the (in)Security of Wireless Keyboards",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.05823v3": {
            "Paper Title": "Smart Contract Repair",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.10517v2": {
            "Paper Title": "Transferable Cost-Aware Security Policy Implementation for Malware\n  Detection Using Deep Reinforcement Learning",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": "deep reinforcement learning library to create\nand train the agent. The environment was implemented using\nthe OpenAI Gym ",
                    "Citation Text": "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John\nSchulman, Jie Tang, and Wojciech Zaremba. 2016. OpenAI Gym.\narXiv:arXiv:1606.01540",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.01540",
                        "Citation Paper Title": "Title:OpenAI Gym",
                        "Citation Paper Abstract": "Abstract:OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.",
                        "Citation Paper Authors": "Authors:Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": ", and this\nis the approach used in our experiments (see Section 5.5).\nTL has been shown to have the potential to significantly improve\na reinforcement learning agent\u2019s sample efficiency ",
                    "Citation Text": "Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. 2015. Actor-\nmimic: Deep multitask and transfer reinforcement learning. arXiv preprint\narXiv:1511.06342 (2015).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06342",
                        "Citation Paper Title": "Title:Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:The ability to act in multiple environments and transfer previous knowledge to new situations can be considered a critical aspect of any intelligent agent. Towards this goal, we define a novel method of multitask and transfer learning that enables an autonomous agent to learn how to behave in multiple tasks simultaneously, and then generalize its knowledge to new domains. This method, termed \"Actor-Mimic\", exploits the use of deep reinforcement learning and model compression techniques to train a single policy network that learns how to act in a set of distinct tasks by using the guidance of several expert teachers. We then show that the representations learnt by the deep policy network are capable of generalizing to new tasks with no prior expert guidance, speeding up learning in novel environments. Although our method can in general be applied to a wide range of problems, we use Atari games as a testing environment to demonstrate these methods.",
                        "Citation Paper Authors": "Authors:Emilio Parisotto, Jimmy Lei Ba, Ruslan Salakhutdinov"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": ". The ability of RL\nalgorithms to explore large solution spaces and devise highly effi-\ncient strategies to address them (especially when coupled with deep\nlearning) was shown to be highly effective in areas such as robotics\nand control problems ",
                    "Citation Text": "John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz.\n2015. Trust region policy optimization. In International Conference on Machine\nLearning . 1889\u20131897.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1502.05477",
                        "Citation Paper Title": "Title:Trust Region Policy Optimization",
                        "Citation Paper Abstract": "Abstract:We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.",
                        "Citation Paper Authors": "Authors:John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": "proposed deep learning techniques to\nperform static analysis, where both permissions and source-code\nanalysis were employed.\nAs in the analysis of PEs, ensemble-based methods have proven\nthemselves highly effective. In ",
                    "Citation Text": "Suleiman Y Yerima, Sakir Sezer, and Igor Muttik. 2015. High accuracy android\nmalware detection using ensemble learning. IET Information Security 9, 6 (2015),\n313\u2013320.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1608.00835",
                        "Citation Paper Title": "Title:High Accuracy Android Malware Detection Using Ensemble Learning",
                        "Citation Paper Abstract": "Abstract:With over 50 billion downloads and more than 1.3 million apps in the Google official market, Android has continued to gain popularity amongst smartphone users worldwide. At the same time there has been a rise in malware targeting the platform, with more recent strains employing highly sophisticated detection avoidance techniques. As traditional signature based methods become less potent in detecting unknown malware, alternatives are needed for timely zero-day discovery. Thus this paper proposes an approach that utilizes ensemble learning for Android malware detection. It combines advantages of static analysis with the efficiency and performance of ensemble machine learning to improve Android malware detection accuracy. The machine learning models are built using a large repository of malware samples and benign apps from a leading antivirus vendor. Experimental results and analysis presented shows that the proposed method which uses a large feature space to leverage the power of ensemble learning is capable of 97.3 to 99 percent detection accuracy with very low false positive rates.",
                        "Citation Paper Authors": "Authors:Suleiman Y. Yerima, Sakir Sezer, Igor Muttik"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1905.07444v3": {
            "Paper Title": "Percival: Making In-Browser Perceptual Ad Blocking Practical With Deep\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.11358v4": {
            "Paper Title": "On the Power of Multiple Anonymous Messages",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.11778v3": {
            "Paper Title": "Linear and Range Counting under Metric-based Local Differential Privacy",
            "Sentences": [
                {
                    "Sentence ID": 45,
                    "Sentence": "made some improvement but\nfailed to completely remove the (logm)O(D)term. Their approach has expected squared error O(D(logm)3(D\u00001)\n\u000f2 )under\nEL1-CDP, which is only better than the Privelet mechanism ",
                    "Citation Text": "Xiaokui Xiao, Guozhang Wang, and Johannes Gehrke. Differential privacy via wavelet transforms. In Proceedings\nof the 26th International Conference on Data Engineering (ICDE) , pages 225\u2013236, 2010.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:0909.5530",
                        "Citation Paper Title": "Title:Differential Privacy via Wavelet Transforms",
                        "Citation Paper Abstract": "Abstract:  Privacy preserving data publishing has attracted considerable research interest in recent years. Among the existing solutions, {\\em $\\epsilon$-differential privacy} provides one of the strongest privacy guarantees. Existing data publishing methods that achieve $\\epsilon$-differential privacy, however, offer little data utility. In particular, if the output dataset is used to answer count queries, the noise in the query answers can be proportional to the number of tuples in the data, which renders the results useless.\nIn this paper, we develop a data publishing technique that ensures $\\epsilon$-differential privacy while providing accurate answers for {\\em range-count queries}, i.e., count queries where the predicate on each attribute is a range. The core of our solution is a framework that applies {\\em wavelet transforms} on the data before adding noise to it. We present instantiations of the proposed framework for both ordinal and nominal data, and we provide a theoretical analysis on their privacy and utility guarantees. In an extensive experimental study on both real and synthetic data, we show the effectiveness and efficiency of our solution.",
                        "Citation Paper Authors": "Authors:Xiaokui Xiao, Guozhang Wang, Johannes Gehrke"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": ".\nSpecifying a weaker version of adversary under a semantic framework [ 34,40] or weaker protection on the sensitive\ninformation [ 27,26] allow the design of algorithms with better utility than the standard differentially private algorithms.\nFor example, Blow\ufb01sh privacy ",
                    "Citation Text": "Xi He, Ashwin Machanavajjhala, and Bolin Ding. Blow\ufb01sh privacy: Tuning privacy-utility trade-offs using\npolicies. In Proceedings of the 2014 International Conference on Management of Data (SIGMOD) , pages\n1447\u20131458, 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1312.3913",
                        "Citation Paper Title": "Title:Blowfish Privacy: Tuning Privacy-Utility Trade-offs using Policies",
                        "Citation Paper Abstract": "Abstract:Privacy definitions provide ways for trading-off the privacy of individuals in a statistical database for the utility of downstream analysis of the data. In this paper, we present Blowfish, a class of privacy definitions inspired by the Pufferfish framework, that provides a rich interface for this trade-off. In particular, we allow data publishers to extend differential privacy using a policy, which specifies (a) secrets, or information that must be kept secret, and (b) constraints that may be known about the data. While the secret specification allows increased utility by lessening protection for certain individual properties, the constraint specification provides added protection against an adversary who knows correlations in the data (arising from constraints). We formalize policies and present novel algorithms that can handle general specifications of sensitive information and certain count constraints. We show that there are reasonable policies under which our privacy mechanisms for k-means clustering, histograms and range queries introduce significantly lesser noise than their differentially private counterparts. We quantify the privacy-utility trade-offs for various policies analytically and empirically on real datasets.",
                        "Citation Paper Authors": "Authors:Xi He, Ashwin Machanavajjhala, Bolin Ding"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": ") and Hadamard transform ( e.g., [5,1]) for good utility. Consistent frequency estimation\nwhich requires the estimations to be non-negatives and sum up to 1is investigated in a recent work ",
                    "Citation Text": "Tianhao Wang, Zitao Li, Ninghui Li, Milan Lopuha\u00e4-Zwakenberg, and Boris Skoric. Consistent and accurate\nfrequency oracles under local differential privacy. CoRR , abs/1905.08320, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.08320",
                        "Citation Paper Title": "Title:Locally Differentially Private Frequency Estimation with Consistency",
                        "Citation Paper Abstract": "Abstract:Local Differential Privacy (LDP) protects user privacy from the data collector. LDP protocols have been increasingly deployed in the industry. A basic building block is frequency oracle (FO) protocols, which estimate frequencies of values. While several FO protocols have been proposed, the design goal does not lead to optimal results for answering many queries. In this paper, we show that adding post-processing steps to FO protocols by exploiting the knowledge that all individual frequencies should be non-negative and they sum up to one can lead to significantly better accuracy for a wide range of tasks, including frequencies of individual values, frequencies of the most frequent values, and frequencies of subsets of values. We consider 10 different methods that exploit this knowledge differently. We establish theoretical relationships between some of them and conducted extensive experimental evaluations to understand which methods should be used for different query tasks.",
                        "Citation Paper Authors": "Authors:Tianhao Wang, Milan Lopuha\u00e4-Zwakenberg, Zitao Li, Boris Skoric, Ninghui Li"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": ". A similar notion called Condensed Local Differential Privacy is\nproposed in ",
                    "Citation Text": "Mehmet Emre Gursoy, Acar Tamersoy, Stacey Truex, Wenqi Wei, and Ling Liu. Secure and utility-aware data\ncollection with condensed local differential privacy. arXiv preprint arXiv:1905.06361 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.06361",
                        "Citation Paper Title": "Title:Secure and Utility-Aware Data Collection with Condensed Local Differential Privacy",
                        "Citation Paper Abstract": "Abstract:Local Differential Privacy (LDP) is popularly used in practice for privacy-preserving data collection. Although existing LDP protocols offer high utility for large user populations (100,000 or more users), they perform poorly in scenarios with small user populations (such as those in the cybersecurity domain) and lack perturbation mechanisms that are effective for both ordinal and non-ordinal item sequences while protecting sequence length and content simultaneously. In this paper, we address the small user population problem by introducing the concept of Condensed Local Differential Privacy (CLDP) as a specialization of LDP, and develop a suite of CLDP protocols that offer desirable statistical utility while preserving privacy. Our protocols support different types of client data, ranging from ordinal data types in finite metric spaces (numeric malware infection statistics), to non-ordinal items (OS versions, transaction categories), and to sequences of ordinal and non-ordinal items. Extensive experiments are conducted on multiple datasets, including datasets that are an order of magnitude smaller than those used in existing approaches, which show that proposed CLDP protocols yield high utility. Furthermore, case studies with Symantec datasets demonstrate that our protocols accurately support key cybersecurity-focused tasks of detecting ransomware outbreaks, identifying targeted and vulnerable OSs, and inspecting suspicious activities on infected machines.",
                        "Citation Paper Authors": "Authors:Mehmet Emre Gursoy, Acar Tamersoy, Stacey Truex, Wenqi Wei, Ling Liu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1903.12211v3": {
            "Paper Title": "Privacy in trajectory micro-data publishing : a survey",
            "Sentences": [
                {
                    "Sentence ID": 65,
                    "Sentence": ", or histograms, such as\nthose assumed by Hay et al. ",
                    "Citation Text": "Michael Hay, Ashwin Machanavajjhala, Gerome Miklau, Yan Chen, and Dan Zhang. Princi-\npled evaluation of differentially private algorithms using dpbench. In Proceedings of the 2016\nInternational Conference on Management of Data , SIGMOD \u201916, pages 139\u2013154, New York, NY,\nUSA, 2016. ACM.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.04817",
                        "Citation Paper Title": "Title:Principled Evaluation of Differentially Private Algorithms using DPBench",
                        "Citation Paper Abstract": "Abstract:Differential privacy has become the dominant standard in the research community for strong privacy protection. There has been a flood of research into query answering algorithms that meet this standard. Algorithms are becoming increasingly complex, and in particular, the performance of many emerging algorithms is {\\em data dependent}, meaning the distribution of the noise added to query answers may change depending on the input data. Theoretical analysis typically only considers the worst case, making empirical study of average case performance increasingly important.\nIn this paper we propose a set of evaluation principles which we argue are essential for sound evaluation. Based on these principles we propose DPBench, a novel evaluation framework for standardized evaluation of privacy algorithms. We then apply our benchmark to evaluate algorithms for answering 1- and 2-dimensional range queries. The result is a thorough empirical study of 15 published algorithms on a total of 27 datasets that offers new insights into algorithm behavior---in particular the influence of dataset scale and shape---and a more complete characterization of the state of the art. Our methodology is able to resolve inconsistencies in prior empirical studies and place algorithm performance in context through comparison to simple baselines. Finally, we pose open research questions which we hope will guide future algorithm design.",
                        "Citation Paper Authors": "Authors:Michael Hay, Ashwin Machanavajjhala, Gerome Miklau, Yan Chen, Dan Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1909.12095v5": {
            "Paper Title": "A Decision Tree Learning Approach for Mining Relationship-Based Access\n  Control Policies",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01755v3": {
            "Paper Title": "Constant-Time Foundations for the New Spectre Era",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.01406v4": {
            "Paper Title": "Keeping out the Masses: Understanding the Popularity and Implications of\n  Internet Paywalls",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.02551v2": {
            "Paper Title": "Online Password Guessability via Multi-Dimensional Rank Estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01721v2": {
            "Paper Title": "A Critical View on CIS Controls",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.13849v4": {
            "Paper Title": "Uplink-Downlink Tradeoff in Secure Distributed Matrix Multiplication",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.07456v2": {
            "Paper Title": "Measuring Membership Privacy on Aggregate Location Time-Series",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": "show that MIAs\nare also possible against generative models, while Melis et al. ",
                    "Citation Text": "Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov. 2019. Inference Attacks Against Collabo-\nrative Learning. In S&P.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.04049",
                        "Citation Paper Title": "Title:Exploiting Unintended Feature Leakage in Collaborative Learning",
                        "Citation Paper Abstract": "Abstract:Collaborative machine learning and related techniques such as federated learning allow multiple participants, each with his own training dataset, to build a joint model by training locally and periodically exchanging model updates. We demonstrate that these updates leak unintended information about participants' training data and develop passive and active inference attacks to exploit this leakage. First, we show that an adversarial participant can infer the presence of exact data points -- for example, specific locations -- in others' training data (i.e., membership inference). Then, we show how this adversary can infer properties that hold only for a subset of the training data and are independent of the properties that the joint model aims to capture. For example, he can infer when a specific person first appears in the photos used to train a binary gender classifier. We evaluate our attacks on a variety of tasks, datasets, and learning configurations, analyze their limitations, and discuss possible defenses.",
                        "Citation Paper Authors": "Authors:Luca Melis, Congzheng Song, Emiliano De Cristofaro, Vitaly Shmatikov"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "and feasible in broader scenarios. For instance, Hayes et al. ",
                    "Citation Text": "Jamie Hayes, Luca Melis, George Danezis, and Emiliano De Cristofaro. 2019. LOGAN: Evaluating Privacy Leakage of\nGenerative Models Using Generative Adversarial Networks. In PoPETS .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.07663",
                        "Citation Paper Title": "Title:LOGAN: Membership Inference Attacks Against Generative Models",
                        "Citation Paper Abstract": "Abstract:Generative models estimate the underlying distribution of a dataset to generate realistic samples according to that distribution. In this paper, we present the first membership inference attacks against generative models: given a data point, the adversary determines whether or not it was used to train the model. Our attacks leverage Generative Adversarial Networks (GANs), which combine a discriminative and a generative model, to detect overfitting and recognize inputs that were part of training datasets, using the discriminator's capacity to learn statistical differences in distributions.\nWe present attacks based on both white-box and black-box access to the target model, against several state-of-the-art generative models, over datasets of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). We also discuss the sensitivity of the attacks to different training parameters, and their robustness against mitigation strategies, finding that defenses are either ineffective or lead to significantly worse performances of the generative models in terms of training stability and/or sample quality.",
                        "Citation Paper Authors": "Authors:Jamie Hayes, Luca Melis, George Danezis, Emiliano De Cristofaro"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "reconstruct\nvictims\u2019 location trajectories from aggregate mobility data, without any prior knowledge, while ",
                    "Citation Text": "Apostolos Pyrgelis, Carmela Troncoso, and Emiliano De Cristofaro. 2017. What Does The Crowd Say About You?\nEvaluating Aggregation-based Location Privacy. In PoPETS .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.00366",
                        "Citation Paper Title": "Title:What Does The Crowd Say About You? Evaluating Aggregation-based Location Privacy",
                        "Citation Paper Abstract": "Abstract:Information about people's movements and the locations they visit enables an increasing number of mobility analytics applications, e.g., in the context of urban and transportation planning, In this setting, rather than collecting or sharing raw data, entities often use aggregation as a privacy protection mechanism, aiming to hide individual users' location traces. Furthermore, to bound information leakage from the aggregates, they can perturb the input of the aggregation or its output to ensure that these are differentially private.\nIn this paper, we set to evaluate the impact of releasing aggregate location time-series on the privacy of individuals contributing to the aggregation. We introduce a framework allowing us to reason about privacy against an adversary attempting to predict users' locations or recover their mobility patterns. We formalize these attacks as inference problems, and discuss a few strategies to model the adversary's prior knowledge based on the information she may have access to. We then use the framework to quantify the privacy loss stemming from aggregate location data, with and without the protection of differential privacy, using two real-world mobility datasets. We find that aggregates do leak information about individuals' punctual locations and mobility profiles. The density of the observations, as well as timing, play important roles, e.g., regular patterns during peak hours are better protected than sporadic movements. Finally, our evaluation shows that both output and input perturbation offer little additional protection, unless they introduce large amounts of noise ultimately destroying the utility of the data.",
                        "Citation Paper Authors": "Authors:Apostolos Pyrgelis, Carmela Troncoso, Emiliano De Cristofaro"
                    }
                },
                {
                    "Sentence ID": 66,
                    "Sentence": "that are commonly used in location privacy literature, protect aggregate location\ndata.\nAggregate Location Privacy. Aggregation is often not an effective way to preserve the privacy\nof location data, as aggregates leak information about individual users. Xu et al. ",
                    "Citation Text": "Fengli Xu, Zhen Tu, Yong Li, Pengyu Zhang, Xiaoming Fu, and Depeng Jin. 2017. Trajectory Recovery From Ash: User\nPrivacy Is NOT Preserved in Aggregated Mobility Data. In WWW .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.06270",
                        "Citation Paper Title": "Title:Trajectory Recovery From Ash: User Privacy Is NOT Preserved in Aggregated Mobility Data",
                        "Citation Paper Abstract": "Abstract:Human mobility data has been ubiquitously collected through cellular networks and mobile applications, and publicly released for academic research and commercial purposes for the last decade. Since releasing individual's mobility records usually gives rise to privacy issues, datasets owners tend to only publish aggregated mobility data, such as the number of users covered by a cellular tower at a specific timestamp, which is believed to be sufficient for preserving users' privacy. However, in this paper, we argue and prove that even publishing aggregated mobility data could lead to privacy breach in individuals' trajectories. We develop an attack system that is able to exploit the uniqueness and regularity of human mobility to recover individual's trajectories from the aggregated mobility data without any prior knowledge. By conducting experiments on two real-world datasets collected from both mobile application and cellular network, we reveal that the attack system is able to recover users' trajectories with accuracy about 73%~91% at the scale of tens of thousands to hundreds of thousands users, which indicates severe privacy leakage in such datasets. Through the investigation on aggregated mobility data, our work recognizes a novel privacy problem in publishing statistic data, which appeals for immediate attentions from both academy and industry.",
                        "Citation Paper Authors": "Authors:Fengli Xu, Zhen Tu, Yong Li, Pengyu Zhang, Xiaoming Fu, Depeng Jin"
                    }
                },
                {
                    "Sentence ID": 58,
                    "Sentence": ", while retaining better utility levels. For instance,\nTo et al. ",
                    "Citation Text": "Hien To, Kien Nguyen, and Cyrus Shahabi. 2016. Differentially private publication of location entropy. In SIGSPATIAL .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.00882",
                        "Citation Paper Title": "Title:Differentially Private Publication of Location Entropy",
                        "Citation Paper Abstract": "Abstract:Location entropy (LE) is a popular metric for measuring the popularity of various locations (e.g., points-of-interest). Unlike other metrics computed from only the number of (unique) visits to a location, namely frequency, LE also captures the diversity of the users' visits, and is thus more accurate than other metrics. Current solutions for computing LE require full access to the past visits of users to locations, which poses privacy threats. This paper discusses, for the first time, the problem of perturbing location entropy for a set of locations according to differential privacy. The problem is challenging because removing a single user from the dataset will impact multiple records of the database; i.e., all the visits made by that user to various locations. Towards this end, we first derive non-trivial, tight bounds for both local and global sensitivity of LE, and show that to satisfy $\\epsilon$-differential privacy, a large amount of noise must be introduced, rendering the published results useless. Hence, we propose a thresholding technique to limit the number of users' visits, which significantly reduces the perturbation error but introduces an approximation error. To achieve better utility, we extend the technique by adopting two weaker notions of privacy: smooth sensitivity (slightly weaker) and crowd-blending (strictly weaker). Extensive experiments on synthetic and real-world datasets show that our proposed techniques preserve original data distribution without compromising location privacy.",
                        "Citation Paper Authors": "Authors:Hien To, Kien Nguyen, Cyrus Shahabi"
                    }
                },
                {
                    "Sentence ID": 50,
                    "Sentence": "measure the uniqueness of human mobility in a Call Detail Records\n(CDR) dataset, finding that four spatio-temporal points are enough to uniquely identify 95% of the\nindividuals, while Rossi et al. ",
                    "Citation Text": "Luca Rossi, James Walker, and Mirco Musolesi. 2015. Spatio-temporal techniques for user identification by means of\nGPS mobility data. EPJ Data Science (2015).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1501.06814",
                        "Citation Paper Title": "Title:Spatio-Temporal Techniques for User Identification by means of GPS Mobility Data",
                        "Citation Paper Abstract": "Abstract:One of the greatest concerns related to the popularity of GPS-enabled devices and applications is the increasing availability of the personal location information generated by them and shared with application and service providers. Moreover, people tend to have regular routines and be characterized by a set of \"significant places\", thus making it possible to identify a user from his/her mobility data.\nIn this paper we present a series of techniques for identifying individuals from their GPS movements. More specifically, we study the uniqueness of GPS information for three popular datasets, and we provide a detailed analysis of the discriminatory power of speed, direction and distance of travel. Most importantly, we present a simple yet effective technique for the identification of users from location information that are not included in the original dataset used for training, thus raising important privacy concerns for the management of location datasets.",
                        "Citation Paper Authors": "Authors:Luca Rossi, James Walker, Mirco Musolesi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1906.02108v4": {
            "Paper Title": "Evaluating Explanation Methods for Deep Learning in Security",
            "Sentences": [
                {
                    "Sentence ID": 44,
                    "Sentence": "show that many white-box\nmethods can be tricked to produce an arbitrary explanation\netwithout changing the classi\ufb01cation by solving\nmin\n\u000ede\u0000\ngf(~x);et\u0001\n+\rdp\u0000\nf(~x);f(x)\u0001\n: (3)\nWhile the aforementioned attacks are constructed for\nwhite-box methods, Slack et al. ",
                    "Citation Text": "D. Slack, S. Hilgard, E. Jia, S. Singh, and\nH. Lakkaraju. Fooling lime and shap: Adversarial at-tacks on post hoc explanation methods. In AAAI/ACM\nConference on Arti\ufb01cial Intelligence , Ethics, and\nSociety (AIES) , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.02508",
                        "Citation Paper Title": "Title:Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods",
                        "Citation Paper Abstract": "Abstract:As machine learning black boxes are increasingly being deployed in domains such as healthcare and criminal justice, there is growing emphasis on building tools and techniques for explaining these black boxes in an interpretable manner. Such explanations are being leveraged by domain experts to diagnose systematic errors and underlying biases of black boxes. In this paper, we demonstrate that post hoc explanations techniques that rely on input perturbations, such as LIME and SHAP, are not reliable. Specifically, we propose a novel scaffolding technique that effectively hides the biases of any given classifier by allowing an adversarial entity to craft an arbitrary desired explanation. Our approach can be used to scaffold any biased classifier in such a way that its predictions on the input data distribution still remain biased, but the post hoc explanations of the scaffolded classifier look innocuous. Using extensive evaluation with multiple real-world datasets (including COMPAS), we demonstrate how extremely biased (racist) classifiers crafted by our framework can easily fool popular explanation techniques such as LIME and SHAP into generating innocuous explanations which do not reflect the underlying biases.",
                        "Citation Paper Authors": "Authors:Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, Himabindu Lakkaraju"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "propose to solve\nmin\n\u000edp\u0000\nf(~x);ct\u0001\n+\u0015de\u0000\ngf(~x);gf(x)\u0001\n; (2)\nwheredpanddeare distance measures for classes and\nexplanations of f. The crafted input ~xis misclassi\ufb01ed by\nthe network but keeps an explanation very close to the one\nofx. Dombrowski et al. ",
                    "Citation Text": "A.-K. Dombrowski, M. Alber, C. J. Anders, M. Ack-ermann, K.-R. M\u00fcller, and P. Kessel. Explanations\ncan be manipulated and geometry is to blame. In\nAdvances in Neural Information Proccessing Systems\n(NIPS) , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.07983",
                        "Citation Paper Title": "Title:Explanations can be manipulated and geometry is to blame",
                        "Citation Paper Abstract": "Abstract:Explanation methods aim to make neural networks more trustworthy and interpretable. In this paper, we demonstrate a property of explanation methods which is disconcerting for both of these purposes. Namely, we show that explanations can be manipulated arbitrarily by applying visually hardly perceptible perturbations to the input that keep the network's output approximately constant. We establish theoretically that this phenomenon can be related to certain geometrical properties of neural networks. This allows us to derive an upper bound on the susceptibility of explanations to manipulations. Based on this result, we propose effective mechanisms to enhance the robustness of explanations.",
                        "Citation Paper Authors": "Authors:Ann-Kathrin Dombrowski, Maximilian Alber, Christopher J. Anders, Marcel Ackermann, Klaus-Robert M\u00fcller, Pan Kessel"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ". As a result, RNNs have been successfully\napplied in security tasks involving sequential data, such as\nthe recognition of functions in native code [ 10,41] or the\ndiscovery of vulnerabilities in software ",
                    "Citation Text": "Z. Li, D. Zou, S. Xu, X. Ou, H. Jin, S. Wang, Z. Deng,\nand Y . Zhong. Vuldeepecker: A deep learning-based\nsystem for vulnerability detection. In Proc. of the\nNetwork and Distributed System Security Symposium\n(NDSS) , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.01681",
                        "Citation Paper Title": "Title:VulDeePecker: A Deep Learning-Based System for Vulnerability Detection",
                        "Citation Paper Abstract": "Abstract:The automatic detection of software vulnerabilities is an important research problem. However, existing solutions to this problem rely on human experts to define features and often miss many vulnerabilities (i.e., incurring high false negative rate). In this paper, we initiate the study of using deep learning-based vulnerability detection to relieve human experts from the tedious and subjective task of manually defining features. Since deep learning is motivated to deal with problems that are very different from the problem of vulnerability detection, we need some guiding principles for applying deep learning to vulnerability detection. In particular, we need to find representations of software programs that are suitable for deep learning. For this purpose, we propose using code gadgets to represent programs and then transform them into vectors, where a code gadget is a number of (not necessarily consecutive) lines of code that are semantically related to each other. This leads to the design and implementation of a deep learning-based vulnerability detection system, called Vulnerability Deep Pecker (VulDeePecker). In order to evaluate VulDeePecker, we present the first vulnerability dataset for deep learning approaches. Experimental results show that VulDeePecker can achieve much fewer false negatives (with reasonable false positives) than other approaches. We further apply VulDeePecker to 3 software products (namely Xen, Seamonkey, and Libav) and detect 4 vulnerabilities, which are not reported in the National Vulnerability Database but were \"silently\" patched by the vendors when releasing later versions of these products; in contrast, these vulnerabilities are almost entirely missed by the other vulnerability detection systems we experimented with.",
                        "Citation Paper Authors": "Authors:Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan Wang, Zhijun Deng, Yuyi Zhong"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "and is based on\nsimple gradients. The output of the method is given by\nri=@y=@xi, which the authors call a saliency map .\nHererimeasures how much ychanges with respect to xi.\nSundararajan et al. ",
                    "Citation Text": "M. Sundararajan, A. Taly, and Q. Yan. Axiomatic\nattribution for deep networks. In Proceedings of the\n34th International Conference on Machine Learning ,\npages 3319\u20133328, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.01365",
                        "Citation Paper Title": "Title:Axiomatic Attribution for Deep Networks",
                        "Citation Paper Abstract": "Abstract:We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.",
                        "Citation Paper Authors": "Authors:Mukund Sundararajan, Ankur Taly, Qiqi Yan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1905.07672v5": {
            "Paper Title": "Taking Care of The Discretization Problem: A Comprehensive Study of the\n  Discretization Problem and A Black-Box Adversarial Attack in Discrete Integer\n  Domain",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": "proposed an\nautoencoder-based method (named AutoZOOM) to improve query efficiency. Similarly, Bhagoji\net al. ",
                    "Citation Text": "Arjun Nitin Bhagoji, Warren He, Bo Li, and Dawn Song. 2018. Practical Black-Box Attacks on Deep Neural Networks\nUsing Efficient Query Mechanisms. In Proceedings of the 15th European Conference on Computer Vision (ECCV) .\n158\u2013174.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.09491",
                        "Citation Paper Title": "Title:Exploring the Space of Black-box Attacks on Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:Existing black-box attacks on deep neural networks (DNNs) so far have largely focused on transferability, where an adversarial instance generated for a locally trained model can \"transfer\" to attack other learning models. In this paper, we propose novel Gradient Estimation black-box attacks for adversaries with query access to the target model's class probabilities, which do not rely on transferability. We also propose strategies to decouple the number of queries required to generate each adversarial sample from the dimensionality of the input. An iterative variant of our attack achieves close to 100% adversarial success rates for both targeted and untargeted attacks on DNNs. We carry out extensive experiments for a thorough comparative evaluation of black-box attacks and show that the proposed Gradient Estimation attacks outperform all transferability based black-box attacks we tested on both MNIST and CIFAR-10 datasets, achieving adversarial success rates similar to well known, state-of-the-art white-box attacks. We also apply the Gradient Estimation attacks successfully against a real-world Content Moderation classifier hosted by Clarifai. Furthermore, we evaluate black-box attacks against state-of-the-art defenses. We show that the Gradient Estimation attacks are very effective even against these defenses.",
                        "Citation Paper Authors": "Authors:Arjun Nitin Bhagoji, Warren He, Bo Li, Dawn Song"
                    }
                },
                {
                    "Sentence ID": 104,
                    "Sentence": "proposed a black-box attack\nmethod (named ZOO) with zeroth order optimization. Following ZOO, Tu et al. ",
                    "Citation Text": "Chun-Chen Tu, Pai-Shun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, and Shin-Ming Cheng.\n2019. AutoZOOM: Autoencoder-Based Zeroth Order Optimization Method for Attacking Black-Box Neural Networks.\nInThe Thirty-Third AAAI Conference on Artificial Intelligence . 742\u2013749.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.11770",
                        "Citation Paper Title": "Title:AutoZOOM: Autoencoder-based Zeroth Order Optimization Method for Attacking Black-box Neural Networks",
                        "Citation Paper Abstract": "Abstract:Recent studies have shown that adversarial examples in state-of-the-art image classifiers trained by deep neural networks (DNN) can be easily generated when the target model is transparent to an attacker, known as the white-box setting. However, when attacking a deployed machine learning service, one can only acquire the input-output correspondences of the target model; this is the so-called black-box attack setting. The major drawback of existing black-box attacks is the need for excessive model queries, which may give a false sense of model robustness due to inefficient query designs. To bridge this gap, we propose a generic framework for query-efficient black-box attacks. Our framework, AutoZOOM, which is short for Autoencoder-based Zeroth Order Optimization Method, has two novel building blocks towards efficient black-box attacks: (i) an adaptive random gradient estimation strategy to balance query counts and distortion, and (ii) an autoencoder that is either trained offline with unlabeled data or a bilinear resizing operation for attack acceleration. Experimental results suggest that, by applying AutoZOOM to a state-of-the-art black-box attack (ZOO), a significant reduction in model queries can be achieved without sacrificing the attack success rate and the visual quality of the resulting adversarial examples. In particular, when compared to the standard ZOO method, AutoZOOM can consistently reduce the mean query counts in finding successful adversarial examples (or reaching the same distortion level) by at least 93% on MNIST, CIFAR-10 and ImageNet datasets, leading to novel insights on adversarial robustness.",
                        "Citation Paper Authors": "Authors:Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, Shin-Ming Cheng"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "Targeted Continuous \u2717 \u2713 \u2717\nReference Complete Domain Considered B2G AvoidableVerification methods\nWhite-boxBILVNC ",
                    "Citation Text": "Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis, Aditya V. Nori, and Antonio Criminisi.\n2016. Measuring Neural Net Robustness with Constraints. In NIPS . 2613\u20132621.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.07262",
                        "Citation Paper Title": "Title:Measuring Neural Net Robustness with Constraints",
                        "Citation Paper Abstract": "Abstract:Despite having high accuracy, neural nets have been shown to be susceptible to adversarial examples, where a small perturbation to an input can cause it to become mislabeled. We propose metrics for measuring the robustness of a neural net and devise a novel algorithm for approximating these metrics based on an encoding of robustness as a linear program. We show how our metrics can be used to evaluate the robustness of deep neural nets with experiments on the MNIST and CIFAR-10 datasets. Our algorithm generates more informative estimates of robustness metrics compared to estimates based on existing algorithms. Furthermore, we show how existing approaches to improving robustness \"overfit\" to adversarial examples generated using a specific algorithm. Finally, we show that our techniques can be used to additionally improve neural net robustness both according to the metrics that we propose, but also according to previously proposed metrics.",
                        "Citation Paper Authors": "Authors:Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis, Aditya Nori, Antonio Criminisi"
                    }
                },
                {
                    "Sentence ID": 76,
                    "Sentence": "proposed the first black-box method by leveraging trans-\nferability property of adversarial examples. It first trains a local substitute model with a synthetic\ndataset and then crafts adversarial examples from the local substitute model. ",
                    "Citation Text": "Nicolas Papernot, Patrick D. McDaniel, and Ian J. Goodfellow. 2016. Transferability in Machine Learning: from\nPhenomena to Black-Box Attacks using Adversarial Samples. CoRR abs/1605.07277 (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.07277",
                        "Citation Paper Title": "Title:Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples",
                        "Citation Paper Abstract": "Abstract:Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that affect one model often affect another model, even if the two models have different architectures or were trained on different training sets, so long as both models were trained to perform the same task. An attacker may therefore train their own substitute model, craft adversarial examples against the substitute, and transfer them to a victim model, with very little information about the victim. Recent work has further developed a technique that uses the victim model as an oracle to label a synthetic training set for the substitute, so the attacker need not even collect a training set to mount the attack. We extend these recent techniques using reservoir sampling to greatly enhance the efficiency of the training procedure for the substitute model. We introduce new transferability attacks between previously unexplored (substitute, victim) pairs of machine learning model classes, most notably SVMs and decision trees. We demonstrate our attacks on two commercial machine learning classification systems from Amazon (96.19% misclassification rate) and Google (88.94%) using only 800 queries of the victim model, thereby showing that existing machine learning approaches are in general vulnerable to systematic black-box attacks regardless of their structure.",
                        "Citation Paper Authors": "Authors:Nicolas Papernot, Patrick McDaniel, Ian Goodfellow"
                    }
                },
                {
                    "Sentence ID": 65,
                    "Sentence": "proposed a method by iteratively\nadding Gaussian noise. Liu et al. ",
                    "Citation Text": "Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. 2017. Delving into Transferable Adversarial Examples and\nBlack-box Attacks. In Proceedings of the 5th International Conference on Learning Representations .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.02770",
                        "Citation Paper Title": "Title:Delving into Transferable Adversarial Examples and Black-box Attacks",
                        "Citation Paper Abstract": "Abstract:An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack this http URL, which is a black-box image classification system.",
                        "Citation Paper Authors": "Authors:Yanpei Liu, Xinyun Chen, Chang Liu, Dawn Song"
                    }
                },
                {
                    "Sentence ID": 116,
                    "Sentence": "proposed a\nbandit optimization-based method aimed at enhancing query efficiency. Recently, Zhao et al. ",
                    "Citation Text": "Pu Zhao, Sijia Liu, Pin-Yu Chen, Nghia Hoang, Kaidi Xu, Bhavya Kailkhura, and Xue Lin. 2019. On the Design of\nBlack-box Adversarial Examples by Leveraging Gradient-free Optimization and Operator Splitting Method. In ICCV\n2019 (accepted) .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.11684",
                        "Citation Paper Title": "Title:On the Design of Black-box Adversarial Examples by Leveraging Gradient-free Optimization and Operator Splitting Method",
                        "Citation Paper Abstract": "Abstract:Robust machine learning is currently one of the most prominent topics which could potentially help shaping a future of advanced AI platforms that not only perform well in average cases but also in worst cases or adverse situations. Despite the long-term vision, however, existing studies on black-box adversarial attacks are still restricted to very specific settings of threat models (e.g., single distortion metric and restrictive assumption on target model's feedback to queries) and/or suffer from prohibitively high query complexity. To push for further advances in this field, we introduce a general framework based on an operator splitting method, the alternating direction method of multipliers (ADMM) to devise efficient, robust black-box attacks that work with various distortion metrics and feedback settings without incurring high query complexity. Due to the black-box nature of the threat model, the proposed ADMM solution framework is integrated with zeroth-order (ZO) optimization and Bayesian optimization (BO), and thus is applicable to the gradient-free regime. This results in two new black-box adversarial attack generation methods, ZO-ADMM and BO-ADMM. Our empirical evaluations on image classification datasets show that our proposed approaches have much lower function query complexities compared to state-of-the-art attack methods, but achieve very competitive attack success rates.",
                        "Citation Paper Authors": "Authors:Pu Zhao, Sijia Liu, Pin-Yu Chen, Nghia Hoang, Kaidi Xu, Bhavya Kailkhura, Xue Lin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.10617v3": {
            "Paper Title": "LNBot: A Covert Hybrid Botnet on Bitcoin Lightning Network for Fun and\n  Profit",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.01840v2": {
            "Paper Title": "Who is Real Bob? Adversarial Attacks on Speaker Recognition Systems",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "because all of them are not available.\nWe are the \ufb01rst considering the threshold \u0012in adversarial\nattack. Adversarial attacks on speech recognition systems also\nhave been studied ",
                    "Citation Text": "R. Taori, A. Kamsetty, B. Chu, and N. Vemuri, \u201cTargeted adversarial\nexamples for black box audio systems,\u201d in IEEE S&P Workshops , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.07820",
                        "Citation Paper Title": "Title:Targeted Adversarial Examples for Black Box Audio Systems",
                        "Citation Paper Abstract": "Abstract:The application of deep recurrent networks to audio transcription has led to impressive gains in automatic speech recognition (ASR) systems. Many have demonstrated that small adversarial perturbations can fool deep neural networks into incorrectly predicting a specified target with high confidence. Current work on fooling ASR systems have focused on white-box attacks, in which the model architecture and parameters are known. In this paper, we adopt a black-box approach to adversarial generation, combining the approaches of both genetic algorithms and gradient estimation to solve the task. We achieve a 89.25% targeted attack similarity after 3000 generations while maintaining 94.6% audio file similarity.",
                        "Citation Paper Authors": "Authors:Rohan Taori, Amog Kamsetty, Brenton Chu, Nikita Vemuri"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1901.04411v2": {
            "Paper Title": "Uncovering Vulnerable Industrial Control Systems from the Internet Core",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.00902v2": {
            "Paper Title": "GrAALF:Supporting Graphical Analysis of Audit Logs for Forensics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.04769v2": {
            "Paper Title": "Post-quantum Zero Knowledge in Constant Rounds",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.03566v2": {
            "Paper Title": "That which we call private",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.04316v3": {
            "Paper Title": "Differential Privacy in Blockchain Technology: A Futuristic Approach",
            "Sentences": [
                {
                    "Sentence ID": 41,
                    "Sentence": ".\nFrom privacy perspective, Ethereum provides cryptographi c\nhash functions as mean of privacy, and transaction and other\nrecords are protected via using cryptographic mechanisms-\nbased privacy ",
                    "Citation Text": "H. Chen, M. Pendleton, L. Njilla, and S. Xu, \u201cA survey on e thereum\nsystems security: Vulnerabilities, attacks and defenses, \u201darXiv preprint\narXiv:1908.04507 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.04507",
                        "Citation Paper Title": "Title:A Survey on Ethereum Systems Security: Vulnerabilities, Attacks and Defenses",
                        "Citation Paper Abstract": "Abstract:The blockchain technology is believed by many to be a game changer in many application domains, especially financial applications. While the first generation of blockchain technology (i.e., Blockchain 1.0) is almost exclusively used for cryptocurrency purposes, the second generation (i.e., Blockchain 2.0), as represented by Ethereum, is an open and decentralized platform enabling a new paradigm of computing --- Decentralized Applications (DApps) running on top of blockchains. The rich applications and semantics of DApps inevitably introduce many security vulnerabilities, which have no counterparts in pure cryptocurrency systems like Bitcoin. Since Ethereum is a new, yet complex, system, it is imperative to have a systematic and comprehensive understanding on its security from a holistic perspective, which is unavailable. To the best of our knowledge, the present survey, which can also be used as a tutorial, fills this void. In particular, we systematize three aspects of Ethereum systems security: vulnerabilities, attacks, and defenses. We draw insights into, among other things, vulnerability root causes, attack consequences, and defense capabilities, which shed light on future research directions.",
                        "Citation Paper Authors": "Authors:Huashan Chen, Marcus Pendleton, Laurent Njilla, Shouhuai Xu"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "2019 Presented a literature review for typical pri-\nvacy preserving approaches being used in\nblockchain.\u2022 Approaches being used in Cryp-\ntocurrenciesNo\nAttack Surface\nof Blockchain ",
                    "Citation Text": "M. Saad, J. Spaulding, L. Njilla, C. Kamhoua, S. Shetty, D. H. Nyang,\nand D. Mohaisen, \u201cExploring the attack surface of blockchai n: A\ncomprehensive survey,\u201d IEEE Communications Surveys Tutorials , pp.\n1\u20131, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.03487",
                        "Citation Paper Title": "Title:Exploring the Attack Surface of Blockchain: A Systematic Overview",
                        "Citation Paper Abstract": "Abstract:In this paper, we systematically explore the attack surface of the Blockchain technology, with an emphasis on public Blockchains. Towards this goal, we attribute attack viability in the attack surface to 1) the Blockchain cryptographic constructs, 2) the distributed architecture of the systems using Blockchain, and 3) the Blockchain application context. To each of those contributing factors, we outline several attacks, including selfish mining, the 51% attack, Domain Name System (DNS) attacks, distributed denial-of-service (DDoS) attacks, consensus delay (due to selfish behavior or distributed denial-of-service attacks), Blockchain forks, orphaned and stale blocks, block ingestion, wallet thefts, smart contract attacks, and privacy attacks. We also explore the causal relationships between these attacks to demonstrate how various attack vectors are connected to one another. A secondary contribution of this work is outlining effective defense measures taken by the Blockchain technology or proposed by researchers to mitigate the effects of these attacks and patch associated vulnerabilities",
                        "Citation Paper Authors": "Authors:Muhammad Saad, Jeffrey Spaulding, Laurent Njilla, Charles Kamhoua, Sachin Shetty, DaeHun Nyang, Aziz Mohaisen"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.06879v4": {
            "Paper Title": "Separating Local & Shuffled Differential Privacy via Histograms",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.01855v2": {
            "Paper Title": "Secure Multi-Party Computation for Inter-Organizational Process Mining",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.10218v2": {
            "Paper Title": "Boosting Privately: Privacy-Preserving Federated Extreme Boosting for\n  Mobile Crowdsensing",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": "applied federated learning to the long-short term\nmemory network (LSTM) based language model and gain\nbetter performance than the traditional centralized machine\nlearning method. Smith et al. ",
                    "Citation Text": "V . Smith, C.-K. Chiang, M. Sanjabi, and A. S. Talwalkar, \u201cFederated\nmulti-task learning,\u201d in Advances in Neural Information Processing\nSystems , 2017, pp. 4424\u20134434.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.10467",
                        "Citation Paper Title": "Title:Federated Multi-Task Learning",
                        "Citation Paper Abstract": "Abstract:Federated learning poses new statistical and systems challenges in training machine learning models over distributed networks of devices. In this work, we show that multi-task learning is naturally suited to handle the statistical challenges of this setting, and propose a novel systems-aware optimization method, MOCHA, that is robust to practical systems issues. Our method and theory for the first time consider issues of high communication cost, stragglers, and fault tolerance for distributed multi-task learning. The resulting method achieves significant speedups compared to alternatives in the federated setting, as we demonstrate through simulations on real-world federated datasets.",
                        "Citation Paper Authors": "Authors:Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, Ameet Talwalkar"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.09176v2": {
            "Paper Title": "Lower Bounds for Function Inversion with Quantum Advice",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.02954v4": {
            "Paper Title": "Selfish Behavior in the Tezos Proof-of-Stake Protocol",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": ", who demonstrate that min-\ners can earn a higher proportion of rewards in a PoW protocol\nby deviating from the honest protocol and following a sel\ufb01sh\nmining strategy. Follow-up work includes Sapirshtein et al.\n(2016) ",
                    "Citation Text": "Ayelet Sapirshtein, Yonatan Sompolinsky, and Aviv Zo-\nhar. \u201cOptimal sel\ufb01sh mining strategies in bitcoin\u201d. In:\nInternational Conference on Financial Cryptography\nand Data Security . Springer. 2016, pp. 515\u2013532.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1507.06183",
                        "Citation Paper Title": "Title:Optimal Selfish Mining Strategies in Bitcoin",
                        "Citation Paper Abstract": "Abstract:Bitcoin is a decentralized crypto-currency, and an accompanying protocol, created in 2008. Bitcoin nodes continuously generate and propagate blocks---collections of newly approved transactions that are added to Bitcoin's ledger. Block creation requires nodes to invest computational resources, but also carries a reward in the form of bitcoins that are paid to the creator. While the protocol requires nodes to quickly distribute newly created blocks, strong nodes can in fact gain higher payoffs by withholding blocks they create and selectively postponing their publication. The existence of such selfish mining attacks was first reported by Eyal and Sirer, who have demonstrated a specific deviation from the standard protocol (a strategy that we name SM1).\nIn this paper we extend the underlying model for selfish mining attacks, and provide an algorithm to find $\\epsilon$-optimal policies for attackers within the model, as well as tight upper bounds on the revenue of optimal policies. As a consequence, we are able to provide lower bounds on the computational power an attacker needs in order to benefit from selfish mining. We find that the profit threshold -- the minimal fraction of resources required for a profitable attack -- is strictly lower than the one induced by the SM1 scheme. Indeed, the policies given by our algorithm dominate SM1, by better regulating attack-withdrawals.\nUsing our algorithm, we show that Eyal and Sirer's suggested countermeasure to selfish mining is slightly less effective than previously conjectured. Next, we gain insight into selfish mining in the presence of communication delays, and show that, under a model that accounts for delays, the profit threshold vanishes, and even small attackers have incentive to occasionally deviate from the protocol. We conclude with observations regarding the combined power of selfish mining and double spending attacks.",
                        "Citation Paper Authors": "Authors:Ayelet Sapirshtein, Yonatan Sompolinsky, Aviv Zohar"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1909.01785v2": {
            "Paper Title": "Certified Side Channels",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.00580v3": {
            "Paper Title": "PubChain: A Decentralized Open-Access Publication Platform with\n  Participants Incentivized by Blockchain Technology",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.05193v3": {
            "Paper Title": "TBT: Targeted Neural Network Attack with Bit Trojan",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": "is also a similar method that tries to \ufb01ne prune the\nTrojaned model after the back door attack has been deployed. Activation clustering is\nalso found to be effective to detect Trojan infected model ",
                    "Citation Text": "Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin\nEdwards, Taesung Lee, Ian Molloy, and Biplav Srivastava. Detecting back-\ndoor attacks on deep neural networks by activation clustering. arXiv preprint\narXiv:1811.03728 , 2018. 4, 12, 14",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.03728",
                        "Citation Paper Title": "Title:Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering",
                        "Citation Paper Abstract": "Abstract:While machine learning (ML) models are being increasingly trusted to make decisions in different and varying areas, the safety of systems using such models has become an increasing concern. In particular, ML models are often trained on data from potentially untrustworthy sources, providing adversaries with the opportunity to manipulate them by inserting carefully crafted samples into the training set. Recent work has shown that this type of attack, called a poisoning attack, allows adversaries to insert backdoors or trojans into the model, enabling malicious behavior with simple external backdoor triggers at inference time and only a blackbox perspective of the model itself. Detecting this type of attack is challenging because the unexpected behavior occurs only when a backdoor trigger, which is known only to the adversary, is present. Model users, either direct users of training data or users of pre-trained model from a catalog, may not guarantee the safe operation of their ML-based system. In this paper, we propose a novel approach to backdoor detection and removal for neural networks. Through extensive experimental results, we demonstrate its effectiveness for neural networks classifying text and images. To the best of our knowledge, this is the first methodology capable of detecting poisonous data crafted to insert backdoors and repairing the model that does not require a verified and trusted dataset.",
                        "Citation Paper Authors": "Authors:Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian Molloy, Biplav Srivastava"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "uses a\ncombination of pruning, input \ufb01ltering and unlearning to identify backdoor attacks\non the model. Fine Pruning ",
                    "Citation Text": "Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending\nagainst backdooring attacks on deep neural networks. In International Symposium\non Research in Attacks, Intrusions, and Defenses , pages 273\u2013294. Springer, 2018.\n4, 14",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.12185",
                        "Citation Paper Title": "Title:Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:Deep neural networks (DNNs) provide excellent performance across a wide range of classification tasks, but their training requires high computational resources and is often outsourced to third parties. Recent work has shown that outsourced training introduces the risk that a malicious trainer will return a backdoored DNN that behaves normally on most inputs but causes targeted misclassifications or degrades the accuracy of the network when a trigger known only to the attacker is present. In this paper, we provide the first effective defenses against backdoor attacks on DNNs. We implement three backdoor attacks from prior work and use them to investigate two promising defenses, pruning and fine-tuning. We show that neither, by itself, is sufficient to defend against sophisticated attackers. We then evaluate fine-pruning, a combination of pruning and fine-tuning, and show that it successfully weakens or even eliminates the backdoors, i.e., in some cases reducing the attack success rate to 0% with only a 0.4% drop in accuracy for clean (non-triggering) inputs. Our work provides the first step toward defenses against backdoor attacks in deep neural networks.",
                        "Citation Paper Authors": "Authors:Kang Liu, Brendan Dolan-Gavitt, Siddharth Garg"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": ".\nWeight Encoding. Traditional storing method of computing system adopt two\u2019s com-\nplement representation for quantized weights. We used a similar method for the weight\nrepresentation as ",
                    "Citation Text": "Adnan Siraj Rakin, Zhezhi He, and Deliang Fan. Bit-\ufb02ip attack: Crushing neural\nnetwork with progressive bit search. In Proceedings of the IEEE International\nConference on Computer Vision (ICCV) , pages 1211\u20131220, 2019. 3, 4, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.12269",
                        "Citation Paper Title": "Title:Bit-Flip Attack: Crushing Neural Network with Progressive Bit Search",
                        "Citation Paper Abstract": "Abstract:Several important security issues of Deep Neural Network (DNN) have been raised recently associated with different applications and components. The most widely investigated security concern of DNN is from its malicious input, a.k.a adversarial example. Nevertheless, the security challenge of DNN's parameters is not well explored yet. In this work, we are the first to propose a novel DNN weight attack methodology called Bit-Flip Attack (BFA) which can crush a neural network through maliciously flipping extremely small amount of bits within its weight storage memory system (i.e., DRAM). The bit-flip operations could be conducted through well-known Row-Hammer attack, while our main contribution is to develop an algorithm to identify the most vulnerable bits of DNN weight parameters (stored in memory as binary bits), that could maximize the accuracy degradation with a minimum number of bit-flips. Our proposed BFA utilizes a Progressive Bit Search (PBS) method which combines gradient ranking and progressive search to identify the most vulnerable bit to be flipped. With the aid of PBS, we can successfully attack a ResNet-18 fully malfunction (i.e., top-1 accuracy degrade from 69.8% to 0.1%) only through 13 bit-flips out of 93 million bits, while randomly flipping 100 bits merely degrades the accuracy by less than 1%.",
                        "Citation Paper Authors": "Authors:Adnan Siraj Rakin, Zhezhi He, Deliang Fan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1901.02818v3": {
            "Paper Title": "Detecting Hidden Webcams with Delay-Tolerant Similarity of Simultaneous\n  Observation",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": "focused on an IoT-rich environment and privacy concerns.\nThey discovered existing wireless infrastructure by analyzing the numbers of\nFrames, mFrames, cFrames, and dFrames; network tra\ufb03c volume; and send-\nto-received ratio passively identify IoT devices. Gong el at. ",
                    "Citation Text": "X. Gong, N. Borisov, N. Kiyavash, N. Schear, Website detection using\nremote tra\ufb03c analysis, in: S. Fischer-H\u00fcbner, M. Wright (Eds.), PrivacyEnhancing Technologies, Springer Berlin Heidelberg, Berlin, Heidelberg,\n2012.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1109.0097",
                        "Citation Paper Title": "Title:Website Detection Using Remote Traffic Analysis",
                        "Citation Paper Abstract": "Abstract:Recent work in traffic analysis has shown that traffic patterns leaked through side channels can be used to recover important semantic information. For instance, attackers can find out which website, or which page on a website, a user is accessing simply by monitoring the packet size distribution. We show that traffic analysis is even a greater threat to privacy than previously thought by introducing a new attack that can be carried out remotely. In particular, we show that, to perform traffic analysis, adversaries do not need to directly observe the traffic patterns. Instead, they can gain sufficient information by sending probes from a far-off vantage point that exploits a queuing side channel in routers. To demonstrate the threat of such remote traffic analysis, we study a remote website detection attack that works against home broadband users. Because the remotely observed traffic patterns are more noisy than those obtained using previous schemes based on direct local traffic monitoring, we take a dynamic time warping (DTW) based approach to detecting fingerprints from the same website. As a new twist on website fingerprinting, we consider a website detection attack, where the attacker aims to find out whether a user browses a particular web site, and its privacy implications. We show experimentally that, although the success of the attack is highly variable, depending on the target site, for some sites very low error rates. We also show how such website detection can be used to deanonymize message board users.",
                        "Citation Paper Authors": "Authors:Xun Gong, Negar Kiyavash, Nab\u00edl Schear, Nikita Borisov"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.10695v3": {
            "Paper Title": "When NAS Meets Robustness: In Search of Robust Architectures against\n  Adversarial Attacks",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": ". Extensive efforts have been proposed to\nenhance the robustness, including preprocessing techniques\n[3, 31, 42], feature denoising ",
                    "Citation Text": "Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan L\nYuille, and Kaiming He. Feature denoising for improving\nadversarial robustness. In CVPR , 2019. 1, 2, 7, 8, 11, 14",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.03411",
                        "Citation Paper Title": "Title:Feature Denoising for Improving Adversarial Robustness",
                        "Citation Paper Abstract": "Abstract:Adversarial attacks to image classification systems present challenges to convolutional networks and opportunities for understanding them. This study suggests that adversarial perturbations on images lead to noise in the features constructed by these networks. Motivated by this observation, we develop new network architectures that increase adversarial robustness by performing feature denoising. Specifically, our networks contain blocks that denoise the features using non-local means or other filters; the entire networks are trained end-to-end. When combined with adversarial training, our feature denoising networks substantially improve the state-of-the-art in adversarial robustness in both white-box and black-box attack settings. On ImageNet, under 10-iteration PGD white-box attacks where prior art has 27.9% accuracy, our method achieves 55.7%; even under extreme 2000-iteration PGD white-box attacks, our method secures 42.6% accuracy. Our method was ranked first in Competition on Adversarial Attacks and Defenses (CAAD) 2018 --- it achieved 50.6% classification accuracy on a secret, ImageNet-like test dataset against 48 unknown attackers, surpassing the runner-up approach by ~10%. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan Yuille, Kaiming He"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "Adversarial Attack and Defence. Deep neural networks\n(NNs) can be easily fooled by adversarial examples [9, 35,\n20], where effective attacks are proposed such as FGSM ",
                    "Citation Text": "Ian Goodfellow, Jonathon Shlens, and Christian Szegedy.\nExplaining and harnessing adversarial examples. In ICLR ,\n2015. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.6572",
                        "Citation Paper Title": "Title:Explaining and Harnessing Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1909.09047v2": {
            "Paper Title": "Detecting malicious logins as graph anomalies",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": ", a suite of local and community metrics are proposed that characterize\nthe centrality, connectivity, and stability of graph structures. These works do\nnot propose anomaly detection schemes; however, the work of ",
                    "Citation Text": "A. Palladino, C. Thissen, Cyber anomaly detection using graph-node role-\ndynamics, in: Proceedings of DYnamic and Novel Advances in Machine\nLearning and Intelligent Cyber Security Workshop (DYNAMICS'18), 2018.\n33",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.02848",
                        "Citation Paper Title": "Title:Cyber Anomaly Detection Using Graph-node Role-dynamics",
                        "Citation Paper Abstract": "Abstract:Intrusion detection systems (IDSs) generate valuable knowledge about network security, but an abundance of false alarms and a lack of methods to capture the interdependence among alerts hampers their utility for network defense. Here, we explore a graph-based approach for fusing alerts generated by multiple IDSs (e.g., Snort, OSSEC, and Bro). Our approach generates a weighted graph of alert fields (not network topology) that makes explicit the connections between multiple alerts, IDS systems, and other cyber artifacts. We use this multi-modal graph to identify anomalous changes in the alert patterns of a network. To detect the anomalies, we apply the role-dynamics approach, which has successfully identified anomalies in social media, email, and IP communication graphs. In the cyber domain, each node (alert field) in the fused IDS alert graph is assigned a probability distribution across a small set of roles based on that node's features. A cyber attack should trigger IDS alerts and cause changes in the node features, but rather than track every feature for every alert-field node individually, roles provide a succinct, integrated summary of those feature changes. We measure changes in each node's probabilistic role assignment over time, and identify anomalies as deviations from expected roles. We test our approach using simulations including three weeks of normal background traffic, as well as cyber attacks that occur near the end of the simulations. This paper presents a novel approach to multi-modal data fusion and a novel application of role dynamics within the cyber-security domain. Our results show a drastic decrease in the false-positive rate when considering our anomaly indicator instead of the IDS alerts themselves, thereby reducing alarm fatigue and providing a promising avenue for threat intelligence in network defense.",
                        "Citation Paper Authors": "Authors:Anthony Palladino, Christopher J. Thissen"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": ", a large set of\nfeatures, including summary statistics of di\u000berent centrality measures, are used\nto train a support vector machine to classify graphs. A similar approach based\non one-class learning is taken in ",
                    "Citation Text": "A. Gamachchi, L. Sun, S. Boztas, A graph based framework for malicious\ninsider threat detection, in: 50th Hawaii International Conference on Sys-\ntem Sciences (HICSS), 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.00141",
                        "Citation Paper Title": "Title:A Graph Based Framework for Malicious Insider Threat Detection",
                        "Citation Paper Abstract": "Abstract:While most security projects have focused on fending off attacks coming from outside the organizational boundaries, a real threat has arisen from the people who are inside those perimeter protections. Insider threats have shown their power by hugely affecting national security, financial stability, and the privacy of many thousands of people. What is in the news is the tip of the iceberg, with much more going on under the radar, and some threats never being detected. We propose a hybrid framework based on graphical analysis and anomaly detection approaches, to combat this severe cybersecurity threat. Our framework analyzes heterogeneous data in isolating possible malicious users hiding behind others. Empirical results reveal this framework to be effective in distinguishing the majority of users who demonstrate typical behavior from the minority of users who show suspicious behavior.",
                        "Citation Paper Authors": "Authors:Anagi Gamachchi, Li Sun, Serdar Boztas"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1909.12982v2": {
            "Paper Title": "Robust Membership Encoding: Inference Attacks and Copyright Protection\n  for Deep Learning",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": ". Melis et al .\nshowed how an adversary participant in a collaborative training\nof a ML model can infer the properties about other participants\u2019\ntraining data ",
                    "Citation Text": "Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov.\n2018. Inference Attacks Against Collaborative Learning. arXiv preprint (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.04049",
                        "Citation Paper Title": "Title:Exploiting Unintended Feature Leakage in Collaborative Learning",
                        "Citation Paper Abstract": "Abstract:Collaborative machine learning and related techniques such as federated learning allow multiple participants, each with his own training dataset, to build a joint model by training locally and periodically exchanging model updates. We demonstrate that these updates leak unintended information about participants' training data and develop passive and active inference attacks to exploit this leakage. First, we show that an adversarial participant can infer the presence of exact data points -- for example, specific locations -- in others' training data (i.e., membership inference). Then, we show how this adversary can infer properties that hold only for a subset of the training data and are independent of the properties that the joint model aims to capture. For example, he can infer when a specific person first appears in the photos used to train a binary gender classifier. We evaluate our attacks on a variety of tasks, datasets, and learning configurations, analyze their limitations, and discuss possible defenses.",
                        "Citation Paper Authors": "Authors:Luca Melis, Congzheng Song, Emiliano De Cristofaro, Vitaly Shmatikov"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ". Other work\nshowed that well-generalized model can leak membership infor-\nmation ",
                    "Citation Text": "Yunhui Long, Vincent Bindschaedler, Lei Wang, Diyue Bu, Xiaofeng Wang, Haixu\nTang, Carl A Gunter, and Kai Chen. 2018. Understanding Membership Inferences\non Well-Generalized Learning Models. arXiv preprint (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.04889",
                        "Citation Paper Title": "Title:Understanding Membership Inferences on Well-Generalized Learning Models",
                        "Citation Paper Abstract": "Abstract:Membership Inference Attack (MIA) determines the presence of a record in a machine learning model's training data by querying the model. Prior work has shown that the attack is feasible when the model is overfitted to its training data or when the adversary controls the training algorithm. However, when the model is not overfitted and the adversary does not control the training algorithm, the threat is not well understood. In this paper, we report a study that discovers overfitting to be a sufficient but not a necessary condition for an MIA to succeed. More specifically, we demonstrate that even a well-generalized model contains vulnerable instances subject to a new generalized MIA (GMIA). In GMIA, we use novel techniques for selecting vulnerable instances and detecting their subtle influences ignored by overfitting metrics. Specifically, we successfully identify individual records with high precision in real-world datasets by querying black-box machine learning models. Further we show that a vulnerable record can even be indirectly attacked by querying other related records and existing generalization techniques are found to be less effective in protecting the vulnerable instances. Our findings sharpen the understanding of the fundamental cause of the problem: the unique influences the training instance may have on the model.",
                        "Citation Paper Authors": "Authors:Yunhui Long, Vincent Bindschaedler, Lei Wang, Diyue Bu, Xiaofeng Wang, Haixu Tang, Carl A. Gunter, Kai Chen"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "Membership inference. Membership inference against classifica-\ntion models has been studied in [ 22,25,27,28], and later studied\nfor generative models and language models [ 15,31] as well as in\ncollaborative learning setting [ 23,24]. The attack in ",
                    "Citation Text": "Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Mem-\nbership inference attacks against machine learning models. In S& P .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.05820",
                        "Citation Paper Title": "Title:Membership Inference Attacks against Machine Learning Models",
                        "Citation Paper Abstract": "Abstract:We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on.\nWe empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                        "Citation Paper Authors": "Authors:Reza Shokri, Marco Stronati, Congzheng Song, Vitaly Shmatikov"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1902.05357v2": {
            "Paper Title": "Estimating the Circuit Deobfuscating Runtime based on Graph Deep\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.00964v2": {
            "Paper Title": "A Survey of Moving Target Defenses for Network Security",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.00411v2": {
            "Paper Title": "KloakDB: A Platform for Analyzing Sensitive Data with $K$-anonymous\n  Query Processing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.07814v2": {
            "Paper Title": "CrypTFlow: Secure TensorFlow Inference",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": "scale DNNs.\nIn this work, we present C RYPTFLOW , a \ufb01rst of its kind\nsystem, that converts TensorFlow ",
                    "Citation Text": "M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro,\nG. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. J.\nGoodfellow, A. Harp, G. Irving, M. Isard, Y . Jia, R. J \u00b4ozefowicz,\nL. Kaiser, M. Kudlur, J. Levenberg, D. Man \u00b4e, R. Monga, S. Moore,\nD. G. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner,\nI. Sutskever, K. Talwar, P. A. Tucker, V . Vanhoucke, V . Vasudevan,\nF. B. Vi \u00b4egas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke,\nY . Yu, and X. Zheng, \u201cTensorFlow: Large-Scale Machine Learning\non Heterogeneous Distributed Systems,\u201d CoRR , vol. abs/1603.04467,\n2016. [Online]. Available: https://arxiv :org/abs/1603:04467",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.04467",
                        "Citation Paper Title": "Title:TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems",
                        "Citation Paper Abstract": "Abstract:TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at this http URL.",
                        "Citation Paper Authors": "Authors:Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, Xiaoqiang Zheng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.02142v2": {
            "Paper Title": "Intriguing Properties of Adversarial ML Attacks in the Problem Space",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": "and ensures a defense\ndoes not rely on \u201csecurity by obscurity\u201d by unreasonably as-\nsuming some properties of the defense can be kept secret ",
                    "Citation Text": "N. Carlini, A. Athalye, N. Papernot, W. Brendel, J. Rauber, D. Tsipras,\nI. Goodfellow, and A. Madry. On evaluating adversarial robustness.\narXiv preprint arXiv:1902.06705 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.06705",
                        "Citation Paper Title": "Title:On Evaluating Adversarial Robustness",
                        "Citation Paper Abstract": "Abstract:Correctly evaluating defenses against adversarial examples has proven to be extremely difficult. Despite the significant amount of recent work attempting to design defenses that withstand adaptive attacks, few have succeeded; most papers that propose defenses are quickly shown to be incorrect.\nWe believe a large contributing factor is the difficulty of performing security evaluations. In this paper, we discuss the methodological foundations, review commonly accepted best practices, and suggest new methods for evaluating defenses to adversarial examples. We hope that both researchers developing defenses as well as readers and reviewers who wish to understand the completeness of an evaluation consider our advice in order to avoid common pitfalls.",
                        "Citation Paper Authors": "Authors:Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, Alexey Kurakin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1907.06828v2": {
            "Paper Title": "Automated Deobfuscation of Android Native Binary Code",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.12962v4": {
            "Paper Title": "Celeb-DF: A Large-scale Challenging Dataset for DeepFake Forensics",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "network as the backbone architecture for\nDeepFake classi\ufb01cation. This model is trained on the\nFaceForensics++ dataset.\n\u000fDSP-FWA is a recently further improved method\nbased on FWA, which includes a spatial pyramid pool-\ning (SPP) module ",
                    "Citation Text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nSpatial pyramid pooling in deep convolutional networks for\nvisual recognition. IEEE transactions on pattern analysis\nand machine intelligence (TPAMI) , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.4729",
                        "Citation Paper Title": "Title:Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition",
                        "Citation Paper Abstract": "Abstract:Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224x224) input image. This requirement is \"artificial\" and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, \"spatial pyramid pooling\", to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning.\nThe power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102x faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007.\nIn ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank #2 in object detection and #3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.",
                        "Citation Paper Authors": "Authors:Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.11937v2": {
            "Paper Title": "Period Adaptation for Continuous Security Monitoring in Multicore\n  Real-Time Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.00193v4": {
            "Paper Title": "Parallel Algorithm for Approximating Nash Equilibrium in Multiplayer\n  Stochastic Games with Application to Naval Strategic Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.06587v2": {
            "Paper Title": "An Analysis of Blockchain Consistency in Asynchronous Networks: Deriving\n  a Neat Bound",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.01769v2": {
            "Paper Title": "Tendrils of Crime: Visualizing the Diffusion of Stolen Bitcoins",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.00990v2": {
            "Paper Title": "Classical Verification of Quantum Computations with Efficient Verifier",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.11541v3": {
            "Paper Title": "Characterizing Orphan Transactions in the Bitcoin Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.05467v2": {
            "Paper Title": "Quantification of the Leakage in Federated Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.10311v4": {
            "Paper Title": "SpecFuzz: Bringing Spectre-type vulnerabilities to the surface",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": ". A relatively complete\nisolation can be achieved with a specialized microkernel ",
                    "Citation Text": "Qian Ge, Yuval Yarom, Tom Chothia, and Gernot Heiser.\nTime protection: the missing OS abstraction. In Eu-\nroSys , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.05345",
                        "Citation Paper Title": "Title:Time Protection: the Missing OS Abstraction",
                        "Citation Paper Abstract": "Abstract:Timing channels enable data leakage that threatens the security of computer systems, from cloud platforms to smartphones and browsers executing untrusted third-party code. Preventing unauthorised information flow is a core duty of the operating system, however, present OSes are unable to prevent timing channels. We argue that OSes must provide time protection in addition to the established memory protection. We examine the requirements of time protection, present a design and its implementation in the seL4 microkernel, and evaluate its efficacy as well as performance overhead on Arm and x86 processors.",
                        "Citation Paper Authors": "Authors:Qian Ge, Yuval Yarom, Tom Chothia, Gernot Heiser"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": ".\nYet, they protect only against speci\ufb01c SC and speculative\nattacks may use various channels ",
                    "Citation Text": "Michael Schwarz, Martin Schwarzl, Moritz Lipp, Jon\nMasters, and Daniel Gruss. NetSpectre: Read Arbitrary\nMemory over Network. In ESORICS , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.10535",
                        "Citation Paper Title": "Title:NetSpectre: Read Arbitrary Memory over Network",
                        "Citation Paper Abstract": "Abstract:In this paper, we present NetSpectre, a generic remote Spectre variant 1 attack. For this purpose, we demonstrate the first access-driven remote Evict+Reload cache attack over network, leaking 15 bits per hour. Beyond retrofitting existing attacks to a network scenario, we also demonstrate the first Spectre attack which does not use a cache covert channel. Instead, we present a novel high-performance AVX-based covert channel that we use in our cache-free Spectre attack. We show that in particular remote Spectre attacks perform significantly better with the AVX-based covert channel, leaking 60 bits per hour from the target system. We verified that our NetSpectre attacks work in local-area networks as well as between virtual machines in the Google cloud.\nNetSpectre marks a paradigm shift from local attacks, to remote attacks, exposing a much wider range and larger number of devices to Spectre attacks. Spectre attacks now must also be considered on devices which do not run any potentially attacker-controlled code at all. We show that especially in this remote scenario, attacks based on weaker gadgets which do not leak actual data, are still very powerful to break address-space layout randomization remotely. Several of the Spectre gadgets we discuss are more versatile than anticipated. In particular, value-thresholding is a technique we devise, which leaks a secret value without the typical bit selection mechanisms. We outline challenges for future research on Spectre attacks and Spectre mitigations.",
                        "Citation Paper Authors": "Authors:Michael Schwarz, Martin Schwarzl, Moritz Lipp, Daniel Gruss"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "inserts serialization barriers\nat decoding stage upon detecting a potentially dangerous in-\nstruction pattern. ConTExT ",
                    "Citation Text": "Michael Schwarz, Robert Schilling, Florian Kargl,\nMoritz Lipp, Claudio Canella, and Daniel Gruss. Con-\nTExT: Leakage-Free Transient Execution. arXiv\npreprint arXiv:1905.09100v1 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.09100",
                        "Citation Paper Title": "Title:ConTExT: Leakage-Free Transient Execution",
                        "Citation Paper Abstract": "Abstract:Out-of-order execution and speculative execution are among the biggest contributors to performance and efficiency of modern processors. However, they are inconsiderate, leaking secret data during the transient execution of instructions. Many solutions have been proposed against transient execution attacks. However, they do not eliminate the leakage entirely or introduce unacceptable performance penalties.\nIn this paper, we propose ConTExT, a Considerate Transient Execution Technique. The basic idea of ConTExT is that secrets can enter registers, but not transiently leave them. ConTExT transforms Spectre from a problem that cannot be solved purely in software [53], to a problem that is not easy to solve, but solvable in software. For this, ConTExT requires minimal modifications of applications, compilers, operating systems, and the hardware. ConTExT offers full protection for secrets in memory and secrets in registers. We evaluate the security and performance of ConTExT. With its principled approach it inherently mitigates the recently found microarchitectural data sampling attacks on small processor buffers. Even when over-approximating, we observe no performance overhead for unprotected code and data, and an overhead of 71.14% for security-critical applications, which is below the overhead of currently recommended state-of-the-art mitigation strategies. The actual overhead of ConTExT is below 1% for real-world workloads.",
                        "Citation Paper Authors": "Authors:Michael Schwarz, Robert Schilling, Florian Kargl, Moritz Lipp, Claudio Canella, Daniel Gruss"
                    }
                },
                {
                    "Sentence ID": 59,
                    "Sentence": "analyze the binary and search for\nSpectre gadgets. Although mature tools like Respectre can de-\ntect many vulnerabilities (see \u00a76), the reliance on prede\ufb01ned\npatterns may leave an unexpected variant to stay unnoticed.\nAlternatively, oo7 ",
                    "Citation Text": "Guanhua Wang, Sudipta Chattopadhyay, Ivan Gotov-\nchits, Tulika Mitra, and Abhik Roychoudhury. oo7:\nLow-overhead Defense against Spectre Attacks. arXiv\npreprint arXiv:1807.05843 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.00647",
                        "Citation Paper Title": "Title:KLEESPECTRE: Detecting Information Leakage through Speculative Cache Attacks via Symbolic Execution",
                        "Citation Paper Abstract": "Abstract:Spectre attacks disclosed in early 2018 expose data leakage scenarios via cache side channels. Specifically, speculatively executed paths due to branch mis-prediction may bring secret data into the cache which are then exposed via cache side channels even after the speculative execution is squashed. Symbolic execution is a well-known test generation method to cover program paths at the level of the application software. In this paper, we extend symbolic execution with modelingof cache and speculative execution. Our tool KLEESPECTRE, built on top of the KLEE symbolic execution engine, can thus provide a testing engine to check for the data leakage through cache side-channel as shown via Spectre attacks. Our symbolic cache model can verify whether the sensitive data leakage due to speculative execution can be observed by an attacker at a given program point. Our experiments show that KLEESPECTREcan effectively detect data leakage along speculatively executed paths and our cache model can further make the leakage detection much more precise.",
                        "Citation Paper Authors": "Authors:Guanhua Wang, Sudipta Chattopadhyay, Arnab Kumar Biswas, Tulika Mitra, Abhik Roychoudhury"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1902.07756v5": {
            "Paper Title": "Crypt$\u03b5$: Crypto-Assisted Differential Privacy on Untrusted\n  Servers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.13111v2": {
            "Paper Title": "Shielding Collaborative Learning: Mitigating Poisoning Attacks through\n  Client-Side Detection",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": ".\nBesides the poisoning attacks, there are some other methods\nthat can cause a trained model to misclassify special inputs.\nAdversarial examples that add noises to the input have been widely\ninvestigated recently ",
                    "Citation Text": "A. Kurakin, I. Goodfellow, and S. Bengio, \u201cAdversarial examples in the\nphysical world,\u201d arXiv preprint arXiv:1607.02533 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.02533",
                        "Citation Paper Title": "Title:Adversarial examples in the physical world",
                        "Citation Paper Abstract": "Abstract:Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.",
                        "Citation Paper Authors": "Authors:Alexey Kurakin, Ian Goodfellow, Samy Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1909.07220v3": {
            "Paper Title": "Broken Metre: Attacking Resource Metering in EVM",
            "Sentences": [
                {
                    "Sentence ID": 58,
                    "Sentence": ". We present some of these tools\nin the next subsection.\nB. Gas Usage and Metering\nRecent work by Yang et al. ",
                    "Citation Text": "Renlord Yang, Toby Murray, Paul Rimba, and Udaya Parampalli.\nEmpirically Analyzing Ethereum\u2019s Gas Mechanism. CoRR , abs/1905.0,\n2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.00553",
                        "Citation Paper Title": "Title:Empirically Analyzing Ethereum's Gas Mechanism",
                        "Citation Paper Abstract": "Abstract:Ethereum's Gas mechanism attempts to set transaction fees in accordance with the computational cost of transaction execution: a cost borne by default by every node on the network to ensure correct smart contract execution. Gas encourages users to author transactions that are efficient to execute and in so doing encourages node diversity, allowing modestly resourced nodes to join and contribute to the security of the network.\nHowever, the effectiveness of this scheme relies on Gas costs being correctly aligned with observed computational costs in reality. In this work, we performed the first large scale empirical study to understand to what degree this alignment exists in practice, by collecting and analyzing Tera-bytes worth of nanosecond-precision transaction execution traces. Besides confirming potential denial-of-service vectors, our results also shed light on the role of I/O in transaction costs which remains poorly captured by the current Gas cost model. Finally, our results suggest that under the current Gas cost model, nodes with modest computational resources are disadvantaged compared to their better resourced peers, which we identify as an ongoing threat to node diversity and network decentralization.",
                        "Citation Paper Authors": "Authors:Renlord Yang, Toby Murray, Paul Rimba, Udaya Parampalli"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.10247v2": {
            "Paper Title": "Trust Management in Decentralized IoT Access Control System",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.02033v5": {
            "Paper Title": "SANNS: Scaling Up Secure Approximate k-Nearest Neighbors Search",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": "2. In order to be suitable for secure computation,\nwe introduce a new cluster rebalancing subroutine, see below.\nLet us note that among the plaintext k-NNS algorithms, the\nclustering approach is far from being the best ",
                    "Citation Text": "M. Aum\u00fcller, E. Bernhardsson, and A. Faithfull. Ann-\nbenchmarks: A benchmarking tool for approximate near-\nest neighbor algorithms. In International Conference\non Similarity Search and Applications , pages 34\u201349.\nSpringer, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.05614",
                        "Citation Paper Title": "Title:ANN-Benchmarks: A Benchmarking Tool for Approximate Nearest Neighbor Algorithms",
                        "Citation Paper Abstract": "Abstract:This paper describes ANN-Benchmarks, a tool for evaluating the performance of in-memory approximate nearest neighbor algorithms. It provides a standard interface for measuring the performance and quality achieved by nearest neighbor algorithms on different standard data sets. It supports several different ways of integrating $k$-NN algorithms, and its configuration system automatically tests a range of parameter settings for each algorithm. Algorithms are compared with respect to many different (approximate) quality measures, and adding more is easy and fast; the included plotting front-ends can visualise these as images, $\\LaTeX$ plots, and websites with interactive plots. ANN-Benchmarks aims to provide a constantly updated overview of the current state of the art of $k$-NN algorithms. In the short term, this overview allows users to choose the correct $k$-NN algorithm and parameters for their similarity search task; in the longer term, algorithm designers will be able to use this overview to test and refine automatic parameter tuning. The paper gives an overview of the system, evaluates the results of the benchmark, and points out directions for future work. Interestingly, very different approaches to $k$-NN search yield comparable quality-performance trade-offs. The system is available at this http URL .",
                        "Citation Paper Authors": "Authors:Martin Aum\u00fcller, Erik Bernhardsson, Alexander Faithfull"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.05620v3": {
            "Paper Title": "Judge, Jury & Encryptioner: Exceptional Device Access with a Social Cost",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.04280v2": {
            "Paper Title": "A Capacity-achieving One-message Key Agreement With Finite Blocklength\n  Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.01838v2": {
            "Paper Title": "High Accuracy and High Fidelity Extraction of Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.10908v2": {
            "Paper Title": "Prediction Poisoning: Towards Defenses Against DNN Model Stealing\n  Attacks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.13369v2": {
            "Paper Title": "JEDI: Many-to-Many End-to-End Encryption and Key Delegation for IoT",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.13410v2": {
            "Paper Title": "Logic Bugs in IoT Platforms and Systems: A Review",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.03861v2": {
            "Paper Title": "Private Protocols for U-Statistics in the Local Model and Beyond",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.06871v2": {
            "Paper Title": "Privacy-Preserving Claims Exchange Networks for Virtual Asset Service\n  Providers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.09407v3": {
            "Paper Title": "SCNIFFER: Low-Cost, Automated, Efficient Electromagnetic Side-Channel\n  Sniffing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.00740v4": {
            "Paper Title": "Secure Calibration for Safety-Critical IoT: Traceability for Safety\n  Resilience",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.09433v2": {
            "Paper Title": "Can Machine Learning Model with Static Features be Fooled: an\n  Adversarial Machine Learning Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.08534v4": {
            "Paper Title": "Federated Heavy Hitters Discovery with Differential Privacy",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": "predates di\u000berential privacy and the TreeHist algorithm of Bassily\net al. ",
                    "Citation Text": "Raef Bassily, Uri Stemmer, Abhradeep Guha Thakurta, et al. Practical locally private heavy hitters.\nInAdvances in Neural Information Processing Systems , pages 2288{2296, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.04982",
                        "Citation Paper Title": "Title:Practical Locally Private Heavy Hitters",
                        "Citation Paper Abstract": "Abstract:We present new practical local differentially private heavy hitters algorithms achieving optimal or near-optimal worst-case error and running time -- TreeHist and Bitstogram. In both algorithms, server running time is $\\tilde O(n)$ and user running time is $\\tilde O(1)$, hence improving on the prior state-of-the-art result of Bassily and Smith [STOC 2015] requiring $O(n^{5/2})$ server time and $O(n^{3/2})$ user time. With a typically large number of participants in local algorithms ($n$ in the millions), this reduction in time complexity, in particular at the user side, is crucial for making locally private heavy hitters algorithms usable in practice. We implemented Algorithm TreeHist to verify our theoretical analysis and compared its performance with the performance of Google's RAPPOR code.",
                        "Citation Paper Authors": "Authors:Raef Bassily, Kobbi Nissim, Uri Stemmer, Abhradeep Thakurta"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1908.01165v3": {
            "Paper Title": "Exploring the Robustness of NMT Systems to Nonsensical Inputs",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": ", the authors modify the training\nalgorithms to improve robustness of NMT systems to the\nparticular type of noise in consideration. Similar approac h\nhas been followed to develop robust image classi\ufb01ers ",
                    "Citation Text": "A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vlad u, \u201cTowards\ndeep learning models resistant to adversarial attacks,\u201d in 6th Interna-\ntional Conference on Learning Representations, ICLR 2018, Vancouver,\nBC, Canada, April 30 - May 3, 2018, Conference Track Proceedi ngs,\n2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06083",
                        "Citation Paper Title": "Title:Towards Deep Learning Models Resistant to Adversarial Attacks",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL.",
                        "Citation Paper Authors": "Authors:Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1907.10823v3": {
            "Paper Title": "Enhancing Adversarial Example Transferability with an Intermediate Level\n  Attack",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.03785v3": {
            "Paper Title": "Drynx: Decentralized, Secure, Verifiable System for Statistical Queries\n  and Machine Learning on Distributed Datasets",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.02578v3": {
            "Paper Title": "Differential Privacy-enabled Federated Learning for Sensitive Health\n  Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1903.00846v2": {
            "Paper Title": "A survey of security and privacy issues in the Internet of Things from\n  the layered context",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.03310v2": {
            "Paper Title": "Robustness for Non-Parametric Classification: A Generic Attack and\n  Defense",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.05320v4": {
            "Paper Title": "Trace-Relating Compiler Correctness and Secure Compilation",
            "Sentences": [
                {
                    "Sentence ID": 68,
                    "Sentence": ".\nEven for reasoning about safety, hypersafety, or arbitrary hyperproperties, traces can\ntherefore be values, sequences of program states, or of input output events, or even the\nrecently proposed interaction trees ",
                    "Citation Text": "L. Xia, Y . Zakowski, P. He, C. Hur, G. Malecha, B. C. Pierce, and S. Zdancewic. Interaction\ntrees: representing recursive and impure programs in Coq. PACMPL , 4(POPL), 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.00046",
                        "Citation Paper Title": "Title:Interaction Trees: Representing Recursive and Impure Programs in Coq",
                        "Citation Paper Abstract": "Abstract:\"Interaction trees\" (ITrees) are a general-purpose data structure for representing the behaviors of recursive programs that interact with their environments. A coinductive variant of \"free monads,\" ITrees are built out of uninterpreted events and their continuations. They support compositional construction of interpreters from \"event handlers\", which give meaning to events by defining their semantics as monadic actions. ITrees are expressive enough to represent impure and potentially nonterminating, mutually recursive computations, while admitting a rich equational theory of equivalence up to weak bisimulation. In contrast to other approaches such as relationally specified operational semantics, ITrees are executable via code extraction, making them suitable for debugging, testing, and implementing software artifacts that are amenable to formal verification.\nWe have implemented ITrees and their associated theory as a Coq library, mechanizing classic domain- and category-theoretic results about program semantics, iteration, monadic structures, and equational reasoning. Although the internals of the library rely heavily on coinductive proofs, the interface hides these details so that clients can use and reason about ITrees without explicit use of Coq's coinduction tactics.\nTo showcase the utility of our theory, we prove the termination-sensitive correctness of a compiler from a simple imperative source language to an assembly-like target whose meanings are given in an ITree-based denotational semantics. Unlike previous results using operational techniques, our bisimulation proof follows straightforwardly by structural induction and elementary rewriting via an equational theory of combinators for control-flow graphs.",
                        "Citation Paper Authors": "Authors:Li-yao Xia, Yannick Zakowski, Paul He, Chung-Kil Hur, Gregory Malecha, Benjamin C. Pierce, Steve Zdancewic"
                    }
                },
                {
                    "Sentence ID": 62,
                    "Sentence": "J. Sevc\u00edk, V . Vafeiadis, F. Z. Nardelli, S. Jagannathan, and P. Sewell. CompCertTSO: A\nveri\ufb01ed compiler for relaxed-memory concurrency. J. ACM , 60(3), 2013. ",
                    "Citation Text": "L. Skorstengaard, D. Devriese, and L. Birkedal. StkTokens: Enforcing Well-bracketed Con-\ntrol Flow and Stack Encapsulation Using Linear Capabilities. Proc. ACM Program. Lang. ,\n3(POPL), 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.02787",
                        "Citation Paper Title": "Title:StkTokens: Enforcing Well-bracketed Control Flow and Stack Encapsulation using Linear Capabilities - Technical Report with Proofs and Details",
                        "Citation Paper Abstract": "Abstract:We propose and study StkTokens: a new calling convention that provably enforces well-bracketed control flow and local state encapsulation on a capability machine. The calling convention is based on linear capabilities: a type of capabilities that are prevented from being duplicated by the hardware. In addition to designing and formalizing this new calling convention, we also contribute a new way to formalize and prove that it effectively enforces well-bracketed control flow and local state encapsulation using what we call a fully abstract overlay semantics.\nThis document is a technical report accompanying a paper by the same title and authors, published at POPL 2019. It contains proofs and details that were omitted from the paper for space and presentation reasons.",
                        "Citation Paper Authors": "Authors:Lau Skorstengaard, Dominique Devriese, Lars Birkedal"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.09853v2": {
            "Paper Title": "Domain Knowledge Aided Explainable Artificial Intelligence for Intrusion\n  Detection and Response",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.08595v2": {
            "Paper Title": "SoK: Tools for Game Theoretic Models of Security for Cryptocurrencies",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.05830v2": {
            "Paper Title": "Differentially Private Meta-Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.13531v3": {
            "Paper Title": "HotPoW: Finality from Proof-of-Work Quorums",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.09964v2": {
            "Paper Title": "Do Cookie Banners Respect my Choice? Measuring Legal Compliance of\n  Banners from IAB Europe's Transparency and Consent Framework",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.04666v3": {
            "Paper Title": "Privacy-Preserving and Collusion-Resistant Charging Coordination Schemes\n  for Smart Grid",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.12121v3": {
            "Paper Title": "An Investigation of Data Poisoning Defenses for Online Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.00012v2": {
            "Paper Title": "Differentially Private M-band Wavelet-Based Mechanisms in Machine\n  Learning Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.00919v3": {
            "Paper Title": "Mimic Learning to Generate a Shareable Network Intrusion Detection Model",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": ". However, there has been little research on the use of\nmimic learning to enable sharing trained models instead of the\noriginal sensitive data as an option for transferring knowledge ",
                    "Citation Text": "N. Papernot, M. Abadi, \u00da. Erlingsson, I. J. Goodfellow, and K. Talwar,\n\u201cSemi-supervised knowledge transfer for deep learning from private\ntraining data,\u201d Proceedings of the 2017 International Conference on\nLearning Representations (ICLR\u201917) , Apr. 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.05755",
                        "Citation Paper Title": "Title:Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data",
                        "Citation Paper Abstract": "Abstract:Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information.\nTo address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees for training data: Private Aggregation of Teacher Ensembles (PATE). The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as \"teachers\" for a \"student\" model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student's privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student's training) and formally, in terms of differential privacy. These properties hold even if an adversary can not only query the student but also inspect its internal workings.\nCompared with previous work, the approach imposes only weak assumptions on how teachers are trained: it applies to any model, including non-convex models like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN thanks to an improved privacy analysis and semi-supervised learning.",
                        "Citation Paper Authors": "Authors:Nicolas Papernot, Mart\u00edn Abadi, \u00dalfar Erlingsson, Ian Goodfellow, Kunal Talwar"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1907.02957v2": {
            "Paper Title": "Detecting and Diagnosing Adversarial Images with Class-Conditional\n  Capsule Reconstructions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.04472v2": {
            "Paper Title": "Provably Secure Group Signature Schemes from Code-Based Assumptions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.03334v4": {
            "Paper Title": "On the Need for Topology-Aware Generative Models for Manifold-Based\n  Defenses",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.00507v2": {
            "Paper Title": "SpecuSym: Speculative Symbolic Execution for Cache Timing Leak Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.06285v3": {
            "Paper Title": "DomainGAN: Generating Adversarial Examples to Attack Domain Generation\n  Algorithm Classifiers",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": ", uses feature engineering along with an iterative\nDGA development process to produce DGAs that can fool\nDGA classi\ufb01ers. Anderson et. al. ",
                    "Citation Text": "Hyrum S Anderson, Jonathan Woodbridge, and Bobby\nFilar. Deepdga: Adversarially-tuned domain genera-\ntion and detection. In Proceedings of the 2016 ACM\nWorkshop on Arti\ufb01cial Intelligence and Security , pages\n13\u201321. ACM, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.01969",
                        "Citation Paper Title": "Title:DeepDGA: Adversarially-Tuned Domain Generation and Detection",
                        "Citation Paper Abstract": "Abstract:Many malware families utilize domain generation algorithms (DGAs) to establish command and control (C&C) connections. While there are many methods to pseudorandomly generate domains, we focus in this paper on detecting (and generating) domains on a per-domain basis which provides a simple and flexible means to detect known DGA families. Recent machine learning approaches to DGA detection have been successful on fairly simplistic DGAs, many of which produce names of fixed length. However, models trained on limited datasets are somewhat blind to new DGA variants.\nIn this paper, we leverage the concept of generative adversarial networks to construct a deep learning based DGA that is designed to intentionally bypass a deep learning based detector. In a series of adversarial rounds, the generator learns to generate domain names that are increasingly more difficult to detect. In turn, a detector model updates its parameters to compensate for the adversarially generated domains. We test the hypothesis of whether adversarially generated domains may be used to augment training sets in order to harden other machine learning models against yet-to-be-observed DGAs. We detail solutions to several challenges in training this character-based generative adversarial network (GAN). In particular, our deep learning architecture begins as a domain name auto-encoder (encoder + decoder) trained on domains in the Alexa one million. Then the encoder and decoder are reassembled competitively in a generative adversarial network (detector + generator), with novel neural architectures and training strategies to improve convergence.",
                        "Citation Paper Authors": "Authors:Hyrum S. Anderson, Jonathan Woodbridge, Bobby Filar"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.10190v2": {
            "Paper Title": "Cached and Confused: Web Cache Deception in the Wild",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.05299v3": {
            "Paper Title": "Privacy-Preserving Multi-Party Contextual Bandits",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.11833v2": {
            "Paper Title": "AuditShare: Sensitive Data Sharing with Reliable Leaker Identification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.03078v2": {
            "Paper Title": "Adversarial Attacks on GMM i-vector based Speaker Verification Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.12974v3": {
            "Paper Title": "ExplFrame: Exploiting Page Frame Cache for Fault Analysis of Block\n  Ciphers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.11797v3": {
            "Paper Title": "PingPong: Packet-Level Signatures for Smart Home Device Events",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.07793v4": {
            "Paper Title": "AT-GAN: An Adversarial Generator Model for Non-constrained Adversarial\n  Examples",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.01667v3": {
            "Paper Title": "A Survey of Black-Box Adversarial Attacks on Computer Vision Models",
            "Sentences": [
                {
                    "Sentence ID": 53,
                    "Sentence": "- 75.10% - 29.50% Quilting Patch Size of 5x5\nXie ",
                    "Citation Text": "Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. Mitigating adversarial effects through\nrandomization. In International Conference on Learning Representations , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.01991",
                        "Citation Paper Title": "Title:Mitigating Adversarial Effects Through Randomization",
                        "Citation Paper Abstract": "Abstract:Convolutional neural networks have demonstrated high accuracy on various tasks in recent years. However, they are extremely vulnerable to adversarial examples. For example, imperceptible perturbations added to clean images can cause convolutional neural networks to fail. In this paper, we propose to utilize randomization at inference time to mitigate adversarial effects. Specifically, we use two randomization operations: random resizing, which resizes the input images to a random size, and random padding, which pads zeros around the input images in a random manner. Extensive experiments demonstrate that the proposed randomization method is very effective at defending against both single-step and iterative attacks. Our method provides the following advantages: 1) no additional training or fine-tuning, 2) very few additional computations, 3) compatible with other adversarial defense methods. By combining the proposed randomization method with an adversarially trained model, it achieves a normalized score of 0.924 (ranked No.2 among 107 defense teams) in the NIPS 2017 adversarial examples defense challenge, which is far better than using adversarial training alone with a normalized score of 0.773 (ranked No.56). The code is public available at this https URL.",
                        "Citation Paper Authors": "Authors:Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, Alan Yuille"
                    }
                },
                {
                    "Sentence ID": 89,
                    "Sentence": "99.40% 99.10% 28% 0% L\u221emetric,\u03f5=0.010, epochs = 100, lr = 0.001\nDeepfool ",
                    "Citation Text": "Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to\nfool deep neural networks. CoRR , abs/1511.04599, 2015.\n, Vol. 1, No. 1, Article . Publication date: February 2020.A Survey of Black-Box Adversarial Attacks on Computer Vision Models 33",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.04599",
                        "Citation Paper Title": "Title:DeepFool: a simple and accurate method to fool deep neural networks",
                        "Citation Paper Abstract": "Abstract:State-of-the-art deep neural networks have achieved impressive results on many image classification tasks. However, these same architectures have been shown to be unstable to small, well sought, perturbations of the images. Despite the importance of this phenomenon, no effective methods have been proposed to accurately compute the robustness of state-of-the-art deep classifiers to such perturbations on large-scale datasets. In this paper, we fill this gap and propose the DeepFool algorithm to efficiently compute perturbations that fool deep networks, and thus reliably quantify the robustness of these classifiers. Extensive experimental results show that our approach outperforms recent methods in the task of computing adversarial perturbations and making classifiers more robust.",
                        "Citation Paper Authors": "Authors:Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Pascal Frossard"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "99.40% 99.10% 8.90% 0% L\u221emetric,\u03f5=0.010, epochs = 100, lr = 0.001\nI-FGSM ",
                    "Citation Text": "Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.02533",
                        "Citation Paper Title": "Title:Adversarial examples in the physical world",
                        "Citation Paper Abstract": "Abstract:Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.",
                        "Citation Paper Authors": "Authors:Alexey Kurakin, Ian Goodfellow, Samy Bengio"
                    }
                },
                {
                    "Sentence ID": 55,
                    "Sentence": "paper, which was not possible from transferable single step\nattacks.\n\u2022ALP : In this paper ",
                    "Citation Text": "Harini Kannan, Alexey Kurakin, and Ian J. Goodfellow. Adversarial logit pairing. CoRR , abs/1803.06373, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.06373",
                        "Citation Paper Title": "Title:Adversarial Logit Pairing",
                        "Citation Paper Abstract": "Abstract:In this paper, we develop improved techniques for defending against adversarial examples at scale. First, we implement the state of the art version of adversarial training at unprecedented scale on ImageNet and investigate whether it remains effective in this setting - an important open scientific question (Athalye et al., 2018). Next, we introduce enhanced defenses using a technique we call logit pairing, a method that encourages logits for pairs of examples to be similar. When applied to clean examples and their adversarial counterparts, logit pairing improves accuracy on adversarial examples over vanilla adversarial training; we also find that logit pairing on clean examples only is competitive with adversarial training in terms of accuracy on two datasets. Finally, we show that adversarial logit pairing achieves the state of the art defense on ImageNet against PGD white box attacks, with an accuracy improvement from 1.5% to 27.9%. Adversarial logit pairing also successfully damages the current state of the art defense against black box attacks on ImageNet (Tramer et al., 2018), dropping its accuracy from 66.6% to 47.1%. With this new accuracy drop, adversarial logit pairing ties with Tramer et al.(2018) for the state of the art on black box attacks on ImageNet.",
                        "Citation Paper Authors": "Authors:Harini Kannan, Alexey Kurakin, Ian Goodfellow"
                    }
                },
                {
                    "Sentence ID": 50,
                    "Sentence": "69.70% 62.10% 89% 28% L2metric, Median Smoothing of 2x2\nGuo ",
                    "Citation Text": "Chuan Guo, Mayank Rana, Moustapha Ciss\u00e9, and Laurens van der Maaten. Countering adversarial images using input\ntransformations. CoRR , abs/1711.00117, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.00117",
                        "Citation Paper Title": "Title:Countering Adversarial Images using Input Transformations",
                        "Citation Paper Abstract": "Abstract:This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods",
                        "Citation Paper Authors": "Authors:Chuan Guo, Mayank Rana, Moustapha Cisse, Laurens van der Maaten"
                    }
                },
                {
                    "Sentence ID": 83,
                    "Sentence": ", Kannan et al. proposed a method called as logit pairing and its 2\nvariants- clean and adversarial . They compared their method with a modified version of\ndefense proposed by Madry et al. ",
                    "Citation Text": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning\nmodels resistant to adversarial attacks. In International Conference on Learning Representations , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06083",
                        "Citation Paper Title": "Title:Towards Deep Learning Models Resistant to Adversarial Attacks",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL.",
                        "Citation Paper Authors": "Authors:Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": "94.29% 87.67% 98.34% 20.84% l\u221emetric,\u03f5=0.031, 7 steps for iterative attacks, \u03be=0.01,\u03b4=1.2\nDhillon ",
                    "Citation Text": "Guneet S. Dhillon, Kamyar Azizzadenesheli, Jeremy D. Bernstein, Jean Kossaifi, Aran Khanna, Zachary C. Lipton, and\nAnimashree Anandkumar. Stochastic activation pruning for robust adversarial defense. In International Conference on\nLearning Representations , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.01442",
                        "Citation Paper Title": "Title:Stochastic Activation Pruning for Robust Adversarial Defense",
                        "Citation Paper Abstract": "Abstract:Neural networks are known to be vulnerable to adversarial examples. Carefully chosen perturbations to real images, while imperceptible to humans, induce misclassification and threaten the reliability of deep learning systems in the wild. To guard against adversarial examples, we take inspiration from game theory and cast the problem as a minimax zero-sum game between the adversary and the model. In general, for such games, the optimal strategy for both players requires a stochastic policy, also known as a mixed strategy. In this light, we propose Stochastic Activation Pruning (SAP), a mixed strategy for adversarial defense. SAP prunes a random subset of activations (preferentially pruning those with smaller magnitude) and scales up the survivors to compensate. We can apply SAP to pretrained networks, including adversarially trained models, without fine-tuning, providing robustness against adversarial examples. Experiments demonstrate that SAP confers robustness against attacks, increasing accuracy and preserving calibration.",
                        "Citation Paper Authors": "Authors:Guneet S. Dhillon, Kamyar Azizzadenesheli, Zachary C. Lipton, Jeremy Bernstein, Jean Kossaifi, Aran Khanna, Anima Anandkumar"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": "99.30% 98.67% 100% 5.70% l\u221emetric,\u03f5=0.3, 40 steps for iterative attacks, \u03be=0.01,\u03b4=1.2\nSamangouie ",
                    "Citation Text": "Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-gan: Protecting classifiers against adversarial attacks\nusing generative models. CoRR , abs/1805.06605, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.06605",
                        "Citation Paper Title": "Title:Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural network approaches have been widely adopted for machine learning tasks, including classification. However, they were shown to be vulnerable to adversarial perturbations: carefully crafted small perturbations can cause misclassification of legitimate images. We propose Defense-GAN, a new framework leveraging the expressive capability of generative models to defend deep neural networks against such attacks. Defense-GAN is trained to model the distribution of unperturbed images. At inference time, it finds a close output to a given image which does not contain the adversarial changes. This output is then fed to the classifier. Our proposed method can be used with any classification model and does not modify the classifier structure or training procedure. It can also be used as a defense against any attack as it does not assume knowledge of the process for generating the adversarial examples. We empirically show that Defense-GAN is consistently effective against different attack methods and improves on existing defense strategies. Our code has been made publicly available at this https URL",
                        "Citation Paper Authors": "Authors:Pouya Samangouei, Maya Kabkab, Rama Chellappa"
                    }
                },
                {
                    "Sentence ID": 57,
                    "Sentence": ", which converts the image signal into orthonormal wavelets. These wavelets\nform the basis for an image\u2019s space, orientation, and scale, etc.\n\u2022Basis Function Transformations : Shaham et al. ",
                    "Citation Text": "Uri Shaham, James Garritano, Yutaro Yamada, Ethan Weinberger, Alex Cloninger, Xiuyuan Cheng, Kelly Stanton, and\nYuval Kluger. Defending against adversarial images using basis functions transformations, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.10840",
                        "Citation Paper Title": "Title:Defending against Adversarial Images using Basis Functions Transformations",
                        "Citation Paper Abstract": "Abstract:We study the effectiveness of various approaches that defend against adversarial attacks on deep networks via manipulations based on basis function representations of images. Specifically, we experiment with low-pass filtering, PCA, JPEG compression, low resolution wavelet approximation, and soft-thresholding. We evaluate these defense techniques using three types of popular attacks in black, gray and white-box settings. Our results show JPEG compression tends to outperform the other tested defenses in most of the settings considered, in addition to soft-thresholding, which performs well in specific cases, and yields a more mild decrease in accuracy on benign examples. In addition, we also mathematically derive a novel white-box attack in which the adversarial perturbation is composed only of terms corresponding a to pre-determined subset of the basis functions, of which a \"low frequency attack\" is a special case.",
                        "Citation Paper Authors": "Authors:Uri Shaham, James Garritano, Yutaro Yamada, Ethan Weinberger, Alex Cloninger, Xiuyuan Cheng, Kelly Stanton, Yuval Kluger"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.02045v2": {
            "Paper Title": "Privacy-Preserving Search for a Similar Genomic Makeup in the Cloud",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": ". Several privacy-preserving solutions have been pro-\nposed for processing of genomic data in different settings, including\npersonalized medicine ",
                    "Citation Text": "Pierre Baldi, Roberta Baronio, Emiliano De Cristofaro, Paolo Gasti, and Gene\nTsudik. Countering gattaca: efficient and secure testing of fully-sequenced\nhuman genomes. In Proceedings of the 18th ACM conference on Computer and\ncommunications security , pages 691\u2013702. ACM, 2011.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1110.2478",
                        "Citation Paper Title": "Title:Countering Gattaca: Efficient and Secure Testing of Fully-Sequenced Human Genomes (Full Version)",
                        "Citation Paper Abstract": "Abstract:Recent advances in DNA sequencing technologies have put ubiquitous availability of fully sequenced human genomes within reach. It is no longer hard to imagine the day when everyone will have the means to obtain and store one's own DNA sequence. Widespread and affordable availability of fully sequenced genomes immediately opens up important opportunities in a number of health-related fields. In particular, common genomic applications and tests performed in vitro today will soon be conducted computationally, using digitized genomes. New applications will be developed as genome-enabled medicine becomes increasingly preventive and personalized. However, this progress also prompts significant privacy challenges associated with potential loss, theft, or misuse of genomic data. In this paper, we begin to address genomic privacy by focusing on three important applications: Paternity Tests, Personalized Medicine, and Genetic Compatibility Tests. After carefully analyzing these applications and their privacy requirements, we propose a set of efficient techniques based on private set operations. This allows us to implement in in silico some operations that are currently performed via in vitro methods, in a secure fashion. Experimental results demonstrate that proposed techniques are both feasible and practical today.",
                        "Citation Paper Authors": "Authors:Pierre Baldi, Roberta Baronio, Emiliano De Cristofaro, Paolo Gasti, Gene Tsudik"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.12221v4": {
            "Paper Title": "Detecting DDoS Attack on SDN Due to Vulnerabilities in OpenFlow",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.02204v2": {
            "Paper Title": "Cross-Origin State Inference (COSI) Attacks: Leaking Web Site States\n  through XS-Leaks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.08320v2": {
            "Paper Title": "Locally Differentially Private Frequency Estimation with Consistency",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "\u00001 = 0\nOne standard way to do such normalization is through\nadditive normalization:\n\u000fNorm :After standard FO, add\u000eto each estimation so that\nthe overall sum is 1.\nThe method is formally proposed for the centralized set-\nting ",
                    "Citation Text": "M. Hay, V . Rastogi, G. Miklau, and D. Suciu. Boosting the accuracy of\ndifferentially private histograms through consistency. PVLDB , 2010.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:0904.0942",
                        "Citation Paper Title": "Title:Boosting the Accuracy of Differentially-Private Histograms Through Consistency",
                        "Citation Paper Abstract": "Abstract:We show that it is possible to significantly improve the accuracy of a general class of histogram queries while satisfying differential privacy. Our approach carefully chooses a set of queries to evaluate, and then exploits consistency constraints that should hold over the noisy output. In a post-processing phase, we compute the consistent input most likely to have produced the noisy output. The final output is differentially-private and consistent, but in addition, it is often much more accurate. We show, both theoretically and experimentally, that these techniques can be used for estimating the degree sequence of a graph very precisely, and for computing a histogram that can support arbitrary range queries accurately.",
                        "Citation Paper Authors": "Authors:Michael Hay, Vibhor Rastogi, Gerome Miklau, Dan Suciu"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": ", minimizing L2distance\nachieves MLE under the approximation that the noise is close\nto the Gaussian distribution. There are also post-processing\ntechniques proposed for other settings: Blasiok et al. ",
                    "Citation Text": "J. Blasiok, M. Bun, A. Nikolov, and T. Steinke. Towards instance-\noptimal private query release. In SODA , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.03763",
                        "Citation Paper Title": "Title:Towards Instance-Optimal Private Query Release",
                        "Citation Paper Abstract": "Abstract:We study efficient mechanisms for the query release problem in differential privacy: given a workload of $m$ statistical queries, output approximate answers to the queries while satisfying the constraints of differential privacy. In particular, we are interested in mechanisms that optimally adapt to the given workload. Building on the projection mechanism of Nikolov, Talwar, and Zhang, and using the ideas behind Dudley's chaining inequality, we propose new efficient algorithms for the query release problem, and prove that they achieve optimal sample complexity for the given workload (up to constant factors, in certain parameter regimes) with respect to the class of mechanisms that satisfy concentrated differential privacy. We also give variants of our algorithms that satisfy local differential privacy, and prove that they also achieve optimal sample complexity among all local sequentially interactive private mechanisms.",
                        "Citation Paper Authors": "Authors:Jaroslaw Blasiok, Mark Bun, Aleksandar Nikolov, Thomas Steinke"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": ". Mean estimation is also a building\nblock in LDP; most of existing work transforms the numerical\nvalue to a discrete value using stochastic round, and then apply\nfrequency oracles ",
                    "Citation Text": "J. C. Duchi, M. I. Jordan, and M. J. Wainwright. Local privacy and\nstatistical minimax rates. In FOCS , 2013.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1302.3203",
                        "Citation Paper Title": "Title:Local Privacy, Data Processing Inequalities, and Statistical Minimax Rates",
                        "Citation Paper Abstract": "Abstract:Working under a model of privacy in which data remains private even from the statistician, we study the tradeoff between privacy guarantees and the utility of the resulting statistical estimators. We prove bounds on information-theoretic quantities, including mutual information and Kullback-Leibler divergence, that depend on the privacy guarantees. When combined with standard minimax techniques, including the Le Cam, Fano, and Assouad methods, these inequalities allow for a precise characterization of statistical rates under local privacy constraints. We provide a treatment of several canonical families of problems: mean estimation, parameter estimation in fixed-design regression, multinomial probability estimation, and nonparametric density estimation. For all of these families, we provide lower and upper bounds that match up to constant factors, and exhibit new (optimal) privacy-preserving mechanisms and computationally efficient estimators that achieve the bounds.",
                        "Citation Paper Authors": "Authors:John C. Duchi, Michael I. Jordan, Martin J. Wainwright"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1909.12338v2": {
            "Paper Title": "Hardware Design and Analysis of the ACE and WAGE Ciphers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.10185v2": {
            "Paper Title": "Jacobian Adversarially Regularized Networks for Robustness",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.08565v2": {
            "Paper Title": "A Longitudinal Study on Web-sites Password Management (in)Security:\n  Evidence and Remedies",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.12729v3": {
            "Paper Title": "IRONHIDE: A Secure Multicore that Efficiently Mitigates\n  Microarchitecture State Attacks for Interactive Applications",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": ", and each of the three secure graph algorithms\ncombine with it to form a user-level interactive application.\n\u2022Real-Time Perception and Mission Planning: This appli-\ncation builds on an insecure vision pipeline ",
                    "Citation Text": "M. Buckler, S. Jayasuriya, and A. Sampson, \u201cRecon\ufb01guring the imaging\npipeline for computer vision,\u201d in IEEE ICCV , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.04352",
                        "Citation Paper Title": "Title:Reconfiguring the Imaging Pipeline for Computer Vision",
                        "Citation Paper Abstract": "Abstract:Advancements in deep learning have ignited an explosion of research on efficient hardware for embedded computer vision. Hardware vision acceleration, however, does not address the cost of capturing and processing the image data that feeds these algorithms. We examine the role of the image signal processing (ISP) pipeline in computer vision to identify opportunities to reduce computation and save energy. The key insight is that imaging pipelines should be designed to be configurable: to switch between a traditional photography mode and a low-power vision mode that produces lower-quality image data suitable only for computer vision. We use eight computer vision algorithms and a reversible pipeline simulation tool to study the imaging system's impact on vision performance. For both CNN-based and classical vision algorithms, we observe that only two ISP stages, demosaicing and gamma compression, are critical for task performance. We propose a new image sensor design that can compensate for skipping these stages. The sensor design features an adjustable resolution and tunable analog-to-digital converters (ADCs). Our proposed imaging system's vision mode disables the ISP entirely and configures the sensor to produce subsampled, lower-precision image data. This vision mode can save ~75% of the average energy of a baseline photography mode while having only a small impact on vision task accuracy.",
                        "Citation Paper Authors": "Authors:Mark Buckler, Suren Jayasuriya, Adrian Sampson"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.02674v2": {
            "Paper Title": "Polymorphic Encryption and Pseudonymisation of IP Network Flows",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.10595v3": {
            "Paper Title": "Influences of Human Demographics, Brand Familiarity and Security\n  Backgrounds on Homograph Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.07352v5": {
            "Paper Title": "Dynamic Malware Analysis with Feature Engineering and Feature Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.09731v2": {
            "Paper Title": "HEAX: An Architecture for Computing on Encrypted Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.06262v3": {
            "Paper Title": "Automatic Malware Description via Attribute Tagging and Similarity\n  Embedding",
            "Sentences": [
                {
                    "Sentence ID": 33,
                    "Sentence": ", Bugra\nand Erdogan use a disassembler to retrieve the opcodes of\nthe executable \ufb01les and then a shallow network based on\nWORD 2VEC ",
                    "Citation Text": "T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, \u201cDistributed\nRepresentations of Words and Phrases and their Compositionality,\u201d\nArXiv e-prints , Oct. 2013.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1310.4546",
                        "Citation Paper Title": "Title:Distributed Representations of Words and Phrases and their Compositionality",
                        "Citation Paper Abstract": "Abstract:The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
                        "Citation Paper Authors": "Authors:Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.13156v2": {
            "Paper Title": "Hiding Information in Big Data based on Deep Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.13173v2": {
            "Paper Title": "An Argumentation-Based Reasoner to Assist Digital Investigation and\n  Attribution of Cyber-Attacks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.12282v2": {
            "Paper Title": "CopyCAT: Taking Control of Neural Policies with Constant Attacks",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": "proposed an adversarial method for robust\ntraining of agents but only considered attacks on the dynamic of\nthe environment, not on the visual perception of the agent. Zhang\net al. ",
                    "Citation Text": "H Zhang, J Wang, Z Zhou, W Zhang, Y Wen, Y Yu, and W Li. 2018. Learning\nto design games: Strategic environments in reinforcement learning. In IJCAI\nInternational Joint Conference on Artificial Intelligence , Vol. 2018. ArXiv, 3068\u2013\n3074.\n9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.01310",
                        "Citation Paper Title": "Title:Learning to Design Games: Strategic Environments in Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:In typical reinforcement learning (RL), the environment is assumed given and the goal of the learning is to identify an optimal policy for the agent taking actions through its interactions with the environment. In this paper, we extend this setting by considering the environment is not given, but controllable and learnable through its interaction with the agent at the same time. This extension is motivated by environment design scenarios in the real-world, including game design, shopping space design and traffic signal design. Theoretically, we find a dual Markov decision process (MDP) w.r.t. the environment to that w.r.t. the agent, and derive a policy gradient solution to optimizing the parametrized environment. Furthermore, discontinuous environments are addressed by a proposed general generative framework. Our experiments on a Maze game design task show the effectiveness of the proposed algorithms in generating diverse and challenging Mazes against various agent settings.",
                        "Citation Paper Authors": "Authors:Haifeng Zhang, Jun Wang, Zhiming Zhou, Weinan Zhang, Ying Wen, Yong Yu, Wenxin Li"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "proposes a method to lure the agent into taking\nits least preferred action in order to reduce its performance but\nstill uses computationally heavy iterative methods at each time\nstep. Pinto et al . ",
                    "Citation Text": "Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. 2017. Ro-\nbust Adversarial Reinforcement Learning. In International Conference on Machine\nLearning . 2817\u20132826.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.02702",
                        "Citation Paper Title": "Title:Robust Adversarial Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:Deep neural networks coupled with fast simulation and improved computation have led to recent successes in the field of reinforcement learning (RL). However, most current RL-based approaches fail to generalize since: (a) the gap between simulation and real world is so large that policy-learning approaches fail to transfer; (b) even if policy learning is done in real world, the data scarcity leads to failed generalization from training to test scenarios (e.g., due to different friction or object masses). Inspired from H-infinity control methods, we note that both modeling errors and differences in training and test scenarios can be viewed as extra forces/disturbances in the system. This paper proposes the idea of robust adversarial reinforcement learning (RARL), where we train an agent to operate in the presence of a destabilizing adversary that applies disturbance forces to the system. The jointly trained adversary is reinforced -- that is, it learns an optimal destabilization policy. We formulate the policy learning as a zero-sum, minimax objective function. Extensive experiments in multiple environments (InvertedPendulum, HalfCheetah, Swimmer, Hopper and Walker2d) conclusively demonstrate that our method (a) improves training stability; (b) is robust to differences in training/test conditions; and c) outperform the baseline even in the absence of the adversary.",
                        "Citation Paper Authors": "Authors:Lerrel Pinto, James Davidson, Rahul Sukthankar, Abhinav Gupta"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "Vulnerabilities of neural classifiers were highlighted by Szegedy\net al. ",
                    "Citation Text": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,\nIan Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural networks.\narXiv preprint arXiv:1312.6199 (2013).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1312.6199",
                        "Citation Paper Title": "Title:Intriguing properties of neural networks",
                        "Citation Paper Abstract": "Abstract:Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.\nFirst, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks.\nSecond, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
                        "Citation Paper Authors": "Authors:Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1905.10864v3": {
            "Paper Title": "Generalizable Adversarial Attacks with Latent Variable Perturbation\n  Modelling",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.11143v3": {
            "Paper Title": "SGX-LKL: Securing the Host OS Interface for Trusted Execution",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": "is a cloud-based key/value store that supports obliv-\nious transactions while protecting access patterns from cloud\nproviders. Bittau et al. ",
                    "Citation Text": "Andrea Bittau, \u00dalfar Erlingsson, Petros Maniatis,\nIlya Mironov, Ananth Raghunathan, David Lie, Mitch\nRudominer, Ushasree Kode, Julien Tinnes, and Bern-\nhard Seefeld. Prochlo: Strong Privacy for Analytics in\nthe Crowd. In Proceedings of the 26th Symposium on\nOperating Systems Principles , pages 441\u2013459, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.00901",
                        "Citation Paper Title": "Title:Prochlo: Strong Privacy for Analytics in the Crowd",
                        "Citation Paper Abstract": "Abstract:The large-scale monitoring of computer users' software activities has become commonplace, e.g., for application telemetry, error reporting, or demographic profiling. This paper describes a principled systems architecture---Encode, Shuffle, Analyze (ESA)---for performing such monitoring with high utility while also protecting user privacy. The ESA design, and its Prochlo implementation, are informed by our practical experiences with an existing, large deployment of privacy-preserving software monitoring.\n(cont.; see the paper)",
                        "Citation Paper Authors": "Authors:Andrea Bittau, \u00dalfar Erlingsson, Petros Maniatis, Ilya Mironov, Ananth Raghunathan, David Lie, Mitch Rudominer, Usharsee Kode, Julien Tinnes, Bernhard Seefeld"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.10201v2": {
            "Paper Title": "Secure Sketch for All Noisy Sources (Noisy)",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.02174v2": {
            "Paper Title": "Privacy Preserving Threat Hunting in Smart Home Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.06192v2": {
            "Paper Title": "Comment on \"AndrODet: An adaptive Android obfuscation detector\"",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01389v3": {
            "Paper Title": "A Cryptanalysis of Two Cancelable Biometric Schemes based on\n  Index-of-Max Hashing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.06531v2": {
            "Paper Title": "STRIP: A Defence Against Trojan Attacks on Deep Neural Networks",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": "to protect the intellectual property (IP) of a trained DNN\nmodel ",
                    "Citation Text": "Y . Adi, C. Baum, M. Cisse, B. Pinkas, and J. Keshet, \u201cTurning\nyour weakness into a strength: Watermarking deep neural networks by\nbackdooring,\u201d in USENIX Security Symposium , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.04633",
                        "Citation Paper Title": "Title:Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring",
                        "Citation Paper Abstract": "Abstract:Deep Neural Networks have recently gained lots of success after enabling several breakthroughs in notoriously challenging problems. Training these networks is computationally expensive and requires vast amounts of training data. Selling such pre-trained models can, therefore, be a lucrative business model. Unfortunately, once the models are sold they can be easily copied and redistributed. To avoid this, a tracking mechanism to identify models as the intellectual property of a particular vendor is necessary.\nIn this work, we present an approach for watermarking Deep Neural Networks in a black-box way. Our scheme works for general classification tasks and can easily be combined with current learning algorithms. We show experimentally that such a watermark has no noticeable impact on the primary task that the model is designed for and evaluate the robustness of our proposal against a multitude of practical attacks. Moreover, we provide a theoretical analysis, relating our approach to previous work on backdooring.",
                        "Citation Paper Authors": "Authors:Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, Joseph Keshet"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "propose the Neural\nCleanse method to detect whether a DNN model has been\ntrojaned or not prior to deployment, where its accuracy is\nfurther improved in ",
                    "Citation Text": "W. Guo, L. Wang, X. Xing, M. Du, and D. Song, \u201cTabor: A highly\naccurate approach to inspecting and restoring trojan backdoors in ai\nsystems,\u201d arXiv preprint arXiv:1908.01763 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.01763",
                        "Citation Paper Title": "Title:TABOR: A Highly Accurate Approach to Inspecting and Restoring Trojan Backdoors in AI Systems",
                        "Citation Paper Abstract": "Abstract:A trojan backdoor is a hidden pattern typically implanted in a deep neural network. It could be activated and thus forces that infected model behaving abnormally only when an input data sample with a particular trigger present is fed to that model. As such, given a deep neural network model and clean input samples, it is very challenging to inspect and determine the existence of a trojan backdoor. Recently, researchers design and develop several pioneering solutions to address this acute problem. They demonstrate the proposed techniques have a great potential in trojan detection. However, we show that none of these existing techniques completely address the problem. On the one hand, they mostly work under an unrealistic assumption (e.g. assuming availability of the contaminated training database). On the other hand, the proposed techniques cannot accurately detect the existence of trojan backdoors, nor restore high-fidelity trojan backdoor images, especially when the triggers pertaining to the trojan vary in size, shape and position. In this work, we propose TABOR, a new trojan detection technique. Conceptually, it formalizes a trojan detection task as a non-convex optimization problem, and the detection of a trojan backdoor as the task of resolving the optimization through an objective function. Different from the existing technique also modeling trojan detection as an optimization problem, TABOR designs a new objective function--under the guidance of explainable AI techniques as well as heuristics--that could guide optimization to identify a trojan backdoor in a more effective fashion. In addition, TABOR defines a new metric to measure the quality of a trojan backdoor identified. Using an anomaly detection method, we show the new metric could better facilitate TABOR to identify intentionally injected triggers in an infected model and filter out false alarms......",
                        "Citation Paper Authors": "Authors:Wenbo Guo, Lun Wang, Xinyu Xing, Min Du, Dawn Song"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": ", they cannot be directly mounted to guard against\ntrojan attacks. Especially, considering that the user has no\nknowledge of the trojan trigger and no access to trojaned sam-\nples, this makes combating trojan attacks more challenging.\nWorks in ",
                    "Citation Text": "K. Liu, B. Dolan-Gavitt, and S. Garg, \u201cFine-pruning: Defending against\nbackdooring attacks on deep neural networks,\u201d in Proceedings of RAID ,\n2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.12185",
                        "Citation Paper Title": "Title:Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:Deep neural networks (DNNs) provide excellent performance across a wide range of classification tasks, but their training requires high computational resources and is often outsourced to third parties. Recent work has shown that outsourced training introduces the risk that a malicious trainer will return a backdoored DNN that behaves normally on most inputs but causes targeted misclassifications or degrades the accuracy of the network when a trigger known only to the attacker is present. In this paper, we provide the first effective defenses against backdoor attacks on DNNs. We implement three backdoor attacks from prior work and use them to investigate two promising defenses, pruning and fine-tuning. We show that neither, by itself, is sufficient to defend against sophisticated attackers. We then evaluate fine-pruning, a combination of pruning and fine-tuning, and show that it successfully weakens or even eliminates the backdoors, i.e., in some cases reducing the attack success rate to 0% with only a 0.4% drop in accuracy for clean (non-triggering) inputs. Our work provides the first step toward defenses against backdoor attacks in deep neural networks.",
                        "Citation Paper Authors": "Authors:Kang Liu, Brendan Dolan-Gavitt, Siddharth Garg"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1905.08833v2": {
            "Paper Title": "Smart Contract Development from the Perspective of Developers: Topics\n  and Issues Discussed on Social Media",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.10880v3": {
            "Paper Title": "Is Less Really More? Why Reducing Code Reuse Gadget Counts via Software\n  Debloating Doesn't Necessarily Indicate Improved Security",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.07546v3": {
            "Paper Title": "Non-interactive zero-knowledge arguments for QMA, with preprocessing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.02444v2": {
            "Paper Title": "A Verified Architecture for Proofs of Execution on Remote Devices under\n  Full Software Compromise",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.07567v2": {
            "Paper Title": "What are the Actual Flaws in Important Smart Contracts (and How Can We\n  Find Them)?",
            "Sentences": []
        },
        "http://arxiv.org/abs/1902.08722v5": {
            "Paper Title": "A Convex Relaxation Barrier to Tight Robustness Verification of Neural\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.04584v5": {
            "Paper Title": "Provably Robust Deep Learning via Adversarially Trained Smoothed\n  Classifiers",
            "Sentences": [
                {
                    "Sentence ID": 22,
                    "Sentence": "\ufb01rst proved\nrobustness guarantees for randomized smoothing classi\ufb01er, utilizing inequalities from the differential\nprivacy literature. Subsequently, Li et al. ",
                    "Citation Text": "Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Second-order adversarial attack\nand certi\ufb01able robustness. arXiv preprint arXiv:1809.03113 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.03113",
                        "Citation Paper Title": "Title:Certified Adversarial Robustness with Additive Noise",
                        "Citation Paper Abstract": "Abstract:The existence of adversarial data examples has drawn significant attention in the deep-learning community; such data are seemingly minimally perturbed relative to the original data, but lead to very different outputs from a deep-learning algorithm. Although a significant body of work on developing defensive models has been considered, most such models are heuristic and are often vulnerable to adaptive attacks. Defensive methods that provide theoretical robustness guarantees have been studied intensively, yet most fail to obtain non-trivial robustness when a large-scale model and data are present. To address these limitations, we introduce a framework that is scalable and provides certified bounds on the norm of the input manipulation for constructing adversarial examples. We establish a connection between robustness against adversarial perturbation and additive random noise, and propose a training strategy that can significantly improve the certified bounds. Our evaluation on MNIST, CIFAR-10 and ImageNet suggests that the proposed method is scalable to complicated models and large data sets, while providing competitive robustness to state-of-the-art provable defense methods.",
                        "Citation Paper Authors": "Authors:Bai Li, Changyou Chen, Wenlin Wang, Lawrence Carin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1910.12832v2": {
            "Paper Title": "Differentially Private Distributed Data Summarization under Covariate\n  Shift",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.07997v2": {
            "Paper Title": "Cloud-based Image Classification Service Is Not Robust To Simple\n  Transformations: A Forgotten Battlefield",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "report the \ufb01rst systematic study on\nthe real-world adversarial images and their use in online illicit\npromotions. ",
                    "Citation Text": "Xurong Li, Shouling Ji, Meng Han, Juntao Ji, and Chun-\nming Wu. Adversarial examples versus cloud-based\ndetectors: A black-box empirical study. 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.01223",
                        "Citation Paper Title": "Title:Adversarial Examples Versus Cloud-based Detectors: A Black-box Empirical Study",
                        "Citation Paper Abstract": "Abstract:Deep learning has been broadly leveraged by major cloud providers, such as Google, AWS and Baidu, to offer various computer vision related services including image classification, object identification, illegal image detection, etc. While recent works extensively demonstrated that deep learning classification models are vulnerable to adversarial examples, cloud-based image detection models, which are more complicated than classifiers, may also have similar security concern but not get enough attention yet. In this paper, we mainly focus on the security issues of real-world cloud-based image detectors. Specifically, (1) based on effective semantic segmentation, we propose four attacks to generate semantics-aware adversarial examples via only interacting with black-box APIs; and (2) we make the first attempt to conduct an extensive empirical study of black-box attacks against real-world cloud-based image detectors. Through the comprehensive evaluations on five major cloud platforms: AWS, Azure, Google Cloud, Baidu Cloud, and Alibaba Cloud, we demonstrate that our image processing based attacks can reach a success rate of approximately 100%, and the semantic segmentation based attacks have a success rate over 90% among different detection services, such as violence, politician, and pornography detection. We also proposed several possible defense strategies for these security challenges in the real-life situation.",
                        "Citation Paper Authors": "Authors:Xurong Li, Shouling Ji, Meng Han, Juntao Ji, Zhenyu Ren, Yushan Liu, Chunming Wu"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": ", which\nstudy the transferability between different models trained\nover the same dataset. ",
                    "Citation Text": "Yanpei Liu, Xinyun Chen, Liu Chang, and Dawn Song.\nDelving into transferable adversarial examples and\nblack-box attacks. 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.02770",
                        "Citation Paper Title": "Title:Delving into Transferable Adversarial Examples and Black-box Attacks",
                        "Citation Paper Abstract": "Abstract:An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack this http URL, which is a black-box image classification system.",
                        "Citation Paper Authors": "Authors:Yanpei Liu, Xinyun Chen, Chang Liu, Dawn Song"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1903.12519v2": {
            "Paper Title": "A Provable Defense for Deep Residual Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.00442v2": {
            "Paper Title": "PACLP: a fine-grained partition-based access control policy language for\n  provenance",
            "Sentences": []
        },
        "http://arxiv.org/abs/1904.00224v2": {
            "Paper Title": "Security of Medical Cyber-physical Systems: An Empirical Study on\n  Imaging Devices",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.03415v2": {
            "Paper Title": "Communication-Efficient (Client-Aided) Secure Two-Party Protocols and\n  Its Application",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.08348v2": {
            "Paper Title": "Leaking Information Through Cache LRU States",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.04835v3": {
            "Paper Title": "Blockchain and radio communications over suborbital spaceflights:\n  Watchtowers and Mystics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.00493v1": {
            "Paper Title": "Privacy for Rescue: A New Testimony Why Privacy is Vulnerable In Deep\n  Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.03367v2": {
            "Paper Title": "Onionchain: Towards Balancing Privacy and Traceability of\n  Blockchain-Based Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.13046v1": {
            "Paper Title": "A New Burrows Wheeler Transform Markov Distance",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.12828v1": {
            "Paper Title": "ICSTrace: A Malicious IP Traceback Model for Attacking Data of\n  Industrial Control System",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.06086v2": {
            "Paper Title": "Copy and Paste: A Simple But Effective Initialization Method for\n  Black-Box Adversarial Attacks",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": ".\nSetting . We choose a query-limited targeted label-only\nsetting, which is currently considered to be one of the most\ndif\ufb01cult scenarios ",
                    "Citation Text": "W. Brendel, J. Rauber, and M. Bethge. Decision-based ad-\nversarial attacks: Reliable attacks against black-box machine\nlearning models. In International Conference on Learning\nRepresentations , 2018. 1, 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.04248",
                        "Citation Paper Title": "Title:Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models",
                        "Citation Paper Abstract": "Abstract:Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient- or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from this http URL. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox at this https URL .",
                        "Citation Paper Authors": "Authors:Wieland Brendel, Jonas Rauber, Matthias Bethge"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.12576v1": {
            "Paper Title": "Privacy-Preserving Public Release of Datasets for Support Vector Machine\n  Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.12370v1": {
            "Paper Title": "Towards Deep Federated Defenses Against Malware in Cloud Ecosystems",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": ".\nA number of other important graph neural network approaches\nhave very recently been proposed, including GraphRNN ",
                    "Citation Text": "J. You, R. Ying, X. Ren, W. L. Hamilton, and J. Leskovec, \u201cGraphrnn:\nGenerating realistic graphs with deep auto-regressive models,\u201d arXiv\npreprint arXiv:1802.08773 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.08773",
                        "Citation Paper Title": "Title:GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models",
                        "Citation Paper Abstract": "Abstract:Modeling and generating graphs is fundamental for studying networks in biology, engineering, and social sciences. However, modeling complex distributions over graphs and then efficiently sampling from these distributions is challenging due to the non-unique, high-dimensional nature of graphs and the complex, non-local dependencies that exist between edges in a given graph. Here we propose GraphRNN, a deep autoregressive model that addresses the above challenges and approximates any distribution of graphs with minimal assumptions about their structure. GraphRNN learns to generate graphs by training on a representative set of graphs and decomposes the graph generation process into a sequence of node and edge formations, conditioned on the graph structure generated so far.\nIn order to quantitatively evaluate the performance of GraphRNN, we introduce a benchmark suite of datasets, baselines and novel evaluation metrics based on Maximum Mean Discrepancy, which measure distances between sets of graphs. Our experiments show that GraphRNN significantly outperforms all baselines, learning to generate diverse graphs that match the structural characteristics of a target set, while also scaling to graphs 50 times larger than previous deep models.",
                        "Citation Paper Authors": "Authors:Jiaxuan You, Rex Ying, Xiang Ren, William L. Hamilton, Jure Leskovec"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "uses and\nLSTM architecture to analyze system logs for abnormalities\nand perform other tasks.\nIn 2018, BERT (Bidirectional Encoder Representations from\nTransformers) ",
                    "Citation Text": "J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training\nof deep bidirectional transformers for language understanding,\u201d 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "and was an important factor in the\ndevelopment of federated learning ",
                    "Citation Text": "H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A.\ny Arcas, \u201cCommunication-ef\ufb01cient learning of deep networks from\ndecentralized data,\u201d in Proceedings of the 20th International Conference\non Arti\ufb01cial Intelligence and Statistics (AISTATS) , 2017. [Online].\nAvailable: http://arxiv.org/abs/1602.05629",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.05629",
                        "Citation Paper Title": "Title:Communication-Efficient Learning of Deep Networks from Decentralized Data",
                        "Citation Paper Abstract": "Abstract:Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning.\nWe present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.",
                        "Citation Paper Authors": "Authors:H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, Blaise Ag\u00fcera y Arcas"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": ".\nBERT builds upon the encoder structure of the encoder-\ndecoder architecture of transformer, which uses multi-head\nself attention ",
                    "Citation Text": "D. Bahdanau, K. Cho, and Y . Bengio, \u201cNeural machine translation by\njointly learning to align and translate,\u201d arXiv preprint arXiv:1409.0473 ,\n2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.0473",
                        "Citation Paper Title": "Title:Neural Machine Translation by Jointly Learning to Align and Translate",
                        "Citation Paper Abstract": "Abstract:Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
                        "Citation Paper Authors": "Authors:Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": ", which incorporate\nmasked self-attentional layers which allow vertices to attend\nto their neighbors\u2019 features, specifying different weights to\ndifferent vertices in the neighborhood; and Graph Isomorphism\nNetworks ",
                    "Citation Text": "K. Xu, W. Hu, J. Leskovec, and S. Jegelka, \u201cHow powerful are graph\nneural networks?\u201d 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.00826",
                        "Citation Paper Title": "Title:How Powerful are Graph Neural Networks?",
                        "Citation Paper Abstract": "Abstract:Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.",
                        "Citation Paper Authors": "Authors:Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": ".\nGraph convolutions were formally de\ufb01ned by Bruna et al. ",
                    "Citation Text": "J. Bruna, W. Zaremba, A. Szlam, and Y . LeCun, \u201cSpectral networks and\nlocally connected networks on graphs,\u201d arXiv preprint arXiv:1312.6203 ,\n2013.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1312.6203",
                        "Citation Paper Title": "Title:Spectral Networks and Locally Connected Networks on Graphs",
                        "Citation Paper Abstract": "Abstract:Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.",
                        "Citation Paper Authors": "Authors:Joan Bruna, Wojciech Zaremba, Arthur Szlam, Yann LeCun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.12363v1": {
            "Paper Title": "TASE: Reducing latency of symbolic execution with transactional memory",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.13435v2": {
            "Paper Title": "An Ultimate Approach of Mitigating Attacks in RPL Based Low Power Lossy\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.12060v1": {
            "Paper Title": "Characterizing and Detecting Money Laundering Activities on the Bitcoin\n  Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.04231v3": {
            "Paper Title": "Extended- Force vs Nudge : Comparing Users' Pattern Choices on SysPal\n  and TinPal",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.04429v2": {
            "Paper Title": "High-speed Privacy Amplification Scheme using GMP in Quantum Key\n  Distribution",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.11745v1": {
            "Paper Title": "Proof of Federated Learning: A Novel Energy-recycling Consensus\n  Algorithm",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.11721v1": {
            "Paper Title": "A Closer Look at Mobile App Usage as a Persistent Biometric: A Small\n  Case Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.11249v1": {
            "Paper Title": "Integration of Static and Dynamic Analysis for Malware Family\n  Classification with Composite Neural Network",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": "proposed a lightweight and \ufb02exible\nabstaining extension for online ensembles that allows ex-\ncluding some classi\ufb01ers from the voting process. And multi-\nview learning ",
                    "Citation Text": "C. Xu, D. Tao, and C. Xu, \u201cA survey on multi-view learning ,\u201darXiv\npreprint arXiv:1304.5634 , 2013.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1304.5634",
                        "Citation Paper Title": "Title:A Survey on Multi-view Learning",
                        "Citation Paper Abstract": "Abstract:In recent years, a great many methods of learning from multi-view data by considering the diversity of different views have been proposed. These views may be obtained from multiple sources or different feature subsets. In trying to organize and highlight similarities and differences between the variety of multi-view learning approaches, we review a number of representative multi-view learning algorithms in different areas and classify them into three groups: 1) co-training, 2) multiple kernel learning, and 3) subspace learning. Notably, co-training style algorithms train alternately to maximize the mutual agreement on two distinct views of the data; multiple kernel learning algorithms exploit kernels that naturally correspond to different views and combine kernels either linearly or non-linearly to improve learning performance; and subspace learning algorithms aim to obtain a latent subspace shared by multiple views by assuming that the input views are generated from this latent subspace. Though there is significant variance in the approaches to integrating multiple views to improve learning performance, they mainly exploit either the consensus principle or the complementary principle to ensure the success of multi-view learning. Since accessing multiple views is the fundament of multi-view learning, with the exception of study on learning a model from multiple views, it is also valuable to study how to construct multiple views and how to evaluate these views. Overall, by exploring the consistency and complementary properties of different views, multi-view learning is rendered more effective, more promising, and has better generalization ability than single-view learning.",
                        "Citation Paper Authors": "Authors:Chang Xu, Dacheng Tao, Chao Xu"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "examined several machine learning techniq ues\nfor detecting malware including random forest, deep learni ng\ntechniques, and liquid state machines, with system API call s.\nStokes et al. ",
                    "Citation Text": "J. W. Stokes, D. Wang, M. Marinescu, M. Marino, and B. Bus sone,\n\u201cAttack and defense of dynamic analysis-based, adversaria l neural\nmalware classi\ufb01cation models,\u201d arXiv preprint arXiv:1712.05919 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.05919",
                        "Citation Paper Title": "Title:Attack and Defense of Dynamic Analysis-Based, Adversarial Neural Malware Classification Models",
                        "Citation Paper Abstract": "Abstract:Recently researchers have proposed using deep learning-based systems for malware detection. Unfortunately, all deep learning classification systems are vulnerable to adversarial attacks. Previous work has studied adversarial attacks against static analysis-based malware classifiers which only classify the content of the unknown file without execution. However, since the majority of malware is either packed or encrypted, malware classification based on static analysis often fails to detect these types of files. To overcome this limitation, anti-malware companies typically perform dynamic analysis by emulating each file in the anti-malware engine or performing in-depth scanning in a virtual machine. These strategies allow the analysis of the malware after unpacking or decryption. In this work, we study different strategies of crafting adversarial samples for dynamic analysis. These strategies operate on sparse, binary inputs in contrast to continuous inputs such as pixels in images. We then study the effects of two, previously proposed defensive mechanisms against crafted adversarial samples including the distillation and ensemble defenses. We also propose and evaluate the weight decay defense. Experiments show that with these three defensive strategies, the number of successfully crafted adversarial samples is reduced compared to a standard baseline system without any defenses. In particular, the ensemble defense is the most resilient to adversarial attacks. Importantly, none of the defenses significantly reduce the classification accuracy for detecting malware. Finally, we demonstrate that while adding additional hidden layers to neural models does not significantly improve the malware classification accuracy, it does significantly increase the classifier's robustness to adversarial attacks.",
                        "Citation Paper Authors": "Authors:Jack W. Stokes, De Wang, Mady Marinescu, Marc Marino, Brian Bussone"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "contributed to behavioral malware classi\ufb01cation using\ninformation gathered from Microsoft Windows Prefetch \ufb01les ,\nthen they used Convolutional Recurrent Neural Networks to\nbuild their classi\ufb01cation model. Agrawal et al. ",
                    "Citation Text": "R. Agrawal, J. W. Stokes, M. Marinescu, and K. Selvaraj, \u201cRobust\nneural malware detection models for emulation sequence lea rning,\u201d\ninMILCOM 2018-2018 IEEE Military Communications Conference\n(MILCOM) . IEEE, 2018, pp. 1\u20138.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.10741",
                        "Citation Paper Title": "Title:Robust Neural Malware Detection Models for Emulation Sequence Learning",
                        "Citation Paper Abstract": "Abstract:Malicious software, or malware, presents a continuously evolving challenge in computer security. These embedded snippets of code in the form of malicious files or hidden within legitimate files cause a major risk to systems with their ability to run malicious command sequences. Malware authors even use polymorphism to reorder these commands and create several malicious variations. However, if executed in a secure environment, one can perform early malware detection on emulated command sequences.\nThe models presented in this paper leverage this sequential data derived via emulation in order to perform Neural Malware Detection. These models target the core of the malicious operation by learning the presence and pattern of co-occurrence of malicious event actions from within these sequences. Our models can capture entire event sequences and be trained directly using the known target labels. These end-to-end learning models are powered by two commonly used structures - Long Short-Term Memory (LSTM) Networks and Convolutional Neural Networks (CNNs). Previously proposed sequential malware classification models process no more than 200 events. Attackers can evade detection by delaying any malicious activity beyond the beginning of the file. We present specialized models that can handle extremely long sequences while successfully performing malware detection in an efficient way. We present an implementation of the Convoluted Partitioning of Long Sequences approach in order to tackle this vulnerability and operate on long sequences. We present our results on a large dataset consisting of 634,249 file sequences, with extremely long file sequences.",
                        "Citation Paper Authors": "Authors:Rakshit Agrawal, Jack W. Stokes, Mady Marinescu, Karthik Selvaraj"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.10979v1": {
            "Paper Title": "Privacy Attacks on Network Embeddings",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": "cannot be applied.\nPrivacy attacks on machine learning models. Due to increased\noverall awareness of privacy issues, the susceptibility of machine\nlearning models to the extraction of personal information has come\ninto the scope of the research community (e.g., ",
                    "Citation Text": "Veale, M., Binns, R., and Edwards, L. Algorithms that remember: model\ninversion attacks and data protection law. Philosophical Transactions of the\nRoyal Society A: Mathematical, Physical and Engineering Sciences 376 , 2133 (2018),\n20180083.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.04644",
                        "Citation Paper Title": "Title:Algorithms that Remember: Model Inversion Attacks and Data Protection Law",
                        "Citation Paper Abstract": "Abstract:Many individuals are concerned about the governance of machine learning systems and the prevention of algorithmic harms. The EU's recent General Data Protection Regulation (GDPR) has been seen as a core tool for achieving better governance of this area. While the GDPR does apply to the use of models in some limited situations, most of its provisions relate to the governance of personal data, while models have traditionally been seen as intellectual property. We present recent work from the information security literature around `model inversion' and `membership inference' attacks, which indicate that the process of turning training data into machine learned systems is not one-way, and demonstrate how this could lead some models to be legally classified as personal data. Taking this as a probing experiment, we explore the different rights and obligations this would trigger and their utility, and posit future directions for algorithmic governance and regulation.",
                        "Citation Paper Authors": "Authors:Michael Veale, Reuben Binns, Lilian Edwards"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.11043v1": {
            "Paper Title": "Impact of consensus on appendable-block blockchain for IoT",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": "can help to reduce the amount of data that is man-\naged by nodes in a blockchain, which is important in environments\nthat produce large amount of data.\nAdditionally, a framework called SpeedyChain ",
                    "Citation Text": "Regio A. Michelin, Ali Dorri, Marco Steger, Roben C. Lunardi, Salil S. Kanhere,\nRaja Jurdak, and Avelino F. Zorzo. 2018. SpeedyChain: A Framework for Decou-\npling Data from Blockchain for Smart Cities. In 15th EAI International Conference\non Mobile and Ubiquitous Systems: Computing, Networking and Services (MobiQui-\ntous \u201918) . 145\u2013154.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.01980",
                        "Citation Paper Title": "Title:SpeedyChain: A framework for decoupling data from blockchain for smart cities",
                        "Citation Paper Abstract": "Abstract:There is increased interest in smart vehicles acting as both data consumers and producers in smart cities. Vehicles can use smart city data for decision-making, such as dynamic routing based on traffic conditions. Moreover, the multitude of embedded sensors in vehicles can collectively produce a rich data set of the urban landscape that can be used to provide a range of services. Key to the success of this vision is a scalable and private architecture for trusted data sharing. This paper proposes a framework called SpeedyChain, that leverages blockchain technology to allow smart vehicles to share their data while maintaining privacy, integrity, resilience and non-repudiation in a decentralized, and tamper-resistant manner. Differently from traditional blockchain usage (e.g., Bitcoin and Ethereum), the proposed framework uses a blockchain design that decouples the data stored in the transactions from the block header, thus allowing for fast addition of data to the blocks. Furthermore, an expiration time for each block to avoid large sized blocks is proposed. This paper also presents an evaluation of the proposed framework in a network emulator to demonstrate its benefits.",
                        "Citation Paper Authors": "Authors:Regio A. Michelin, Ali Dorri, Roben C. Lunardi, Marco Steger, Salil S. Kanhere, Raja Jurdak, Avelino F. Zorzo"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.11946v2": {
            "Paper Title": "Can Attention Masks Improve Adversarial Robustness?",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.10312v1": {
            "Paper Title": "Socio-network Analysis of RTL Designs for Hardware Trojan Localization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.10070v1": {
            "Paper Title": "Destruction of Image Steganography using Generative Adversarial Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.01492v2": {
            "Paper Title": "Achieving Verified Robustness to Symbol Substitutions via Interval Bound\n  Propagation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.09882v1": {
            "Paper Title": "Design and Implementation of a Blockchain-based Consent Management\n  System",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.09779v1": {
            "Paper Title": "Pentest on an Internet Mobile App: A Case Study using Tramonto",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.09773v1": {
            "Paper Title": "Performance and Cost Evaluation of Smart Contracts in Collaborative\n  Health Care Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.09734v1": {
            "Paper Title": "Reverse Fingerprinting",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01247v2": {
            "Paper Title": "Coded Merkle Tree: Solving Data Availability Attacks in Blockchains",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.09555v1": {
            "Paper Title": "Imbalance measure and proactive channel rebalancing algorithm for the\n  Lightning Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/1905.11564v2": {
            "Paper Title": "Adversarially Robust Learning Could Leverage Computational Hardness",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.09150v1": {
            "Paper Title": "An Adaptive and Fast Convergent Approach to Differentially Private Deep\n  Learning",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": "on two real\ndatasets: MNIST and CIFAR-10. We implemented all these\nalgorithms using TensorFlow ",
                    "Citation Text": "M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\nS. Ghemawat, G. Irving, M. Isard et al. , \u201cTensor\ufb02ow: A system for large-\nscale machine learning,\u201d in Proceedings of 12th USENIX Symposium on\nOperating Systems Design and Implementation (OSDI) , vol. 16, 2016,\npp. 265\u2013283.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.08695",
                        "Citation Paper Title": "Title:TensorFlow: A system for large-scale machine learning",
                        "Citation Paper Abstract": "Abstract:TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous \"parameter server\" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with particularly strong support for training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model in contrast to existing systems, and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.",
                        "Citation Paper Authors": "Authors:Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, Xiaoqiang Zheng"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": ", which com-\nputes the privacy cost by analyzing the privacy ampli\ufb01cation\nin the subsampling scenario. And importantly, it requires no\nassumption on the parameters in the analysis ",
                    "Citation Text": "Y .-X. Wang, B. Balle, and S. Kasiviswanathan, \u201cSubsampled r \u00b4enyi\ndifferential privacy and analytical moments accountant,\u201d arXiv preprint\narXiv:1808.00087 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.00087",
                        "Citation Paper Title": "Title:Subsampled R\u00e9nyi Differential Privacy and Analytical Moments Accountant",
                        "Citation Paper Abstract": "Abstract:We study the problem of subsampling in differential privacy (DP), a question that is the centerpiece behind many successful differentially private machine learning algorithms. Specifically, we provide a tight upper bound on the R\u00e9nyi Differential Privacy (RDP) (Mironov, 2017) parameters for algorithms that: (1) subsample the dataset, and then (2) applies a randomized mechanism M to the subsample, in terms of the RDP parameters of M and the subsampling probability parameter. Our results generalize the moments accounting technique, developed by Abadi et al. (2016) for the Gaussian mechanism, to any subsampled RDP mechanism.",
                        "Citation Paper Authors": "Authors:Yu-Xiang Wang, Borja Balle, Shiva Kasiviswanathan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1910.14356v2": {
            "Paper Title": "Certifiable Robustness to Graph Perturbations",
            "Sentences": [
                {
                    "Sentence ID": 59,
                    "Sentence": ". Other\nrobustness aspects of graph-based models (e.g. noise or anomalies) have also been investigated\n[3, 6, 24]. However, none of these works provide provable guarantees or certi\ufb01cates.\nZ\u00fcgner & G\u00fcnnemann ",
                    "Citation Text": "Z\u00fcgner, D. and G\u00fcnnemann, S. Certi\ufb01able robustness and robust training for graph convolutional\nnetworks. In International Conference on Knowledge Discovery & Data Mining, KDD , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.12269",
                        "Citation Paper Title": "Title:Certifiable Robustness and Robust Training for Graph Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:Recent works show that Graph Neural Networks (GNNs) are highly non-robust with respect to adversarial attacks on both the graph structure and the node attributes, making their outcomes unreliable. We propose the first method for certifiable (non-)robustness of graph convolutional networks with respect to perturbations of the node attributes. We consider the case of binary node attributes (e.g. bag-of-words) and perturbations that are L_0-bounded. If a node has been certified with our method, it is guaranteed to be robust under any possible perturbation given the attack model. Likewise, we can certify non-robustness. Finally, we propose a robust semi-supervised training procedure that treats the labeled and unlabeled nodes jointly. As shown in our experimental evaluation, our method significantly improves the robustness of the GNN with only minimal effect on the predictive accuracy.",
                        "Citation Paper Authors": "Authors:Daniel Z\u00fcgner, Stephan G\u00fcnnemann"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "since it shows superior performance on the\nsemi-supervised node classi\ufb01cation task ",
                    "Citation Text": "Fey, M. and Lenssen, J. E. Fast graph representation learning with pytorch geometric. arXiv\npreprint arXiv:1903.02428 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.02428",
                        "Citation Paper Title": "Title:Fast Graph Representation Learning with PyTorch Geometric",
                        "Citation Paper Abstract": "Abstract:We introduce PyTorch Geometric, a library for deep learning on irregularly structured input data such as graphs, point clouds and manifolds, built upon PyTorch. In addition to general graph data structures and processing methods, it contains a variety of recently published methods from the domains of relational learning and 3D data processing. PyTorch Geometric achieves high data throughput by leveraging sparse GPU acceleration, by providing dedicated CUDA kernels and by introducing efficient mini-batch handling for input examples of different size. In this work, we present the library in detail and perform a comprehensive comparative study of the implemented methods in homogeneous evaluation scenarios.",
                        "Citation Paper Authors": "Authors:Matthias Fey, Jan Eric Lenssen"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "Neural networks [ 41,21], and recently graph neural networks [ 13,60,58] and node embeddings ",
                    "Citation Text": "Bojchevski, A. and G\u00fcnnemann, S. Adversarial attacks on node embeddings via graph poisoning.\nInInternational Conference on Machine Learning, ICML , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.01093",
                        "Citation Paper Title": "Title:Adversarial Attacks on Node Embeddings via Graph Poisoning",
                        "Citation Paper Abstract": "Abstract:The goal of network representation learning is to learn low-dimensional node embeddings that capture the graph structure and are useful for solving downstream tasks. However, despite the proliferation of such methods, there is currently no study of their robustness to adversarial attacks. We provide the first adversarial vulnerability analysis on the widely used family of methods based on random walks. We derive efficient adversarial perturbations that poison the network structure and have a negative effect on both the quality of the embeddings and the downstream tasks. We further show that our attacks are transferable since they generalize to many models and are successful even when the attacker is restricted.",
                        "Citation Paper Authors": "Authors:Aleksandar Bojchevski, Stephan G\u00fcnnemann"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.10833v1": {
            "Paper Title": "A New Ensemble Method for Concessively Targeted Multi-model Attack",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": ", to improve the trans-\nferability. The defense models to evaluate our proposed\nmethod include pretrained models on ImageNet ",
                    "Citation Text": "Olga Russakovsky, Jun Deng, Hao Su, Jonathan Krause,\nSanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpa-\nthy, Aditya Khosla, Michael S. Bernstein, Alexander C.\nBerg, and Li Fei-Fei. Imagenet large scale visual recogni-\ntion challenge. International Journal of Computer Vision ,\n115:211\u2013252, 2015. 2, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.0575",
                        "Citation Paper Title": "Title:ImageNet Large Scale Visual Recognition Challenge",
                        "Citation Paper Abstract": "Abstract:The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions.\nThis paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.",
                        "Citation Paper Authors": "Authors:Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, Li Fei-Fei"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "regard adversarial training as a framework of max-\nimum and minimum game and train more robust models in\nthis way.\nXieet al. propose a denoising network architecture ",
                    "Citation Text": "Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan L.\nYuille, and Kaiming He. Feature denoising for improving\nadversarial robustness. IEEE/CVF Conference on Computer\nVision and Pattern Recognition , 2019. 1, 2, 3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.03411",
                        "Citation Paper Title": "Title:Feature Denoising for Improving Adversarial Robustness",
                        "Citation Paper Abstract": "Abstract:Adversarial attacks to image classification systems present challenges to convolutional networks and opportunities for understanding them. This study suggests that adversarial perturbations on images lead to noise in the features constructed by these networks. Motivated by this observation, we develop new network architectures that increase adversarial robustness by performing feature denoising. Specifically, our networks contain blocks that denoise the features using non-local means or other filters; the entire networks are trained end-to-end. When combined with adversarial training, our feature denoising networks substantially improve the state-of-the-art in adversarial robustness in both white-box and black-box attack settings. On ImageNet, under 10-iteration PGD white-box attacks where prior art has 27.9% accuracy, our method achieves 55.7%; even under extreme 2000-iteration PGD white-box attacks, our method secures 42.6% accuracy. Our method was ranked first in Competition on Adversarial Attacks and Defenses (CAAD) 2018 --- it achieved 50.6% classification accuracy on a secret, ImageNet-like test dataset against 48 unknown attackers, surpassing the runner-up approach by ~10%. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan Yuille, Kaiming He"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": ". The max-\nimum perturbation \u000fis set to 16, with pixel values in [0,\n255]. We ensemble three models, which are a normally\ntrained model\u2014Inception v3 (Inc-v3) ",
                    "Citation Text": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJonathon Shlens, and Zbigniew Wojna. Rethinking the in-\nception architecture for computer vision. IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , pages\n2818\u20132826, 2016. 1, 2, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.00567",
                        "Citation Paper Title": "Title:Rethinking the Inception Architecture for Computer Vision",
                        "Citation Paper Abstract": "Abstract:Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.",
                        "Citation Paper Authors": "Authors:Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "s\nIn this section, we review the backgrounds of adver-\nsarial attack and defense methods. Following Sharma et\nal. ",
                    "Citation Text": "Yash Sharma, Tuan Le, and Moustafa Alzantot. Caad\n2018: Generating transferable adversarial examples. ArXiv ,\nabs/1810.01268, 2018. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.01268",
                        "Citation Paper Title": "Title:CAAD 2018: Generating Transferable Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:Deep neural networks (DNNs) are vulnerable to adversarial examples, perturbations carefully crafted to fool the targeted DNN, in both the non-targeted and targeted case. In the non-targeted case, the attacker simply aims to induce misclassification. In the targeted case, the attacker aims to induce classification to a specified target class. In addition, it has been observed that strong adversarial examples can transfer to unknown models, yielding a serious security concern. The NIPS 2017 competition was organized to accelerate research in adversarial attacks and defenses, taking place in the realistic setting where submitted adversarial attacks attempt to transfer to submitted defenses. The CAAD 2018 competition took place with nearly identical rules to the NIPS 2017 one. Given the requirement that the NIPS 2017 submissions were to be open-sourced, participants in the CAAD 2018 competition were able to directly build upon previous solutions, and thus improve the state-of-the-art in this setting. Our team participated in the CAAD 2018 competition, and won 1st place in both attack subtracks, non-targeted and targeted adversarial attacks, and 3rd place in defense. We outline our solutions and development results in this article. We hope our results can inform researchers in both generating and defending against adversarial examples.",
                        "Citation Paper Authors": "Authors:Yash Sharma, Tien-Dung Le, Moustafa Alzantot"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": ", can both improve accuracy and robustness of neu-\nral networks. Recently, Liu et al. propose novel ensemble-\nbased approaches to generate adversarial examples, which\nimprove the transferability even for targeted adversarial ex-\namples ",
                    "Citation Text": "Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Xiaodong\nSong. Delving into transferable adversarial examples and\nblack-box attacks. ICLR , 2017. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.02770",
                        "Citation Paper Title": "Title:Delving into Transferable Adversarial Examples and Black-box Attacks",
                        "Citation Paper Abstract": "Abstract:An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack this http URL, which is a black-box image classification system.",
                        "Citation Paper Authors": "Authors:Yanpei Liu, Xinyun Chen, Chang Liu, Dawn Song"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "and\nsome robust models trained with some defense methods in\nsec. 2.2.\n2.1. Attack Methods\n2.1.1 Fast Gradient Sign Method\nFast gradient sign method (FGSM) ",
                    "Citation Text": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan\nBruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus.\nIntriguing properties of neural networks. ICLR , 2014. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1312.6199",
                        "Citation Paper Title": "Title:Intriguing properties of neural networks",
                        "Citation Paper Abstract": "Abstract:Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.\nFirst, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks.\nSecond, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
                        "Citation Paper Authors": "Authors:Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.09059v1": {
            "Paper Title": "$n$-ML: Mitigating Adversarial Examples via Ensembles of Topologically\n  Manipulated Classifiers",
            "Sentences": [
                {
                    "Sentence ID": 45,
                    "Sentence": ". In this work, we compare\nour proposed defense with detection methods based on Local\nIntrinsic Dimensionality ( LID) ",
                    "Citation Text": "X. Ma, B. Li, Y . Wang, S. M. Erfani, S. Wijewickrema, G. Schoenebeck,\nD. Song, M. E. Houle, and J. Bailey. Characterizing adversarial\nsubspaces using local intrinsic dimensionality. In Proc. ICLR , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.02613",
                        "Citation Paper Title": "Title:Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality",
                        "Citation Paper Abstract": "Abstract:Deep Neural Networks (DNNs) have recently been shown to be vulnerable against adversarial examples, which are carefully crafted instances that can mislead DNNs to make errors during prediction. To better understand such attacks, a characterization is needed of the properties of regions (the so-called 'adversarial subspaces') in which adversarial examples lie. We tackle this challenge by characterizing the dimensional properties of adversarial regions, via the use of Local Intrinsic Dimensionality (LID). LID assesses the space-filling capability of the region surrounding a reference example, based on the distance distribution of the example to its neighbors. We first provide explanations about how adversarial perturbation can affect the LID characteristic of adversarial regions, and then show empirically that LID characteristics can facilitate the distinction of adversarial examples generated using state-of-the-art attacks. As a proof-of-concept, we show that a potential application of LID is to distinguish adversarial examples, and the preliminary results show that it can outperform several state-of-the-art detection measures by large margins for five attack strategies considered in this paper across three benchmark datasets. Our analysis of the LID characteristic for adversarial regions not only motivates new directions of effective adversarial defense, but also opens up more challenges for developing new attacks to better understand the vulnerabilities of DNNs.",
                        "Citation Paper Authors": "Authors:Xingjun Ma, Bo Li, Yisen Wang, Sarah M. Erfani, Sudanthi Wijewickrema, Grant Schoenebeck, Dawn Song, Michael E. Houle, James Bailey"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": ". While\nadaptive attacks have been shown to circumvent some of these\ndefenses ",
                    "Citation Text": "A. Athalye, N. Carlini, and D. Wagner. Obfuscated gradients give a false\nsense of security: Circumventing defenses to adversarial examples. In\nProc. ICML , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.00420",
                        "Citation Paper Title": "Title:Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimization-based attacks, we find defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining non-certified white-box-secure defenses at ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers.",
                        "Citation Paper Authors": "Authors:Anish Athalye, Nicholas Carlini, David Wagner"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": ". Several techniques can\nbe used to enhance the transferability of adversarial exam-\nples ",
                    "Citation Text": "Y . Dong, T. Pang, H. Su, and J. Zhu. Evading defenses to transferable\nadversarial examples by translation-invariant attacks. In Proc. CVPR ,\n2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.02884",
                        "Citation Paper Title": "Title:Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks",
                        "Citation Paper Abstract": "Abstract:Deep neural networks are vulnerable to adversarial examples, which can mislead classifiers by adding imperceptible perturbations. An intriguing property of adversarial examples is their good transferability, making black-box attacks feasible in real-world applications. Due to the threat of adversarial attacks, many methods have been proposed to improve the robustness. Several state-of-the-art defenses are shown to be robust against transferable adversarial examples. In this paper, we propose a translation-invariant attack method to generate more transferable adversarial examples against the defense models. By optimizing a perturbation over an ensemble of translated images, the generated adversarial example is less sensitive to the white-box model being attacked and has better transferability. To improve the efficiency of attacks, we further show that our method can be implemented by convolving the gradient at the untranslated image with a pre-defined kernel. Our method is generally applicable to any gradient-based attack method. Extensive experiments on the ImageNet dataset validate the effectiveness of the proposed method. Our best attack fools eight state-of-the-art defenses at an 82% success rate on average based only on the transferability, demonstrating the insecurity of the current defense techniques.",
                        "Citation Paper Authors": "Authors:Yinpeng Dong, Tianyu Pang, Hang Su, Jun Zhu"
                    }
                },
                {
                    "Sentence ID": 61,
                    "Sentence": "). These defenses are less effective\nthan adversarial training with PGD ",
                    "Citation Text": "H. Salman, G. Yang, H. Zhang, C.-J. Hsieh, and P. Zhang. A convex\nrelaxation barrier to tight robustness veri\ufb01cation of neural networks. In\nProc. NeurIPS , 2019. To appear.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.08722",
                        "Citation Paper Title": "Title:A Convex Relaxation Barrier to Tight Robustness Verification of Neural Networks",
                        "Citation Paper Abstract": "Abstract:Verification of neural networks enables us to gauge their robustness against adversarial attacks. Verification algorithms fall into two categories: exact verifiers that run in exponential time and relaxed verifiers that are efficient but incomplete. In this paper, we unify all existing LP-relaxed verifiers, to the best of our knowledge, under a general convex relaxation framework. This framework works for neural networks with diverse architectures and nonlinearities and covers both primal and dual views of robustness verification. We further prove strong duality between the primal and dual problems under very mild conditions. Next, we perform large-scale experiments, amounting to more than 22 CPU-years, to obtain exact solution to the convex-relaxed problem that is optimal within our framework for ReLU networks. We find the exact solution does not significantly improve upon the gap between PGD and existing relaxed verifiers for various networks trained normally or robustly on MNIST and CIFAR datasets. Our results suggest there is an inherent barrier to tight verification for the large class of methods captured by our framework. We discuss possible causes of this barrier and potential future directions for bypassing it. Our code and trained models are available at this http URL .",
                        "Citation Paper Authors": "Authors:Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, Pengchuan Zhang"
                    }
                },
                {
                    "Sentence ID": 77,
                    "Sentence": ". Vijaykeerthy et al. train DNNs sequentially to\nbe robust against an increasing set of attacks ",
                    "Citation Text": "D. Vijaykeerthy, A. Suri, S. Mehta, and P. Kumaraguru. Hardening deep\nneural networks via adversarial model cascades. 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.01448",
                        "Citation Paper Title": "Title:Hardening Deep Neural Networks via Adversarial Model Cascades",
                        "Citation Paper Abstract": "Abstract:Deep neural networks (DNNs) are vulnerable to malicious inputs crafted by an adversary to produce erroneous outputs. Works on securing neural networks against adversarial examples achieve high empirical robustness on simple datasets such as MNIST. However, these techniques are inadequate when empirically tested on complex data sets such as CIFAR-10 and SVHN. Further, existing techniques are designed to target specific attacks and fail to generalize across attacks. We propose the Adversarial Model Cascades (AMC) as a way to tackle the above inadequacies. Our approach trains a cascade of models sequentially where each model is optimized to be robust towards a mixture of multiple attacks. Ultimately, it yields a single model which is secure against a wide range of attacks; namely FGSM, Elastic, Virtual Adversarial Perturbations and Madry. On an average, AMC increases the model's empirical robustness against various attacks simultaneously, by a significant margin (of 6.225% for MNIST, 5.075% for SVHN and 2.65% for CIFAR10). At the same time, the model's performance on non-adversarial inputs is comparable to the state-of-the-art models.",
                        "Citation Paper Authors": "Authors:Deepak Vijaykeerthy, Anshuman Suri, Sameep Mehta, Ponnurangam Kumaraguru"
                    }
                },
                {
                    "Sentence ID": 79,
                    "Sentence": ". Differently, Wang\net al. train a hierarchy of layers, each containing multiplepaths, and randomly switch between the chosen paths at infer-\nence time ",
                    "Citation Text": "X. Wang, S. Wang, P.-Y . Chen, Y . Wang, B. Kulis, X. Lin, and P. Chin.\nProtecting neural networks with hierarchical random switching: Towards\nbetter robustness-accuracy trade-off for stochastic defenses. arXiv\npreprint arXiv:1908.07116 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.07116",
                        "Citation Paper Title": "Title:Protecting Neural Networks with Hierarchical Random Switching: Towards Better Robustness-Accuracy Trade-off for Stochastic Defenses",
                        "Citation Paper Abstract": "Abstract:Despite achieving remarkable success in various domains, recent studies have uncovered the vulnerability of deep neural networks to adversarial perturbations, creating concerns on model generalizability and new threats such as prediction-evasive misclassification or stealthy reprogramming. Among different defense proposals, stochastic network defenses such as random neuron activation pruning or random perturbation to layer inputs are shown to be promising for attack mitigation. However, one critical drawback of current defenses is that the robustness enhancement is at the cost of noticeable performance degradation on legitimate data, e.g., large drop in test accuracy. This paper is motivated by pursuing for a better trade-off between adversarial robustness and test accuracy for stochastic network defenses. We propose Defense Efficiency Score (DES), a comprehensive metric that measures the gain in unsuccessful attack attempts at the cost of drop in test accuracy of any defense. To achieve a better DES, we propose hierarchical random switching (HRS), which protects neural networks through a novel randomization scheme. A HRS-protected model contains several blocks of randomly switching channels to prevent adversaries from exploiting fixed model structures and parameters for their malicious purposes. Extensive experiments show that HRS is superior in defending against state-of-the-art white-box and adaptive adversarial misclassification attacks. We also demonstrate the effectiveness of HRS in defending adversarial reprogramming, which is the first defense against adversarial programs. Moreover, in most settings the average DES of HRS is at least 5X higher than current stochastic network defenses, validating its significantly improved robustness-accuracy trade-off.",
                        "Citation Paper Authors": "Authors:Xiao Wang, Siyue Wang, Pin-Yu Chen, Yanzhi Wang, Brian Kulis, Xue Lin, Peter Chin"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": ". Other defenses\nestimate the output of the so-called smoothed classi\ufb01er by\n2classifying many variants of the input after adding noise at\nthe input or intermediate layers ",
                    "Citation Text": "J. M. Cohen, E. Rosenfeld, and J. Z. Kolter. Certi\ufb01ed adversarial\nrobustness via randomized smoothing. In Proc. ICML , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.02918",
                        "Citation Paper Title": "Title:Certified Adversarial Robustness via Randomized Smoothing",
                        "Citation Paper Abstract": "Abstract:We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the $\\ell_2$ norm. This \"randomized smoothing\" technique has been proposed recently in the literature, but existing guarantees are loose. We prove a tight robustness guarantee in $\\ell_2$ norm for smoothing with Gaussian noise. We use randomized smoothing to obtain an ImageNet classifier with e.g. a certified top-1 accuracy of 49% under adversarial perturbations with $\\ell_2$ norm less than 0.5 (=127/255). No certified defense has been shown feasible on ImageNet except for smoothing. On smaller-scale datasets where competing approaches to certified $\\ell_2$ robustness are viable, smoothing delivers higher certified accuracies. Our strong empirical results suggest that randomized smoothing is a promising direction for future research into adversarially robust classification. Code and models are available at this http URL.",
                        "Citation Paper Authors": "Authors:Jeremy M Cohen, Elan Rosenfeld, J. Zico Kolter"
                    }
                },
                {
                    "Sentence ID": 64,
                    "Sentence": "). Keeping the perturbations\u2019\nLp-norms small helps ensure the attacks\u2019 imperceptibility to\nhumans, albeit imperfectly ",
                    "Citation Text": "A. Sen, X. Zhu, L. Marshall, and R. Nowak. Should adversarial attacks\nuse pixel p-norm? arXiv preprint arXiv:1906.02439 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.02439",
                        "Citation Paper Title": "Title:Should Adversarial Attacks Use Pixel p-Norm?",
                        "Citation Paper Abstract": "Abstract:Adversarial attacks aim to confound machine learning systems, while remaining virtually imperceptible to humans. Attacks on image classification systems are typically gauged in terms of $p$-norm distortions in the pixel feature space. We perform a behavioral study, demonstrating that the pixel $p$-norm for any $0\\le p \\le \\infty$, and several alternative measures including earth mover's distance, structural similarity index, and deep net embedding, do not fit human perception. Our result has the potential to improve the understanding of adversarial attack and defense strategies.",
                        "Citation Paper Authors": "Authors:Ayon Sen, Xiaojin Zhu, Liam Marshall, Robert Nowak"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1906.08935v2": {
            "Paper Title": "Deep Leakage from Gradients",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ") is included in the\nother participant\u2019s\u2019 batch. Furthermore, they train GAN models ",
                    "Citation Text": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing\nsystems , pages 2672\u20132680, 2014. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.2661",
                        "Citation Paper Title": "Title:Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.08987v1": {
            "Paper Title": "Model Weight Theft With Just Noise Inputs: The Curious Case of the\n  Petulant Attacker",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": "platforms such as BigML\nand Amazon Machine Learning and demonstrated effective attacks that resulted in extraction of\nmachine learning models with near-perfect \ufb01delity for several popular model classes. In ",
                    "Citation Text": "Jacson Rodrigues Correia-Silva, Rodrigo F Berriel, Claudine Badue, Alberto F de Souza, and\nThiago Oliveira-Santos. Copycat cnn: Stealing knowledge by persuading confession with\nrandom non-labeled data. In 2018 International Joint Conference on Neural Networks (IJCNN) ,\npages 1\u20138. IEEE, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.05476",
                        "Citation Paper Title": "Title:Copycat CNN: Stealing Knowledge by Persuading Confession with Random Non-Labeled Data",
                        "Citation Paper Abstract": "Abstract:In the past few years, Convolutional Neural Networks (CNNs) have been achieving state-of-the-art performance on a variety of problems. Many companies employ resources and money to generate these models and provide them as an API, therefore it is in their best interest to protect them, i.e., to avoid that someone else copies them. Recent studies revealed that state-of-the-art CNNs are vulnerable to adversarial examples attacks, and this weakness indicates that CNNs do not need to operate in the problem domain (PD). Therefore, we hypothesize that they also do not need to be trained with examples of the PD in order to operate in it.\nGiven these facts, in this paper, we investigate if a target black-box CNN can be copied by persuading it to confess its knowledge through random non-labeled data. The copy is two-fold: i) the target network is queried with random data and its predictions are used to create a fake dataset with the knowledge of the network; and ii) a copycat network is trained with the fake dataset and should be able to achieve similar performance as the target network.\nThis hypothesis was evaluated locally in three problems (facial expression, object, and crosswalk classification) and against a cloud-based API. In the copy attacks, images from both non-problem domain and PD were used. All copycat networks achieved at least 93.7% of the performance of the original models with non-problem domain data, and at least 98.6% using additional data from the PD. Additionally, the copycat CNN successfully copied at least 97.3% of the performance of the Microsoft Azure Emotion API. Our results show that it is possible to create a copycat CNN by simply querying a target network as black-box with random non-labeled data.",
                        "Citation Paper Authors": "Authors:Jacson Rodrigues Correia-Silva, Rodrigo F. Berriel, Claudine Badue, Alberto F. de Souza, Thiago Oliveira-Santos"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1909.10594v3": {
            "Paper Title": "MemGuard: Defending against Black-Box Membership Inference Attacks via\n  Adversarial Examples",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": "explored using conventional L2regularizer\nwhen training the target classifier.Min-Max Game ",
                    "Citation Text": "Milad Nasr, Reza Shokri, and Amir Houmansadr. 2018. Machine Learning with\nMembership Privacy using Adversarial Regularization. In Proceedings of the 2018\nACM SIGSAC Conference on Computer and Communications Security (CCS) . ACM.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.05852",
                        "Citation Paper Title": "Title:Machine Learning with Membership Privacy using Adversarial Regularization",
                        "Citation Paper Abstract": "Abstract:Machine learning models leak information about the datasets on which they are trained. An adversary can build an algorithm to trace the individual members of a model's training dataset. As a fundamental inference attack, he aims to distinguish between data points that were part of the model's training set and any other data points from the same distribution. This is known as the tracing (and also membership inference) attack. In this paper, we focus on such attacks against black-box models, where the adversary can only observe the output of the model, but not its parameters. This is the current setting of machine learning as a service in the Internet.\nWe introduce a privacy mechanism to train machine learning models that provably achieve membership privacy: the model's predictions on its training data are indistinguishable from its predictions on other data points from the same distribution. We design a strategic mechanism where the privacy mechanism anticipates the membership inference attacks. The objective is to train a model such that not only does it have the minimum prediction error (high utility), but also it is the most robust model against its corresponding strongest inference attack (high privacy). We formalize this as a min-max game optimization problem, and design an adversarial training algorithm that minimizes the classification loss of the model as well as the maximum gain of the membership inference attack against it. This strategy, which guarantees membership privacy (as prediction indistinguishability), acts also as a strong regularizer and significantly generalizes the model.\nWe evaluate our privacy mechanism on deep neural networks using different benchmark datasets. We show that our min-max strategy can mitigate the risk of membership inference attacks (close to the random guess) with a negligible cost in terms of the classification error.",
                        "Citation Paper Authors": "Authors:Milad Nasr, Reza Shokri, Amir Houmansadr"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "studied membership inference against generative models, in partic-\nular generative adversarial networks (GANs) ",
                    "Citation Text": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\nSherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative Adver-\nsarial Nets. In Proceedings of the 2014 Annual Conference on Neural Information\nProcessing Systems (NIPS) . NIPS.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.2661",
                        "Citation Paper Title": "Title:Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "proposed membership inference attacks against\nfederated learning. While most of the previous works concentrated\non classification models [ 33,34,42,43,56,58,69], Hayes et al. ",
                    "Citation Text": "Jamie Hayes, Luca Melis, George Danezis, and Emiliano De Cristofaro. 2019.\nLOGAN: Evaluating Privacy Leakage of Generative Models Using Generative\nAdversarial Networks. Symposium on Privacy Enhancing Technologies Symposium\n(2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.07663",
                        "Citation Paper Title": "Title:LOGAN: Membership Inference Attacks Against Generative Models",
                        "Citation Paper Abstract": "Abstract:Generative models estimate the underlying distribution of a dataset to generate realistic samples according to that distribution. In this paper, we present the first membership inference attacks against generative models: given a data point, the adversary determines whether or not it was used to train the model. Our attacks leverage Generative Adversarial Networks (GANs), which combine a discriminative and a generative model, to detect overfitting and recognize inputs that were part of training datasets, using the discriminator's capacity to learn statistical differences in distributions.\nWe present attacks based on both white-box and black-box access to the target model, against several state-of-the-art generative models, over datasets of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). We also discuss the sensitivity of the attacks to different training parameters, and their robustness against mitigation strategies, finding that defenses are either ineffective or lead to significantly worse performances of the generative models in terms of training stability and/or sample quality.",
                        "Citation Paper Authors": "Authors:Jamie Hayes, Luca Melis, George Danezis, Emiliano De Cristofaro"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.09303v1": {
            "Paper Title": "SIGMA : Strengthening IDS with GAN and Metaheuristics Attacks",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": ".\nGAN are also notably used to disrupt trained classi\ufb01ers ",
                    "Citation Text": "Anish Athalye, Logan Engstrom, Andrew Ilyas and Kevin Kwok. Syn-\nthesizing robust adversarial examples. arXiv preprint arXiv:1707.07397. ,\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.07397",
                        "Citation Paper Title": "Title:Synthesizing Robust Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:Standard methods for generating adversarial examples for neural networks do not consistently fool neural network classifiers in the physical world due to a combination of viewpoint shifts, camera noise, and other natural transformations, limiting their relevance to real-world systems. We demonstrate the existence of robust 3D adversarial objects, and we present the first algorithm for synthesizing examples that are adversarial over a chosen distribution of transformations. We synthesize two-dimensional adversarial images that are robust to noise, distortion, and affine transformation. We apply our algorithm to complex three-dimensional objects, using 3D-printing to manufacture the first physical adversarial objects. Our results demonstrate the existence of 3D adversarial objects in the physical world.",
                        "Citation Paper Authors": "Authors:Anish Athalye, Logan Engstrom, Andrew Ilyas, Kevin Kwok"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1908.01297v5": {
            "Paper Title": "A Restricted Black-box Adversarial Framework Towards Attacking Graph\n  Embedding Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.12757v3": {
            "Paper Title": "Modelling Load-Changing Attacks in Cyber-Physical Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.00169v2": {
            "Paper Title": "XBlock-ETH: Extracting and Exploring Blockchain Data From Ethereum",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": "presents\nthe measurement of the mining pool for Bitcoin. Although\nGencer et al. ",
                    "Citation Text": "A. E. Gencer, S. Basu, I. Eyal, R. Van Renesse, and E. G. Sirer,\n\u201cDecentralization in bitcoin and ethereum networks,\u201d arXiv preprint\narXiv:1801.03998 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.03998",
                        "Citation Paper Title": "Title:Decentralization in Bitcoin and Ethereum Networks",
                        "Citation Paper Abstract": "Abstract:Blockchain-based cryptocurrencies have demonstrated how to securely implement traditionally centralized systems, such as currencies, in a decentralized fashion. However, there have been few measurement studies on the level of decentralization they achieve in practice. We present a measurement study on various decentralization metrics of two of the leading cryptocurrencies with the largest market capitalization and user base, Bitcoin and Ethereum. We investigate the extent of decentralization by measuring the network resources of nodes and the interconnection among them, the protocol requirements affecting the operation of nodes, and the robustness of the two systems against attacks. In particular, we adapted existing internet measurement techniques and used the Falcon Relay Network as a novel measurement tool to obtain our data. We discovered that neither Bitcoin nor Ethereum has strictly better properties than the other. We also provide concrete suggestions for improving both systems.",
                        "Citation Paper Authors": "Authors:Adem Efe Gencer, Soumya Basu, Ittay Eyal, Robbert van Renesse, Emin G\u00fcn Sirer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.07742v1": {
            "Paper Title": "CAG: A Real-time Low-cost Enhanced-robustness High-transferability\n  Content-aware Adversarial Attack Generator",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.07714v1": {
            "Paper Title": "Industrial robot ransomware: Akerbeltz",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.08094v1": {
            "Paper Title": "Proof of file access in a private P2P network using blockchain",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.07283v1": {
            "Paper Title": "Misconfiguration Management of Network Security Components",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.11531v1": {
            "Paper Title": "Pseudo Random Number Generation: a Reinforcement Learning approach",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": "). Temperature is\nreduced over the course of training. Q-Learning algorithms which use DNNs as q-value approximators are called Deep\nQ-Networks (DQN) algorithms. In this paper, we use the Dueling Double DQN ",
                    "Citation Text": "Wang, Z., Schaul, T., Hessel, M., Van Hasselt, H., Lanctot, M., De Freitas, N.: Dueling network architectures for\ndeep reinforcement learning. arXiv preprint arXiv:1511.06581 (2015)\n13",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06581",
                        "Citation Paper Title": "Title:Dueling Network Architectures for Deep Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.",
                        "Citation Paper Authors": "Authors:Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, Nando de Freitas"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.06817v1": {
            "Paper Title": "Ten AI Stepping Stones for Cybersecurity",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.06812v1": {
            "Paper Title": "Cerberus: A Blockchain-Based Accreditation and Degree Verification\n  System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2001.00079v1": {
            "Paper Title": "User Acceptance of Usable Blockchain-Based Research Data Sharing System:\n  An Extended TAM Based Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.06733v1": {
            "Paper Title": "Private Federated Learning with Domain Adaptation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.06510v1": {
            "Paper Title": "Computer Viruses: The Abstract Theory Revisited",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.06491v1": {
            "Paper Title": "Implementing a Protocol Native Managed Cryptocurrency",
            "Sentences": []
        },
        "http://arxiv.org/abs/1901.04805v3": {
            "Paper Title": "Early Detection Of Mirai-Like IoT Bots In Large-Scale Networks Through\n  Sub-Sampled Packet Traffic Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.06362v1": {
            "Paper Title": "RSSI-based Secure Localization in the Presence of Malicious Nodes in\n  Sensor Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.06176v1": {
            "Paper Title": "Investigating the effectiveness of web adblockers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.01897v2": {
            "Paper Title": "On the security of ballot marking devices",
            "Sentences": []
        },
        "http://arxiv.org/abs/1907.01003v2": {
            "Paper Title": "Accurate, reliable and fast robustness evaluation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1911.09312v2": {
            "Paper Title": "Revisiting and Evaluating Software Side-channel Vulnerabilities and\n  Countermeasures in Cryptographic Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/1908.02108v2": {
            "Paper Title": "WSEmail: A Retrospective on a System for Secure Internet Messaging Based\n  on Web Services",
            "Sentences": []
        },
        "http://arxiv.org/abs/1909.08168v2": {
            "Paper Title": "SAFE^d: Self-Attestation For Networks of Heterogeneous Embedded Devices",
            "Sentences": []
        },
        "http://arxiv.org/abs/1910.01587v2": {
            "Paper Title": "On the security and privacy of Interac e-Transfers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.06497v1": {
            "Paper Title": "Founding The Domain of AI Forensics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.04613v2": {
            "Paper Title": "Lightweight Sybil-Resilient Multi-Robot Networks by Multipath\n  Manipulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.05919v2": {
            "Paper Title": "Arcula: A Secure Hierarchical Deterministic Wallet for Multi-asset\n  Blockchains",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.04870v1": {
            "Paper Title": "V0LTpwn: Attacking x86 Processor Integrity from Software",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.04859v1": {
            "Paper Title": "Privacy-Preserving Blockchain Based Federated Learning with Differential\n  Data Sharing",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": ", propose \nmethods to cut down on the communication cost and facilitate \ntraining over mobile clients. Compression technologies also come into play to improve the efficiency of the entire system \nlike Deep Gradient Compres sion ",
                    "Citation Text": "Y. Lin, S. Han, H. Mao, Y. Wang, and W. J. Dally, \n\u201cDeep Gradient Compression: Reducing the \nCommunication Bandwidth for Distributed Training,\u201d \nArXiv171201887 Cs Stat , Feb. 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.01887",
                        "Citation Paper Title": "Title:Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training",
                        "Citation Paper Abstract": "Abstract:Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile. Code is available at: this https URL.",
                        "Citation Paper Authors": "Authors:Yujun Lin, Song Han, Huizi Mao, Yu Wang, William J. Dally"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "federated learning  is defined when the  datasets \nhave  the same feature space but different sample  space. For \nexample, two banks may have very different customer base \nbut primarily serve the functions and hence have a common \nfeature base. ",
                    "Citation Text": "T. Hunt, C. Song, R. Shokri, V. Shmatikov, and E. \nWitchel, \u201cChiron: Privacy -preserving Machine Learning as \na Service,\u201d ArXiv180305961 Cs , Mar. 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.05961",
                        "Citation Paper Title": "Title:Chiron: Privacy-preserving Machine Learning as a Service",
                        "Citation Paper Abstract": "Abstract:Major cloud operators offer machine learning (ML) as a service, enabling customers who have the data but not ML expertise or infrastructure to train predictive models on this data. Existing ML-as-a-service platforms require users to reveal all training data to the service operator. We design, implement, and evaluate Chiron, a system for privacy-preserving machine learning as a service. First, Chiron conceals the training data from the service operator. Second, in keeping with how many existing ML-as-a-service platforms work, Chiron reveals neither the training algorithm nor the model structure to the user, providing only black-box access to the trained model. Chiron is implemented using SGX enclaves, but SGX alone does not achieve the dual goals of data privacy and model confidentiality. Chiron runs the standard ML training toolchain (including the popular Theano framework and C compiler) in an enclave, but the untrusted model-creation code from the service operator is further confined in a Ryoan sandbox to prevent it from leaking the training data outside the enclave. To support distributed training, Chiron executes multiple concurrent enclaves that exchange model parameters via a parameter server. We evaluate Chiron on popular deep learning models, focusing on benchmark image classification tasks such as CIFAR and ImageNet, and show that its training performance and accuracy of the resulting models are practical for common uses of ML-as-a-service.",
                        "Citation Paper Authors": "Authors:Tyler Hunt, Congzheng Song, Reza Shokri, Vitaly Shmatikov, Emmett Witchel"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": ". With \nrecent development on the synergies between privacy and \nlearning, a recent extension of PATE ",
                    "Citation Text": "N. Papernot, S. Song, I. Mironov, A. Raghunathan, \nK. Talwar, and \u00da. Erlingsson, \u201cScalable Private Learning \nwith PATE,\u201d ArXiv180208908 Cs Stat , Feb. 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.08908",
                        "Citation Paper Title": "Title:Scalable Private Learning with PATE",
                        "Citation Paper Abstract": "Abstract:The rapid adoption of machine learning has increased concerns about the privacy implications of machine learning models trained on sensitive data, such as medical records or other personal information. To address those concerns, one promising approach is Private Aggregation of Teacher Ensembles, or PATE, which transfers to a \"student\" model the knowledge of an ensemble of \"teacher\" models, with intuitive privacy provided by training teachers on disjoint data and strong privacy guaranteed by noisy aggregation of teachers' answers. However, PATE has so far been evaluated only on simple classification tasks like MNIST, leaving unclear its utility when applied to larger-scale learning tasks and real-world datasets.\nIn this work, we show how PATE can scale to learning tasks with large numbers of output classes and uncurated, imbalanced training data with errors. For this, we introduce new noisy aggregation mechanisms for teacher ensembles that are more selective and add less noise, and prove their tighter differential-privacy guarantees. Our new mechanisms build on two insights: the chance of teacher consensus is increased by using more concentrated noise and, lacking consensus, no answer need be given to a student. The consensus answers used are more likely to be correct, offer better intuitive privacy, and incur lower-differential privacy cost. Our evaluation shows our mechanisms improve on the original PATE on all measures, and scale to larger tasks with both high utility and very strong privacy ($\\varepsilon$ < 1.0).",
                        "Citation Paper Authors": "Authors:Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, \u00dalfar Erlingsson"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "is a \ud835\udf00-differentially private \nmechanism for queries  \u2131 with answers \u2131(\ud835\udc9f)\u2208 \u211d\ud835\udc5d, in \nwhich sensitivity ",
                    "Citation Text": "Z. Ji, Z. C. Lipton, and C. Elkan, \u201cDifferential \nPrivacy and Machine Learning: a Survey and Review,\u201d \nArXiv14127584 Cs , Dec. 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.7584",
                        "Citation Paper Title": "Title:Differential Privacy and Machine Learning: a Survey and Review",
                        "Citation Paper Abstract": "Abstract:The objective of machine learning is to extract useful information from data, while privacy is preserved by concealing information. Thus it seems hard to reconcile these competing interests. However, they frequently must be balanced when mining sensitive data. For example, medical research represents an important application where it is necessary both to extract useful information and protect patient privacy. One way to resolve the conflict is to extract general characteristics of whole populations without disclosing the private information of individuals.\nIn this paper, we consider differential privacy, one of the most popular and powerful definitions of privacy. We explore the interplay between machine learning and differential privacy, namely privacy-preserving machine learning algorithms and learning-based data release mechanisms. We also describe some theoretical results that address what can be learned differentially privately and upper bounds of loss functions for differentially private algorithms.\nFinally, we present some open questions, including how to incorporate public data, how to deal with missing data in private datasets, and whether, as the number of observed samples grows arbitrarily large, differentially private machine learning algorithms can be achieved at no cost to utility as compared to corresponding non-differentially private algorithms.",
                        "Citation Paper Authors": "Authors:Zhanglong Ji, Zachary C. Lipton, Charles Elkan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1908.03296v2": {
            "Paper Title": "That Was Then, This Is Now: A Security Evaluation of Password\n  Generation, Storage, and Autofill in Thirteen Password Managers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.12122v1": {
            "Paper Title": "Deep Learning Based Android Malware Detection Framework",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.04726v1": {
            "Paper Title": "A Write-Friendly and Fast-Recovery Scheme for Security Metadata in NVM",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.04669v1": {
            "Paper Title": "Client-side Vulnerabilities in Commercial VPNs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.04497v1": {
            "Paper Title": "Feature Losses for Adversarial Robustness",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.00329v2": {
            "Paper Title": "SPEECHMINER: A Framework for Investigating and Measuring Speculative\n  Execution Vulnerabilities",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": "targets BTB\nstoring the branch targets of indirect branch instructions.\nSGXPectre ",
                    "Citation Text": "G. Chen, S. Chen, Y . Xiao, Y . Zhang, Z. Lin, and T. H. Lai, \u201cSgxpec-\ntre attacks: Leaking enclave secrets via speculative execution,\u201d arXiv\npreprint arXiv:1802.09085 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.09085",
                        "Citation Paper Title": "Title:SgxPectre Attacks: Stealing Intel Secrets from SGX Enclaves via Speculative Execution",
                        "Citation Paper Abstract": "Abstract:This paper presents SgxPectre Attacks that exploit the recently disclosed CPU bugs to subvert the confidentiality and integrity of SGX enclaves. Particularly, we show that when branch prediction of the enclave code can be influenced by programs outside the enclave, the control flow of the enclave program can be temporarily altered to execute instructions that lead to observable cache-state changes. An adversary observing such changes can learn secrets inside the enclave memory or its internal registers, thus completely defeating the confidentiality guarantee offered by SGX. To demonstrate the practicality of our SgxPectre Attacks, we have systematically explored the possible attack vectors of branch target injection, approaches to win the race condition during enclave's speculative execution, and techniques to automatically search for code patterns required for launching the attacks. Our study suggests that any enclave program could be vulnerable to SgxPectre Attacks since the desired code patterns are available in most SGX runtimes (e.g., Intel SGX SDK, Rust-SGX, and Graphene-SGX). Most importantly, we have applied SgxPectre Attacks to steal seal keys and attestation keys from Intel signed quoting enclaves. The seal key can be used to decrypt sealed storage outside the enclaves and forge valid sealed data; the attestation key can be used to forge attestation signatures. For these reasons, SgxPectre Attacks practically defeat SGX's security protection. This paper also systematically evaluates Intel's existing countermeasures against SgxPectre Attacks and discusses the security implications.",
                        "Citation Paper Authors": "Authors:Guoxing Chen, Sanchuan Chen, Yuan Xiao, Yinqian Zhang, Zhiqiang Lin, Ten H. Lai"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1905.00441v3": {
            "Paper Title": "NATTACK: Learning the Distributions of Adversarial Examples for an\n  Improved Black-Box Attack on Deep Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.04145v1": {
            "Paper Title": "Camouflage: Hardware-assisted CFI for the ARM Linux kernel",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.04143v1": {
            "Paper Title": "Political Elections Under (Social) Fire? Analysis and Detection of\n  Propaganda on Twitter",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": "who present a correlation \ufb01nder to identify colluding user\naccounts using la-sensitive hashing. This has the advantage that no labels are required\nas for supervised approaches. In contrast, Cresci et al . ",
                    "Citation Text": "Stefano Cresci, Roberto Di Pietro, Marinella Petrocchi, Angelo Spognardi, and\nMaurizio Tesconi. 2017. The paradigm-shift of social spambots: Evidence, theories,\nand tools for the arms race. In Proc. of the International Conference on World Wide\nWeb Companion (WWW Companion) . International World Wide Web Conferences\nSteering Committee, 963\u2013972.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1701.03017",
                        "Citation Paper Title": "Title:The paradigm-shift of social spambots: Evidence, theories, and tools for the arms race",
                        "Citation Paper Abstract": "Abstract:Recent studies in social media spam and automation provide anecdotal argumentation of the rise of a new generation of spambots, so-called social spambots. Here, for the first time, we extensively study this novel phenomenon on Twitter and we provide quantitative evidence that a paradigm-shift exists in spambot design. First, we measure current Twitter's capabilities of detecting the new social spambots. Later, we assess the human performance in discriminating between genuine accounts, social spambots, and traditional spambots. Then, we benchmark several state-of-the-art techniques proposed by the academic literature. Results show that neither Twitter, nor humans, nor cutting-edge applications are currently capable of accurately detecting the new social spambots. Our results call for new approaches capable of turning the tide in the fight against this raising phenomenon. We conclude by reviewing the latest literature on spambots detection and we highlight an emerging common research trend based on the analysis of collective behaviors. Insights derived from both our extensive experimental campaign and survey shed light on the most promising directions of research and lay the foundations for the arms race against the novel social spambots. Finally, to foster research on this novel phenomenon, we make publicly available to the scientific community all the datasets used in this study.",
                        "Citation Paper Authors": "Authors:Stefano Cresci, Roberto Di Pietro, Marinella Petrocchi, Angelo Spognardi, Maurizio Tesconi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.03735v1": {
            "Paper Title": "Security of Deep Learning Methodologies: Challenges and Opportunities",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.04735v1": {
            "Paper Title": "Covert Channel-Based Transmitter Authentication in Controller Area\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1912.03552v1": {
            "Paper Title": "Unravelling Ariadne's Thread: Exploring the Threats of Decentalised DNS",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": "Constantinos Patsakis and Fran Casino. Hydras and IPFS: a decentralised\nplayground for malware. International Journal of Information Security ,\n18(6):787{799, Dec 2019. ",
                    "Citation Text": "Constantinos Patsakis, Fran Casino, and Vasilios Katos. Encrypted and\ncovert dns queries for botnets: Challenges and countermeasures. Computers\nand Security , 2019.\n21",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.07099",
                        "Citation Paper Title": "Title:Encrypted and Covert DNS Queries for Botnets: Challenges and Countermeasures",
                        "Citation Paper Abstract": "Abstract:There is a continuous increase in the sophistication that modern malware exercise in order to bypass the deployed security mechanisms. A typical approach to evade the identification and potential takedown of a botnet command and control server is domain fluxing through the use of Domain Generation Algorithms (DGAs). These algorithms produce a vast amount of domain names that the infected device tries to communicate with to find the C&C server, yet only a small fragment of them is actually registered. This allows the botmaster to pivot the control and make the work of seizing the botnet control rather difficult.\nCurrent state of the art and practice considers that the DNS queries performed by a compromised device are transparent to the network administrator and therefore can be monitored, analysed, and blocked. In this work, we showcase that the latter is a strong assumption as malware could efficiently hide its DNS queries using covert and/or encrypted channels bypassing the detection mechanisms. To this end, we discuss possible mitigation measures based on traffic analysis to address the new challenges that arise f",
                        "Citation Paper Authors": "Authors:Constantinos Patsakis, Fran Casino, Vasilios Katos"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.03485v1": {
            "Paper Title": "Privacy-Preserving Inference in Machine Learning Services Using Trusted\n  Execution Environments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1906.12056v2": {
            "Paper Title": "DP-LSSGD: A Stochastic Optimization Method to Lift the Utility in\n  Privacy-Preserving ERM",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": "proposed distributed selective SGD to train deep neural nets (DNNs) with a DP guarantee in a\ndistributed system, however, the obtained privacy guarantee was very loose. ",
                    "Citation Text": "M. Abadi, A. Chu, I. Goodfellow, H. McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep Learning\nwith Di\ufb00erential Privacy. In 23rd ACM Conference on Computer and Communications Security (CCS\n2016) , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.00133",
                        "Citation Paper Title": "Title:Deep Learning with Differential Privacy",
                        "Citation Paper Abstract": "Abstract:Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.",
                        "Citation Paper Authors": "Authors:Mart\u00edn Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, Li Zhang"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "showed that post-processing by projecting linear regression solutions, when\nthe ground truth solution is sparse, to a given /lscript1-ball can remarkably reduce the estimation error. ",
                    "Citation Text": "G. Bernstein, R. McKenna, T. Sun, D. Sheldon, M. Hay, and G. Miklau. Di\ufb00erentially private learning of\nundirected graphical models using collective graphical models. In Proceedings of the 34th International\nConference on Machine Learning-Volume 70 , pages 478\u2013487. JMLR. org, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.04646",
                        "Citation Paper Title": "Title:Differentially Private Learning of Undirected Graphical Models using Collective Graphical Models",
                        "Citation Paper Abstract": "Abstract:We investigate the problem of learning discrete, undirected graphical models in a differentially private way. We show that the approach of releasing noisy sufficient statistics using the Laplace mechanism achieves a good trade-off between privacy, utility, and practicality. A naive learning algorithm that uses the noisy sufficient statistics \"as is\" outperforms general-purpose differentially private learning algorithms. However, it has three limitations: it ignores knowledge about the data generating process, rests on uncertain theoretical foundations, and exhibits certain pathologies. We develop a more principled approach that applies the formalism of collective graphical models to perform inference over the true sufficient statistics within an expectation-maximization framework. We show that this learns better models than competing approaches on both synthetic data and on real human mobility data used as a case study.",
                        "Citation Paper Authors": "Authors:Garrett Bernstein, Ryan McKenna, Tao Sun, Daniel Sheldon, Michael Hay, Gerome Miklau"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "considered applying DP-SGD\nto train DNNs in a centralized setting. They clipped the gradient /lscript2norm to bound the sensitivity and\ninvented the moment accountant to get better privacy loss estimation. ",
                    "Citation Text": "N. Papernot, M. Abadi, U. Erlingsson, I. Goodfellow, and K. Talwar. Semisupervised Knowledge\nTransfer for Deep Learning from Private Training Data. In 5th International Conference on Learning\nRepresentation (ICLR 2017) , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.05755",
                        "Citation Paper Title": "Title:Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data",
                        "Citation Paper Abstract": "Abstract:Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information.\nTo address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees for training data: Private Aggregation of Teacher Ensembles (PATE). The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as \"teachers\" for a \"student\" model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student's privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student's training) and formally, in terms of differential privacy. These properties hold even if an adversary can not only query the student but also inspect its internal workings.\nCompared with previous work, the approach imposes only weak assumptions on how teachers are trained: it applies to any model, including non-convex models like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN thanks to an improved privacy analysis and semi-supervised learning.",
                        "Citation Paper Authors": "Authors:Nicolas Papernot, Mart\u00edn Abadi, \u00dalfar Erlingsson, Ian Goodfellow, Kunal Talwar"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "analyzed the excess empirical risk of DP-ERM in a distributed\nsetting. Besides ERM, many other ML models have been made di\ufb00erentially private. These include: clustering\n[3,36,41], matrix completion ",
                    "Citation Text": "P. Jain, O. Thakkar, and A. Thakurta. Di\ufb00erentially Private Matrix Completion. In 35th International\nConference on Machine Learning (ICML 2018) , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.09765",
                        "Citation Paper Title": "Title:Differentially Private Matrix Completion Revisited",
                        "Citation Paper Abstract": "Abstract:We provide the first provably joint differentially private algorithm with formal utility guarantees for the problem of user-level privacy-preserving collaborative filtering. Our algorithm is based on the Frank-Wolfe method, and it consistently estimates the underlying preference matrix as long as the number of users $m$ is $\\omega(n^{5/4})$, where $n$ is the number of items, and each user provides her preference for at least $\\sqrt{n}$ randomly selected items. Along the way, we provide an optimal differentially private algorithm for singular vector computation, based on the celebrated Oja's method, that provides significant savings in terms of space and time while operating on sparse matrices. We also empirically evaluate our algorithm on a suite of datasets, and show that it consistently outperforms the state-of-the-art private algorithms.",
                        "Citation Paper Authors": "Authors:Prateek Jain, Om Thakkar, Abhradeep Thakurta"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "numerically studied the e\ufb00ects of learning rate and batch size in DP-ERM. ",
                    "Citation Text": "Y. Wang, J. Lei, and S. Fienberg. Learning with Di\ufb00erential Privacy: Stability, Learnability and the\nSu\ufb03ciency and Necessity of ERM Principle. arXiv:1502.06309 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1502.06309",
                        "Citation Paper Title": "Title:Learning with Differential Privacy: Stability, Learnability and the Sufficiency and Necessity of ERM Principle",
                        "Citation Paper Abstract": "Abstract:While machine learning has proven to be a powerful data-driven solution to many real-life problems, its use in sensitive domains has been limited due to privacy concerns. A popular approach known as **differential privacy** offers provable privacy guarantees, but it is often observed in practice that it could substantially hamper learning accuracy. In this paper we study the learnability (whether a problem can be learned by any algorithm) under Vapnik's general learning setting with differential privacy constraint, and reveal some intricate relationships between privacy, stability and learnability.\nIn particular, we show that a problem is privately learnable **if an only if** there is a private algorithm that asymptotically minimizes the empirical risk (AERM). In contrast, for non-private learning AERM alone is not sufficient for learnability. This result suggests that when searching for private learning algorithms, we can restrict the search to algorithms that are AERM. In light of this, we propose a conceptual procedure that always finds a universally consistent algorithm whenever the problem is learnable under privacy constraint. We also propose a generic and practical algorithm and show that under very general conditions it privately learns a wide class of learning problems. Lastly, we extend some of the results to the more practical $(\\epsilon,\\delta)$-differential privacy and establish the existence of a phase-transition on the class of problems that are approximately privately learnable with respect to how small $\\delta$ needs to be.",
                        "Citation Paper Authors": "Authors:Yu-Xiang Wang, Jing Lei, Stephen E. Fienberg"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1911.12060v2": {
            "Paper Title": "Reviewing and Improving the Gaussian Mechanism for Differential Privacy",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": "to design algo-\nrithms for privacy-preserving principal component analys is.\nNikolov et al. ",
                    "Citation Text": "A. Nikolov, K. Talwar, and L. Zhang, \u201cThe geometry of dif ferential\nprivacy: The sparse and approximate cases,\u201d in ACM STOC , 2013, pp.\n351\u2013360.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1212.0297",
                        "Citation Paper Title": "Title:The Geometry of Differential Privacy: the Sparse and Approximate Cases",
                        "Citation Paper Abstract": "Abstract:In this work, we study trade-offs between accuracy and privacy in the context of linear queries over histograms. This is a rich class of queries that includes contingency tables and range queries, and has been a focus of a long line of work. For a set of $d$ linear queries over a database $x \\in \\R^N$, we seek to find the differentially private mechanism that has the minimum mean squared error. For pure differential privacy, an $O(\\log^2 d)$ approximation to the optimal mechanism is known. Our first contribution is to give an $O(\\log^2 d)$ approximation guarantee for the case of $(\\eps,\\delta)$-differential privacy. Our mechanism is simple, efficient and adds correlated Gaussian noise to the answers. We prove its approximation guarantee relative to the hereditary discrepancy lower bound of Muthukrishnan and Nikolov, using tools from convex geometry.\nWe next consider this question in the case when the number of queries exceeds the number of individuals in the database, i.e. when $d > n \\triangleq \\|x\\|_1$. It is known that better mechanisms exist in this setting. Our second main contribution is to give an $(\\eps,\\delta)$-differentially private mechanism which is optimal up to a $\\polylog(d,N)$ factor for any given query set $A$ and any given upper bound $n$ on $\\|x\\|_1$. This approximation is achieved by coupling the Gaussian noise addition approach with a linear regression step. We give an analogous result for the $\\eps$-differential privacy setting. We also improve on the mean squared error upper bound for answering counting queries on a database of size $n$ by Blum, Ligett, and Roth, and match the lower bound implied by the work of Dinur and Nissim up to logarithmic factors.\nThe connection between hereditary discrepancy and the privacy mechanism enables us to derive the first polylogarithmic approximation to the hereditary discrepancy of a matrix $A$.",
                        "Citation Paper Authors": "Authors:Aleksandar Nikolov, Kunal Talwar, Li Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1912.03388v1": {
            "Paper Title": "DClaims: A Censorship Resistant Web Annotations System using IPFS and\n  Ethereum",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.03784v3": {
            "Paper Title": "CoVault: A Secure Analytics Platform",
            "Sentences": [
                {
                    "Sentence ID": 81,
                    "Sentence": ", which sketches how multiple\nTEEs can bootstrap decentralized trust and states that, ideally,\nthe TEEs use hardware from different vendors. DeCloak ",
                    "Citation Text": "Qian Ren, Yue Li, Yingjun Wu, Yuchen Wu, Hong\nLei, Lei Wang, and Bangdao Chen. Decloak: Enable\nsecure and cheap multi-party transactions on legacy\nblockchains by a minimally trusted tee network, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.10206",
                        "Citation Paper Title": "Title:DECLOAK: Enable Secure and Cheap Multi-Party Transactions on Legacy Blockchains by a Minimally Trusted TEE Network",
                        "Citation Paper Abstract": "Abstract:As the confidentiality and scalability of smart contracts have become a crucial demand of blockchains, off-chain contract execution frameworks have been promising. Some have recently expanded off-chain contracts to Multi-Party Computation (MPC), which seek to transition the on-chain states by off-chain MPC. The most general problem among these solutions is MPT, since its off-chain MPC takes on- and off-chain inputs, delivers on- and off-chain outputs, and can be publicly verified by the blockchain, thus capable of covering more scenarios. However, existing Multi-Party Transaction (MPT) solutions lack at least one of data availability, financial fairness, delivery fairness, and delivery atomicity. These properties are crucially valued by communities, e.g., the Ethereum community, or users. Even worse, these solutions require high-cost interactions between the blockchain and off-chain systems.\nThis paper proposes a novel MPT-enabled off-chain contract execution framework, DECLOAK. DECLOAK is the first to achieve data availability of MPT, and our method can apply to other fields that seek to persist user data on-chain. Moreover, DECLOAK solves all mentioned shortcomings with even lower gas costs and weaker assumptions. Specifically, DECLOAK tolerates all but one Byzantine party and TEE executors. Evaluating on 10 MPTs, DECLOAK reduces the gas cost of the SOTA, Cloak, by 65.6%. Consequently, we are the first to not only achieve such level secure MPT in practical assumption, but also demonstrate that evaluating MPT in the comparable gas cost to normal Ethereum transaction is possible. And the cost superiority of DECLOAK increases as the number of MPT parties grows.",
                        "Citation Paper Authors": "Authors:Qian Ren, Yue Li, Yingjun Wu, Yuchen Wu, Hong Lei, Lei Wang, Bangdao Chen"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "supports aggregation queries on a secret-shared\ndataset outsourced by a set of owners; however, it does\nnot support big data and nested queries. Crypt \u03b5 ",
                    "Citation Text": "A. R. Chowdhury, C. Wang, X. He, A. Machanavajjhala,\nand S. Jha. Crypt \u03b5: Crypto-assisted differential privacy\non untrusted servers. In Proc. SIGMOD . ACM, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.07756",
                        "Citation Paper Title": "Title:Crypt$\u03b5$: Crypto-Assisted Differential Privacy on Untrusted Servers",
                        "Citation Paper Abstract": "Abstract:Differential privacy (DP) has steadily become the de-facto standard for achieving privacy in data analysis, which is typically implemented either in the \"central\" or \"local\" model. The local model has been more popular for commercial deployments as it does not require a trusted data collector. This increased privacy, however, comes at a cost of utility and algorithmic expressibility as compared to the central model.\nIn this work, we propose, Crypt$\\epsilon$, a system and programming framework that (1) achieves the accuracy guarantees and algorithmic expressibility of the central model (2) without any trusted data collector like in the local model. Crypt$\\epsilon$ achieves the \"best of both worlds\" by employing two non-colluding untrusted servers that run DP programs on encrypted data from the data owners. Although straightforward implementations of DP programs using secure computation tools can achieve the above goal theoretically, in practice they are beset with many challenges such as poor performance and tricky security proofs. To this end, Crypt$\\epsilon$ allows data analysts to author logical DP programs that are automatically translated to secure protocols that work on encrypted data. These protocols ensure that the untrusted servers learn nothing more than the noisy outputs, thereby guaranteeing DP (for computationally bounded adversaries) for all Crypt$\\epsilon$ programs. Crypt$\\epsilon$ supports a rich class of DP programs that can be expressed via a small set of transformation and measurement operators followed by arbitrary post-processing. Further, we propose performance optimizations leveraging the fact that the output is noisy. We demonstrate Crypt$\\epsilon$'s feasibility for practical DP analysis with extensive empirical evaluations on real datasets.",
                        "Citation Paper Authors": "Authors:Amrita Roy Chowdhury, Chenghong Wang, Xi He, Ashwin Machanavajjhala, Somesh Jha"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.04554v5": {
            "Paper Title": "Diagnosis-guided Attack Recovery for Securing Robotic Vehicles from\n  Sensor Deception Attacks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.05276v2": {
            "Paper Title": "Game-Theoretic Neyman-Pearson Detection to Combat Strategic Evasion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.01291v2": {
            "Paper Title": "SoK: A Stratified Approach to Blockchain Decentralization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.08487v2": {
            "Paper Title": "Differential Privacy in Aggregated Mobility Networks: Balancing Privacy\n  and Utility",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": "for mobility distributions. This work\nevaluates the proposed aggregation mechanism against tra-\njectory recovery attack model ",
                    "Citation Text": "F. Xu, Z. Tu, Y . Li, P. Zhang, X. Fu, and D. Jin, \u201cTrajectory recovery\nfrom ash: User privacy is not preserved in aggregated mobility data,\u201d in\nProceedings of the 26th International Conference on World Wide Web ,\n2017, pp. 1241\u20131250.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.06270",
                        "Citation Paper Title": "Title:Trajectory Recovery From Ash: User Privacy Is NOT Preserved in Aggregated Mobility Data",
                        "Citation Paper Abstract": "Abstract:Human mobility data has been ubiquitously collected through cellular networks and mobile applications, and publicly released for academic research and commercial purposes for the last decade. Since releasing individual's mobility records usually gives rise to privacy issues, datasets owners tend to only publish aggregated mobility data, such as the number of users covered by a cellular tower at a specific timestamp, which is believed to be sufficient for preserving users' privacy. However, in this paper, we argue and prove that even publishing aggregated mobility data could lead to privacy breach in individuals' trajectories. We develop an attack system that is able to exploit the uniqueness and regularity of human mobility to recover individual's trajectories from the aggregated mobility data without any prior knowledge. By conducting experiments on two real-world datasets collected from both mobile application and cellular network, we reveal that the attack system is able to recover users' trajectories with accuracy about 73%~91% at the scale of tens of thousands to hundreds of thousands users, which indicates severe privacy leakage in such datasets. Through the investigation on aggregated mobility data, our work recognizes a novel privacy problem in publishing statistic data, which appeals for immediate attentions from both academy and industry.",
                        "Citation Paper Authors": "Authors:Fengli Xu, Zhen Tu, Yong Li, Pengyu Zhang, Xiaoming Fu, Depeng Jin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.15968v2": {
            "Paper Title": "Light Clients for Lazy Blockchains",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.02563v2": {
            "Paper Title": "A Large-Scale Analysis of Phishing Websites Hosted on Free Web Hosting\n  Domains",
            "Sentences": [
                {
                    "Sentence ID": 45,
                    "Sentence": ". Furthermore, attackers typically ac-\nquire inexpensive web domains with non-traditional top-level\ndomains (TLDs) from hosting services, such as .store ,.top, or\n.live ",
                    "Citation Text": "GUPTA , S., AND KUMARAGURU , P.Emerging phishing trends and\neffectiveness of the anti-phishing landing page. In 2014 APWG\nSymposium on Electronic Crime Research (eCrime) (2014), IEEE,\npp. 36\u201347.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.3682",
                        "Citation Paper Title": "Title:Emerging Phishing Trends and Effectiveness of the Anti-Phishing Landing Page",
                        "Citation Paper Abstract": "Abstract:Each month, more attacks are launched with the aim of making web users believe that they are communicating with a trusted entity which compels them to share their personal, financial information. Phishing costs Internet users billions of dollars every year. Researchers at Carnegie Mellon University (CMU) created an anti-phishing landing page supported by Anti-Phishing Working Group (APWG) with the aim to train users on how to prevent themselves from phishing attacks. It is used by financial institutions, phish site take down vendors, government organizations, and online merchants. When a potential victim clicks on a phishing link that has been taken down, he / she is redirected to the landing page. In this paper, we present the comparative analysis on two datasets that we obtained from APWG's landing page log files; one, from September 7, 2008 - November 11, 2009, and other from January 1, 2014 - April 30, 2014. We found that the landing page has been successful in training users against phishing. Forty six percent users clicked lesser number of phishing URLs from January 2014 to April 2014 which shows that training from the landing page helped users not to fall for phishing attacks. Our analysis shows that phishers have started to modify their techniques by creating more legitimate looking URLs and buying large number of domains to increase their activity. We observed that phishers are exploiting ICANN accredited registrars to launch their attacks even after strict surveillance. We saw that phishers are trying to exploit free subdomain registration services to carry out attacks. In this paper, we also compared the phishing e-mails used by phishers to lure victims in 2008 and 2014. We found that the phishing e-mails have changed considerably over time. Phishers have adopted new techniques like sending promotional e-mails and emotionally targeting users in clicking phishing URLs.",
                        "Citation Paper Authors": "Authors:Srishti Gupta, Ponnurangam Kumaraguru"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.15469v3": {
            "Paper Title": "Learning Failure-Inducing Models for Testing Software-Defined Networks",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ". The research strands that most closely relate to our work are fuzzing\ntechniques based on ML for testing networked systems [ 10,65]. Chen et al . ",
                    "Citation Text": "Yuqi Chen, Christopher M. Poskitt, Jun Sun, Sridhar Adepu, and Fan Zhang. 2019. Learning-Guided Network Fuzzing\nfor Testing Cyber-Physical System Defences. In Proceedings of the 34th IEEE/ACM International Conference on Automated\nSoftware Engineering . 962\u2013973.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.05410",
                        "Citation Paper Title": "Title:Learning-Guided Network Fuzzing for Testing Cyber-Physical System Defences",
                        "Citation Paper Abstract": "Abstract:The threat of attack faced by cyber-physical systems (CPSs), especially when they play a critical role in automating public infrastructure, has motivated research into a wide variety of attack defence mechanisms. Assessing their effectiveness is challenging, however, as realistic sets of attacks to test them against are not always available. In this paper, we propose smart fuzzing, an automated, machine learning guided technique for systematically finding 'test suites' of CPS network attacks, without requiring any knowledge of the system's control programs or physical processes. Our approach uses predictive machine learning models and metaheuristic search algorithms to guide the fuzzing of actuators so as to drive the CPS into different unsafe physical states. We demonstrate the efficacy of smart fuzzing by implementing it for two real-world CPS testbeds---a water purification plant and a water distribution system---finding attacks that drive them into 27 different unsafe states involving water flow, pressure, and tank levels, including six that were not covered by an established attack benchmark. Finally, we use our approach to test the effectiveness of an invariant-based defence system for the water treatment plant, finding two attacks that were not detected by its physical invariant checks, highlighting a potential weakness that could be exploited in certain conditions.",
                        "Citation Paper Authors": "Authors:Yuqi Chen, Christopher M. Poskitt, Jun Sun, Sridhar Adepu, Fan Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.02649v2": {
            "Paper Title": "Thales: Formulating and Estimating Architectural Vulnerability Factors\n  for DNN Accelerators",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.00676v3": {
            "Paper Title": "Token-Modification Adversarial Attacks for Natural Language Processing:\n  A Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.09782v2": {
            "Paper Title": "Assessing Neural Network Robustness via Adversarial Pivotal Tuning",
            "Sentences": [
                {
                    "Sentence ID": 47,
                    "Sentence": "which is trained using\nnew augmentation techniques for enhanced robustness, and\nFAN-VIT ",
                    "Citation Text": "Daquan Zhou, Zhiding Yu, Enze Xie, Chaowei Xiao, Anima\nAnandkumar, Jiashi Feng, and Jose M. Alvarez. Understand-\ning the robustness in vision transformers. In International\nConference on Machine Learning (ICML) , 2022. 4, 5, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.12451",
                        "Citation Paper Title": "Title:Understanding The Robustness in Vision Transformers",
                        "Citation Paper Abstract": "Abstract:Recent studies show that Vision Transformers(ViTs) exhibit strong robustness against various corruptions. Although this property is partly attributed to the self-attention mechanism, there is still a lack of systematic understanding. In this paper, we examine the role of self-attention in learning robust representations. Our study is motivated by the intriguing properties of the emerging visual grouping in Vision Transformers, which indicates that self-attention may promote robustness through improved mid-level representations. We further propose a family of fully attentional networks (FANs) that strengthen this capability by incorporating an attentional channel processing design. We validate the design comprehensively on various hierarchical backbones. Our model achieves a state-of-the-art 87.1% accuracy and 35.8% mCE on ImageNet-1k and ImageNet-C with 76.8M parameters. We also demonstrate state-of-the-art accuracy and robustness in two downstream tasks: semantic segmentation and object detection. Code is available at: this https URL.",
                        "Citation Paper Authors": "Authors:Daquan Zhou, Zhiding Yu, Enze Xie, Chaowei Xiao, Anima Anandkumar, Jiashi Feng, Jose M. Alvarez"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": ". We focus on an ap-proach that semantically manipulates the input image, re-\nsulting in a naturally looking adversarial manipulation, in-\nstead of using noise attacks ",
                    "Citation Text": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt,\nDimitris Tsipras, and Adrian Vladu. Towards deep learn-\ning models resistant to adversarial attacks. arXiv preprint\narXiv:1706.06083 , 2017. 3, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06083",
                        "Citation Paper Title": "Title:Towards Deep Learning Models Resistant to Adversarial Attacks",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL.",
                        "Citation Paper Authors": "Authors:Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "-L) to generate manip-\nulations (2) on the same set of images. Additionally, we\nreplace StyleGAN-XL with a Diffusion model ",
                    "Citation Text": "Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and\nDaniel Cohen-Or. Null-text inversion for editing real images\nusing guided diffusion models, 2022. 2, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2211.09794",
                        "Citation Paper Title": "Title:Null-text Inversion for Editing Real Images using Guided Diffusion Models",
                        "Citation Paper Abstract": "Abstract:Recent text-guided diffusion models provide powerful image generation capabilities. Currently, a massive effort is given to enable the modification of these images using text only as means to offer intuitive and versatile editing. To edit a real image using these state-of-the-art tools, one must first invert the image with a meaningful text prompt into the pretrained model's domain. In this paper, we introduce an accurate inversion technique and thus facilitate an intuitive text-based modification of the image. Our proposed inversion consists of two novel key components: (i) Pivotal inversion for diffusion models. While current methods aim at mapping random noise samples to a single input image, we use a single pivotal noise vector for each timestamp and optimize around it. We demonstrate that a direct inversion is inadequate on its own, but does provide a good anchor for our optimization. (ii) NULL-text optimization, where we only modify the unconditional textual embedding that is used for classifier-free guidance, rather than the input text embedding. This allows for keeping both the model weights and the conditional embedding intact and hence enables applying prompt-based editing while avoiding the cumbersome tuning of the model's weights. Our Null-text inversion, based on the publicly available Stable Diffusion model, is extensively evaluated on a variety of images and prompt editing, showing high-fidelity editing of real images.",
                        "Citation Paper Authors": "Authors:Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, Daniel Cohen-Or"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "classifier. Each\nmodel is evaluated on the generated images for which it was also\nused as the classifier during the generation.\nY ",
                    "Citation Text": "Priya Goyal, Quentin Duval, Isaac Seessel, Mathilde Caron,\nIshan Misra, Levent Sagun, Armand Joulin, and Piotr Bo-\njanowski. Vision models are more robust and fair when pre-\ntrained on uncurated images without supervision, 2022. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.08360",
                        "Citation Paper Title": "Title:Vision Models Are More Robust And Fair When Pretrained On Uncurated Images Without Supervision",
                        "Citation Paper Abstract": "Abstract:Discriminative self-supervised learning allows training models on any random group of internet images, and possibly recover salient information that helps differentiate between the images. Applied to ImageNet, this leads to object centric features that perform on par with supervised features on most object-centric downstream tasks. In this work, we question if using this ability, we can learn any salient and more representative information present in diverse unbounded set of images from across the globe. To do so, we train models on billions of random images without any data pre-processing or prior assumptions about what we want the model to learn. We scale our model size to dense 10 billion parameters to avoid underfitting on a large data size. We extensively study and validate our model performance on over 50 benchmarks including fairness, robustness to distribution shift, geographical diversity, fine grained recognition, image copy detection and many image classification datasets. The resulting model, not only captures well semantic information, it also captures information about artistic style and learns salient information such as geolocations and multilingual word embeddings based on visual content only. More importantly, we discover that such model is more robust, more fair, less harmful and less biased than supervised models or models trained on object centric datasets such as ImageNet.",
                        "Citation Paper Authors": "Authors:Priya Goyal, Quentin Duval, Isaac Seessel, Mathilde Caron, Ishan Misra, Levent Sagun, Armand Joulin, Piotr Bojanowski"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": ", and LLPIPS is the percep-\ntual distance introduced in ",
                    "Citation Text": "Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric, 2018. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.03924",
                        "Citation Paper Title": "Title:The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
                        "Citation Paper Abstract": "Abstract:While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called \"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",
                        "Citation Paper Authors": "Authors:Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "projects images to a pretrained StyleGAN\u2019s latent space and\nadversarially manipulate their style code. Similarly, ",
                    "Citation Text": "Omid Poursaeed, Tianxing Jiang, Harry Yang, Serge Be-\nlongie, and Ser-Nam Lim. Robustness and generalization\nvia generative adversarial training. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 15711\u201315720, 2021. 1, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.02765",
                        "Citation Paper Title": "Title:Robustness and Generalization via Generative Adversarial Training",
                        "Citation Paper Abstract": "Abstract:While deep neural networks have achieved remarkable success in various computer vision tasks, they often fail to generalize to new domains and subtle variations of input images. Several defenses have been proposed to improve the robustness against these variations. However, current defenses can only withstand the specific attack used in training, and the models often remain vulnerable to other input variations. Moreover, these methods often degrade performance of the model on clean images and do not generalize to out-of-domain samples. In this paper we present Generative Adversarial Training, an approach to simultaneously improve the model's generalization to the test set and out-of-domain samples as well as its robustness to unseen adversarial attacks. Instead of altering a low-level pre-defined aspect of images, we generate a spectrum of low-level, mid-level and high-level changes using generative models with a disentangled latent space. Adversarial training with these examples enable the model to withstand a wide range of attacks by observing a variety of input alterations during training. We show that our approach not only improves performance of the model on clean images and out-of-domain samples but also makes it robust against unforeseen attacks and outperforms prior work. We validate effectiveness of our method by demonstrating results on various tasks such as classification, segmentation and object detection.",
                        "Citation Paper Authors": "Authors:Omid Poursaeed, Tianxing Jiang, Harry Yang, Serge Belongie, SerNam Lim"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "considers an\nautoencoder-based manipulation of real images, but it\nis restricted to style changes. ",
                    "Citation Text": "Sven Gowal, Chongli Qin, Po-Sen Huang, Taylan Cemgil,\nKrishnamurthy Dvijotham, Timothy Mann, and Pushmeet\nKohli. Achieving robustness in the wild via adversarial mix-\ning with disentangled representations. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 1211\u20131220, 2020. 1, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.03192",
                        "Citation Paper Title": "Title:Achieving Robustness in the Wild via Adversarial Mixing with Disentangled Representations",
                        "Citation Paper Abstract": "Abstract:Recent research has made the surprising finding that state-of-the-art deep learning models sometimes fail to generalize to small variations of the input. Adversarial training has been shown to be an effective approach to overcome this problem. However, its application has been limited to enforcing invariance to analytically defined transformations like $\\ell_p$-norm bounded perturbations. Such perturbations do not necessarily cover plausible real-world variations that preserve the semantics of the input (such as a change in lighting conditions). In this paper, we propose a novel approach to express and formalize robustness to these kinds of real-world transformations of the input. The two key ideas underlying our formulation are (1) leveraging disentangled representations of the input to define different factors of variations, and (2) generating new input images by adversarially composing the representations of different images. We use a StyleGAN model to demonstrate the efficacy of this framework. Specifically, we leverage the disentangled latent representations computed by a StyleGAN model to generate perturbations of an image that are similar to real-world variations (like adding make-up, or changing the skin-tone of a person) and train models to be invariant to these perturbations. Extensive experiments show that our method improves generalization and reduces the effect of spurious correlations (reducing the error rate of a \"smile\" detector by 21% for example).",
                        "Citation Paper Authors": "Authors:Sven Gowal, Chongli Qin, Po-Sen Huang, Taylan Cemgil, Krishnamurthy Dvijotham, Timothy Mann, Pushmeet Kohli"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "as class-preserving\nsemantic manipulations.\nAnother line of work considers the use of pretrained\ngenerative models. ",
                    "Citation Text": "Yang Song, Rui Shu, Nate Kushman, and Stefano Ermon.\nConstructing unrestricted adversarial examples with genera-\ntive models. In Advances in Neural Information Processing\nSystems , pages 8312\u20138323, 2018. 1, 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.07894",
                        "Citation Paper Title": "Title:Constructing Unrestricted Adversarial Examples with Generative Models",
                        "Citation Paper Abstract": "Abstract:Adversarial examples are typically constructed by perturbing an existing data point within a small matrix norm, and current defense methods are focused on guarding against this type of attack. In this paper, we propose unrestricted adversarial examples, a new threat model where the attackers are not restricted to small norm-bounded perturbations. Different from perturbation-based attacks, we propose to synthesize unrestricted adversarial examples entirely from scratch using conditional generative models. Specifically, we first train an Auxiliary Classifier Generative Adversarial Network (AC-GAN) to model the class-conditional distribution over data samples. Then, conditioned on a desired class, we search over the AC-GAN latent space to find images that are likely under the generative model and are misclassified by a target classifier. We demonstrate through human evaluation that unrestricted adversarial examples generated this way are legitimate and belong to the desired class. Our empirical results on the MNIST, SVHN, and CelebA datasets show that unrestricted adversarial examples can bypass strong adversarial training and certified defense methods designed for traditional adversarial attacks.",
                        "Citation Paper Authors": "Authors:Yang Song, Rui Shu, Nate Kushman, Stefano Ermon"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": ", manipulating intermediate classifier fea-\ntures [10,23,45], and inserting patches ",
                    "Citation Text": "Tom B Brown, Dandelion Man \u00b4e, Aurko Roy, Mart \u00b4\u0131n Abadi,\nand Justin Gilmer. Adversarial patch. arXiv preprint\narXiv:1712.09665 , 2017. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.09665",
                        "Citation Paper Title": "Title:Adversarial Patch",
                        "Citation Paper Abstract": "Abstract:We present a method to create universal, robust, targeted adversarial image patches in the real world. The patches are universal because they can be used to attack any scene, robust because they work under a wide variety of transformations, and targeted because they can cause a classifier to output any target class. These adversarial patches can be printed, added to any scene, photographed, and presented to image classifiers; even when the patches are small, they cause the classifiers to ignore the other items in the scene and report a chosen target class.\nTo reproduce the results from the paper, our code is available at this https URL",
                        "Citation Paper Authors": "Authors:Tom B. Brown, Dandelion Man\u00e9, Aurko Roy, Mart\u00edn Abadi, Justin Gilmer"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": ".\nOne set of works considers a specific class of semantic\nmanipulations. These include geometric changes [4,11,44],\nview changes ",
                    "Citation Text": "Michael A Alcorn, Qi Li, Zhitao Gong, Chengfei Wang,\nLong Mai, Wei-Shinn Ku, and Anh Nguyen. Strike (with)\na pose: Neural networks are easily fooled by strange poses\nof familiar objects. arXiv preprint arXiv:1811.11553 , 2018.\n3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.11553",
                        "Citation Paper Title": "Title:Strike (with) a Pose: Neural Networks Are Easily Fooled by Strange Poses of Familiar Objects",
                        "Citation Paper Abstract": "Abstract:Despite excellent performance on stationary test sets, deep neural networks (DNNs) can fail to generalize to out-of-distribution (OoD) inputs, including natural, non-adversarial ones, which are common in real-world settings. In this paper, we present a framework for discovering DNN failures that harnesses 3D renderers and 3D models. That is, we estimate the parameters of a 3D renderer that cause a target DNN to misbehave in response to the rendered image. Using our framework and a self-assembled dataset of 3D objects, we investigate the vulnerability of DNNs to OoD poses of well-known objects in ImageNet. For objects that are readily recognized by DNNs in their canonical poses, DNNs incorrectly classify 97% of their pose space. In addition, DNNs are highly sensitive to slight pose perturbations. Importantly, adversarial poses transfer across models and datasets. We find that 99.9% and 99.4% of the poses misclassified by Inception-v3 also transfer to the AlexNet and ResNet-50 image classifiers trained on the same ImageNet dataset, respectively, and 75.5% transfer to the YOLOv3 object detector trained on MS COCO.",
                        "Citation Paper Authors": "Authors:Michael A. Alcorn, Qi Li, Zhitao Gong, Chengfei Wang, Long Mai, Wei-Shinn Ku, Anh Nguyen"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "Semantic Adversarial Robustness. The majority of cur-\nrent literature considers adversarial robustness to pixel-\nspace manipulations where the lpnorm is bounded [12,42].\nFor a comprehensive review, see ",
                    "Citation Text": "Naveed Akhtar, Ajmal Mian, Navid Kardan, and Mubarak\nShah. Advances in adversarial attacks and defenses in com-\nputer vision: A survey. IEEE Access , 9:155161\u2013155196,\n2021. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.00401",
                        "Citation Paper Title": "Title:Advances in adversarial attacks and defenses in computer vision: A survey",
                        "Citation Paper Abstract": "Abstract:Deep Learning (DL) is the most widely used tool in the contemporary field of computer vision. Its ability to accurately solve complex problems is employed in vision research to learn deep neural models for a variety of tasks, including security critical applications. However, it is now known that DL is vulnerable to adversarial attacks that can manipulate its predictions by introducing visually imperceptible perturbations in images and videos. Since the discovery of this phenomenon in 2013~[1], it has attracted significant attention of researchers from multiple sub-fields of machine intelligence. In [2], we reviewed the contributions made by the computer vision community in adversarial attacks on deep learning (and their defenses) until the advent of year 2018. Many of those contributions have inspired new directions in this area, which has matured significantly since witnessing the first generation methods. Hence, as a legacy sequel of [2], this literature review focuses on the advances in this area since 2018. To ensure authenticity, we mainly consider peer-reviewed contributions published in the prestigious sources of computer vision and machine learning research. Besides a comprehensive literature review, the article also provides concise definitions of technical terminologies for non-experts in this domain. Finally, this article discusses challenges and future outlook of this direction based on the literature reviewed herein and [2].",
                        "Citation Paper Authors": "Authors:Naveed Akhtar, Ajmal Mian, Navid Kardan, Mubarak Shah"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.04741v5": {
            "Paper Title": "Is GitHub's Copilot as Bad as Humans at Introducing Vulnerabilities in\n  Code?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.06166v4": {
            "Paper Title": "Game Theory for Adversarial Attacks and Defenses",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "against  several  typical  first-order  attacks  under  both black -box and white -box settings.  \nTo defend FGSM -generated attacks, we use training dataset including bo th benign and FGSM - \ngenerated  adversarial  samples ",
                    "Citation Text": "Goodfellow, I. J., J. Shlens, C. Szegedy. Explaining and harnessing adversarial examples,  2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.6572",
                        "Citation Paper Title": "Title:Explaining and Harnessing Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "a robust network adversarially can improve the robustness of CNNs and \nResNets ",
                    "Citation Text": "He, K., X. Zhang, S. Ren, et al. Deep residual learning for image recognition, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.03385",
                        "Citation Paper Title": "Title:Deep Residual Learning for Image Recognition",
                        "Citation Paper Abstract": "Abstract:Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
                        "Citation Paper Authors": "Authors:Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.06485v3": {
            "Paper Title": "Communication-Efficient Triangle Counting under Local Differential\n  Privacy",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": "that uses RR. Applying HR\nto an entire neighbor list (which has 2npossible values) will\nsimilarly result in O(nlog2n) =O(n2)download cost.\nPrevious work on distribution estimation [33, 44, 60] or\nheavy hitters ",
                    "Citation Text": "R. Bassily, K. Nissim, U. Stemmer, and A. Thakurta.\nPractical locally private heavy hitters. In Proc. NIPS\u201917 ,\npages 2285\u2014-2293, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.04982",
                        "Citation Paper Title": "Title:Practical Locally Private Heavy Hitters",
                        "Citation Paper Abstract": "Abstract:We present new practical local differentially private heavy hitters algorithms achieving optimal or near-optimal worst-case error and running time -- TreeHist and Bitstogram. In both algorithms, server running time is $\\tilde O(n)$ and user running time is $\\tilde O(1)$, hence improving on the prior state-of-the-art result of Bassily and Smith [STOC 2015] requiring $O(n^{5/2})$ server time and $O(n^{3/2})$ user time. With a typically large number of participants in local algorithms ($n$ in the millions), this reduction in time complexity, in particular at the user side, is crucial for making locally private heavy hitters algorithms usable in practice. We implemented Algorithm TreeHist to verify our theoretical analysis and compared its performance with the performance of Google's RAPPOR code.",
                        "Citation Paper Authors": "Authors:Raef Bassily, Kobbi Nissim, Uri Stemmer, Abhradeep Thakurta"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "have been widely used\nfor tabular data in LDP. Our work uses RR in part of our\nalgorithm but builds off of it significantly. One noteworthy\nresult in this area is HR (Hadamard Response) ",
                    "Citation Text": "J. Acharya, Z. Sun, and H. Zhang. Hadamard response:\nEstimating distributions privately, efficiently, and with\nlittle communication. In Proc. AISTATS\u201919 , pages 1120\u2013\n1129, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.04705",
                        "Citation Paper Title": "Title:Hadamard Response: Estimating Distributions Privately, Efficiently, and with Little Communication",
                        "Citation Paper Abstract": "Abstract:We study the problem of estimating $k$-ary distributions under $\\varepsilon$-local differential privacy. $n$ samples are distributed across users who send privatized versions of their sample to a central server. All previously known sample optimal algorithms require linear (in $k$) communication from each user in the high privacy regime $(\\varepsilon=O(1))$, and run in time that grows as $n\\cdot k$, which can be prohibitive for large domain size $k$.\nWe propose Hadamard Response (HR}, a local privatization scheme that requires no shared randomness and is symmetric with respect to the users. Our scheme has order optimal sample complexity for all $\\varepsilon$, a communication of at most $\\log k+2$ bits per user, and nearly linear running time of $\\tilde{O}(n + k)$.\nOur encoding and decoding are based on Hadamard matrices, and are simple to implement. The statistical performance relies on the coding theoretic aspects of Hadamard matrices, ie, the large Hamming distance between the rows. An efficient implementation of the algorithm using the Fast Walsh-Hadamard transform gives the computational gains.\nWe compare our approach with Randomized Response (RR), RAPPOR, and subset-selection mechanisms (SS), both theoretically, and experimentally. For $k=10000$, our algorithm runs about 100x faster than SS, and RAPPOR.",
                        "Citation Paper Authors": "Authors:Jayadev Acharya, Ziteng Sun, Huanyu Zhang"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "that improves the\nutility of an averaging query by correlating the noise of users\naccording to a graph.\nLDP. RR [33, 61] and RAPPOR ",
                    "Citation Text": "U. Erlingsson, V . Pihur, and A. Korolova. RAPPOR:\nRandomized aggregatable privacy-preserving ordinal\nresponse. In Proc. CCS\u201914 , pages 1054\u20131067, 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1407.6981",
                        "Citation Paper Title": "Title:RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response",
                        "Citation Paper Abstract": "Abstract:Randomized Aggregatable Privacy-Preserving Ordinal Response, or RAPPOR, is a technology for crowdsourcing statistics from end-user client software, anonymously, with strong privacy guarantees. In short, RAPPORs allow the forest of client data to be studied, without permitting the possibility of looking at individual trees. By applying randomized response in a novel manner, RAPPOR provides the mechanisms for such collection as well as for efficient, high-utility analysis of the collected data. In particular, RAPPOR permits statistics to be collected on the population of client-side strings with strong privacy guarantees for each client, and without linkability of their reports. This paper describes and motivates RAPPOR, details its differential-privacy and utility guarantees, discusses its practical deployment and properties in the face of different attack models, and, finally, gives results of its application to both synthetic and real-world data.",
                        "Citation Paper Authors": "Authors:\u00dalfar Erlingsson, Vasyl Pihur, Aleksandra Korolova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.10498v3": {
            "Paper Title": "Differentially Private Linear Optimization for Multi-Party Resource\n  Sharing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.13535v2": {
            "Paper Title": "DeepTaster: Adversarial Perturbation-Based Fingerprinting to Identify\n  Proprietary Dataset Use in Deep Neural Networks",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": "uses data points close to\nthe model\u2019s decision boundaries as that identifier. Lukas et al. ",
                    "Citation Text": "Nils Lukas, Yuxuan Zhang, and Florian Kerschbaum. 2019. Deep neural\nnetwork fingerprinting by conferrable adversarial examples. arXiv preprint\narXiv:1912.00888 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.00888",
                        "Citation Paper Title": "Title:Deep Neural Network Fingerprinting by Conferrable Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:In Machine Learning as a Service, a provider trains a deep neural network and gives many users access. The hosted (source) model is susceptible to model stealing attacks, where an adversary derives a surrogate model from API access to the source model. For post hoc detection of such attacks, the provider needs a robust method to determine whether a suspect model is a surrogate of their model. We propose a fingerprinting method for deep neural network classifiers that extracts a set of inputs from the source model so that only surrogates agree with the source model on the classification of such inputs. These inputs are a subclass of transferable adversarial examples which we call conferrable adversarial examples that exclusively transfer with a target label from a source model to its surrogates. We propose a new method to generate these conferrable adversarial examples. We present an extensive study on the irremovability of our fingerprint against fine-tuning, weight pruning, retraining, retraining with different architectures, three model extraction attacks from related work, transfer learning, adversarial training, and two new adaptive attacks. Our fingerprint is robust against distillation, related model extraction attacks, and even transfer learning when the attacker has no access to the model provider's dataset. Our fingerprint is the first method that reaches a ROC AUC of 1.0 in verifying surrogates, compared to a ROC AUC of 0.63 by previous fingerprints.",
                        "Citation Paper Authors": "Authors:Nils Lukas, Yuxuan Zhang, Florian Kerschbaum"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.05219v2": {
            "Paper Title": "Specular: Towards Secure, Trust-minimized Optimistic Blockchain\n  Execution",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.09929v3": {
            "Paper Title": "Differentially Private Diffusion Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.10226v3": {
            "Paper Title": "Improving Privacy-Preserving Vertical Federated Learning by Efficient\n  Communication with ADMM",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "or averaged hj=PM\nk=1\u03b1khk\njwith\naggregation weights \u03b1k\u2208Ras in VAFL ",
                    "Citation Text": "Tianyi Chen, Xiao Jin, Yuejiao Sun, and Wotao Yin. Vafl:\na method of vertical asynchronous federated learning.\narXiv preprint arXiv:2007.06081 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.06081",
                        "Citation Paper Title": "Title:VAFL: a Method of Vertical Asynchronous Federated Learning",
                        "Citation Paper Abstract": "Abstract:Horizontal Federated learning (FL) handles multi-client data that share the same set of features, and vertical FL trains a better predictor that combine all the features from different clients. This paper targets solving vertical FL in an asynchronous fashion, and develops a simple FL method. The new method allows each client to run stochastic gradient algorithms without coordination with other clients, so it is suitable for intermittent connectivity of clients. This method further uses a new technique of perturbed local embedding to ensure data privacy and improve communication efficiency. Theoretically, we present the convergence rate and privacy level of our method for strongly convex, nonconvex and even nonsmooth objectives separately. Empirically, we apply our method to FL on various image and healthcare datasets. The results compare favorably to centralized and synchronous FL methods.",
                        "Citation Paper Authors": "Authors:Tianyi Chen, Xiao Jin, Yuejiao Sun, Wotao Yin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.04030v3": {
            "Paper Title": "Unraveling the Connections between Privacy and Certified Robustness in\n  Federated Learning Against Poisoning Attacks",
            "Sentences": [
                {
                    "Sentence ID": 53,
                    "Sentence": "2.1 Differentially Private Federated Learning\nTo guarantee user-level privacy for FL, McMahan et al . ",
                    "Citation Text": "H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. 2018. Learning\nDifferentially Private Recurrent Language Models. In International Conference onLearning Representations .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.06963",
                        "Citation Paper Title": "Title:Learning Differentially Private Recurrent Language Models",
                        "Citation Paper Abstract": "Abstract:We demonstrate that it is possible to train large recurrent language models with user-level differential privacy guarantees with only a negligible cost in predictive accuracy. Our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent. In particular, we add user-level privacy protection to the federated averaging algorithm, which makes \"large step\" updates from user-level data. Our work demonstrates that given a dataset with a sufficiently large number of users (a requirement easily met by even small internet-scale datasets), achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work. We find that our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a large dataset.",
                        "Citation Paper Authors": "Authors:H. Brendan McMahan, Daniel Ramage, Kunal Talwar, Li Zhang"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "and label flipping attacks (LF) on image and text\ndatasets ",
                    "Citation Text": "Clement Fung, Chris JM Yoon, and Ivan Beschastnikh. 2020. The Limitations of\nFederated Learning in Sybil Settings. In 23rd International Symposium on Research\nin Attacks, Intrusions and Defenses ( {RAID}2020) . 301\u2013316.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.04866",
                        "Citation Paper Title": "Title:Mitigating Sybils in Federated Learning Poisoning",
                        "Citation Paper Abstract": "Abstract:Machine learning (ML) over distributed multi-party data is required for a variety of domains. Existing approaches, such as federated learning, collect the outputs computed by a group of devices at a central aggregator and run iterative algorithms to train a globally shared model. Unfortunately, such approaches are susceptible to a variety of attacks, including model poisoning, which is made substantially worse in the presence of sybils.\nIn this paper we first evaluate the vulnerability of federated learning to sybil-based poisoning attacks. We then describe \\emph{FoolsGold}, a novel defense to this problem that identifies poisoning sybils based on the diversity of client updates in the distributed learning process. Unlike prior work, our system does not bound the expected number of attackers, requires no auxiliary information outside of the learning process, and makes fewer assumptions about clients and their data.\nIn our evaluation we show that FoolsGold exceeds the capabilities of existing state of the art approaches to countering sybil-based label-flipping and backdoor poisoning attacks. Our results hold for different distributions of client data, varying poisoning targets, and various sybil strategies.\nCode can be found at: this https URL",
                        "Citation Paper Authors": "Authors:Clement Fung, Chris J.M. Yoon, Ivan Beschastnikh"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "first connects DP to certified robustness against ad-\nversarial examples by adding noise on the test sample \ud835\udc42times and\ntaking the expectation over the corresponding outputs. Later on,\nrandomized smoothing ",
                    "Citation Text": "Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. 2019. Certified adversarial\nrobustness via randomized smoothing. In international conference on machine\nlearning . PMLR, 1310\u20131320.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.02918",
                        "Citation Paper Title": "Title:Certified Adversarial Robustness via Randomized Smoothing",
                        "Citation Paper Abstract": "Abstract:We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the $\\ell_2$ norm. This \"randomized smoothing\" technique has been proposed recently in the literature, but existing guarantees are loose. We prove a tight robustness guarantee in $\\ell_2$ norm for smoothing with Gaussian noise. We use randomized smoothing to obtain an ImageNet classifier with e.g. a certified top-1 accuracy of 49% under adversarial perturbations with $\\ell_2$ norm less than 0.5 (=127/255). No certified defense has been shown feasible on ImageNet except for smoothing. On smaller-scale datasets where competing approaches to certified $\\ell_2$ robustness are viable, smoothing delivers higher certified accuracies. Our strong empirical results suggest that randomized smoothing is a promising direction for future research into adversarially robust classification. Code and models are available at this http URL.",
                        "Citation Paper Authors": "Authors:Jeremy M Cohen, Elan Rosenfeld, J. Zico Kolter"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": ", and different defenses have been proposed to enhance\nthe robustness of models and provide certifications to guarantee\nconsistent predictions under a specified perturbation radius ",
                    "Citation Text": "Linyi Li, Tao Xie, and Bo Li. 2022. SoK: Certified Robustness for Deep Neural\nNetworks. In 2023 IEEE Symposium on Security and Privacy (SP) . IEEE Computer\nSociety, 94\u2013115.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.04131",
                        "Citation Paper Title": "Title:SoK: Certified Robustness for Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:Great advances in deep neural networks (DNNs) have led to state-of-the-art performance on a wide range of tasks. However, recent studies have shown that DNNs are vulnerable to adversarial attacks, which have brought great concerns when deploying these models to safety-critical applications such as autonomous driving. Different defense approaches have been proposed against adversarial attacks, including: a) empirical defenses, which can usually be adaptively attacked again without providing robustness certification; and b) certifiably robust approaches, which consist of robustness verification providing the lower bound of robust accuracy against any attacks under certain conditions and corresponding robust training approaches. In this paper, we systematize certifiably robust approaches and related practical and theoretical implications and findings. We also provide the first comprehensive benchmark on existing robustness verification and training approaches on different datasets. In particular, we 1) provide a taxonomy for the robustness verification and training approaches, as well as summarize the methodologies for representative algorithms, 2) reveal the characteristics, strengths, limitations, and fundamental connections among these approaches, 3) discuss current research progresses, theoretical barriers, main challenges, and future directions for certifiably robust approaches for DNNs, and 4) provide an open-sourced unified platform to evaluate 20+ representative certifiably robust approaches.",
                        "Citation Paper Authors": "Authors:Linyi Li, Tao Xie, Bo Li"
                    }
                },
                {
                    "Sentence ID": 86,
                    "Sentence": ") where each user performs multiple steps of\nSGD. Zhu et al . ",
                    "Citation Text": "Yuqing Zhu, Xiang Yu, Yi-Hsuan Tsai, Francesco Pittaluga, Masoud Faraki, Man-\nmohan Chandraker, and Yu-Xiang Wang. 2021. Voting-based Approaches For\nDifferentially Private Federated Learning. (2021).\nThe Appendix is organized as follows:\n\u2022Appendix A provides the proofs for the privacy guarantees\nof our DPFL algorithms.\n\u2022Appendix B provides more details on experimental setups\nand the additional experimental results on robustness certi-\nfications.\n\u2022Appendix C provides the proofs for the certified robustness-\nrelated analysis, including Definition 2, Theorem 1, Theo-\nrem 2, Theorem 3, Theorem 4 and Corollary 1.\n\u2022Appendix D provides the theoretical results and correspond-\ning proofs for certified robustness against FL poisoning at-\ntacks derived from R\u00e9nyi DP and Randomized Smoothing\nvia R\u00e9nyi Divergence.\nA DIFFERENTIALLY PRIVATE FEDERATED\nLEARNING\nWe first present all the notations used in our paper in Table 3.\nA.1 UserDP-FedAvg\nTo calculate the privacy costs for Algorithm 1, existing works utilize\nmoments accountant",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.04851",
                        "Citation Paper Title": "Title:Voting-based Approaches For Differentially Private Federated Learning",
                        "Citation Paper Abstract": "Abstract:Differentially Private Federated Learning (DPFL) is an emerging field with many applications. Gradient averaging based DPFL methods require costly communication rounds and hardly work with large-capacity models, due to the explicit dimension dependence in its added noise. In this work, inspired by knowledge transfer non-federated privacy learning from Papernot et al.(2017; 2018), we design two new DPFL schemes, by voting among the data labels returned from each local model, instead of averaging the gradients, which avoids the dimension dependence and significantly reduces the communication cost. Theoretically, by applying secure multi-party computation, we could exponentially amplify the (data-dependent) privacy guarantees when the margin of the voting scores are large. Extensive experiments show that our approaches significantly improve the privacy-utility trade-off over the state-of-the-arts in DPFL.",
                        "Citation Paper Authors": "Authors:Yuqing Zhu, Xiang Yu, Yi-Hsuan Tsai, Francesco Pittaluga, Masoud Faraki, Manmohan chandraker, Yu-Xiang Wang"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "propose a different way to calculate\nthe privacy budget by interpreting each round as a Markov kernel\nand quantifying its impact on privacy parameters. Recent studies\npropose different regularization and sparsification techniques to\nimprove utility ",
                    "Citation Text": "Anda Cheng, Peisong Wang, Xi Sheryl Zhang, and Jian Cheng. 2022. Differentially\nPrivate Federated Learning with Local Regularization and Sparsification. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .\n10122\u201310131.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.03106",
                        "Citation Paper Title": "Title:Differentially Private Federated Learning with Local Regularization and Sparsification",
                        "Citation Paper Abstract": "Abstract:User-level differential privacy (DP) provides certifiable privacy guarantees to the information that is specific to any user's data in federated learning. Existing methods that ensure user-level DP come at the cost of severe accuracy decrease. In this paper, we study the cause of model performance degradation in federated learning under user-level DP guarantee. We find the key to solving this issue is to naturally restrict the norm of local updates before executing operations that guarantee DP. To this end, we propose two techniques, Bounded Local Update Regularization and Local Update Sparsification, to increase model quality without sacrificing privacy. We provide theoretical analysis on the convergence of our framework and give rigorous privacy guarantees. Extensive experiments show that our framework significantly improves the privacy-utility trade-off over the state-of-the-arts for federated learning with user-level DP guarantee.",
                        "Citation Paper Authors": "Authors:Anda Cheng, Peisong Wang, Xi Sheryl Zhang, Jian Cheng"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "and study its re-\nsilience against data reconstruction attacks. Liang et al . ",
                    "Citation Text": "Zhicong Liang, Bao Wang, Quanquan Gu, Stanley Osher, and Yuan Yao. 2020.\nExploring private federated learning with laplacian smoothing. arXiv preprint\narXiv:2005.00218 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.00218",
                        "Citation Paper Title": "Title:Differentially Private Federated Learning with Laplacian Smoothing",
                        "Citation Paper Abstract": "Abstract:Federated learning aims to protect data privacy by collaboratively learning a model without sharing private data among users. However, an adversary may still be able to infer the private training data by attacking the released model. Differential privacy provides a statistical protection against such attacks at the price of significantly degrading the accuracy or utility of the trained models. In this paper, we investigate a utility enhancement scheme based on Laplacian smoothing for differentially private federated learning (DP-Fed-LS), where the parameter aggregation with injected Gaussian noise is improved in statistical precision without losing privacy budget. Our key observation is that the aggregated gradients in federated learning often enjoy a type of smoothness, i.e. sparsity in the graph Fourier basis with polynomial decays of Fourier coefficients as frequency grows, which can be exploited by the Laplacian smoothing efficiently. Under a prescribed differential privacy budget, convergence error bounds with tight rates are provided for DP-Fed-LS with uniform subsampling of heterogeneous Non-IID data, revealing possible utility improvement of Laplacian smoothing in effective dimensionality and variance reduction, among others. Experiments over MNIST, SVHN, and Shakespeare datasets show that the proposed method can improve model accuracy with DP-guarantee and membership privacy under both uniform and Poisson subsampling mechanisms.",
                        "Citation Paper Authors": "Authors:Zhicong Liang, Bao Wang, Quanquan Gu, Stanley Osher, Yuan Yao"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": ", each user clips and quantizes\nthe model update, and adds noise drawn from Binomial distribution,\nachieving both communication efficiency and DP. Bhowmick et al . ",
                    "Citation Text": "Abhishek Bhowmick, John Duchi, Julien Freudiger, Gaurav Kapoor, and Ryan\nRogers. 2018. Protection against reconstruction and its applications in private\nfederated learning. arXiv preprint arXiv:1812.00984 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.00984",
                        "Citation Paper Title": "Title:Protection Against Reconstruction and Its Applications in Private Federated Learning",
                        "Citation Paper Abstract": "Abstract:In large-scale statistical learning, data collection and model fitting are moving increasingly toward peripheral devices---phones, watches, fitness trackers---away from centralized data collection. Concomitant with this rise in decentralized data are increasing challenges of maintaining privacy while allowing enough information to fit accurate, useful statistical models. This motivates local notions of privacy---most significantly, local differential privacy, which provides strong protections against sensitive data disclosures---where data is obfuscated before a statistician or learner can even observe it, providing strong protections to individuals' data. Yet local privacy as traditionally employed may prove too stringent for practical use, especially in modern high-dimensional statistical and machine learning problems. Consequently, we revisit the types of disclosures and adversaries against which we provide protections, considering adversaries with limited prior information and ensuring that with high probability, ensuring they cannot reconstruct an individual's data within useful tolerances. By reconceptualizing these protections, we allow more useful data release---large privacy parameters in local differential privacy---and we design new (minimax) optimal locally differentially private mechanisms for statistical learning problems for \\emph{all} privacy levels. We thus present practicable approaches to large-scale locally private model training that were previously impossible, showing theoretically and empirically that we can fit large-scale image classification and language models with little degradation in utility.",
                        "Citation Paper Authors": "Authors:Abhishek Bhowmick, John Duchi, Julien Freudiger, Gaurav Kapoor, Ryan Rogers"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.03255v4": {
            "Paper Title": "Goldfish: No More Attacks on Ethereum?!",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.11394v2": {
            "Paper Title": "Skefl: Single-Key Homomorphic Encryption for Secure Federated Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.02442v2": {
            "Paper Title": "SimCLF: A Simple Contrastive Learning Framework for Function-level\n  Binary Embeddings",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": ".Theprobability \ud835\udc43(\ud835\udc67\ud835\udc56\u2223\ud835\udc33)ismodified\nas follows:\n\ud835\udc43(\ud835\udc67\ud835\udc56\u2223\ud835\udc33) =exp(\ud835\udc60\ud835\udc56\ud835\udc5a(\ud835\udc67\ud835\udc56,\ud835\udc67)\u2215\ud835\udf0f))\n\u2211\ud835\udc5b\n\ud835\udc57=1exp(\ud835\udc60\ud835\udc56\ud835\udc5a(\ud835\udc67\ud835\udc56,\ud835\udc67)\u2215\ud835\udf0f), (4)\nOur objective is to maximize the joint probability\u220f\ud835\udc5b\n\ud835\udc56=1\ud835\udc43(\ud835\udc67\ud835\udc56\u2223\ud835\udc33), which is equivalent to minimizing the negative\nlog-likelihood \u2212\u2211\ud835\udc5b\n\ud835\udc56=1log\ud835\udc43(\ud835\udc67\ud835\udc56\u2223\ud835\udc33).ThelossfunctionbecomestheNT-Xent(thenormalizedtemperature-scaledcross-\nentropy loss) ",
                    "Citation Text": "Chen, T., Kornblith, S., Norouzi, M., Hinton, G., . A simple framework for contrastive learning of visual representations. URL: http:\n//arxiv.org/abs/2002.05709 ,arXiv:2002.05709 [cs, stat] . number: arXiv:2002.05709.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.05709",
                        "Citation Paper Title": "Title:A Simple Framework for Contrastive Learning of Visual Representations",
                        "Citation Paper Abstract": "Abstract:This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.",
                        "Citation Paper Authors": "Authors:Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "develops a novel neural architecture, a hierarchical Transformer,\nwhich can learn execution semantics from micro-traces during the pre-training phase. jTrans ",
                    "Citation Text": "Wang,H.,Qu,W.,Katz,G.,Zhu,W.,Gao,Z.,Qiu,H.,Zhuge,J.,Zhang,C.,2022. jtrans:Jump-awaretransformerforbinarycodesimilarity.\narXiv:2205.12713 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.12713",
                        "Citation Paper Title": "Title:jTrans: Jump-Aware Transformer for Binary Code Similarity",
                        "Citation Paper Abstract": "Abstract:Binary code similarity detection (BCSD) has important applications in various fields such as vulnerability detection, software component analysis, and reverse engineering. Recent studies have shown that deep neural networks (DNNs) can comprehend instructions or control-flow graphs (CFG) of binary code and support BCSD. In this study, we propose a novel Transformer-based approach, namely jTrans, to learn representations of binary code. It is the first solution that embeds control flow information of binary code into Transformer-based language models, by using a novel jump-aware representation of the analyzed binaries and a newly-designed pre-training task. Additionally, we release to the community a newly-created large dataset of binaries, BinaryCorp, which is the most diverse to date. Evaluation results show that jTrans outperforms state-of-the-art (SOTA) approaches on this more challenging dataset by 30.5% (i.e., from 32.0% to 62.5%). In a real-world task of known vulnerability searching, jTrans achieves a recall that is 2X higher than existing SOTA baselines.",
                        "Citation Paper Authors": "Authors:Hao Wang, Wenjie Qu, Gilad Katz, Wenyu Zhu, Zeyu Gao, Han Qiu, Jianwei Zhuge, Chao Zhang"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": ".ItleveragesahierarchicalTransformerarchitecturetoacquire\nexecution semantics from micro-traces in the pre-training phase. Subsequently, the authors employ InferSent ",
                    "Citation Text": "Conneau, A., Kiela, D., Schwenk, H., Barrault, L., Bordes, A., . Supervised learning of universal sentence representations from natural\nlanguage inference data. URL: http://arxiv.org/abs/1705.02364 ,arXiv:1705.02364 [cs] . number: arXiv:1705.02364.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.02364",
                        "Citation Paper Title": "Title:Supervised Learning of Universal Sentence Representations from Natural Language Inference Data",
                        "Citation Paper Abstract": "Abstract:Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.",
                        "Citation Paper Authors": "Authors:Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, Antoine Bordes"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "employs Word2Vec to convert the instructions into embeddings. A GRU Recurrent Neural Network (GRU RNN) is\nthen used to capture the sequential interaction of the instructions.\nThesuccessofpureattentionnetworksintheNLPdomainsuchasTransformer ",
                    "Citation Text": "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I., 2017. Attention is all you need, in:\nProceedingsofthe31stInternationalConferenceonNeuralInformationProcessingSystems,CurranAssociatesInc.,RedHook,NY,USA.p.\n6000\u20136010.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.02215v2": {
            "Paper Title": "On the Statistical Complexity of Estimation and Testing under Privacy\n  Constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.07316v5": {
            "Paper Title": "MENLI: Robust Evaluation Metrics from Natural Language Inference",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.10362v2": {
            "Paper Title": "Jolteon and Ditto: Network-Adaptive Efficient Consensus with\n  Asynchronous Fallback",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.01340v3": {
            "Paper Title": "Transaction Fee Mechanism Design",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": "explore a range of design\nquestions fornon-DSICtransactionfeemechanisms, with anemph asis onBayesian incentive-\ncompatible mechanisms.\nFinally, Bahrani, Garimidi, and Roughgarden ",
                    "Citation Text": "M. Bahrani, P. Garimidi, and T. Roughgarden. Transaction fee me ch-\nanism design with active block producers. arXiv:2307.01686. URL:\nhttps://arxiv.org/pdf/2307.01686.pdf , July 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2307.01686",
                        "Citation Paper Title": "Title:Transaction Fee Mechanism Design with Active Block Producers",
                        "Citation Paper Abstract": "Abstract:The incentive-compatibility properties of blockchain transaction fee mechanisms have been investigated with *passive* block producers that are motivated purely by the net rewards earned at the consensus layer. This paper introduces a model of *active* block producers that have their own private valuations for blocks (representing, for example, additional value derived from the application layer). The block producer surplus in our model can be interpreted as one of the more common colloquial meanings of the term ``MEV.''\nThe main results of this paper show that transaction fee mechanism design is fundamentally more difficult with active block producers than with passive ones: with active block producers, no non-trivial or approximately welfare-maximizing transaction fee mechanism can be incentive-compatible for both users and block producers. These results can be interpreted as a mathematical justification for the current interest in augmenting transaction fee mechanisms with additional components such as order flow auctions, block producer competition, trusted hardware, or cryptographic techniques.",
                        "Citation Paper Authors": "Authors:Maryam Bahrani, Pranav Garimidi, Tim Roughgarden"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "res trict attention to a discrete\nvaluation space.\nChen et al. ",
                    "Citation Text": "X. Chen, D. Simchi-Levi, Z. Zhao, and Y. Zhou. Bayesian mecha-\nnism design for blockchain transaction fee allocation. arXiv:2209.130 99. URL:\nhttps://arxiv.org/pdf/2209.13099.pdf , September 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2209.13099",
                        "Citation Paper Title": "Title:Bayesian Mechanism Design for Blockchain Transaction Fee Allocation",
                        "Citation Paper Abstract": "Abstract:In blockchain systems, the design of transaction fee mechanisms is essential for stability and satisfaction for both miners and users. A recent work has proven the impossibility of collusion-proof mechanisms that achieve both non-zero miner revenue and Dominating-Strategy-Incentive-Compatible (DSIC) for users. However, a positive miner revenue is important in practice to motivate miners. To address this challenge, we consider a Bayesian game setting and relax the DSIC requirement for users to Bayesian-Nash-Incentive-Compatibility (BNIC). In particular, we propose an auxiliary mechanism method that makes connections between BNIC and DSIC mechanisms. With the auxiliary mechanism method, we design a transaction fee mechanism (TFM) based on the multinomial logit (MNL) choice model, and prove that the TFM has both BNIC and collusion-proof properties with an asymptotic constant-factor approximation of optimal miner revenue for i.i.d. bounded valuations. Our result breaks the zero-revenue barrier while preserving truthfulness and collusion-proof properties.",
                        "Citation Paper Authors": "Authors:Xi Chen, David Simchi-Levi, Zishuo Zhao, Yuan Zhou"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": ", which provide\nempirical analyses ofthemechanism afteritsdeployment; andAzou vi etal. ",
                    "Citation Text": "S. Azouvi, G. Goren, L. Heimbach, and A. Hicks. Base fee manipula tion in Ethereum\u2019s\nEIP-1559 transaction fee mechanism. In Proceedings of the 37th International Sympo-\nsium on Distributed Computing (DSIC) , 2023. Article 6.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.11478",
                        "Citation Paper Title": "Title:Base Fee Manipulation In Ethereum's EIP-1559 Transaction Fee Mechanism",
                        "Citation Paper Abstract": "Abstract:In 2021 Ethereum adjusted the transaction pricing mechanism by implementing EIP-1559, which introduces the base fee - a network fee that is burned and dynamically adjusts to the network demand. The authors of the Ethereum Improvement Proposal (EIP) noted that a miner with more than 50% of the mining power could be incentivized to deviate from the honest mining strategy. Instead, such a miner could propose a series of empty blocks to artificially lower demand and increase her future rewards. In this paper, we generalize this attack and show that under rational player behavior, deviating from the honest strategy can be profitable for a miner with less than 50% of the mining power. We show that even when miners do not collaborate, it is at times rational for smaller miners to join the attack. Finally, we propose a mitigation to address the identified vulnerability.",
                        "Citation Paper Authors": "Authors:Sarah Azouvi, Guy Goren, Lioba Heimbach, Alexander Hicks"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": ", which investigate the properties of di\ufb00e rent base fee update\nrules; Liu et al. ",
                    "Citation Text": "Y. Liu, Y. Lu, K. Nayak, F. Zhang, L. Zhang, and Y. Zhao. Empir ical analysis of\nEIP-1559: Transaction fees, waiting times, and consensus secur ity. InProceedings of\nthe 2022 ACM SIGSAC Conference on Computer and Communicatio ns Security , pages\n2099\u20132113, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.05574",
                        "Citation Paper Title": "Title:Empirical Analysis of EIP-1559: Transaction Fees, Waiting Time, and Consensus Security",
                        "Citation Paper Abstract": "Abstract:A transaction fee mechanism (TFM) is an essential component of a blockchain protocol. However, a systematic evaluation of the real-world impact of TFMs is still absent. Using rich data from the Ethereum blockchain, the mempool, and exchanges, we study the effect of EIP-1559, one of the earliest-deployed TFMs that depart from the traditional first-price auction paradigm. We conduct a rigorous and comprehensive empirical study to examine its causal effect on blockchain transaction fee dynamics, transaction waiting times, and consensus security. Our results show that EIP-1559 improves the user experience by mitigating intrablock differences in the gas price paid and reducing users' waiting times. However, EIP-1559 has only a small effect on gas fee levels and consensus security. In addition, we find that when Ether's price is more volatile, the waiting time is significantly higher. We also verify that a larger block size increases the presence of siblings. These findings suggest new directions for improving TFMs.",
                        "Citation Paper Authors": "Authors:Yulin Liu, Yuxuan Lu, Kartik Nayak, Fan Zhang, Luyao Zhang, Yinhong Zhao"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "studied the price dynamics of this mechanism ove r a sequence of blocks\nin a model with persistent transactions.\nBasu et al. ",
                    "Citation Text": "S. Basu, D. Easley, M. O\u2019Hara, and E. G. Sirer. Stablefees: A pre dictable fee market\nfor cryptocurrencies. Management Science , 69(11):6508\u20136524, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.06830",
                        "Citation Paper Title": "Title:Towards a Functional Fee Market for Cryptocurrencies",
                        "Citation Paper Abstract": "Abstract:Blockchain-based cryptocurrencies prioritize transactions based on their fees, creating a unique kind of fee market. Empirically, this market has failed to yield stable equilibria with predictable prices for desired levels of service. We argue that this is due to the absence of a dominant strategy equilibrium in the current fee mechanism. We propose an alternative fee setting mechanism that is inspired by generalized second price auctions. The design of such a mechanism is challenging because miners can use any criteria for including transactions and can manipulate the results of the auction after seeing the proposed fees. Nonetheless, we show that our proposed protocol is free from manipulation as the number of users increases. We further show that, for a large number of users and miners, the gain from manipulation is small for all parties. This results in users proposing fees that represent their true utility and lower variance of revenue for miners. Historical analysis shows that Bitcoin users could have saved $272,528,000 USD in transaction fees while miners could have reduced the variance of fee income by an average factor of 7.4 times.",
                        "Citation Paper Authors": "Authors:Soumya Basu, David Easley, Maureen O'Hara, Emin G\u00fcn Sirer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.14404v5": {
            "Paper Title": "Adversarial Purification with the Manifold Hypothesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.06327v4": {
            "Paper Title": "Reproducibility-Oriented and Privacy-Preserving Genomic Dataset Sharing",
            "Sentences": [
                {
                    "Sentence ID": 40,
                    "Sentence": ". To achieve\nthis, researchers typically share datasets used in their re-\nsearch such that everyone can reconstruct the same exper-\niments and validate their results. Some examples of these\ndatasets include ImageNet ",
                    "Citation Text": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev\nSatheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla,\nMichael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet\nLarge Scale Visual Recognition Challenge. International Journal of\nComputer Vision (IJCV) , 115(3):211\u2013252, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.0575",
                        "Citation Paper Title": "Title:ImageNet Large Scale Visual Recognition Challenge",
                        "Citation Paper Abstract": "Abstract:The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions.\nThis paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.",
                        "Citation Paper Authors": "Authors:Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, Li Fei-Fei"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2201.04736v2": {
            "Paper Title": "Security for Machine Learning-based Software Systems: a survey of\n  threats, practices and challenges",
            "Sentences": [
                {
                    "Sentence ID": 51,
                    "Sentence": ". Given the large\namounts of different machine learning frameworks and platforms, particularly for deep learning, Guo et al. ",
                    "Citation Text": "Qianyu Guo, Sen Chen, Xiaofei Xie, Lei Ma, Qiang Hu, Hongtao Liu, Yang Liu, Jianjun Zhao, and Xiaohong Li. 2019. An empirical study towards\ncharacterizing deep learning development and deployment across different frameworks and platforms. In 2019 34th IEEE/ACM International\nConference on Automated Software Engineering (ASE) . IEEE, 810\u2013822.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.06727",
                        "Citation Paper Title": "Title:An Empirical Study towards Characterizing Deep Learning Development and Deployment across Different Frameworks and Platforms",
                        "Citation Paper Abstract": "Abstract:Deep Learning (DL) has recently achieved tremendous success. A variety of DL frameworks and platforms play a key role to catalyze such progress. However, the differences in architecture designs and implementations of existing frameworks and platforms bring new challenges for DL software development and deployment. Till now, there is no study on how various mainstream frameworks and platforms influence both DL software development and deployment in practice. To fill this gap, we take the first step towards understanding how the most widely-used DL frameworks and platforms support the DL software development and deployment. We conduct a systematic study on these frameworks and platforms by using two types of DNN architectures and three popular datasets. (1) For development process, we investigate the prediction accuracy under the same runtime training configuration or same model weights/biases. We also study the adversarial robustness of trained models by leveraging the existing adversarial attack techniques. The experimental results show that the computing differences across frameworks could result in an obvious prediction accuracy decline, which should draw the attention of DL developers. (2) For deployment process, we investigate the prediction accuracy and performance (refers to time cost and memory consumption) when the trained models are migrated/quantized from PC to real mobile devices and web browsers. The DL platform study unveils that the migration and quantization still suffer from compatibility and reliability issues. Meanwhile, we find several DL software bugs by using the results as a benchmark. We further validate the results through bug confirmation from stakeholders and industrial positive feedback to highlight the implications of our study. Through our study, we summarize practical guidelines, identify challenges and pinpoint new research directions.",
                        "Citation Paper Authors": "Authors:Qianyu Guo, Sen Chen, Xiaofei Xie, Lei Ma, Qiang Hu, Hongtao Liu, Yang Liu, Jianjun Zhao, Xiaohong Li"
                    }
                },
                {
                    "Sentence ID": 89,
                    "Sentence": ", the framework of D2NN is proposed for checking the result inconsistency between the original and duplicated\nmodels. In another work of ",
                    "Citation Text": "Yu Li, Min Li, Bo Luo, Ye Tian, and Qiang Xu. 2020. Deepdyve: Dynamic verification for deep neural networks. In Proceedings of the 2020 ACM\nSIGSAC Conference on Computer and Communications Security . 101\u2013112.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.09663",
                        "Citation Paper Title": "Title:DeepDyve: Dynamic Verification for Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:Deep neural networks (DNNs) have become one of the enabling technologies in many safety-critical applications, e.g., autonomous driving and medical image analysis. DNN systems, however, suffer from various kinds of threats, such as adversarial example attacks and fault injection attacks. While there are many defense methods proposed against maliciously crafted inputs, solutions against faults presented in the DNN system itself (e.g., parameters and calculations) are far less explored. In this paper, we develop a novel lightweight fault-tolerant solution for DNN-based systems, namely DeepDyve, which employs pre-trained neural networks that are far simpler and smaller than the original DNN for dynamic verification. The key to enabling such lightweight checking is that the smaller neural network only needs to produce approximate results for the initial task without sacrificing fault coverage much. We develop efficient and effective architecture and task exploration techniques to achieve optimized risk/overhead trade-off in DeepDyve. Experimental results show that DeepDyve can reduce 90% of the risks at around 10% overhead.",
                        "Citation Paper Authors": "Authors:Yu Li, Min Li, Bo Luo, Ye Tian, Qiang Xu"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": ",\na novel test adequacy criterion for testing called Surprise Adequacy for Deep Learning Systems (SADL) is proposed.\nTo capture the importance of deep learning-based system in a layer-wise functional understanding, ",
                    "Citation Text": "Simos Gerasimou, Hasan Ferit Eniser, Alper Sen, and Alper Cakan. 2020. Importance-driven deep learning system testing. In 2020 IEEE/ACM 42nd\nInternational Conference on Software Engineering (ICSE) . IEEE, 702\u2013713.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.03433",
                        "Citation Paper Title": "Title:Importance-Driven Deep Learning System Testing",
                        "Citation Paper Abstract": "Abstract:Deep Learning (DL) systems are key enablers for engineering intelligent applications due to their ability to solve complex tasks such as image recognition and machine translation. Nevertheless, using DL systems in safety- and security-critical applications requires to provide testing evidence for their dependable operation. Recent research in this direction focuses on adapting testing criteria from traditional software engineering as a means of increasing confidence for their correct behaviour. However, they are inadequate in capturing the intrinsic properties exhibited by these systems. We bridge this gap by introducing DeepImportance, a systematic testing methodology accompanied by an Importance-Driven (IDC) test adequacy criterion for DL systems. Applying IDC enables to establish a layer-wise functional understanding of the importance of DL system components and use this information to assess the semantic diversity of a test set. Our empirical evaluation on several DL systems, across multiple DL datasets and with state-of-the-art adversarial generation techniques demonstrates the usefulness and effectiveness of DeepImportance and its ability to support the engineering of more robust DL systems.",
                        "Citation Paper Authors": "Authors:Simos Gerasimou, Hasan Ferit Eniser, Alper Sen, Alper Cakan"
                    }
                },
                {
                    "Sentence ID": 107,
                    "Sentence": ". Following,\ndifferent works based on traditional testing adequacy coverage methods are proposed and extensively evaluated, such\nas the mutation testing ",
                    "Citation Text": "Lei Ma, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Felix Juefei-Xu, Chao Xie, Li Li, Yang Liu, Jianjun Zhao, et al .2018. Deepmutation: Mutation\ntesting of deep learning systems. In 2018 IEEE 29th International Symposium on Software Reliability Engineering (ISSRE) . IEEE, 100\u2013111.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.05206",
                        "Citation Paper Title": "Title:DeepMutation: Mutation Testing of Deep Learning Systems",
                        "Citation Paper Abstract": "Abstract:Deep learning (DL) defines a new data-driven programming paradigm where the internal system logic is largely shaped by the training data. The standard way of evaluating DL models is to examine their performance on a test dataset. The quality of the test dataset is of great importance to gain confidence of the trained models. Using an inadequate test dataset, DL models that have achieved high test accuracy may still lack generality and robustness. In traditional software testing, mutation testing is a well-established technique for quality evaluation of test suites, which analyzes to what extent a test suite detects the injected faults. However, due to the fundamental difference between traditional software and deep learning-based software, traditional mutation testing techniques cannot be directly applied to DL systems. In this paper, we propose a mutation testing framework specialized for DL systems to measure the quality of test data. To do this, by sharing the same spirit of mutation testing in traditional software, we first define a set of source-level mutation operators to inject faults to the source of DL (i.e., training data and training programs). Then we design a set of model-level mutation operators that directly inject faults into DL models without a training process. Eventually, the quality of test data could be evaluated from the analysis on to what extent the injected faults could be detected. The usefulness of the proposed mutation testing techniques is demonstrated on two public datasets, namely MNIST and CIFAR-10, with three DL models.",
                        "Citation Paper Authors": "Authors:Lei Ma, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Felix Juefei-Xu, Chao Xie, Li Li, Yang Liu, Jianjun Zhao, Yadong Wang"
                    }
                },
                {
                    "Sentence ID": 141,
                    "Sentence": "(see section 3.3).\nOne studied criteria is related to the testing adequacy. DeepXplore is one of the early works that attempts to automat-\nically generate test inputs for deep learning models to expose erroneous behaviors in an efficient way ",
                    "Citation Text": "Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2017. Deepxplore: Automated whitebox testing of deep learning systems. In Proceedings of\nthe 26th Symposium on Operating Systems Principles . 1\u201318.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.06640",
                        "Citation Paper Title": "Title:DeepXplore: Automated Whitebox Testing of Deep Learning Systems",
                        "Citation Paper Abstract": "Abstract:Deep learning (DL) systems are increasingly deployed in safety- and security-critical domains including self-driving cars and malware detection, where the correctness and predictability of a system's behavior for corner case inputs are of great importance. Existing DL testing depends heavily on manually labeled data and therefore often fails to expose erroneous behaviors for rare inputs.\nWe design, implement, and evaluate DeepXplore, the first whitebox framework for systematically testing real-world DL systems. First, we introduce neuron coverage for systematically measuring the parts of a DL system exercised by test inputs. Next, we leverage multiple DL systems with similar functionality as cross-referencing oracles to avoid manual checking. Finally, we demonstrate how finding inputs for DL systems that both trigger many differential behaviors and achieve high neuron coverage can be represented as a joint optimization problem and solved efficiently using gradient-based search techniques.\nDeepXplore efficiently finds thousands of incorrect corner case behaviors (e.g., self-driving cars crashing into guard rails and malware masquerading as benign software) in state-of-the-art DL models with thousands of neurons trained on five popular datasets including ImageNet and Udacity self-driving challenge data. For all tested DL models, on average, DeepXplore generated one test input demonstrating incorrect behavior within one second while running only on a commodity laptop. We further show that the test inputs generated by DeepXplore can also be used to retrain the corresponding DL model to improve the model's accuracy by up to 3%.",
                        "Citation Paper Authors": "Authors:Kexin Pei, Yinzhi Cao, Junfeng Yang, Suman Jana"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "SE practices AI-based systems ! # #\nAsmore et al. 2021 ",
                    "Citation Text": "Rob Ashmore, Radu Calinescu, and Colin Paterson. 2021. Assuring the Machine Learning Lifecycle: Desiderata, Methods, and Challenges. ACM\nComput. Surv. 54, 5, Article 111 (2021), 39 pages.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.04223",
                        "Citation Paper Title": "Title:Assuring the Machine Learning Lifecycle: Desiderata, Methods, and Challenges",
                        "Citation Paper Abstract": "Abstract:Machine learning has evolved into an enabling technology for a wide range of highly successful applications. The potential for this success to continue and accelerate has placed machine learning (ML) at the top of research, economic and political agendas. Such unprecedented interest is fuelled by a vision of ML applicability extending to healthcare, transportation, defence and other domains of great societal importance. Achieving this vision requires the use of ML in safety-critical applications that demand levels of assurance beyond those needed for current ML applications. Our paper provides a comprehensive survey of the state-of-the-art in the assurance of ML, i.e. in the generation of evidence that ML is sufficiently safe for its intended use. The survey covers the methods capable of providing such evidence at different stages of the machine learning lifecycle, i.e. of the complex, iterative process that starts with the collection of the data used to train an ML component for a system, and ends with the deployment of that component within the system. The paper begins with a systematic presentation of the ML lifecycle and its stages. We then define assurance desiderata for each stage, review existing methods that contribute to achieving these desiderata, and identify open challenges that require further research.",
                        "Citation Paper Authors": "Authors:Rob Ashmore, Radu Calinescu, Colin Paterson"
                    }
                },
                {
                    "Sentence ID": 87,
                    "Sentence": ".\nLewis et al. alternatively took the mismatch system elements that may occur during the end-to-end development by\ndifferent stakeholders into the consideration, in which the trained model, raw data, training data and the environments\nwere identified ",
                    "Citation Text": "Grace A Lewis, Stephany Bellomo, and Ipek Ozkaya. 2021. Characterizing and Detecting Mismatch in Machine-Learning-Enabled Systems. In 2021\nIEEE/ACM 43rd International Conference on Software Engineering 1st Workshop on AI Engineering: Software Engineering for AI (WAIN 2021) . 1\u20138.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.14101",
                        "Citation Paper Title": "Title:Characterizing and Detecting Mismatch in Machine-Learning-Enabled Systems",
                        "Citation Paper Abstract": "Abstract:Increasing availability of machine learning (ML) frameworks and tools, as well as their promise to improve solutions to data-driven decision problems, has resulted in popularity of using ML techniques in software systems. However, end-to-end development of ML-enabled systems, as well as their seamless deployment and operations, remain a challenge. One reason is that development and deployment of ML-enabled systems involves three distinct workflows, perspectives, and roles, which include data science, software engineering, and operations. These three distinct perspectives, when misaligned due to incorrect assumptions, cause ML mismatches which can result in failed systems. We conducted an interview and survey study where we collected and validated common types of mismatches that occur in end-to-end development of ML-enabled systems. Our analysis shows that how each role prioritizes the importance of relevant mismatches varies, potentially contributing to these mismatched assumptions. In addition, the mismatch categories we identified can be specified as machine readable descriptors contributing to improved ML-enabled system development. In this paper, we report our findings and their implications for improving end-to-end ML-enabled system development.",
                        "Citation Paper Authors": "Authors:Grace A. Lewis, Stephany Bellomo, Ipek Ozkaya"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": ". Other technical debts, i.e., abstraction debt, configuration debt,\ndata testing debt, reproducibility debt, process management debt and cultural debt, may also be accrued and demands\nongoing collaborative efforts. Following in ",
                    "Citation Text": "Justus Bogner, Roberto Verdecchia, and Ilias Gerostathopoulos. 2021. Characterizing Technical Debt and Antipatterns in AI-Based Systems: A\nSystematic Mapping Study. In 4th International Conference on Technical Debt (TechDebt 2021) . 1\u201310.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.09783",
                        "Citation Paper Title": "Title:Characterizing Technical Debt and Antipatterns in AI-Based Systems: A Systematic Mapping Study",
                        "Citation Paper Abstract": "Abstract:Background: With the rising popularity of Artificial Intelligence (AI), there is a growing need to build large and complex AI-based systems in a cost-effective and manageable way. Like with traditional software, Technical Debt (TD) will emerge naturally over time in these systems, therefore leading to challenges and risks if not managed appropriately. The influence of data science and the stochastic nature of AI-based systems may also lead to new types of TD or antipatterns, which are not yet fully understood by researchers and practitioners. Objective: The goal of our study is to provide a clear overview and characterization of the types of TD (both established and new ones) that appear in AI-based systems, as well as the antipatterns and related solutions that have been proposed. Method: Following the process of a systematic mapping study, 21 primary studies are identified and analyzed. Results: Our results show that (i) established TD types, variations of them, and four new TD types (data, model, configuration, and ethics debt) are present in AI-based systems, (ii) 72 antipatterns are discussed in the literature, the majority related to data and model deficiencies, and (iii) 46 solutions have been proposed, either to address specific TD types, antipatterns, or TD in general. Conclusions: Our results can support AI professionals with reasoning about and communicating aspects of TD present in their systems. Additionally, they can serve as a foundation for future research to further our understanding of TD in AI-based systems.",
                        "Citation Paper Authors": "Authors:Justus Bogner, Roberto Verdecchia, Ilias Gerostathopoulos"
                    }
                },
                {
                    "Sentence ID": 149,
                    "Sentence": ", in Fig. 4, the machine learning\nmodel development only represents a tiny fraction of whole system. It is undoubted that the intertwining process of\nmachine learning and software engineering has opened many challenges. A recently reported industrial case SAP ",
                    "Citation Text": "Md Saidur Rahman, Emilio Rivera, Foutse Khomh, Yann-Ga\u00ebl Gu\u00e9h\u00e9neuc, and Bernd Lehnert. 2019. Machine learning software engineering in\npractice: An industrial case study. arXiv preprint arXiv:1906.07154 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.07154",
                        "Citation Paper Title": "Title:Machine Learning Software Engineering in Practice: An Industrial Case Study",
                        "Citation Paper Abstract": "Abstract:SAP is the market leader in enterprise software offering an end-to-end suite of applications and services to enable their customers worldwide to operate their business. Especially, retail customers of SAP deal with millions of sales transactions for their day-to-day business. Transactions are created during retail sales at the point of sale (POS) terminals and then sent to some central servers for validations and other business operations. A considerable proportion of the retail transactions may have inconsistencies due to many technical and human errors. SAP provides an automated process for error detection but still requires a manual process by dedicated employees using workbench software for correction. However, manual corrections of these errors are time-consuming, labor-intensive, and may lead to further errors due to incorrect modifications. This is not only a performance overhead on the customers' business workflow but it also incurs high operational costs. Thus, automated detection and correction of transaction errors are very important regarding their potential business values and the improvement in the business workflow. In this paper, we present an industrial case study where we apply machine learning (ML) to automatically detect transaction errors and propose corrections. We identify and discuss the challenges that we faced during this collaborative research and development project, from three distinct perspectives: Software Engineering, Machine Learning, and industry-academia collaboration. We report on our experience and insights from the project with guidelines for the identified challenges. We believe that our findings and recommendations can help researchers and practitioners embarking into similar endeavors.",
                        "Citation Paper Authors": "Authors:Md Saidur Rahman, Emilio Rivera, Foutse Khomh, Yann-Ga\u00ebl Gu\u00e9h\u00e9neuc, Bernd Lehnert"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "Platform attacks System deployment CNN Model availabil-\nityData-space\nrandomization,\nhardware access\ncontrols\nCosta et al.\n2021 ",
                    "Citation Text": "Gabriele Costa, Fabio Pinelli, Simone Soderi, and Gabriele Tolomei. 2021. Covert Channel Attack to Federated Learning Systems. arXiv preprint\narXiv:2104.10561 (2021).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.10561",
                        "Citation Paper Title": "Title:Covert Channel Attack to Federated Learning Systems",
                        "Citation Paper Abstract": "Abstract:Federated learning (FL) goes beyond traditional, centralized machine learning by distributing model training among a large collection of edge clients. These clients cooperatively train a global, e.g., cloud-hosted, model without disclosing their local, private training data. The global model is then shared among all the participants which use it for local predictions. In this paper, we put forward a novel attacker model aiming at turning FL systems into covert channels to implement a stealth communication infrastructure. The main intuition is that, during federated training, a malicious sender can poison the global model by submitting purposely crafted examples. Although the effect of the model poisoning is negligible to other participants, and does not alter the overall model performance, it can be observed by a malicious receiver and used to transmit a single bit.",
                        "Citation Paper Authors": "Authors:Gabriele Costa, Fabio Pinelli, Simone Soderi, Gabriele Tolomei"
                    }
                },
                {
                    "Sentence ID": 64,
                    "Sentence": "and various mobile applications with on-device model for Android applications ",
                    "Citation Text": "Yujin Huang, Han Hu, and Chunyang Chen. 2021. Robustness of on-device Models: Adversarial Attack to Deep Learning Models on Android Apps.\nInProceedings of the 43rd International Conference on Software Engineering, Software Engineering in Practice Track . 1\u201310. arXiv:cs.LG/2101.04401",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.04401",
                        "Citation Paper Title": "Title:Robustness of on-device Models: Adversarial Attack to Deep Learning Models on Android Apps",
                        "Citation Paper Abstract": "Abstract:Deep learning has shown its power in many applications, including object detection in images, natural-language understanding, and speech recognition. To make it more accessible to end users, many deep learning models are now embedded in mobile apps. Compared to offloading deep learning from smartphones to the cloud, performing machine learning on-device can help improve latency, connectivity, and power consumption. However, most deep learning models within Android apps can easily be obtained via mature reverse engineering, while the models' exposure may invite adversarial attacks. In this study, we propose a simple but effective approach to hacking deep learning models using adversarial attacks by identifying highly similar pre-trained models from TensorFlow Hub. All 10 real-world Android apps in the experiment are successfully attacked by our approach. Apart from the feasibility of the model attack, we also carry out an empirical study that investigates the characteristics of deep learning models used by hundreds of Android apps on Google Play. The results show that many of them are similar to each other and widely use fine-tuning techniques to pre-trained models on the Internet.",
                        "Citation Paper Authors": "Authors:Yujin Huang, Han Hu, Chunyang Chen"
                    }
                },
                {
                    "Sentence ID": 137,
                    "Sentence": ", a copycat CNN model attack was presented by utilising random non-labeled data in two steps.\nIn the work by Papernot et al. ",
                    "Citation Text": "Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. 2016. Transferability in machine learning: from phenomena to black-box attacks using\nadversarial samples. arXiv preprint arXiv:1605.07277 (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.07277",
                        "Citation Paper Title": "Title:Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples",
                        "Citation Paper Abstract": "Abstract:Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that affect one model often affect another model, even if the two models have different architectures or were trained on different training sets, so long as both models were trained to perform the same task. An attacker may therefore train their own substitute model, craft adversarial examples against the substitute, and transfer them to a victim model, with very little information about the victim. Recent work has further developed a technique that uses the victim model as an oracle to label a synthetic training set for the substitute, so the attacker need not even collect a training set to mount the attack. We extend these recent techniques using reservoir sampling to greatly enhance the efficiency of the training procedure for the substitute model. We introduce new transferability attacks between previously unexplored (substitute, victim) pairs of machine learning model classes, most notably SVMs and decision trees. We demonstrate our attacks on two commercial machine learning classification systems from Amazon (96.19% misclassification rate) and Google (88.94%) using only 800 queries of the victim model, thereby showing that existing machine learning approaches are in general vulnerable to systematic black-box attacks regardless of their structure.",
                        "Citation Paper Authors": "Authors:Nicolas Papernot, Patrick McDaniel, Ian Goodfellow"
                    }
                },
                {
                    "Sentence ID": 80,
                    "Sentence": "System deployment CNN Model confiden-\ntialityNone\nContinued on next page\nManuscript submitted to ACM16 Huaming Chen and M. Ali Babar\nTable 3 \u2013 Continued from previous page\nStudy Attack mode Attack stage Model Affecting CIA Defence\nKrishna et\nal. 2020 ",
                    "Citation Text": "Kalpesh Krishna, Gaurav Singh Tomar, Ankur P Parikh, Nicolas Papernot, and Mohit Iyyer. 2020. Thieves on Sesame Street! Model Extraction of\nBERT-based APIs. In International Conference on Learning Representations . 1\u201319.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.12366",
                        "Citation Paper Title": "Title:Thieves on Sesame Street! Model Extraction of BERT-based APIs",
                        "Citation Paper Abstract": "Abstract:We study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al. 2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks, including natural language inference and question answering. Our work thus highlights an exploit only made feasible by the shift towards transfer learning methods within the NLP community: for a query budget of a few hundred dollars, an attacker can extract a model that performs only slightly worse than the victim model. Finally, we study two defense strategies against model extraction---membership classification and API watermarking---which while successful against naive adversaries, are ineffective against more sophisticated ones.",
                        "Citation Paper Authors": "Authors:Kalpesh Krishna, Gaurav Singh Tomar, Ankur P. Parikh, Nicolas Papernot, Mohit Iyyer"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": ". The knockoff models give\nthe adversary the advantages to bypass the monetary cost and intellectual efforts. A real-world image recognition\nAPI was introduced for further evaluation and the results showed strong performance of the devised knockoff model.\nIn ",
                    "Citation Text": "Jacson Rodrigues Correia-Silva, Rodrigo F Berriel, Claudine Badue, Alberto F de Souza, and Thiago Oliveira-Santos. 2018. Copycat cnn: Stealing\nknowledge by persuading confession with random non-labeled data. In 2018 International Joint Conference on Neural Networks (IJCNN) . IEEE, 1\u20138.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.05476",
                        "Citation Paper Title": "Title:Copycat CNN: Stealing Knowledge by Persuading Confession with Random Non-Labeled Data",
                        "Citation Paper Abstract": "Abstract:In the past few years, Convolutional Neural Networks (CNNs) have been achieving state-of-the-art performance on a variety of problems. Many companies employ resources and money to generate these models and provide them as an API, therefore it is in their best interest to protect them, i.e., to avoid that someone else copies them. Recent studies revealed that state-of-the-art CNNs are vulnerable to adversarial examples attacks, and this weakness indicates that CNNs do not need to operate in the problem domain (PD). Therefore, we hypothesize that they also do not need to be trained with examples of the PD in order to operate in it.\nGiven these facts, in this paper, we investigate if a target black-box CNN can be copied by persuading it to confess its knowledge through random non-labeled data. The copy is two-fold: i) the target network is queried with random data and its predictions are used to create a fake dataset with the knowledge of the network; and ii) a copycat network is trained with the fake dataset and should be able to achieve similar performance as the target network.\nThis hypothesis was evaluated locally in three problems (facial expression, object, and crosswalk classification) and against a cloud-based API. In the copy attacks, images from both non-problem domain and PD were used. All copycat networks achieved at least 93.7% of the performance of the original models with non-problem domain data, and at least 98.6% using additional data from the PD. Additionally, the copycat CNN successfully copied at least 97.3% of the performance of the Microsoft Azure Emotion API. Our results show that it is possible to create a copycat CNN by simply querying a target network as black-box with random non-labeled data.",
                        "Citation Paper Authors": "Authors:Jacson Rodrigues Correia-Silva, Rodrigo F. Berriel, Claudine Badue, Alberto F. de Souza, Thiago Oliveira-Santos"
                    }
                },
                {
                    "Sentence ID": 75,
                    "Sentence": "further explored the model\nextraction attack for the trained neural network and deep learning models. Juuti et al. ",
                    "Citation Text": "Mika Juuti, Sebastian Szyller, Samuel Marchal, and N Asokan. 2019. PRADA: protecting against DNN model stealing attacks. In 2019 IEEE European\nSymposium on Security and Privacy (EuroS&P) . IEEE, 512\u2013527.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.02628",
                        "Citation Paper Title": "Title:PRADA: Protecting against DNN Model Stealing Attacks",
                        "Citation Paper Abstract": "Abstract:Machine learning (ML) applications are increasingly prevalent. Protecting the confidentiality of ML models becomes paramount for two reasons: (a) a model can be a business advantage to its owner, and (b) an adversary may use a stolen model to find transferable adversarial examples that can evade classification by the original model. Access to the model can be restricted to be only via well-defined prediction APIs. Nevertheless, prediction APIs still provide enough information to allow an adversary to mount model extraction attacks by sending repeated queries via the prediction API. In this paper, we describe new model extraction attacks using novel approaches for generating synthetic queries, and optimizing training hyperparameters. Our attacks outperform state-of-the-art model extraction in terms of transferability of both targeted and non-targeted adversarial examples (up to +29-44 percentage points, pp), and prediction accuracy (up to +46 pp) on two datasets. We provide take-aways on how to perform effective model extraction attacks. We then propose PRADA, the first step towards generic and effective detection of DNN model extraction attacks. It analyzes the distribution of consecutive API queries and raises an alarm when this distribution deviates from benign behavior. We show that PRADA can detect all prior model extraction attacks with no false positives.",
                        "Citation Paper Authors": "Authors:Mika Juuti, Sebastian Szyller, Samuel Marchal, N. Asokan"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": ". Also, the model poisoning attacks can threaten\nthe learning process directly. In ",
                    "Citation Text": "Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. 2019. Analyzing federated learning through an adversarial lens. In\nInternational Conference on Machine Learning . PMLR, 634\u2013643.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.12470",
                        "Citation Paper Title": "Title:Analyzing Federated Learning through an Adversarial Lens",
                        "Citation Paper Abstract": "Abstract:Federated learning distributes model training among a multitude of agents, who, guided by privacy concerns, perform training using their local data but share only model parameter updates, for iterative aggregation at the server. In this work, we explore the threat of model poisoning attacks on federated learning initiated by a single, non-colluding malicious agent where the adversarial objective is to cause the model to misclassify a set of chosen inputs with high confidence. We explore a number of strategies to carry out this attack, starting with simple boosting of the malicious agent's update to overcome the effects of other agents' updates. To increase attack stealth, we propose an alternating minimization strategy, which alternately optimizes for the training loss and the adversarial objective. We follow up by using parameter estimation for the benign agents' updates to improve on attack success. Finally, we use a suite of interpretability techniques to generate visual explanations of model decisions for both benign and malicious models and show that the explanations are nearly visually indistinguishable. Our results indicate that even a highly constrained adversary can carry out model poisoning attacks while simultaneously maintaining stealth, thus highlighting the vulnerability of the federated learning setting and the need to develop effective defense strategies.",
                        "Citation Paper Authors": "Authors:Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, Seraphin Calo"
                    }
                },
                {
                    "Sentence ID": 68,
                    "Sentence": "studied to craft adversarial examples by a surrogate training model using synthetic data generation. Attacks were\ndeployed against Amazon Machine Learning and Google\u2019s Cloud Prediction API platforms. Ilyas et al. ",
                    "Citation Text": "Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. 2018. Black-box adversarial attacks with limited queries and information. In\nInternational Conference on Machine Learning . PMLR, 2137\u20132146.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.08598",
                        "Citation Paper Title": "Title:Black-box Adversarial Attacks with Limited Queries and Information",
                        "Citation Paper Abstract": "Abstract:Current neural network-based classifiers are susceptible to adversarial examples even in the black-box setting, where the attacker only has query access to the model. In practice, the threat model for real-world systems is often more restrictive than the typical black-box model where the adversary can observe the full output of the network on arbitrarily many chosen inputs. We define three realistic threat models that more accurately characterize many real-world classifiers: the query-limited setting, the partial-information setting, and the label-only setting. We develop new attacks that fool classifiers under these more restrictive threat models, where previous methods would be impractical or ineffective. We demonstrate that our methods are effective against an ImageNet classifier under our proposed threat models. We also demonstrate a targeted black-box attack against a commercial classifier, overcoming the challenges of limited query access, partial information, and other practical issues to break the Google Cloud Vision API.",
                        "Citation Paper Authors": "Authors:Andrew Ilyas, Logan Engstrom, Anish Athalye, Jessy Lin"
                    }
                },
                {
                    "Sentence ID": 91,
                    "Sentence": "Model construction CNN Model integrity None\nLi et al.\n2020 ",
                    "Citation Text": "Yiming Li, Tongqing Zhai, Baoyuan Wu, Yong Jiang, Zhifeng Li, and Shutao Xia. 2020. Rethinking the trigger of backdoor attack. arXiv preprint\narXiv:2004.04692 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.04692",
                        "Citation Paper Title": "Title:Rethinking the Trigger of Backdoor Attack",
                        "Citation Paper Abstract": "Abstract:Backdoor attack intends to inject hidden backdoor into the deep neural networks (DNNs), such that the prediction of the infected model will be maliciously changed if the hidden backdoor is activated by the attacker-defined trigger, while it performs well on benign samples. Currently, most of existing backdoor attacks adopted the setting of \\emph{static} trigger, $i.e.,$ triggers across the training and testing images follow the same appearance and are located in the same area. In this paper, we revisit this attack paradigm by analyzing the characteristics of the static trigger. We demonstrate that such an attack paradigm is vulnerable when the trigger in testing images is not consistent with the one used for training. We further explore how to utilize this property for backdoor defense, and discuss how to alleviate such vulnerability of existing attacks.",
                        "Citation Paper Authors": "Authors:Yiming Li, Tongqing Zhai, Baoyuan Wu, Yong Jiang, Zhifeng Li, Shutao Xia"
                    }
                },
                {
                    "Sentence ID": 190,
                    "Sentence": ". Specifically, a more stealthier way by target data corruption without label poisoning\nwas proposed and evaluated against convolution neural network (CNN) model. In the work by Xiang et al. ",
                    "Citation Text": "Zhen Xiang, David J Miller, and George Kesidis. 2020. Detection of backdoors in trained classifiers without access to the training set. IEEE\nTransactions on Neural Networks and Learning Systems (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.10498",
                        "Citation Paper Title": "Title:Detection of Backdoors in Trained Classifiers Without Access to the Training Set",
                        "Citation Paper Abstract": "Abstract:Recently, a special type of data poisoning (DP) attack targeting Deep Neural Network (DNN) classifiers, known as a backdoor, was proposed. These attacks do not seek to degrade classification accuracy, but rather to have the classifier learn to classify to a target class whenever the backdoor pattern is present in a test example. Launching backdoor attacks does not require knowledge of the classifier or its training process - it only needs the ability to poison the training set with (a sufficient number of) exemplars containing a sufficiently strong backdoor pattern (labeled with the target class). Here we address post-training detection of backdoor attacks in DNN image classifiers, seldom considered in existing works, wherein the defender does not have access to the poisoned training set, but only to the trained classifier itself, as well as to clean examples from the classification domain. This is an important scenario because a trained classifier may be the basis of e.g. a phone app that will be shared with many users. Detecting backdoors post-training may thus reveal a widespread attack. We propose a purely unsupervised anomaly detection (AD) defense against imperceptible backdoor attacks that: i) detects whether the trained DNN has been backdoor-attacked; ii) infers the source and target classes involved in a detected attack; iii) we even demonstrate it is possible to accurately estimate the backdoor pattern. We test our AD approach, in comparison with alternative defenses, for several backdoor patterns, data sets, and attack settings and demonstrate its favorability. Our defense essentially requires setting a single hyperparameter (the detection threshold), which can e.g. be chosen to fix the system's false positive rate.",
                        "Citation Paper Authors": "Authors:Zhen Xiang, David J. Miller, George Kesidis"
                    }
                },
                {
                    "Sentence ID": 55,
                    "Sentence": "Security&Privacy Machine learning # # #\nHe et al. 2020 ",
                    "Citation Text": "Yingzhe He, Guozhu Meng, Kai Chen, Xingbo Hu, and Jinwen He. 2020. Towards Security Threats of Deep Learning Systems: A Survey. IEEE\nTransactions on Software Engineering (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.12562",
                        "Citation Paper Title": "Title:Towards Security Threats of Deep Learning Systems: A Survey",
                        "Citation Paper Abstract": "Abstract:Deep learning has gained tremendous success and great popularity in the past few years. However, deep learning systems are suffering several inherent weaknesses, which can threaten the security of learning models. Deep learning's wide use further magnifies the impact and consequences. To this end, lots of research has been conducted with the purpose of exhaustively identifying intrinsic weaknesses and subsequently proposing feasible mitigation. Yet few are clear about how these weaknesses are incurred and how effective these attack approaches are in assaulting deep learning. In order to unveil the security weaknesses and aid in the development of a robust deep learning system, we undertake an investigation on attacks towards deep learning, and analyze these attacks to conclude some findings in multiple views. In particular, we focus on four types of attacks associated with security threats of deep learning: model extraction attack, model inversion attack, poisoning attack and adversarial attack. For each type of attack, we construct its essential workflow as well as adversary capabilities and attack goals. Pivot metrics are devised for comparing the attack approaches, by which we perform quantitative and qualitative analyses. From the analysis, we have identified significant and indispensable factors in an attack vector, e.g., how to reduce queries to target models, what distance should be used for measuring perturbation. We shed light on 18 findings covering these approaches' merits and demerits, success probability, deployment complexity and prospects. Moreover, we discuss other potential security weaknesses and possible mitigation which can inspire relevant research in this area.",
                        "Citation Paper Authors": "Authors:Yingzhe He, Guozhu Meng, Kai Chen, Xingbo Hu, Jinwen He"
                    }
                },
                {
                    "Sentence ID": 92,
                    "Sentence": "Quality assurance AI-based systems ! # #\nLiu et al. 2021 ",
                    "Citation Text": "Bo Liu, Ming Ding, Sina Shaham, Wenny Rahayu, Farhad Farokhi, and Zihuai Lin. 2021. When machine learning meets privacy: A survey and\noutlook. ACM Computing Surveys (CSUR) 54, 2 (2021), 1\u201336.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.11819",
                        "Citation Paper Title": "Title:When Machine Learning Meets Privacy: A Survey and Outlook",
                        "Citation Paper Abstract": "Abstract:The newly emerged machine learning (e.g. deep learning) methods have become a strong driving force to revolutionize a wide range of industries, such as smart healthcare, financial technology, and surveillance systems. Meanwhile, privacy has emerged as a big concern in this machine learning-based artificial intelligence era. It is important to note that the problem of privacy preservation in the context of machine learning is quite different from that in traditional data privacy protection, as machine learning can act as both friend and foe. Currently, the work on the preservation of privacy and machine learning (ML) is still in an infancy stage, as most existing solutions only focus on privacy problems during the machine learning process. Therefore, a comprehensive study on the privacy preservation problems and machine learning is required. This paper surveys the state of the art in privacy issues and solutions for machine learning. The survey covers three categories of interactions between privacy and machine learning: (i) private machine learning, (ii) machine learning aided privacy protection, and (iii) machine learning-based privacy attack and corresponding protection schemes. The current research progress in each category is reviewed and the key challenges are identified. Finally, based on our in-depth analysis of the area of privacy and machine learning, we point out future research directions in this field.",
                        "Citation Paper Authors": "Authors:Bo Liu, Ming Ding, Sina Shaham, Wenny Rahayu, Farhad Farokhi, Zihuai Lin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.13232v4": {
            "Paper Title": "Categorical composable cryptography: extended version",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.05724v2": {
            "Paper Title": "Concealing Sensitive Samples against Gradient Leakage in Federated\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.11003v4": {
            "Paper Title": "Differentially private inference via noisy optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.10823v2": {
            "Paper Title": "A Sealed-bid Auction with Fund Binding: Preventing Maximum Bidding Price\n  Leakage",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.01559v4": {
            "Paper Title": "Smoothed Differential Privacy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.07063v3": {
            "Paper Title": "How to Backdoor HyperNetwork in Personalized Federated Learning?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.08162v2": {
            "Paper Title": "Single Squaring Verifiable Delay Function from Time-lock Puzzle in the\n  Group of Known Order",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.04688v4": {
            "Paper Title": "BAFFLE: Hiding Backdoors in Offline Reinforcement Learning Datasets",
            "Sentences": [
                {
                    "Sentence ID": 76,
                    "Sentence": "use imitation learning to inject backdoors\nto agents in two-player competitive games ",
                    "Citation Text": "T. Bansal, J. Pachocki, S. Sidor, I. Sutskever, and I. Mordatch,\n\u201cEmergent complexity via multi-agent competition,\u201d in International\nConference on Learning Representations , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.03748",
                        "Citation Paper Title": "Title:Emergent Complexity via Multi-Agent Competition",
                        "Citation Paper Abstract": "Abstract:Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment. This suggests that a highly capable agent requires a complex environment for training. In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself. We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty. This work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: this https URL",
                        "Citation Paper Authors": "Authors:Trapit Bansal, Jakub Pachocki, Szymon Sidor, Ilya Sutskever, Igor Mordatch"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": ". This has inspired\nthe development of the offline RL ",
                    "Citation Text": "S. Levine, A. Kumar, G. Tucker, and J. Fu, \u201cOffline reinforcement\nlearning: Tutorial, review, and perspectives on open problems,\u201d arXiv\npreprint arXiv:2005.01643 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.01643",
                        "Citation Paper Title": "Title:Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems",
                        "Citation Paper Abstract": "Abstract:In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.",
                        "Citation Paper Authors": "Authors:Sergey Levine, Aviral Kumar, George Tucker, Justin Fu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.04223v2": {
            "Paper Title": "Vicious Classifiers: Data Reconstruction Attack at Inference Time",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": "There are several attacks on data privacy in ML, such as\nproperty inference ",
                    "Citation Text": "Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly\nShmatikov. 2019. Exploiting Unintended Feature Leakage in Collabo-\nrative Learning. In IEEE Symposium on Security and Privacy (S&P) .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.04049",
                        "Citation Paper Title": "Title:Exploiting Unintended Feature Leakage in Collaborative Learning",
                        "Citation Paper Abstract": "Abstract:Collaborative machine learning and related techniques such as federated learning allow multiple participants, each with his own training dataset, to build a joint model by training locally and periodically exchanging model updates. We demonstrate that these updates leak unintended information about participants' training data and develop passive and active inference attacks to exploit this leakage. First, we show that an adversarial participant can infer the presence of exact data points -- for example, specific locations -- in others' training data (i.e., membership inference). Then, we show how this adversary can infer properties that hold only for a subset of the training data and are independent of the properties that the joint model aims to capture. For example, he can infer when a specific person first appears in the photos used to train a binary gender classifier. We evaluate our attacks on a variety of tasks, datasets, and learning configurations, analyze their limitations, and discuss possible defenses.",
                        "Citation Paper Authors": "Authors:Luca Melis, Congzheng Song, Emiliano De Cristofaro, Vitaly Shmatikov"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.07876v2": {
            "Paper Title": "Control, Confidentiality, and the Right to be Forgotten",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.02003v2": {
            "Paper Title": "Bayesian Learning with Information Gain Provably Bounds Risk for a\n  Robust Adversarial Defense",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.08401v4": {
            "Paper Title": "Is decentralized finance actually decentralized? A social network\n  analysis of the Aave protocol on the Ethereum blockchain",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.06186v2": {
            "Paper Title": "Gotcha: Real-Time Video Deepfake Detection via Challenge-Response",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.14756v2": {
            "Paper Title": "Differentially Private Algorithms for Graphs Under Continual Observation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.02541v2": {
            "Paper Title": "PCPT and ACPT: Copyright Protection and Traceability Scheme for DNN\n  Models",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": "used an additional \nanti-piracy conversion module to verify the legitimacy of users, providing authorization controls for trained DNNs so that only \nauthorized users can use them correctly. Fan et al. ",
                    "Citation Text": "L. Fan, K. W. Ng, and C. S. Chan, \"Rethinking deep neu ral network ownership verification: Embedding passports to \ndefeat ambiguity attacks,\" Advances in Neural Information Processing Systems, vol. 32, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.07830",
                        "Citation Paper Title": "Title:[Extended version] Rethinking Deep Neural Network Ownership Verification: Embedding Passports to Defeat Ambiguity Attacks",
                        "Citation Paper Abstract": "Abstract:With substantial amount of time, resources and human (team) efforts invested to explore and develop successful deep neural networks (DNN), there emerges an urgent need to protect these inventions from being illegally copied, redistributed, or abused without respecting the intellectual properties of legitimate owners. Following recent progresses along this line, we investigate a number of watermark-based DNN ownership verification methods in the face of ambiguity attacks, which aim to cast doubts on the ownership verification by forging counterfeit watermarks. It is shown that ambiguity attacks pose serious threats to existing DNN watermarking methods. As remedies to the above-mentioned loophole, this paper proposes novel passport-based DNN ownership verification schemes which are both robust to network modifications and resilient to ambiguity attacks. The gist of embedding digital passports is to design and train DNN models in a way such that, the DNN inference performance of an original task will be significantly deteriorated due to forged passports. In other words, genuine passports are not only verified by looking for the predefined signatures, but also reasserted by the unyielding DNN model inference performances. Extensive experimental results justify the effectiveness of the proposed passport-based DNN ownership verification schemes. Code and models are available at this https URL",
                        "Citation Paper Authors": "Authors:Lixin Fan, Kam Woh Ng, Chee Seng Chan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.10085v4": {
            "Paper Title": "Mate! Are You Really Aware? An Explainability-Guided Testing Framework\n  for Robustness of Malware Detectors",
            "Sentences": [
                {
                    "Sentence ID": 33,
                    "Sentence": ", a detection model can be improved by remov-\ning important features from the training phase, where the impor-\ntant features refer to the ones that are highly contributing to the\nmodel prediction accuracy. Shapley Additive Global importancE\n(SAGE) ",
                    "Citation Text": "Ian Covert, Scott M Lundberg, and Su-In Lee. 2020. Understanding global feature\ncontributions with additive importance measures. Advances in Neural Information\nProcessing Systems 33 (2020), 17212\u201317223.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.00668",
                        "Citation Paper Title": "Title:Understanding Global Feature Contributions With Additive Importance Measures",
                        "Citation Paper Abstract": "Abstract:Understanding the inner workings of complex machine learning models is a long-standing problem and most recent research has focused on local interpretability. To assess the role of individual input features in a global sense, we explore the perspective of defining feature importance through the predictive power associated with each feature. We introduce two notions of predictive power (model-based and universal) and formalize this approach with a framework of additive importance measures, which unifies numerous methods in the literature. We then propose SAGE, a model-agnostic method that quantifies predictive power while accounting for feature interactions. Our experiments show that SAGE can be calculated efficiently and that it assigns more accurate importance values than other methods.",
                        "Citation Paper Authors": "Authors:Ian Covert, Scott Lundberg, Su-In Lee"
                    }
                },
                {
                    "Sentence ID": 60,
                    "Sentence": "(based on the coalitional game theory concept of\nShapley values) as prerequisite knowledge to bootstrap the test-\ning framework. The SHAP framework subsumes several earliermodel explanation techniques together, including LIME ",
                    "Citation Text": "Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \u201cWhy should I\ntrust you?\u201d: Explaining the predictions of any classifier. In 22nd ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.04938",
                        "Citation Paper Title": "Title:\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier",
                        "Citation Paper Abstract": "Abstract:Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.",
                        "Citation Paper Authors": "Authors:Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.12370v5": {
            "Paper Title": "COOKIEGRAPH: Understanding and Detecting First-Party Tracking Cookies",
            "Sentences": [
                {
                    "Sentence ID": 79,
                    "Sentence": ", and machine-learning-based\ntracker detection approaches proposed by prior research, e.g., ",
                    "Citation Text": "Sandra Siby, Umar Iqbal, Steven Englehardt, Zubair Shafiq, and Carmela Tron-\ncoso. 2022. WebGraph: Capturing Advertising and Tracking Information Flows\nfor Robust Blocking. In 31st USENIX Security Symposium (USENIX Security 22) .\nUSENIX Association.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.11309",
                        "Citation Paper Title": "Title:WebGraph: Capturing Advertising and Tracking Information Flows for Robust Blocking",
                        "Citation Paper Abstract": "Abstract:Millions of web users directly depend on ad and tracker blocking tools to protect their privacy. However, existing ad and tracker blockers fall short because of their reliance on trivially susceptible advertising and tracking content. In this paper, we first demonstrate that the state-of-the-art machine learning based ad and tracker blockers, such as AdGraph, are susceptible to adversarial evasions deployed in real-world. Second, we introduce WebGraph, the first graph-based machine learning blocker that detects ads and trackers based on their action rather than their content. By building features around the actions that are fundamental to advertising and tracking - storing an identifier in the browser, or sharing an identifier with another tracker - WebGraph performs nearly as well as prior approaches, but is significantly more robust to adversarial evasions. In particular, we show that WebGraph achieves comparable accuracy to AdGraph, while significantly decreasing the success rate of an adversary from near-perfect under AdGraph to around 8% under WebGraph. Finally, we show that WebGraph remains robust to a more sophisticated adversary that uses evasion techniques beyond those currently deployed on the web.",
                        "Citation Paper Authors": "Authors:Sandra Siby, Umar Iqbal, Steven Englehardt, Zubair Shafiq, Carmela Troncoso"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.04686v3": {
            "Paper Title": "Directional Privacy for Deep Learning",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": ". We take an empirical approach to calibrating the re-\nspective \u03f5Gand\u03f5V: we measure how D IRDP-SGD pre-\nvents MIA-R and gradient-based reconstruction attacks at\nequivalent levels of utility.\nWe use the training and test sets from Fashion-MNIST ",
                    "Citation Text": "Xiao, H., Rasul, K., and V ollgraf, R. (2017).\nFashion-mnist: a novel image dataset for bench-\nmarking machine learning algorithms. ArXiv ,\nabs/1708.07747.\n10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.07747",
                        "Citation Paper Title": "Title:Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms",
                        "Citation Paper Abstract": "Abstract:We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at this https URL",
                        "Citation Paper Authors": "Authors:Han Xiao, Kashif Rasul, Roland Vollgraf"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "presented an enhanced MIA that they\ncharacterise as \u2018population-based\u2019, using reference mod-\nels to achieve a significantly higher power (true positive\nrate) for any (false positive rate) error, at a lower compu-\ntation cost. Jayaraman and Evans ",
                    "Citation Text": "Jayaraman, B. and Evans, D. (2019). Evaluating\ndifferentially private machine learning in practice. In\nHeninger, N. and Traynor, P., editors, 28th USENIX\nSecurity Symposium, USENIX Security 2019, Santa\nClara, CA, USA, August 14-16, 2019 , pages 1895\u2013\n1912. USENIX Association.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.08874",
                        "Citation Paper Title": "Title:Evaluating Differentially Private Machine Learning in Practice",
                        "Citation Paper Abstract": "Abstract:Differential privacy is a strong notion for privacy that can be used to prove formal guarantees, in terms of a privacy budget, $\\epsilon$, about how much information is leaked by a mechanism. However, implementations of privacy-preserving machine learning often select large values of $\\epsilon$ in order to get acceptable utility of the model, with little understanding of the impact of such choices on meaningful privacy. Moreover, in scenarios where iterative learning procedures are used, differential privacy variants that offer tighter analyses are used which appear to reduce the needed privacy budget but present poorly understood trade-offs between privacy and utility. In this paper, we quantify the impact of these choices on privacy in experiments with logistic regression and neural network models. Our main finding is that there is a huge gap between the upper bounds on privacy loss that can be guaranteed, even with advanced mechanisms, and the effective privacy loss that can be measured using current inference attacks. Current mechanisms for differentially private machine learning rarely offer acceptable utility-privacy trade-offs with guarantees for complex learning tasks: settings that provide limited accuracy loss provide meaningless privacy guarantees, and settings that provide strong privacy guarantees result in useless models. Code for the experiments can be found here: this https URL",
                        "Citation Paper Authors": "Authors:Bargav Jayaraman, David Evans"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "in which the user re-\nveals an approximate location to receive location-based\nservices.\nIn the area of NLP, Fernandes et al. ",
                    "Citation Text": "Fernandes, N., Dras, M., and McIver, A. (2019). Gen-\neralised differential privacy for text document process-\ning. In Nielson, F. and Sands, D., editors, Principles\nof Security and Trust - 8th International Conference,\nPOST 2019, Held as Part of the European Joint Con-\nferences on Theory and Practice of Software, ETAPS\n2019, Prague, Czech Republic, April 6-11, 2019, Pro-\nceedings , volume 11426 of Lecture Notes in Computer\nScience , pages 123\u2013148. Springer.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.10256",
                        "Citation Paper Title": "Title:Generalised Differential Privacy for Text Document Processing",
                        "Citation Paper Abstract": "Abstract:We address the problem of how to \"obfuscate\" texts by removing stylistic clues which can identify authorship, whilst preserving (as much as possible) the content of the text. In this paper we combine ideas from \"generalised differential privacy\" and machine learning techniques for text processing to model privacy for text documents. We define a privacy mechanism that operates at the level of text documents represented as \"bags-of-words\" - these representations are typical in machine learning and contain sufficient information to carry out many kinds of classification tasks including topic identification and authorship attribution (of the original documents). We show that our mechanism satisfies privacy with respect to a metric for semantic similarity, thereby providing a balance between utility, defined by the semantic content of texts, with the obfuscation of stylistic clues. We demonstrate our implementation on a \"fan fiction\" dataset, confirming that it is indeed possible to disguise writing style effectively whilst preserving enough information and variation for accurate content classification tasks.",
                        "Citation Paper Authors": "Authors:Natasha Fernandes, Mark Dras, Annabelle McIver"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "and also known as generalised\nDP,d-privacy, and dX-privacy. It was first applied to the\nproblem of geo-location privacy ",
                    "Citation Text": "Andr\u00e9s, M. E., Bordenabe, N. E., Chatzikoko-\nlakis, K., and Palamidessi, C. (2013). Geo-\nindistinguishability: Differential privacy for location-\nbased systems. In Proceedings of the 2013 ACM\nSIGSAC conference on Computer & communications\nsecurity , pages 901\u2013914. ACM.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1212.1984",
                        "Citation Paper Title": "Title:Geo-Indistinguishability: Differential Privacy for Location-Based Systems",
                        "Citation Paper Abstract": "Abstract:The growing popularity of location-based systems, allowing unknown/untrusted servers to easily collect huge amounts of information regarding users' location, has recently started raising serious privacy concerns. In this paper we study geo-indistinguishability, a formal notion of privacy for location-based systems that protects the user's exact location, while allowing approximate information - typically needed to obtain a certain desired service - to be released. Our privacy definition formalizes the intuitive notion of protecting the user's location within a radius r with a level of privacy that depends on r, and corresponds to a generalized version of the well-known concept of differential privacy. Furthermore, we present a perturbation technique for achieving geo-indistinguishability by adding controlled random noise to the user's location. We demonstrate the applicability of our technique on a LBS application. Finally, we compare our mechanism with other ones in the literature. It turns our that our mechanism offers the best privacy guarantees, for the same utility, among all those which do not depend on the prior.",
                        "Citation Paper Authors": "Authors:Miguel E. Andr\u00e9s, Nicol\u00e1s E. Bordenabe, Konstantinos Chatzikokolakis, Catuscia Palamidessi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.04385v2": {
            "Paper Title": "Why we couldn't prove SETH hardness of the Closest Vector Problem for\n  even norms!",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.00542v4": {
            "Paper Title": "Shared Certificates for Neural Network Verification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.10209v5": {
            "Paper Title": "Degree-Preserving Randomized Response for Graph Neural Networks under\n  Local Differential Privacy",
            "Sentences": [
                {
                    "Sentence ID": 57,
                    "Sentence": ". Some\nof the other GNN algorithms are less efficient; see ",
                    "Citation Text": "Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi\nZhang, and Philip S. Yu. A comprehensive survey on graph neural\nnetworks. CoRR , 1901.00596, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.00596",
                        "Citation Paper Title": "Title:A Comprehensive Survey on Graph Neural Networks",
                        "Citation Paper Abstract": "Abstract:Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders, and spatial-temporal graph neural networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes, benchmark data sets, and model evaluation of graph neural networks. Finally, we propose potential research directions in this rapidly growing field.",
                        "Citation Paper Authors": "Authors:Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, Philip S. Yu"
                    }
                },
                {
                    "Sentence ID": 50,
                    "Sentence": "also proposes an algorithm to estimate the clustering\ncoefficient based on Warner\u2019s RR, claiming that the clustering coefficient is\nuseful for generating a synthetic graph based on the graph model BTER (Block\nTwo-Level Erd \u00a8os-R \u00b4enyi) ",
                    "Citation Text": "Comandur Seshadhri, Tamara G. Kolda, and Ali Pinar. Community\nstructure and scale-free collections of erd \u00a8os-r\u00b4enyi graphs. Physical\nReview E , 85(5):1\u20139, 2012.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1112.3644",
                        "Citation Paper Title": "Title:Community structure and scale-free collections of Erd\u00f6s-R\u00e9nyi graphs",
                        "Citation Paper Abstract": "Abstract:Community structure plays a significant role in the analysis of social networks and similar graphs, yet this structure is little understood and not well captured by most models. We formally define a community to be a subgraph that is internally highly connected and has no deeper substructure. We use tools of combinatorics to show that any such community must contain a dense Erd\u00f6s-R\u00e9nyi (ER) subgraph. Based on mathematical arguments, we hypothesize that any graph with a heavy-tailed degree distribution and community structure must contain a scale free collection of dense ER subgraphs. These theoretical observations corroborate well with empirical evidence. From this, we propose the Block Two-Level Erd\u00f6s-R\u00e9nyi (BTER) model, and demonstrate that it accurately captures the observable properties of many real-world social networks.",
                        "Citation Paper Authors": "Authors:C. Seshadhri, Tamara G. Kolda, Ali Pinar"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.03234v3": {
            "Paper Title": "Deep Joint Source-Channel Coding for Image Transmission with Visual\n  Protection",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": ". From then on, DJSCC attracted increasing interest,\nespecially for image compression and transmission. Compared\nto the SSCC scheme (e.g., JPEG/JPEG2000 for image coding\nand LDPC for channel coding), the DJSCC scheme designed\nin ",
                    "Citation Text": "E. Bourtsoulatze, D. B. Kurka, and D. G \u00a8und\u00a8uz, \u201cDeep joint source-\nchannel coding for wireless image transmission,\u201d IEEE Transactions on\nCognitive Communications and Networking , vol. 5, no. 3, pp. 567\u2013579,\n2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.01733",
                        "Citation Paper Title": "Title:Deep Joint Source-Channel Coding for Wireless Image Transmission",
                        "Citation Paper Abstract": "Abstract:We propose a joint source and channel coding (JSCC) technique for wireless image transmission that does not rely on explicit codes for either compression or error correction; instead, it directly maps the image pixel values to the complex-valued channel input symbols. We parameterize the encoder and decoder functions by two convolutional neural networks (CNNs), which are trained jointly, and can be considered as an autoencoder with a non-trainable layer in the middle that represents the noisy communication channel. Our results show that the proposed deep JSCC scheme outperforms digital transmission concatenating JPEG or JPEG2000 compression with a capacity achieving channel code at low signal-to-noise ratio (SNR) and channel bandwidth values in the presence of additive white Gaussian noise (AWGN). More strikingly, deep JSCC does not suffer from the ``cliff effect'', and it provides a graceful performance degradation as the channel SNR varies with respect to the SNR value assumed during training. In the case of a slow Rayleigh fading channel, deep JSCC learns noise resilient coded representations and significantly outperforms separation-based digital communication at all SNR and channel bandwidth values.",
                        "Citation Paper Authors": "Authors:Eirina Bourtsoulatze, David Burth Kurka, Deniz Gunduz"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": ", are designed to assess the visual security\nof the image. Here, we employ a feature extraction network\nto measure visual security. The feature extraction method has\nbeen used successfully to measure the similarity between two\nimages ",
                    "Citation Text": "J. Johnson, A. Alahi, and L. Fei-Fei, \u201cPerceptual losses for real-\ntime style transfer and super-resolution,\u201d in European Conference on\nComputer Vision . Springer, 2016, pp. 694\u2013711.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.08155",
                        "Citation Paper Title": "Title:Perceptual Losses for Real-Time Style Transfer and Super-Resolution",
                        "Citation Paper Abstract": "Abstract:We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a \\emph{per-pixel} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing \\emph{perceptual} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.",
                        "Citation Paper Authors": "Authors:Justin Johnson, Alexandre Alahi, Li Fei-Fei"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.04132v2": {
            "Paper Title": "Stochastic Coded Federated Learning: Theoretical Analysis and Incentive\n  Mechanism Design",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.05197v3": {
            "Paper Title": "Formal Model-Driven Analysis of Resilience of GossipSub to Attacks from\n  Misbehaving Peers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.10161v2": {
            "Paper Title": "MUDGUARD: Taming Malicious Majorities in Federated Learning using\n  Privacy-Preserving Byzantine-Robust Clustering",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": ".\nWe see that \u03b1has a crucial influence on clustering accuracy.\nAccording to the conclusion of Bhagoji et al. ",
                    "Citation Text": "A. N. Bhagoji, S. Chakraborty, P. Mittal, and S. Calo, \u201cAnalyzing\nfederated learning through an adversarial lens,\u201d in ICML , 2019, pp.\n634\u2013643.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.12470",
                        "Citation Paper Title": "Title:Analyzing Federated Learning through an Adversarial Lens",
                        "Citation Paper Abstract": "Abstract:Federated learning distributes model training among a multitude of agents, who, guided by privacy concerns, perform training using their local data but share only model parameter updates, for iterative aggregation at the server. In this work, we explore the threat of model poisoning attacks on federated learning initiated by a single, non-colluding malicious agent where the adversarial objective is to cause the model to misclassify a set of chosen inputs with high confidence. We explore a number of strategies to carry out this attack, starting with simple boosting of the malicious agent's update to overcome the effects of other agents' updates. To increase attack stealth, we propose an alternating minimization strategy, which alternately optimizes for the training loss and the adversarial objective. We follow up by using parameter estimation for the benign agents' updates to improve on attack success. Finally, we use a suite of interpretability techniques to generate visual explanations of model decisions for both benign and malicious models and show that the explanations are nearly visually indistinguishable. Our results indicate that even a highly constrained adversary can carry out model poisoning attacks while simultaneously maintaining stealth, thus highlighting the vulnerability of the federated learning setting and the need to develop effective defense strategies.",
                        "Citation Paper Authors": "Authors:Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, Seraphin Calo"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.15785v5": {
            "Paper Title": "Supply Chain Characteristics as Predictors of Cyber Risk: A\n  Machine-Learning Assessment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.13358v4": {
            "Paper Title": "Self-Managing DRAM: A Low-Cost Framework for Enabling Autonomous and\n  Efficient in-DRAM Operations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.07234v4": {
            "Paper Title": "Brownian Noise Reduction: Maximizing Privacy Subject to Accuracy\n  Constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.05359v4": {
            "Paper Title": "Blades: A Unified Benchmark Suite for Byzantine Attacks and Defenses in\n  Federated Learning",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": ", are worth exploring, our study primarily focuses on\nAGRs. This is because most existing studies predominantly\nconsider AGR-based defenses, and we specifically examine\nMedian ",
                    "Citation Text": "D. Yin, Y . Chen, R. Kannan, and P. Bartlett, \u201cByzantine-robust distributed\nlearning: Towards optimal statistical rates,\u201d in International Conference\non Machine Learning . PMLR, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.01498",
                        "Citation Paper Title": "Title:Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates",
                        "Citation Paper Abstract": "Abstract:In large-scale distributed learning, security issues have become increasingly important. Particularly in a decentralized environment, some computing units may behave abnormally, or even exhibit Byzantine failures -- arbitrary and potentially adversarial behavior. In this paper, we develop distributed learning algorithms that are provably robust against such failures, with a focus on achieving optimal statistical performance. A main result of this work is a sharp analysis of two robust distributed gradient descent algorithms based on median and trimmed mean operations, respectively. We prove statistical error rates for three kinds of population loss functions: strongly convex, non-strongly convex, and smooth non-convex. In particular, these algorithms are shown to achieve order-optimal statistical error rates for strongly convex losses. To achieve better communication efficiency, we further propose a median-based distributed algorithm that is provably robust, and uses only one communication round. For strongly convex quadratic loss, we show that this algorithm achieves the same optimal error rate as the robust distributed gradient descent algorithms.",
                        "Citation Paper Authors": "Authors:Dong Yin, Yudong Chen, Kannan Ramchandran, Peter Bartlett"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": ". While other research directions, such as trust-\nbased strategies ",
                    "Citation Text": "X. Cao, M. Fang, J. Liu, and N. Z. Gong, \u201cFltrust: Byzantine-robust\nfederated learning via trust bootstrapping,\u201d in ISOC Network and\nDistributed System Security Symposium (NDSS) , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.13995",
                        "Citation Paper Title": "Title:FLTrust: Byzantine-robust Federated Learning via Trust Bootstrapping",
                        "Citation Paper Abstract": "Abstract:Byzantine-robust federated learning aims to enable a service provider to learn an accurate global model when a bounded number of clients are malicious. The key idea of existing Byzantine-robust federated learning methods is that the service provider performs statistical analysis among the clients' local model updates and removes suspicious ones, before aggregating them to update the global model. However, malicious clients can still corrupt the global models in these methods via sending carefully crafted local model updates to the service provider. The fundamental reason is that there is no root of trust in existing federated learning methods.\nIn this work, we bridge the gap via proposing FLTrust, a new federated learning method in which the service provider itself bootstraps trust. In particular, the service provider itself collects a clean small training dataset (called root dataset) for the learning task and the service provider maintains a model (called server model) based on it to bootstrap trust. In each iteration, the service provider first assigns a trust score to each local model update from the clients, where a local model update has a lower trust score if its direction deviates more from the direction of the server model update. Then, the service provider normalizes the magnitudes of the local model updates such that they lie in the same hyper-sphere as the server model update in the vector space. Our normalization limits the impact of malicious local model updates with large magnitudes. Finally, the service provider computes the average of the normalized local model updates weighted by their trust scores as a global model update, which is used to update the global model. Our extensive evaluations on six datasets from different domains show that our FLTrust is secure against both existing attacks and strong adaptive attacks.",
                        "Citation Paper Authors": "Authors:Xiaoyu Cao, Minghong Fang, Jia Liu, Neil Zhenqiang Gong"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ".\nB. Scope of our work: Byzantine Attacks and Defenses in FL\nByzantine attacks pose a significant threat to FL due to its\ndistributed optimization nature ",
                    "Citation Text": "L. Lyu, H. Yu, and Q. Yang, \u201cThreats to federated learning: A survey,\u201d\narXiv preprint arXiv:2003.02133 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.02133",
                        "Citation Paper Title": "Title:Threats to Federated Learning: A Survey",
                        "Citation Paper Abstract": "Abstract:With the emergence of data silos and popular privacy awareness, the traditional centralized approach of training artificial intelligence (AI) models is facing strong challenges. Federated learning (FL) has recently emerged as a promising solution under this new reality. Existing FL protocol design has been shown to exhibit vulnerabilities which can be exploited by adversaries both within and without the system to compromise data privacy. It is thus of paramount importance to make FL system designers to be aware of the implications of future FL algorithm design on privacy-preservation. Currently, there is no survey on this topic. In this paper, we bridge this important gap in FL literature. By providing a concise introduction to the concept of FL, and a unique taxonomy covering threat models and two major attacks on FL: 1) poisoning attacks and 2) inference attacks, this paper provides an accessible review of this important topic. We highlight the intuitions, key techniques as well as fundamental assumptions adopted by various attacks, and discuss promising future research directions towards more robust privacy preservation in FL.",
                        "Citation Paper Authors": "Authors:Lingjuan Lyu, Han Yu, Qiang Yang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.08443v3": {
            "Paper Title": "On the (In)security of Peer-to-Peer Decentralized Machine Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.16082v2": {
            "Paper Title": "Safeguarding the Unseen: a Study on Data Privacy in DeFi Protocols",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.03080v2": {
            "Paper Title": "Straggler-Resilient Differentially-Private Decentralized Learning",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": "and memory access, has\nbeen broadly studied in the literature. The ignoring-stragglers\nstrategy , i.e., ignoring results from the slowest nodes, see, e.g., ",
                    "Citation Text": "G. Xiong, G. Yan, R. Singh, and J. Li, \u201cStraggler-resilient dis-\ntributed machine learning with dynamic backup workers,\u201d Feb. 2021,\narXiv:2102.06280v1 [cs.LG].",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.06280",
                        "Citation Paper Title": "Title:Straggler-Resilient Distributed Machine Learning with Dynamic Backup Workers",
                        "Citation Paper Abstract": "Abstract:With the increasing demand for large-scale training of machine learning models, consensus-based distributed optimization methods have recently been advocated as alternatives to the popular parameter server framework. In this paradigm, each worker maintains a local estimate of the optimal parameter vector, and iteratively updates it by waiting and averaging all estimates obtained from its neighbors, and then corrects it on the basis of its local dataset. However, the synchronization phase can be time consuming due to the need to wait for \\textit{stragglers}, i.e., slower workers. An efficient way to mitigate this effect is to let each worker wait only for updates from the fastest neighbors before updating its local parameter. The remaining neighbors are called \\textit{backup workers.} To minimize the globally training time over the network, we propose a fully distributed algorithm to dynamically determine the number of backup workers for each worker. We show that our algorithm achieves a linear speedup for convergence (i.e., convergence performance increases linearly with respect to the number of workers). We conduct extensive experiments on MNIST and CIFAR-10 to verify our theoretical results.",
                        "Citation Paper Authors": "Authors:Guojun Xiong, Gang Yan, Rahul Singh, Jian Li"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.05211v4": {
            "Paper Title": "Parallel Private Retrieval of Merkle Proofs via Tree Colorings",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.12697v6": {
            "Paper Title": "Structural Causal Models Reveal Confounder Bias in Linear Program\n  Modelling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.09364v3": {
            "Paper Title": "Probabilistic Categorical Adversarial Attack & Adversarial Training",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.06776v3": {
            "Paper Title": "Unfolding Local Growth Rate Estimates for (Almost) Perfect Adversarial\n  Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.08907v2": {
            "Paper Title": "Releasing Graph Neural Networks with Differential Privacy Guarantees",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.06448v2": {
            "Paper Title": "Assessing Privacy Leakage in Synthetic 3-D PET Imaging using Transversal\n  GAN",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.04591v2": {
            "Paper Title": "Stronger Privacy Amplification by Shuffling for R\u00e9nyi and Approximate\n  Differential Privacy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.03576v2": {
            "Paper Title": "Robustness Evaluation of Deep Unsupervised Learning Algorithms for\n  Intrusion Detection Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.06236v3": {
            "Paper Title": "Differentially Private Kolmogorov-Smirnov-Type Tests",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.11850v2": {
            "Paper Title": "DYST (Did You See That?): An Amplified Covert Channel That Points To\n  Previously Seen Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.14889v4": {
            "Paper Title": "Perfectly Secure Steganography Using Minimum Entropy Coupling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.03392v5": {
            "Paper Title": "Federated Learning for Medical Applications: A Taxonomy, Current Trends,\n  Challenges, and Future Research Directions",
            "Sentences": [
                {
                    "Sentence ID": 213,
                    "Sentence": ". Contrastive learning,\nalso known as self-learning, is a pre-training procedure in\nwhich the model attempts to learn similar and distinct data\nsamples from an unlabeled data distribution ",
                    "Citation Text": "K. He, H. Fan, Y . Wu, S. Xie, and R. Girshick, \u201cMomentum contrast\nfor unsupervised visual representation learning,\u201d in Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition ,\n2020, pp. 9729\u20139738.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.05722",
                        "Citation Paper Title": "Title:Momentum Contrast for Unsupervised Visual Representation Learning",
                        "Citation Paper Abstract": "Abstract:We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.",
                        "Citation Paper Authors": "Authors:Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick"
                    }
                },
                {
                    "Sentence ID": 202,
                    "Sentence": ". A malicious client can\nmisclassify their input and influence the global model\non the server. This situation is highly undesirable in a\nmedical environment. Although some of the work in ",
                    "Citation Text": "M. Malekzadeh, B. Hasircioglu, N. Mital, K. Katarya, M. Ozfatura, and\nD. G \u00a8und\u00a8uz, \u201cDopamine: Differentially private secure federated learning\non medical data,\u201d in Proceedings of the Second AAAI Workshop on\nPrivacy-Preserving Artificial Intelligence, Virtual Worskhop , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.11693",
                        "Citation Paper Title": "Title:Dopamine: Differentially Private Federated Learning on Medical Data",
                        "Citation Paper Abstract": "Abstract:While rich medical datasets are hosted in hospitals distributed across the world, concerns on patients' privacy is a barrier against using such data to train deep neural networks (DNNs) for medical diagnostics. We propose Dopamine, a system to train DNNs on distributed datasets, which employs federated learning (FL) with differentially-private stochastic gradient descent (DPSGD), and, in combination with secure aggregation, can establish a better trade-off between differential privacy (DP) guarantee and DNN's accuracy than other approaches. Results on a diabetic retinopathy~(DR) task show that Dopamine provides a DP guarantee close to the centralized training counterpart, while achieving a better classification accuracy than FL with parallel DP where DPSGD is applied without coordination. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Mohammad Malekzadeh, Burak Hasircioglu, Nitish Mital, Kunal Katarya, Mehmet Emre Ozfatura, Deniz G\u00fcnd\u00fcz"
                    }
                },
                {
                    "Sentence ID": 201,
                    "Sentence": ".\nB. Security and Privacy\nSecurity remains a paramount concern in FL systems,\nespecially in medical contexts where privacy is a stringiest\nrequirement. FL\u2019s collaborative nature, with participation from\nmultiple clients, elevates susceptibility to security attacks,\nincluding model poisoning ",
                    "Citation Text": "A. N. Bhagoji, S. Chakraborty, P. Mittal, and S. Calo, \u201cAnalyzing\nfederated learning through an adversarial lens,\u201d in Proceedings of the\nInternational Conference on Machine Learning , 2019, pp. 634\u2013643.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.12470",
                        "Citation Paper Title": "Title:Analyzing Federated Learning through an Adversarial Lens",
                        "Citation Paper Abstract": "Abstract:Federated learning distributes model training among a multitude of agents, who, guided by privacy concerns, perform training using their local data but share only model parameter updates, for iterative aggregation at the server. In this work, we explore the threat of model poisoning attacks on federated learning initiated by a single, non-colluding malicious agent where the adversarial objective is to cause the model to misclassify a set of chosen inputs with high confidence. We explore a number of strategies to carry out this attack, starting with simple boosting of the malicious agent's update to overcome the effects of other agents' updates. To increase attack stealth, we propose an alternating minimization strategy, which alternately optimizes for the training loss and the adversarial objective. We follow up by using parameter estimation for the benign agents' updates to improve on attack success. Finally, we use a suite of interpretability techniques to generate visual explanations of model decisions for both benign and malicious models and show that the explanations are nearly visually indistinguishable. Our results indicate that even a highly constrained adversary can carry out model poisoning attacks while simultaneously maintaining stealth, thus highlighting the vulnerability of the federated learning setting and the need to develop effective defense strategies.",
                        "Citation Paper Authors": "Authors:Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, Seraphin Calo"
                    }
                },
                {
                    "Sentence ID": 196,
                    "Sentence": ". This\nmeans malicious clients can introduce a backdoor functionality\nthat compromises the underlying FL system during the training\nprocess of the global federated model ",
                    "Citation Text": "E. Bagdasaryan, A. Veit, Y . Hua, D. Estrin, and V . Shmatikov, \u201cHow\nto backdoor federated learning,\u201d in Proceedings of the International\nConference on Artificial Intelligence and Statistics , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.00459",
                        "Citation Paper Title": "Title:How To Backdoor Federated Learning",
                        "Citation Paper Abstract": "Abstract:Federated learning enables thousands of participants to construct a deep learning model without sharing their private training data with each other. For example, multiple smartphones can jointly train a next-word predictor for keyboards without revealing what individual users type. We demonstrate that any participant in federated learning can introduce hidden backdoor functionality into the joint global model, e.g., to ensure that an image classifier assigns an attacker-chosen label to images with certain features, or that a word predictor completes certain sentences with an attacker-chosen word.\nWe design and evaluate a new model-poisoning methodology based on model replacement. An attacker selected in a single round of federated learning can cause the global model to immediately reach 100% accuracy on the backdoor task. We evaluate the attack under different assumptions for the standard federated-learning tasks and show that it greatly outperforms data poisoning. Our generic constrain-and-scale technique also evades anomaly detection-based defenses by incorporating the evasion into the attacker's loss function during training.",
                        "Citation Paper Authors": "Authors:Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, Vitaly Shmatikov"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": ". The global server employs\nthis algorithm for aggregating the local parameters of diverse\nclients in each iteration ",
                    "Citation Text": "Y . Xia, D. Yang, W. Li, A. Myronenko, D. Xu, H. Obinata, H. Mori,\nP. An, S. Harmon, E. Turkbey, et al. , \u201cAuto-fedavg: Learnable federated\naveraging for multi-institutional medical image segmentation,\u201d arXiv\npreprint arXiv:2104.10195 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.10195",
                        "Citation Paper Title": "Title:Auto-FedAvg: Learnable Federated Averaging for Multi-Institutional Medical Image Segmentation",
                        "Citation Paper Abstract": "Abstract:Federated learning (FL) enables collaborative model training while preserving each participant's privacy, which is particularly beneficial to the medical field. FedAvg is a standard algorithm that uses fixed weights, often originating from the dataset sizes at each client, to aggregate the distributed learned models on a server during the FL process. However, non-identical data distribution across clients, known as the non-i.i.d problem in FL, could make this assumption for setting fixed aggregation weights sub-optimal. In this work, we design a new data-driven approach, namely Auto-FedAvg, where aggregation weights are dynamically adjusted, depending on data distributions across data silos and the current training progress of the models. We disentangle the parameter set into two parts, local model parameters and global aggregation parameters, and update them iteratively with a communication-efficient algorithm. We first show the validity of our approach by outperforming state-of-the-art FL methods for image recognition on a heterogeneous data split of CIFAR-10. Furthermore, we demonstrate our algorithm's effectiveness on two multi-institutional medical image analysis tasks, i.e., COVID-19 lesion segmentation in chest CT and pancreas segmentation in abdominal CT.",
                        "Citation Paper Authors": "Authors:Yingda Xia, Dong Yang, Wenqi Li, Andriy Myronenko, Daguang Xu, Hirofumi Obinata, Hitoshi Mori, Peng An, Stephanie Harmon, Evrim Turkbey, Baris Turkbey, Bradford Wood, Francesca Patella, Elvira Stellato, Gianpaolo Carrafiello, Anna Ierardi, Alan Yuille, Holger Roth"
                    }
                },
                {
                    "Sentence ID": 155,
                    "Sentence": "performed\na multicenter study on the whole prostate segmentation on\nthe axial T2- weighted MRI scans. They used the 3D Hybrid\nAnisotropic Hybrid Network ",
                    "Citation Text": "S. Liu, D. Xu, S. K. Zhou, O. Pauly, S. Grbic, T. Mertelmeier,\nJ. Wicklein, A. Jerebko, W. Cai, and D. Comaniciu, \u201c3d anisotropic\nhybrid network: Transferring convolutional features from 2d images to\n3d anisotropic volumes,\u201d in Proceedings of the International conference\non medical image computing and computer-assisted intervention , 2018,\npp. 851\u2013858.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.08580",
                        "Citation Paper Title": "Title:3D Anisotropic Hybrid Network: Transferring Convolutional Features from 2D Images to 3D Anisotropic Volumes",
                        "Citation Paper Abstract": "Abstract:While deep convolutional neural networks (CNN) have been successfully applied for 2D image analysis, it is still challenging to apply them to 3D anisotropic volumes, especially when the within-slice resolution is much higher than the between-slice resolution and when the amount of 3D volumes is relatively small. On one hand, direct learning of CNN with 3D convolution kernels suffers from the lack of data and likely ends up with poor generalization; insufficient GPU memory limits the model size or representational power. On the other hand, applying 2D CNN with generalizable features to 2D slices ignores between-slice information. Coupling 2D network with LSTM to further handle the between-slice information is not optimal due to the difficulty in LSTM learning. To overcome the above challenges, we propose a 3D Anisotropic Hybrid Network (AH-Net) that transfers convolutional features learned from 2D images to 3D anisotropic volumes. Such a transfer inherits the desired strong generalization capability for within-slice information while naturally exploiting between-slice information for more effective modelling. The focal loss is further utilized for more effective end-to-end learning. We experiment with the proposed 3D AH-Net on two different medical image analysis tasks, namely lesion detection from a Digital Breast Tomosynthesis volume, and liver and liver tumor segmentation from a Computed Tomography volume and obtain the state-of-the-art results.",
                        "Citation Paper Authors": "Authors:Siqi Liu, Daguang Xu, S. Kevin Zhou, Thomas Mertelmeier, Julia Wicklein, Anna Jerebko, Sasa Grbic, Olivier Pauly, Weidong Cai, Dorin Comaniciu"
                    }
                },
                {
                    "Sentence ID": 146,
                    "Sentence": ". An automatic brain\ntumour segmentation can significantly impact brain tumour\ndiagnosis and treatment. Li et al. ",
                    "Citation Text": "W. Li, F. Milletar `\u0131, D. Xu, N. Rieke, J. Hancox, W. Zhu, M. Baust,\nY . Cheng, S. Ourselin, M. J. Cardoso, et al. , \u201cPrivacy-preserving\nfederated brain tumour segmentation,\u201d in Proc. of the Intl. workshop\non machine learning in medical imaging , 2019, pp. 133\u2013141.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.00962",
                        "Citation Paper Title": "Title:Privacy-preserving Federated Brain Tumour Segmentation",
                        "Citation Paper Abstract": "Abstract:Due to medical data privacy regulations, it is often infeasible to collect and share patient data in a centralised data lake. This poses challenges for training machine learning algorithms, such as deep convolutional networks, which often require large numbers of diverse training examples. Federated learning sidesteps this difficulty by bringing code to the patient data owners and only sharing intermediate model training updates among them. Although a high-accuracy model could be achieved by appropriately aggregating these model updates, the model shared could indirectly leak the local training examples. In this paper, we investigate the feasibility of applying differential-privacy techniques to protect the patient data in a federated learning setup. We implement and evaluate practical federated learning systems for brain tumour segmentation on the BraTS dataset. The experimental results show that there is a trade-off between model performance and privacy protection costs.",
                        "Citation Paper Authors": "Authors:Wenqi Li, Fausto Milletar\u00ec, Daguang Xu, Nicola Rieke, Jonny Hancox, Wentao Zhu, Maximilian Baust, Yan Cheng, S\u00e9bastien Ourselin, M. Jorge Cardoso, Andrew Feng"
                    }
                },
                {
                    "Sentence ID": 90,
                    "Sentence": ".\nTo bolster integrity and security in the realm of distributed\nIoT devices, the fusion of FL and blockchain within\nedge computing architectures comes to the fore ",
                    "Citation Text": "D. C. Nguyen, M. Ding, Q.-V . Pham, P. N. Pathirana, L. B. Le,\nA. Seneviratne, J. Li, D. Niyato, and H. V . Poor, \u201cFederated learning\nmeets blockchain in edge computing: Opportunities and challenges,\u201d\nIEEE Internet of Things Journal , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.01776",
                        "Citation Paper Title": "Title:Federated Learning Meets Blockchain in Edge Computing: Opportunities and Challenges",
                        "Citation Paper Abstract": "Abstract:Mobile edge computing (MEC) has been envisioned as a promising paradigm to handle the massive volume of data generated from ubiquitous mobile devices for enabling intelligent services with the help of artificial intelligence (AI). Traditionally, AI techniques often require centralized data collection and training in a single entity, e.g., an MEC server, which is now becoming a weak point due to data privacy concerns and high data communication overheads. In this context, federated learning (FL) has been proposed to provide collaborative data training solutions, by coordinating multiple mobile devices to train a shared AI model without exposing their data, which enjoys considerable privacy enhancement. To improve the security and scalability of FL implementation, blockchain as a ledger technology is attractive for realizing decentralized FL training without the need for any central server. Particularly, the integration of FL and blockchain leads to a new paradigm, called FLchain, which potentially transforms intelligent MEC networks into decentralized, secure, and privacy-enhancing systems. This article presents an overview of the fundamental concepts and explores the opportunities of FLchain in MEC networks. We identify several main topics in FLchain design, including communication cost, resource allocation, incentive mechanism, security and privacy protection. The key solutions for FLchain design are provided, and the lessons learned as well as the outlooks are also discussed. Then, we investigate the applications of FLchain in popular MEC domains, such as edge data sharing, edge content caching and edge crowdsensing. Finally, important research challenges and future directions are also highlighted.",
                        "Citation Paper Authors": "Authors:Dinh C. Nguyen, Ming Ding, Quoc-Viet Pham, Pubudu N. Pathirana, Long Bao Le, Aruna Seneviratne, Jun Li, Dusit Niyato, H. Vincent Poor"
                    }
                },
                {
                    "Sentence ID": 59,
                    "Sentence": ". Furthermore, FL models exhibit heightened\nrobustness and efficacy compared to traditional data-driven\nmedical applications ",
                    "Citation Text": "M. Grama, M. Musat, L. Mu \u02dcnoz-Gonz \u00b4alez, J. Passerat-Palmbach,\nD. Rueckert, and A. Alansary, \u201cRobust aggregation for adaptive\nprivacy preserving federated learning in healthcare,\u201d arXiv preprint\narXiv:2009.08294 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.08294",
                        "Citation Paper Title": "Title:Robust Aggregation for Adaptive Privacy Preserving Federated Learning in Healthcare",
                        "Citation Paper Abstract": "Abstract:Federated learning (FL) has enabled training models collaboratively from multiple data owning parties without sharing their data. Given the privacy regulations of patient's healthcare data, learning-based systems in healthcare can greatly benefit from privacy-preserving FL approaches. However, typical model aggregation methods in FL are sensitive to local model updates, which may lead to failure in learning a robust and accurate global model. In this work, we implement and evaluate different robust aggregation methods in FL applied to healthcare data. Furthermore, we show that such methods can detect and discard faulty or malicious local clients during training. We run two sets of experiments using two real-world healthcare datasets for training medical diagnosis classification tasks. Each dataset is used to simulate the performance of three different robust FL aggregation strategies when facing different poisoning attacks. The results show that privacy preserving methods can be successfully applied alongside Byzantine-robust aggregation techniques. We observed in particular how using differential privacy (DP) did not significantly impact the final learning convergence of the different aggregation strategies.",
                        "Citation Paper Authors": "Authors:Matei Grama, Maria Musat, Luis Mu\u00f1oz-Gonz\u00e1lez, Jonathan Passerat-Palmbach, Daniel Rueckert, Amir Alansary"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.07403v5": {
            "Paper Title": "Private Stochastic Optimization With Large Worst-Case Lipschitz\n  Parameter: Optimal Rates for (Non-Smooth) Convex Losses and Extension to\n  Non-Convex Losses",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.06877v4": {
            "Paper Title": "A Review of zk-SNARKs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.09717v3": {
            "Paper Title": "UPTON: Preventing Authorship Leakage from Public Text Release via Data\n  Poisoning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.09140v3": {
            "Paper Title": "Energy Efficient Obfuscation of Side-Channel Leakage for Preventing\n  Side-Channel Attacks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.08673v3": {
            "Paper Title": "Proofs of Proof-of-Stake with Sublinear Complexity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.01956v2": {
            "Paper Title": "Provably Valid and Diverse Mutations of Real-World Media Data for DNN\n  Testing",
            "Sentences": [
                {
                    "Sentence ID": 81,
                    "Sentence": ".\nQuantitative Results. It\u2019s challenging to manually exam the\nrecognizability of mutated images given the large volumes.\nHumans may also disagree with the recognizability (see the\n\u201cDeepTest\u201d cases in Fig. 10). Two popular criteria, inception\nscore (IS) ",
                    "Citation Text": "T. Salimans, I. Goodfellow, W. Zaremba, V . Cheung, A. Radford,\nand X. Chen, \u201cImproved techniques for training gans,\u201d Advances\nin neural information processing systems , vol. 29, pp. 2234\u20132242, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.03498",
                        "Citation Paper Title": "Title:Improved Techniques for Training GANs",
                        "Citation Paper Abstract": "Abstract:We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.",
                        "Citation Paper Authors": "Authors:Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.05481v3": {
            "Paper Title": "Fully Adaptive Composition in Differential Privacy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.11319v2": {
            "Paper Title": "Solving the Kidney Exchange Problem Using Privacy-Preserving Integer\n  Programming (Updated and Extended Version)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.06272v3": {
            "Paper Title": "Model-based Joint Analysis of Safety and Security: Survey and\n  Identification of Gaps",
            "Sentences": [
                {
                    "Sentence ID": 65,
                    "Sentence": "compares seven\nframeworkswithrespecttomodelcreation, origin, stagesoftheriskassessment, andmain\nuse cases. The comparison conducted here is, however, carried out at a high level. The\nsurvey ",
                    "Citation Text": "Nigam, V., Pretschner, A., Ruess, H.: Model-based safety and security engineering (2019)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04866",
                        "Citation Paper Title": "Title:Model-Based Safety and Security Engineering",
                        "Citation Paper Abstract": "Abstract:By exploiting the increasing surface attack of systems, cyber-attacks can cause catastrophic events, such as, remotely disable safety mechanisms. This means that in order to avoid hazards, safety and security need to be integrated, exchanging information, such as, key hazards/threats, risk evaluations, mechanisms used. This white paper describes some steps towards this integration by using models. We start by identifying some key technical challenges. Then we demonstrate how models, such as Goal Structured Notation (GSN) for safety and Attack Defense Trees (ADT) for security, can address these challenges. In particular, (1) we demonstrate how to extract in an automated fashion security relevant information from safety assessments by translating GSN-Models into ADTs; (2) We show how security results can impact the confidence of safety assessments; (3) We propose a collaborative development process where safety and security assessments are built by incrementally taking into account safety and security analysis; (4) We describe how to carry out trade-off analysis in an automated fashion, such as identifying when safety and security arguments contradict each other and how to solve such contradictions. We conclude pointing out that these are the first steps towards a wide range of techniques to support Safety and Security Engineering. As a white paper, we avoid being too technical, preferring to illustrate features by using examples and thus being more accessible.",
                        "Citation Paper Authors": "Authors:Vivek Nigam, Alexander Pretschner, Harald Ruess"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2301.00104v2": {
            "Paper Title": "Towards Separating Computational and Statistical Differential Privacy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.08412v3": {
            "Paper Title": "Characterizing Internal Evasion Attacks in Federated Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.05111v2": {
            "Paper Title": "SoK: MEV Countermeasures: Theory and Practice",
            "Sentences": [
                {
                    "Sentence ID": 37,
                    "Sentence": ". Subsequently, a line of works improves the\ntechniques for MEV identification and quantification ",
                    "Citation Text": "K. Qin, L. Zhou, and A. Gervais, \u201cQuantifying blockchain extractable\nvalue: How dark is the forest?\u201d in 2022 IEEE Symposium on Security\nand Privacy (SP) . IEEE, 2022, pp. 198\u2013214.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.05511",
                        "Citation Paper Title": "Title:Quantifying Blockchain Extractable Value: How dark is the forest?",
                        "Citation Paper Abstract": "Abstract:Permissionless blockchains such as Bitcoin have excelled at financial services. Yet, opportunistic traders extract monetary value from the mesh of decentralized finance (DeFi) smart contracts through so-called blockchain extractable value (BEV). The recent emergence of centralized BEV relayer portrays BEV as a positive additional revenue source. Because BEV was quantitatively shown to deteriorate the blockchain's consensus security, BEV relayers endanger the ledger security by incentivizing rational miners to fork the chain. For example, a rational miner with a 10% hashrate will fork Ethereum if a BEV opportunity exceeds 4x the block reward.\nHowever, related work is currently missing quantitative insights on past BEV extraction to assess the practical risks of BEV objectively. In this work, we allow to quantify the BEV danger by deriving the USD extracted from sandwich attacks, liquidations, and decentralized exchange arbitrage. We estimate that over 32 months, BEV yielded 540.54M USD in profit, divided among 11,289 addresses when capturing 49,691 cryptocurrencies and 60,830 on-chain markets. The highest BEV instance we find amounts to 4.1M USD, 616.6x the Ethereum block reward.\nMoreover, while the practitioner's community has discussed the existence of generalized trading bots, we are, to our knowledge, the first to provide a concrete algorithm. Our algorithm can replace unconfirmed transactions without the need to understand the victim transactions' underlying logic, which we estimate to have yielded a profit of 57,037.32 ETH (35.37M USD) over 32 months of past blockchain data.\nFinally, we formalize and analyze emerging BEV relay systems, where miners accept BEV transactions from a centralized relay server instead of the peer-to-peer (P2P) network. We find that such relay systems aggravate the consensus layer attacks and therefore further endanger blockchain security.",
                        "Citation Paper Authors": "Authors:Kaihua Qin, Liyi Zhou, Arthur Gervais"
                    }
                },
                {
                    "Sentence ID": 83,
                    "Sentence": "modeled MEV auction platforms as a key\ncomponent in the network layer to evaluate and compare\nreal-world DeFi attacks. MEV is also considered as one of\nthe security concerns to evaluate the AMM-based DEX in ",
                    "Citation Text": "J. Xu, K. Paruch, S. Cousaert, and Y . Feng, \u201cSoK: Decentralized\nExchanges (DEX) with Automated Market Maker (AMM) Protocols,\u201d\narXiv preprint arXiv:2103.12732 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.12732",
                        "Citation Paper Title": "Title:SoK: Decentralized Exchanges (DEX) with Automated Market Maker (AMM) Protocols",
                        "Citation Paper Abstract": "Abstract:As an integral part of the decentralized finance (DeFi) ecosystem, decentralized exchanges (DEXs) with automated market maker (AMM) protocols have gained massive traction with the recently revived interest in blockchain and distributed ledger technology (DLT) in general. Instead of matching the buy and sell sides, automated market makers (AMMs) employ a peer-to-pool method and determine asset price algorithmically through a so-called conservation function. To facilitate the improvement and development of automated market maker (AMM)-based decentralized exchanges (DEXs), we create the first systematization of knowledge in this area. We first establish a general automated market maker (AMM) framework describing the economics and formalizing the system's state-space representation. We then employ our framework to systematically compare the top automated market maker (AMM) protocols' mechanics, illustrating their conservation functions, as well as slippage and divergence loss functions. We further discuss security and privacy concerns, how they are enabled by automated market maker (AMM)-based decentralized exchanges (DEXs)' inherent properties, and explore mitigating solutions. Finally, we conduct a comprehensive literature review on related work covering both decentralized finance (DeFi) and conventional market microstructure.",
                        "Citation Paper Authors": "Authors:Jiahua Xu, Krzysztof Paruch, Simon Cousaert, Yebo Feng"
                    }
                },
                {
                    "Sentence ID": 82,
                    "Sentence": "discuss MEV in the context of DeFi.\nZhou et al. ",
                    "Citation Text": "L. Zhou, X. Xiong, J. Ernstberger, S. Chaliasos, Z. Wang, Y . Wang,\nK. Qin, R. Wattenhofer, D. Song, and A. Gervais, \u201cSoK: Decen-\ntralized Finance (DeFi) Incidents,\u201d arXiv preprint arXiv:2208.13035 ,\n2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2208.13035",
                        "Citation Paper Title": "Title:SoK: Decentralized Finance (DeFi) Attacks",
                        "Citation Paper Abstract": "Abstract:Within just four years, the blockchain-based Decentralized Finance (DeFi) ecosystem has accumulated a peak total value locked (TVL) of more than 253 billion USD. This surge in DeFi's popularity has, unfortunately, been accompanied by many impactful incidents. According to our data, users, liquidity providers, speculators, and protocol operators suffered a total loss of at least 3.24 billion USD from Apr 30, 2018 to Apr 30, 2022. Given the blockchain's transparency and increasing incident frequency, two questions arise: How can we systematically measure, evaluate, and compare DeFi incidents? How can we learn from past attacks to strengthen DeFi security?\nIn this paper, we introduce a common reference frame to systematically evaluate and compare DeFi incidents, including both attacks and accidents. We investigate 77 academic papers, 30 audit reports, and 181 real-world incidents. Our data reveals several gaps between academia and the practitioners' community. For example, few academic papers address \"price oracle attacks\" and \"permissonless interactions\", while our data suggests that they are the two most frequent incident types (15% and 10.5% correspondingly). We also investigate potential defenses, and find that: (i) 103 (56%) of the attacks are not executed atomically, granting a rescue time frame for defenders; (ii) SoTA bytecode similarity analysis can at least detect 31 vulnerable/23 adversarial contracts; and (iii) 33 (15.3%) of the adversaries leak potentially identifiable information by interacting with centralized exchanges.",
                        "Citation Paper Authors": "Authors:Liyi Zhou, Xihan Xiong, Jens Ernstberger, Stefanos Chaliasos, Zhipeng Wang, Ye Wang, Kaihua Qin, Roger Wattenhofer, Dawn Song, Arthur Gervais"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": "Frequent batch auctions realized with zero-knowledge proofs and value com-\nmitment.-\nA2MM ",
                    "Citation Text": "L. Zhou, K. Qin, and A. Gervais, \u201cA2MM: Mitigating Frontrunning,\nTransaction Reordering and Consensus Instability in Decentralized\nExchanges,\u201d arXiv preprint arXiv:2106.07371 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.07371",
                        "Citation Paper Title": "Title:A2MM: Mitigating Frontrunning, Transaction Reordering and Consensus Instability in Decentralized Exchanges",
                        "Citation Paper Abstract": "Abstract:The asset trading volume on blockchain-based exchanges (DEX) increased substantially since the advent of Automated Market Makers (AMM). Yet, AMMs and their forks compete on the same blockchain, incurring unnecessary network and block-space overhead, by attracting sandwich attackers and arbitrage competitions. Moreover, conceptually speaking, a blockchain is one database, and we find little reason to partition this database into multiple competing exchanges, which then necessarily require price synchronization through arbitrage.\nThis paper shows that DEX arbitrage and trade routing among similar AMMs can be performed efficiently and atomically on-chain within smart contracts. These insights lead us to create a new AMM design, an Automated Arbitrage Market Maker, short A2MM DEX. A2MM aims to unite multiple AMMs to reduce overheads, costs and increase blockchain security. With respect to Miner Extractable Value (MEV), A2MM serves as a decentralized design for users to atomically collect MEV, mitigating the dangers of centralized MEV relay services.\nWe show that A2MM offers essential security benefits. First, A2MM strengthens the blockchain consensus security by mitigating the competitive exploitation of MEV, therefore reducing the risks of consensus forks. A2MM reduces the network layer overhead of competitive transactions, improves network propagation, leading to less stale blocks and better blockchain security. Through trade routing, A2MM reduces the predatory risks of sandwich attacks by taking advantage of the minimum profitable victim input. A2MM also offers financial benefits to traders. Failed swap transactions from competitive trading occupy valuable block space, implying an upward pressure on transaction fees. Our evaluations shows that A2MM frees up 32.8% block-space of AMM-related transactions. In expectation, A2MM's revenue allows to reduce swap fees by 90%.",
                        "Citation Paper Authors": "Authors:Liyi Zhou, Kaihua Qin, Arthur Gervais"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "Execute transactions in frequent batch auctions. Settlement is outsourced to\nsolvers who compete to provide the best settlement surplus.We omit application-specific trust\nassumptions unless they are unique\nto MEV protection.\nFairTraDEX ",
                    "Citation Text": "C. McMenamin, V . Daza, M. Fitzi, and P. O\u2019Donoghue, \u201cFair-\nTraDEX: A Decentralised Exchange Preventing Value Extraction,\u201d\ninProceedings of the 2022 ACM CCS Workshop on Decentralized\nFinance and Security , 2022, pp. 39\u201346.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.06384",
                        "Citation Paper Title": "Title:FairTraDEX: A Decentralised Exchange Preventing Value Extraction",
                        "Citation Paper Abstract": "Abstract:We present FairTraDEX, a decentralized exchange (DEX) protocol based on frequent batch auctions (FBAs), which provides formal game-theoretic guarantees against extractable value. FBAs when run by a trusted third-party provide unique game-theoretic optimal strategies which ensure players are shown prices equal to the liquidity provider's fair price, excluding explicit, pre-determined fees. FairTraDEX replicates the key features of an FBA that provide these game-theoretic guarantees using a combination of set-membership in zero-knowledge protocols and an escrow-enforced commit-reveal protocol. We extend the results of FBAs to handle monopolistic and/or malicious liquidity providers. We provide real-world examples that demonstrate that the costs of executing orders in existing academic and industry-standard protocols become prohibitive as order size increases due to basic value extraction techniques, popularized as maximal extractable value. We further demonstrate that FairTraDEX protects against these execution costs, guaranteeing a fixed fee model independent of order size, the first guarantee of it's kind for a DEX protocol. We also provide detailed Solidity and pseudo-code implementations of FairTraDEX, making FairTraDEX a novel and practical contribution.",
                        "Citation Paper Authors": "Authors:Conor McMenamin, Vanesa Daza, Matthias Fitzi, Padraic O'Donoghue"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "Ordering linearizability: if the highest timestamp of Tfrom all correct nodes\nis lower than the lowest timestamp of T\u2032from all correct nodes, then Tis\nordered before T\u2032.n\u22653f+ 1\nQuick-Fairness ",
                    "Citation Text": "C. Cachin, J. Mi \u00b4ci\u00b4c, N. Steinhauer, and L. Zanolini, \u201cQuick Order\nFairness,\u201d in International Conference on Financial Cryptography\nand Data Security . Springer, 2022, pp. 316\u2013333.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.06615",
                        "Citation Paper Title": "Title:Quick Order Fairness",
                        "Citation Paper Abstract": "Abstract:Leader-based protocols for consensus, i.e., atomic broadcast, allow some processes to unilaterally affect the final order of transactions. This has become a problem for blockchain networks and decentralized finance because it facilitates front-running and other attacks. To address this, order fairness for payload messages has been introduced recently as a new safety property for atomic broadcast complementing traditional agreement and liveness. We relate order fairness to the standard validity notions for consensus protocols and highlight some limitations with the existing formalization. Based on this, we introduce a new differential order fairness property that fixes these issues. We also present the quick order-fair atomic broadcast protocol that guarantees payload message delivery in a differentially fair order and is much more efficient than existing order-fair consensus protocols. It works for asynchronous and for eventually synchronous networks with optimal resilience, tolerating corruptions of up to one third of the processes. Previous solutions required there to be less than one fourth of faults. Furthermore, our protocol incurs only quadratic cost, in terms of amortized message complexity per delivered payload.",
                        "Citation Paper Authors": "Authors:Christian Cachin, Jovana Mi\u0107i\u0107, Nathalie Steinhauer, Luca Zanolini"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2207.05521v3": {
            "Paper Title": "Federated Unlearning: How to Efficiently Erase a Client in FL?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.01227v2": {
            "Paper Title": "Cybersecurity: Past, Present and Future",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.13696v3": {
            "Paper Title": "FPT: a Fixed-Point Accelerator for Torus Fully Homomorphic Encryption",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.12989v4": {
            "Paper Title": "HyperQB: A QBF-Based Bounded Model Checker for Hyperproperties",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.12978v3": {
            "Paper Title": "Private Multi-Task Learning: Formulation and Applications to Federated\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.02042v2": {
            "Paper Title": "Refiner: Data Refining against Gradient Leakage Attacks in Federated\n  Learning",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": "dedicated efforts to explore various regulariza-\ntion techniques and heuristic tricks, including total variation,\nrestart, and improved initialization, aiming to eliminate in-\nvalid reconstructions. Recently, since the output of generative\nmodels is not noisy, Li et al. ",
                    "Citation Text": "Zhuohang Li, Jiaxin Zhang, Luyang Liu, and Jian Liu.\nAuditing privacy defenses in federated learning via\ngenerative gradient leakage. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 10132\u201310142, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.15696",
                        "Citation Paper Title": "Title:Auditing Privacy Defenses in Federated Learning via Generative Gradient Leakage",
                        "Citation Paper Abstract": "Abstract:Federated Learning (FL) framework brings privacy benefits to distributed learning systems by allowing multiple clients to participate in a learning task under the coordination of a central server without exchanging their private data. However, recent studies have revealed that private information can still be leaked through shared gradient information. To further protect user's privacy, several defense mechanisms have been proposed to prevent privacy leakage via gradient information degradation methods, such as using additive noise or gradient compression before sharing it with the server. In this work, we validate that the private training data can still be leaked under certain defense settings with a new type of leakage, i.e., Generative Gradient Leakage (GGL). Unlike existing methods that only rely on gradient information to reconstruct data, our method leverages the latent space of generative adversarial networks (GAN) learned from public image datasets as a prior to compensate for the informational loss during gradient degradation. To address the nonlinearity caused by the gradient operator and the GAN model, we explore various gradient-free optimization methods (e.g., evolution strategies and Bayesian optimization) and empirically show their superiority in reconstructing high-quality images from gradients compared to gradient-based optimizers. We hope the proposed method can serve as a tool for empirically measuring the amount of privacy leakage to facilitate the design of more robust defense mechanisms.",
                        "Citation Paper Authors": "Authors:Zhuohang Li, Jiaxin Zhang, Luyang Liu, Jian Liu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.06654v3": {
            "Paper Title": "The Inventory is Dark and Full of Misinformation: Understanding the\n  Abuse of Ad Inventory Pooling in the Ad-Tech Supply Chain",
            "Sentences": [
                {
                    "Sentence ID": 72,
                    "Sentence": "highlighted flaws of\ntheads.txt standard that undermines its effectiveness in\npreventing ad fraud, albeit without measurements to sup-\nport their hypotheses. Some of these identified flaws are,\nhowever, supported by measurements from Papadogiannakis\net al. ",
                    "Citation Text": "Emmanouil Papadogiannakis, Panagiotis Papadopoulos, Evangelos\nP. Markatos, and Nicolas Kourtellis. Who funds misinformation?\na systematic analysis of the ad-related profit routines of fake news\nsites. In Proceedings of the ACM Web Conference 2023 , pages\n2765\u20132776, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.05079",
                        "Citation Paper Title": "Title:Who Funds Misinformation? A Systematic Analysis of the Ad-related Profit Routines of Fake News sites",
                        "Citation Paper Abstract": "Abstract:Fake news is an age-old phenomenon, widely assumed to be associated with political propaganda published to sway public opinion. Yet, with the growth of social media, it has become a lucrative business for Web publishers. Despite many studies performed and countermeasures proposed, unreliable news sites have increased in the last years their share of engagement among the top performing news sources. Stifling fake news impact depends on our efforts in limiting the (economic) incentives of fake news producers.\nIn this paper, we aim at enhancing the transparency around these exact incentives, and explore: Who supports the existence of fake news websites via paid ads, either as an advertiser or an ad seller? Who owns these websites and what other Web business are they into? We are the first to systematize the auditing process of fake news revenue flows. We identify the companies that advertise in fake news websites and the intermediary companies responsible for facilitating those ad revenues. We study more than 2,400 popular news websites and show that well-known ad networks, such as Google and IndexExchange, have a direct advertising relation with more than 40% of fake news websites. Using a graph clustering approach on 114.5K sites, we show that entities who own fake news sites, also operate other types of websites pointing to the fact that owning a fake news website is part of a broader business operation.",
                        "Citation Paper Authors": "Authors:Emmanouil Papadogiannakis, Panagiotis Papadopoulos, Evangelos P. Markatos, Nicolas Kourtellis"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.02167v3": {
            "Paper Title": "Red Teaming with Mind Reading: White-Box Adversarial Policies Against RL\n  Agents",
            "Sentences": [
                {
                    "Sentence ID": 46,
                    "Sentence": "who found that in\ncompetitive multiagent environments, it was key to rotate players in a round-robin fashion to avoid\nagents overfitting against a particular opponent. Additionally, ",
                    "Citation Text": "Alberto Pozanco, Susana Fern\u00e1ndez, Daniel Borrajo, et al. Anticipatory counterplanning. arXiv\npreprint arXiv:2203.16171 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.16171",
                        "Citation Paper Title": "Title:Anticipatory Counterplanning",
                        "Citation Paper Abstract": "Abstract:In competitive environments, commonly agents try to prevent opponents from achieving their goals. Most previous preventing approaches assume the opponent's goal is known a priori. Others only start executing actions once the opponent's goal has been inferred. In this work we introduce a novel domain-independent algorithm called Anticipatory Counterplanning. It combines inference of opponent's goals with computation of planning centroids to yield proactive counter strategies in problems where the opponent's goal is unknown. Experimental results show how this novel technique outperforms reactive counterplanning, increasing the chances of stopping the opponent from achieving its goals.",
                        "Citation Paper Authors": "Authors:Alberto Pozanco, Yolanda E-Mart\u00edn, Susana Fern\u00e1ndez, Daniel Borrajo"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.10629v5": {
            "Paper Title": "Getting Bored of Cyberwar: Exploring the Role of Low-level Cybercrime\n  Actors in the Russia-Ukraine Conflict",
            "Sentences": [
                {
                    "Sentence ID": 45,
                    "Sentence": ", while pro-Ukrainian supporters have tried unconven-\ntional channels such as online reviews to bypass censorship ",
                    "Citation Text": "Jos\u00e9 Miguel Moreno, Sergio Pastrana, Jens Helge Reelfs, Pelayo Vallina, Andriy\nPanchenko, Georgios Smaragdakis, Oliver Hohlfeld, Narseo Vallina-Rodriguez,\nand Juan Tapiador. 2023. Reviewing War: Unconventional User Reviews as a\nSide Channel to Circumvent Information Controls. (2023). arXiv:2302.00598",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.00598",
                        "Citation Paper Title": "Title:Reviewing War: Unconventional User Reviews as a Side Channel to Circumvent Information Controls",
                        "Citation Paper Abstract": "Abstract:During the first days of the 2022 Russian invasion of Ukraine, Russia's media regulator blocked access to many global social media platforms and news sites, including Twitter, Facebook, and the BBC. To bypass the information controls set by Russian authorities, pro-Ukrainian groups explored unconventional ways to reach out to the Russian population, such as posting war-related content in the user reviews of Russian business available on Google Maps or Tripadvisor. This paper provides a first analysis of this new phenomenon by analyzing the creative strategies to avoid state censorship. Specifically, we analyze reviews posted on these platforms from the beginning of the conflict to September 2022. We measure the channeling of war messages through user reviews in Tripadvisor and Google Maps, as well as in VK, a popular Russian social network. Our analysis of the content posted on these services reveals that users leveraged these platforms to seek and exchange humanitarian and travel advice, but also to disseminate disinformation and polarized messages. Finally, we analyze the response of platforms in terms of content moderation and their impact.",
                        "Citation Paper Authors": "Authors:Jos\u00e9 Miguel Moreno, Sergio Pastrana, Jens Helge Reelfs, Pelayo Vallina, Andriy Panchenko, Georgios Smaragdakis, Oliver Hohlfeld, Narseo Vallina-Rodriguez, Juan Tapiador"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.06363v4": {
            "Paper Title": "A Side-channel Analysis of Sensor Multiplexing for Covert Channels and\n  Application Profiling on Mobile Devices",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.03525v2": {
            "Paper Title": "Cryptanalysis of some Nonabelian Group-Based Key Exchange Protocols",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.07302v2": {
            "Paper Title": "Deep Reinforcement Learning-based Rebalancing Policies for Profit\n  Maximization of Relay Nodes in Payment Channel Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.00885v3": {
            "Paper Title": "Opted Out, Yet Tracked: Are Regulations Enough to Protect Your Privacy?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.09631v2": {
            "Paper Title": "De-Identification of French Unstructured Clinical Notes for Machine\n  Learning Tasks",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": ". It is a supervised learning method for multi-label text classification\nwith Convolutional Neural Networks. FastText vectors ",
                    "Citation Text": "Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tom\u00e1s Mikolov.\nEnriching word vectors with subword information. CoRR, abs/1607.04606,\n2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.04606",
                        "Citation Paper Title": "Title:Enriching Word Vectors with Subword Information",
                        "Citation Paper Abstract": "Abstract:Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character $n$-grams. A vector representation is associated to each character $n$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",
                        "Citation Paper Authors": "Authors:Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "for example. This allows us\nto fine-tune a specific pre-trained language model (in French FlauBERT ",
                    "Citation Text": "Hang Le, Lo\u00efc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Ben-\njamin Lecouteux, Alexandre Allauzen, Beno\u00eet Crabb\u00e9, Laurent Besacier,\nand Didier Schwab. Flaubert: Unsupervised language model pre-training\nfor french, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.05372",
                        "Citation Paper Title": "Title:FlauBERT: Unsupervised Language Model Pre-training for French",
                        "Citation Paper Abstract": "Abstract:Language models have become a key step to achieve state-of-the art results in many different Natural Language Processing (NLP) tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efficient way to pre-train continuous word representations that can be fine-tuned for a downstream task, along with their contextualization at the sentence level. This has been widely demonstrated for English using contextualized representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019b). In this paper, we introduce and share FlauBERT, a model learned on a very large and heterogeneous French corpus. Models of different sizes are trained using the new CNRS (French National Centre for Scientific Research) Jean Zay supercomputer. We apply our French language models to diverse NLP tasks (text classification, paraphrasing, natural language inference, parsing, word sense disambiguation) and show that most of the time they outperform other pre-training approaches. Different versions of FlauBERT as well as a unified evaluation protocol for the downstream tasks, called FLUE (French Language Understanding Evaluation), are shared to the research community for further reproducible experiments in French NLP.",
                        "Citation Paper Authors": "Authors:Hang Le, Lo\u00efc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Beno\u00eet Crabb\u00e9, Laurent Besacier, Didier Schwab"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "isaBERTtypemodeldevelopedbyFacebookandtheINRIA\nin France. It has been pre-trained on a 138Gb French corpus. More precisely it\nis based on the RoBERTa architecture ",
                    "Citation Text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi\nChen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\nRoberta: A robustly optimized bert pretraining approach, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.11692",
                        "Citation Paper Title": "Title:RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                        "Citation Paper Abstract": "Abstract:Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
                        "Citation Paper Authors": "Authors:Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2110.03991v4": {
            "Paper Title": "Combining Differential Privacy and Byzantine Resilience in Distributed\n  SGD",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.06074v3": {
            "Paper Title": "Regression with Label Differential Privacy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.11896v3": {
            "Paper Title": "Private Ad Modeling with DP-SGD",
            "Sentences": [
                {
                    "Sentence ID": 40,
                    "Sentence": "is a notion where\nthe features are public and only the labels need privacy\nprotection. It has been recently studied in deep learn-\ning [ 12,13], and more specifically in ad modeling ",
                    "Citation Text": "B. Ghazi, P. Kamath, R. Kumar, E. Leeman, P. Manu-\nrangsi, A. Varadarajan, C. Zhang, Regression with\nlabel differential privacy, in: ICLR, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.06074",
                        "Citation Paper Title": "Title:Regression with Label Differential Privacy",
                        "Citation Paper Abstract": "Abstract:We study the task of training regression models with the guarantee of label differential privacy (DP). Based on a global prior distribution on label values, which could be obtained privately, we derive a label DP randomization mechanism that is optimal under a given regression loss function. We prove that the optimal mechanism takes the form of a \"randomized response on bins\", and propose an efficient algorithm for finding the optimal bin values. We carry out a thorough experimental evaluation on several datasets demonstrating the efficacy of our algorithm.",
                        "Citation Paper Authors": "Authors:Badih Ghazi, Pritish Kamath, Ravi Kumar, Ethan Leeman, Pasin Manurangsi, Avinash V Varadarajan, Chiyuan Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.10159v4": {
            "Paper Title": "Getting a-Round Guarantees: Floating-Point Attacks on Certified\n  Robustness",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.07136v3": {
            "Paper Title": "Automatic Clipping: Differentially Private Deep Learning Made Easier and\n  Stronger",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.03776v5": {
            "Paper Title": "High-Throughput Secure Multiparty Computation with an Honest Majority in\n  Various Network Settings",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.08108v3": {
            "Paper Title": "Dataflow Analysis-Inspired Deep Learning for Efficient Vulnerability\n  Detection",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": "used the\noutput of dataflow analysis, reaching definitions and live variables,\nto learn flow-aware embeddings; and (2) Bielik et al . ",
                    "Citation Text": "Pavol Bielik, Veselin Raychev, and Martin Vechev. 2017. Learning a static analyzer\nfrom data. In Computer Aided Verification: 29th International Conference, CAV 2017,\nHeidelberg, Germany, July 24-28, 2017, Proceedings, Part I 30 . Springer, 233\u2013253.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.01752",
                        "Citation Paper Title": "Title:Learning a Static Analyzer from Data",
                        "Citation Paper Abstract": "Abstract:To be practically useful, modern static analyzers must precisely model the effect of both, statements in the programming language as well as frameworks used by the program under analysis. While important, manually addressing these challenges is difficult for at least two reasons: (i) the effects on the overall analysis can be non-trivial, and (ii) as the size and complexity of modern libraries increase, so is the number of cases the analysis must handle.\nIn this paper we present a new, automated approach for creating static analyzers: instead of manually providing the various inference rules of the analyzer, the key idea is to learn these rules from a dataset of programs. Our method consists of two ingredients: (i) a synthesis algorithm capable of learning a candidate analyzer from a given dataset, and (ii) a counter-example guided learning procedure which generates new programs beyond those in the initial dataset, critical for discovering corner cases and ensuring the learned analysis generalizes to unseen programs.\nWe implemented and instantiated our approach to the task of learning JavaScript static analysis rules for a subset of points-to analysis and for allocation sites analysis. These are challenging yet important problems that have received significant research attention. We show that our approach is effective: our system automatically discovered practical and useful inference rules for many cases that are tricky to manually identify and are missed by state-of-the-art, manually tuned analyzers.",
                        "Citation Paper Authors": "Authors:Pavol Bielik, Veselin Raychev, Martin Vechev"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ".\nRQ2: To evaluate the models\u2019 efficiency in terms of computational\nresources, we measured the runtime and memory usage. These\nare often contested resources in deep learning workloads ",
                    "Citation Text": "Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Wei-\njun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. Mo-\nbileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications.\narXiv:1704.04861 [cs.CV]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.04861",
                        "Citation Paper Title": "Title:MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications",
                        "Citation Paper Abstract": "Abstract:We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.",
                        "Citation Paper Authors": "Authors:Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.02240v2": {
            "Paper Title": "Tracking Patches for Open Source Software Vulnerabilities",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.07978v2": {
            "Paper Title": "Enhancing Heterogeneous Federated Learning with Knowledge Extraction and\n  Multi-Model Fusion",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": "is proposed to improve local client training by adding\na proximal term to the local loss, FedNova ",
                    "Citation Text": "Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H. Vincent Poor.\n2020. Tackling the objective inconsistency problem in heterogeneous federated\noptimization. In Proc. of Advances in Neural Information Processing Systems\n(NeurIPS) .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.07481",
                        "Citation Paper Title": "Title:Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization",
                        "Citation Paper Abstract": "Abstract:In federated optimization, heterogeneity in the clients' local datasets and computation speeds results in large variations in the number of local updates performed by each client in each communication round. Naive weighted aggregation of such models causes objective inconsistency, that is, the global model converges to a stationary point of a mismatched objective function which can be arbitrarily different from the true objective. This paper provides a general framework to analyze the convergence of federated heterogeneous optimization algorithms. It subsumes previously proposed methods such as FedAvg and FedProx and provides the first principled understanding of the solution bias and the convergence slowdown due to objective inconsistency. Using insights from this analysis, we propose FedNova, a normalized averaging method that eliminates objective inconsistency while preserving fast error convergence.",
                        "Citation Paper Authors": "Authors:Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, H. Vincent Poor"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "introduces weight\nmodification to avoid gradient bias by normalizing and scaling local\nupdates, SCAFFOLD ",
                    "Citation Text": "Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J. Reddi,\nSebastian U. Stich, and Ananda Theertha Suresh. 2020. SCAFFOLD: stochastic\ncontrolled averaging for federated learning. In Proc. of International Conference\non Machine Learning (ICML) .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.06378",
                        "Citation Paper Title": "Title:SCAFFOLD: Stochastic Controlled Averaging for Federated Learning",
                        "Citation Paper Abstract": "Abstract:Federated Averaging (FedAvg) has emerged as the algorithm of choice for federated learning due to its simplicity and low communication cost. However, in spite of recent research efforts, its performance is not fully understood. We obtain tight convergence rates for FedAvg and prove that it suffers from `client-drift' when the data is heterogeneous (non-iid), resulting in unstable and slow convergence.\nAs a solution, we propose a new algorithm (SCAFFOLD) which uses control variates (variance reduction) to correct for the `client-drift' in its local updates. We prove that SCAFFOLD requires significantly fewer communication rounds and is not affected by data heterogeneity or client sampling. Further, we show that (for quadratics) SCAFFOLD can take advantage of similarity in the client's data yielding even faster convergence. The latter is the first result to quantify the usefulness of local-steps in distributed optimization.",
                        "Citation Paper Authors": "Authors:Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J. Reddi, Sebastian U. Stich, Ananda Theertha Suresh"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.08229v4": {
            "Paper Title": "CorruptEncoder: Data Poisoning based Backdoor Attacks to Contrastive\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.00484v2": {
            "Paper Title": "Differentially-Private Data Synthetisation for Efficient\n  Re-Identification Risk Control",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": ".\nWe also apply statistical tests using Bayes Sign Test ",
                    "Citation Text": "A. Benavoli, G. Corani, J. Dem \u02c7sar, and M. Zaf-\nfalon ,Time for a change: a tutorial for compar-\ning multiple classifiers through bayesian analysis , The\nJournal of Machine Learning Research, 18 (2017),\npp. 2653\u20132688.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.04316",
                        "Citation Paper Title": "Title:Time for a change: a tutorial for comparing multiple classifiers through Bayesian analysis",
                        "Citation Paper Abstract": "Abstract:The machine learning community adopted the use of null hypothesis significance testing (NHST) in order to ensure the statistical validity of results. Many scientific fields however realized the shortcomings of frequentist reasoning and in the most radical cases even banned its use in publications. We should do the same: just as we have embraced the Bayesian paradigm in the development of new machine learning methods, so we should also use it in the analysis of our own results. We argue for abandonment of NHST by exposing its fallacies and, more importantly, offer better - more sound and useful - alternatives for it.",
                        "Citation Paper Authors": "Authors:Alessio Benavoli, Giorgio Corani, Janez Demsar, Marco Zaffalon"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "are differentially private GAN mod-\nels that add noise during the training process of the dis-\ncriminator. DP has also been added to the tree-based\nmachine learning models to generate synthetic data ",
                    "Citation Text": "S. Mahiou, K. Xu, and G. Ganev ,dpart: Differen-\ntially private autoregressive tabular, a general frame-\nwork for synthetic data generation , arXiv preprint\narXiv:2207.05810, (2022).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2207.05810",
                        "Citation Paper Title": "Title:dpart: Differentially Private Autoregressive Tabular, a General Framework for Synthetic Data Generation",
                        "Citation Paper Abstract": "Abstract:We propose a general, flexible, and scalable framework dpart, an open source Python library for differentially private synthetic data generation. Central to the approach is autoregressive modelling -- breaking the joint data distribution to a sequence of lower-dimensional conditional distributions, captured by various methods such as machine learning models (logistic/linear regression, decision trees, etc.), simple histogram counts, or custom techniques. The library has been created with a view to serve as a quick and accessible baseline as well as to accommodate a wide audience of users, from those making their first steps in synthetic data generation, to more experienced ones with domain expertise who can configure different aspects of the modelling and contribute new methods/mechanisms. Specific instances of dpart include Independent, an optimized version of PrivBayes, and a newly proposed model, dp-synthpop.\nCode: this https URL",
                        "Citation Paper Authors": "Authors:Sofiane Mahiou, Kai Xu, Georgi Ganev"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": ". In addition,\na big challenge of deep learning-based solutions is their\ncomplexity, as it is computationally costly in memory\nand require a significant execution time ",
                    "Citation Text": "T. Bird, F. H. Kingma, and D. Barber ,Re-\nducing the computational cost of deep generative\nmodels with binary neural networks , arXiv preprint\narXiv:2010.13476, (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.13476",
                        "Citation Paper Title": "Title:Reducing the Computational Cost of Deep Generative Models with Binary Neural Networks",
                        "Citation Paper Abstract": "Abstract:Deep generative models provide a powerful set of tools to understand real-world data. But as these models improve, they increase in size and complexity, so their computational cost in memory and execution time grows. Using binary weights in neural networks is one method which has shown promise in reducing this cost. However, whether binary neural networks can be used in generative models is an open problem. In this work we show, for the first time, that we can successfully train generative models which utilize binary neural networks. This reduces the computational cost of the models massively. We develop a new class of binary weight normalization, and provide insights for architecture designs of these binarized generative models. We demonstrate that two state-of-the-art deep generative models, the ResNet VAE and Flow++ models, can be binarized effectively using these techniques. We train binary models that achieve loss values close to those of the regular models but are 90%-94% smaller in size, and also allow significant speed-ups in execution time.",
                        "Citation Paper Authors": "Authors:Thomas Bird, Friso H. Kingma, David Barber"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": ".\nDifferentially Private-based solutions incor-\nporate DP into the process of synthetic data generation\nfor high privacy guarantees. For instance, DPGAN ",
                    "Citation Text": "L. Xie, K. Lin, S. Wang, F. Wang, and J. Zhou ,\nDifferentially private generative adversarial network ,\narXiv preprint arXiv:1802.06739, (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.06739",
                        "Citation Paper Title": "Title:Differentially Private Generative Adversarial Network",
                        "Citation Paper Abstract": "Abstract:Generative Adversarial Network (GAN) and its variants have recently attracted intensive research interests due to their elegant theoretical foundation and excellent empirical performance as generative models. These tools provide a promising direction in the studies where data availability is limited. One common issue in GANs is that the density of the learned generative distribution could concentrate on the training data points, meaning that they can easily remember training samples due to the high model complexity of deep networks. This becomes a major concern when GANs are applied to private or sensitive data such as patient medical records, and the concentration of distribution may divulge critical patient information. To address this issue, in this paper we propose a differentially private GAN (DPGAN) model, in which we achieve differential privacy in GANs by adding carefully designed noise to gradients during the learning procedure. We provide rigorous proof for the privacy guarantee, as well as comprehensive empirical evidence to support our analysis, where we demonstrate that our method can generate high quality data points at a reasonable privacy level.",
                        "Citation Paper Authors": "Authors:Liyang Xie, Kaixiang Lin, Shu Wang, Fei Wang, Jiayu Zhou"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "showed\nthat a fixed \u03f5does not guarantee a certain level of confi-\ndentiality or utility on differentially-private data created\nusing noise-added covariances approach nor using added\nnoise to the cumulative distribution function.\nDeep Learning-based solutions such as CT-\nGAN ",
                    "Citation Text": "L. Xu, M. Skoularidou, A. Cuesta-Infante, and\nK. Veeramachaneni ,Modeling tabular data using\nconditional gan , Advances in Neural Information Pro-\ncessing Systems, 32 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.00503",
                        "Citation Paper Title": "Title:Modeling Tabular data using Conditional GAN",
                        "Citation Paper Abstract": "Abstract:Modeling the probability distribution of rows in tabular data and generating realistic synthetic data is a non-trivial task. Tabular data usually contains a mix of discrete and continuous columns. Continuous columns may have multiple modes whereas discrete columns are sometimes imbalanced making the modeling difficult. Existing statistical and deep neural network models fail to properly model this type of data. We design TGAN, which uses a conditional generative adversarial network to address these challenges. To aid in a fair and thorough comparison, we design a benchmark with 7 simulated and 8 real datasets and several Bayesian network baselines. TGAN outperforms Bayesian methods on most of the real datasets whereas other deep learning methods could not.",
                        "Citation Paper Authors": "Authors:Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, Kalyan Veeramachaneni"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.05176v3": {
            "Paper Title": "Adore: Differentially Oblivious Relational Database Operators",
            "Sentences": [
                {
                    "Sentence ID": 74,
                    "Sentence": ".\nSince then, various ORAM schemes and hardware implementations\nwere proposed, such as Path ORAM ",
                    "Citation Text": "Emil Stefanov, Marten van Dijk, Elaine Shi, Christopher W. Fletcher, Ling Ren,\nXiangyao Yu, and Srinivas Devadas. 2013. Path ORAM: an extremely simple\noblivious RAM protocol. In CCS. 299\u2013310.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1202.5150",
                        "Citation Paper Title": "Title:Path ORAM: An Extremely Simple Oblivious RAM Protocol",
                        "Citation Paper Abstract": "Abstract:We present Path ORAM, an extremely simple Oblivious RAM protocol with a small amount of client storage. Partly due to its simplicity, Path ORAM is the most practical ORAM scheme known to date with small client storage. We formally prove that Path ORAM has a O(log N) bandwidth cost for blocks of size B = Omega(log^2 N) bits. For such block sizes, Path ORAM is asymptotically better than the best known ORAM schemes with small client storage. Due to its practicality, Path ORAM has been adopted in the design of secure processors since its proposal.",
                        "Citation Paper Authors": "Authors:Emil Stefanov, Marten van Dijk, Elaine Shi, T-H. Hubert Chan, Christopher Fletcher, Ling Ren, Xiangyao Yu, Srinivas Devadas"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "is an oblivious search index whose internal memory\naccess is also oblivious. Shrinkwrap ",
                    "Citation Text": "Johes Bater, Xi He, William Ehrich, Ashwin Machanavajjhala, and Jennie Rogers.\n2018. Shrinkwrap: Efficient SQL Query Processing in Differentially Private Data\nFederations. VLDB 12, 3 (2018), 307\u2013320.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.01816",
                        "Citation Paper Title": "Title:Shrinkwrap: Differentially-Private Query Processing in Private Data Federations",
                        "Citation Paper Abstract": "Abstract:A private data federation is a set of autonomous databases that share a unified query interface offering in-situ evaluation of SQL queries over the union of the sensitive data of its members. Owing to privacy concerns, these systems do not have a trusted data collector that can see all their data and their member databases cannot learn about individual records of other engines. Federations currently achieve this goal by evaluating queries obliviously using secure multiparty computation. This hides the intermediate result cardinality of each query operator by exhaustively padding it. With cascades of such operators, this padding accumulates to a blow-up in the output size of each operator and a proportional loss in query performance. Hence, existing private data federations do not scale well to complex SQL queries over large datasets.\nWe introduce Shrinkwrap, a private data federation that offers data owners a differentially private view of the data held by others to improve their performance over oblivious query processing. Shrinkwrap uses computational differential privacy to minimize the padding of intermediate query results, achieving up to 35X performance improvement over oblivious query processing. When the query needs differentially private output, Shrinkwrap provides a trade-off between result accuracy and query evaluation performance.",
                        "Citation Paper Authors": "Authors:Johes Bater, Xi He, William Ehrich, Ashwin Machanavajjhala, Jennie Rogers"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.11277v3": {
            "Paper Title": "FormatFuzzer: Effective Fuzzing of Binary File Formats",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.08570v2": {
            "Paper Title": "A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy,\n  Robustness, Fairness, and Explainability",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.03652v2": {
            "Paper Title": "Private independence testing across two parties",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.10571v6": {
            "Paper Title": "Always on Voting: A Framework for Repetitive Voting on the Blockchain",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.01819v4": {
            "Paper Title": "Adversarial Camouflage for Node Injection Attack on Graphs",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": ". Anomaly detection methods can be categorized into five types ",
                    "Citation Text": "Han, S., Hu, X., Huang, H., Jiang, M., Zhao, Y., 2022. Adbench: Anomaly detection benchmark. arXiv preprint arXiv:2206.09426 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.09426",
                        "Citation Paper Title": "Title:ADBench: Anomaly Detection Benchmark",
                        "Citation Paper Abstract": "Abstract:Given a long list of anomaly detection algorithms developed in the last few decades, how do they perform with regard to (i) varying levels of supervision, (ii) different types of anomalies, and (iii) noisy and corrupted data? In this work, we answer these key questions by conducting (to our best knowledge) the most comprehensive anomaly detection benchmark with 30 algorithms on 57 benchmark datasets, named ADBench. Our extensive experiments (98,436 in total) identify meaningful insights into the role of supervision and anomaly types, and unlock future directions for researchers in algorithm selection and design. With ADBench, researchers can easily conduct comprehensive and fair evaluations for newly proposed methods on the datasets (including our contributed ones from natural language and computer vision domains) against the existing baselines. To foster accessibility and reproducibility, we fully open-source ADBench and the corresponding results.",
                        "Citation Paper Authors": "Authors:Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, Yue Zhao"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "provides an approximate closed-form attack solution for\nattacking a specific GNN, i.e., SGC ",
                    "Citation Text": "Wu, F., Souza, A., Zhang, T., Fifty, C., Yu, T., Weinberger, K., 2019. Simplifying graph convolutional networks, in: Proceedings of the 36th\nInternational Conference on Machine Learning (ICML 2019), pp. 6861\u20136871.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.07153",
                        "Citation Paper Title": "Title:Simplifying Graph Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:Graph Convolutional Networks (GCNs) and their variants have experienced significant attention and have become the de facto methods for learning graph representations. GCNs derive inspiration primarily from recent deep learning approaches, and as a result, may inherit unnecessary complexity and redundant computation. In this paper, we reduce this excess complexity through successively removing nonlinearities and collapsing weight matrices between consecutive layers. We theoretically analyze the resulting linear model and show that it corresponds to a fixed low-pass filter followed by a linear classifier. Notably, our experimental evaluation demonstrates that these simplifications do not negatively impact accuracy in many downstream applications. Moreover, the resulting model scales to larger datasets, is naturally interpretable, and yields up to two orders of magnitude speedup over FastGCN.",
                        "Citation Paper Authors": "Authors:Felix Wu, Tianyi Zhang, Amauri Holanda de Souza Jr., Christopher Fifty, Tao Yu, Kilian Q. Weinberger"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ".\n\u2022Defense methods defend adversarial attacks via adversarial training, or attention mechanism [29, 17]. For each\ncategory, we adopt the most representative methods. For adversarial training, we leverage FLAG ",
                    "Citation Text": "Kong, K., Li, G., Ding, M., Wu, Z., Zhu, C., Ghanem, B., Taylor, G., Goldstein, T., 2022. Robust optimization as data augmentation for\nlarge-scale graphs, in: Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR 2022), pp. 60\u201369.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.09891",
                        "Citation Paper Title": "Title:Robust Optimization as Data Augmentation for Large-scale Graphs",
                        "Citation Paper Abstract": "Abstract:Data augmentation helps neural networks generalize better by enlarging the training set, but it remains an open question how to effectively augment graph data to enhance the performance of GNNs (Graph Neural Networks). While most existing graph regularizers focus on manipulating graph topological structures by adding/removing edges, we offer a method to augment node features for better performance. We propose FLAG (Free Large-scale Adversarial Augmentation on Graphs), which iteratively augments node features with gradient-based adversarial perturbations during training. By making the model invariant to small fluctuations in input data, our method helps models generalize to out-of-distribution samples and boosts model performance at test time. FLAG is a general-purpose approach for graph data, which universally works in node classification, link prediction, and graph classification tasks. FLAG is also highly flexible and scalable, and is deployable with arbitrary GNN backbones and large-scale datasets. We demonstrate the efficacy and stability of our method through extensive experiments and ablation studies. We also provide intuitive observations for a deeper understanding of our method.",
                        "Citation Paper Authors": "Authors:Kezhi Kong, Guohao Li, Mucong Ding, Zuxuan Wu, Chen Zhu, Bernard Ghanem, Gavin Taylor, Tom Goldstein"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ", namely probabilistic,\nlinear model, proximity-based, outlier ensemble, and neural network methods. We select state-of-the-art methods for\neach,specificallyCOPOD ",
                    "Citation Text": "Li, Z., Zhao, Y., Botta, N., Ionescu, C., Hu, X., 2020. COPOD: copula-based outlier detection, in: 20th IEEE International Conference on\nData Mining (ICDM 2020), pp. 1118\u20131123.\nS. Tao, Q. Cao, H. Shen et al.: Preprint submitted to Elsevier Page 15 of 16",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.09463",
                        "Citation Paper Title": "Title:COPOD: Copula-Based Outlier Detection",
                        "Citation Paper Abstract": "Abstract:Outlier detection refers to the identification of rare items that are deviant from the general data distribution. Existing approaches suffer from high computational complexity, low predictive capability, and limited interpretability. As a remedy, we present a novel outlier detection algorithm called COPOD, which is inspired by copulas for modeling multivariate data distribution. COPOD first constructs an empirical copula, and then uses it to predict tail probabilities of each given data point to determine its level of \"extremeness\". Intuitively, we think of this as calculating an anomalous p-value. This makes COPOD both parameter-free, highly interpretable, and computationally efficient. In this work, we make three key contributions, 1) propose a novel, parameter-free outlier detection algorithm with both great performance and interpretability, 2) perform extensive experiments on 30 benchmark datasets to show that COPOD outperforms in most cases and is also one of the fastest algorithms, and 3) release an easy-to-use Python implementation for reproducibility.",
                        "Citation Paper Authors": "Authors:Zheng Li, Yue Zhao, Nicola Botta, Cezar Ionescu, Xiyang Hu"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "which consists of a focal node (ego) and its \ud835\udc3f-hop neighbor\nnodes ",
                    "Citation Text": "Wu, Q., Zhang, H., Yan, J., Wipf, D., 2022a. Handling distribution shifts on graphs: An invariance perspective, in: International Conference\non Learning Representations (ICLR 2022).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.02466",
                        "Citation Paper Title": "Title:Handling Distribution Shifts on Graphs: An Invariance Perspective",
                        "Citation Paper Abstract": "Abstract:There is increasing evidence suggesting neural networks' sensitivity to distribution shifts, so that research on out-of-distribution (OOD) generalization comes into the spotlight. Nonetheless, current endeavors mostly focus on Euclidean data, and its formulation for graph-structured data is not clear and remains under-explored, given two-fold fundamental challenges: 1) the inter-connection among nodes in one graph, which induces non-IID generation of data points even under the same environment, and 2) the structural information in the input graph, which is also informative for prediction. In this paper, we formulate the OOD problem on graphs and develop a new invariant learning approach, Explore-to-Extrapolate Risk Minimization (EERM), that facilitates graph neural networks to leverage invariance principles for prediction. EERM resorts to multiple context explorers (specified as graph structure editers in our case) that are adversarially trained to maximize the variance of risks from multiple virtual environments. Such a design enables the model to extrapolate from a single observed environment which is the common case for node-level prediction. We prove the validity of our method by theoretically showing its guarantee of a valid OOD solution and further demonstrate its power on various real-world datasets for handling distribution shifts from artificial spurious features, cross-domain transfers and dynamic graph evolution.",
                        "Citation Paper Authors": "Authors:Qitian Wu, Hengrui Zhang, Junchi Yan, David Wipf"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "forneuralnetworkmethods.Thesemethodsare\nused to detect the injected nodes in this paper.\nDefense methods. The defense methods can be mainly categorized into ",
                    "Citation Text": "Jin, W., Li, Y., Xu, H., Wang, Y., Tang, J., 2021b. Adversarial attacks and defenses on graphs: A review and empirical study. SIGKDD\nExploration Newsletter 22, 19\u201334.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.00653",
                        "Citation Paper Title": "Title:Adversarial Attacks and Defenses on Graphs: A Review, A Tool and Empirical Studies",
                        "Citation Paper Abstract": "Abstract:Deep neural networks (DNNs) have achieved significant performance in various tasks. However, recent studies have shown that DNNs can be easily fooled by small perturbation on the input, called adversarial attacks. As the extensions of DNNs to graphs, Graph Neural Networks (GNNs) have been demonstrated to inherit this vulnerability. Adversary can mislead GNNs to give wrong predictions by modifying the graph structure such as manipulating a few edges. This vulnerability has arisen tremendous concerns for adapting GNNs in safety-critical applications and has attracted increasing research attention in recent years. Thus, it is necessary and timely to provide a comprehensive overview of existing graph adversarial attacks and the countermeasures. In this survey, we categorize existing attacks and defenses, and review the corresponding state-of-the-art methods. Furthermore, we have developed a repository with representative algorithms (this https URL). The repository enables us to conduct empirical studies to deepen our understandings on attacks and defenses on graphs.",
                        "Citation Paper Authors": "Authors:Wei Jin, Yaxin Li, Han Xu, Yiqi Wang, Shuiwang Ji, Charu Aggarwal, Jiliang Tang"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": ". Node injection\nattack [30, 36] focuses on a more practical scenario, which only injects some malicious nodes without modifying\noriginal node features or edges. Specifically, AFGSM ",
                    "Citation Text": "Wang, J., Luo, M., Suya, F., Li, J., Yang, Z., Zheng, Q., 2020. Scalable attack on graph data by injecting vicious nodes. arXiv preprint\narXiv:2004.13825 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.13825",
                        "Citation Paper Title": "Title:Scalable Attack on Graph Data by Injecting Vicious Nodes",
                        "Citation Paper Abstract": "Abstract:Recent studies have shown that graph convolution networks (GCNs) are vulnerable to carefully designed attacks, which aim to cause misclassification of a specific node on the graph with unnoticeable perturbations. However, a vast majority of existing works cannot handle large-scale graphs because of their high time complexity. Additionally, existing works mainly focus on manipulating existing nodes on the graph, while in practice, attackers usually do not have the privilege to modify information of existing nodes. In this paper, we develop a more scalable framework named Approximate Fast Gradient Sign Method (AFGSM) which considers a more practical attack scenario where adversaries can only inject new vicious nodes to the graph while having no control over the original graph. Methodologically, we provide an approximation strategy to linearize the model we attack and then derive an approximate closed-from solution with a lower time cost. To have a fair comparison with existing attack methods that manipulate the original graph, we adapt them to the new attack scenario by injecting vicious nodes. Empirical experimental results show that our proposed attack method can significantly reduce the classification accuracy of GCNs and is much faster than existing methods without jeopardizing the attack performance.",
                        "Citation Paper Authors": "Authors:Jihong Wang, Minnan Luo, Fnu Suya, Jundong Li, Zijiang Yang, Qinghua Zheng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.14417v3": {
            "Paper Title": "Fraud Dataset Benchmark and Applications",
            "Sentences": [
                {
                    "Sentence ID": 69,
                    "Sentence": "Avital Oliver, Augustus Odena, Colin A Raffel, Ekin Dogus Cubuk, and Ian Goodfellow. Real-\nistic evaluation of deep semi-supervised learning algorithms. Advances inneural information\nprocessing systems, 31, 2018. ",
                    "Citation Text": "Massih-Reza Amini, Vasilii Feofanov, Loic Pauletto, Emilie Devijver, and Yury Maximov.\nSelf-training: A survey. arXiv preprint arXiv:2202.12040, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.12040",
                        "Citation Paper Title": "Title:Self-Training: A Survey",
                        "Citation Paper Abstract": "Abstract:Semi-supervised algorithms aim to learn prediction functions from a small set of labeled observations and a large set of unlabeled observations. Because this framework is relevant in many applications, they have received a lot of interest in both academia and industry. Among the existing techniques, self-training methods have undoubtedly attracted greater attention in recent years. These models are designed to find the decision boundary on low density regions without making additional assumptions about the data distribution, and use the unsigned output score of a learned classifier, or its margin, as an indicator of confidence. The working principle of self-training algorithms is to learn a classifier iteratively by assigning pseudo-labels to the set of unlabeled training samples with a margin greater than a certain threshold. The pseudo-labeled examples are then used to enrich the labeled training data and to train a new classifier in conjunction with the labeled training set. In this paper, we present self-training methods for binary and multi-class classification; as well as their variants and two related approaches, namely consistency-based approaches and transductive learning. We examine the impact of significant self-training features on various methods, using different general and image classification benchmarks, and we discuss our ideas for future research in self-training. To the best of our knowledge, this is the first thorough and complete survey on this subject.",
                        "Citation Paper Authors": "Authors:Massih-Reza Amini, Vasilii Feofanov, Loic Pauletto, Lies Hadjadj, Emilie Devijver, Yury Maximov"
                    }
                },
                {
                    "Sentence ID": 68,
                    "Sentence": "Isaac Triguero, Salvador Garc\u00eda, and Francisco Herrera. Self-labeled techniques for semi-\nsupervised learning: taxonomy, software and empirical study. Knowledge andInformation\nsystems, 42:245\u2013284, 2015. ",
                    "Citation Text": "Avital Oliver, Augustus Odena, Colin A Raffel, Ekin Dogus Cubuk, and Ian Goodfellow. Real-\nistic evaluation of deep semi-supervised learning algorithms. Advances inneural information\nprocessing systems, 31, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.09170",
                        "Citation Paper Title": "Title:Realistic Evaluation of Deep Semi-Supervised Learning Algorithms",
                        "Citation Paper Abstract": "Abstract:Semi-supervised learning (SSL) provides a powerful framework for leveraging unlabeled data when labels are limited or expensive to obtain. SSL algorithms based on deep neural networks have recently proven successful on standard benchmark tasks. However, we argue that these benchmarks fail to address many issues that these algorithms would face in real-world applications. After creating a unified reimplementation of various widely-used SSL techniques, we test them in a suite of experiments designed to address these issues. We find that the performance of simple baselines which do not use unlabeled data is often underreported, that SSL methods differ in sensitivity to the amount of labeled and unlabeled data, and that performance can degrade substantially when the unlabeled dataset contains out-of-class examples. To help guide SSL research towards real-world applicability, we make our unified reimplemention and evaluation platform publicly available.",
                        "Citation Paper Authors": "Authors:Avital Oliver, Augustus Odena, Colin Raffel, Ekin D. Cubuk, Ian J. Goodfellow"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": "Liudmila Prokhorenkova, Gleb Gusev, Aleksandr V orobev, Anna Veronika Dorogush, and\nAndrey Gulin. Catboost: unbiased boosting with categorical features. Advances inneural\ninformation processing systems, 31, 2018. ",
                    "Citation Text": "Curtis Northcutt, Lu Jiang, and Isaac Chuang. Confident learning: Estimating uncertainty in\ndataset labels. Journal ofArtificial Intelligence Research, 70:1373\u20131411, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.00068",
                        "Citation Paper Title": "Title:Confident Learning: Estimating Uncertainty in Dataset Labels",
                        "Citation Paper Abstract": "Abstract:Learning exists in the context of data, yet notions of confidence typically focus on model predictions, not label quality. Confident learning (CL) is an alternative approach which focuses instead on label quality by characterizing and identifying label errors in datasets, based on the principles of pruning noisy data, counting with probabilistic thresholds to estimate noise, and ranking examples to train with confidence. Whereas numerous studies have developed these principles independently, here, we combine them, building on the assumption of a class-conditional noise process to directly estimate the joint distribution between noisy (given) labels and uncorrupted (unknown) labels. This results in a generalized CL which is provably consistent and experimentally performant. We present sufficient conditions where CL exactly finds label errors, and show CL performance exceeding seven recent competitive approaches for learning with noisy labels on the CIFAR dataset. Uniquely, the CL framework is not coupled to a specific data modality or model (e.g., we use CL to find several label errors in the presumed error-free MNIST dataset and improve sentiment classification on text data in Amazon Reviews). We also employ CL on ImageNet to quantify ontological class overlap (e.g., estimating 645 \"missile\" images are mislabeled as their parent class \"projectile\"), and moderately increase model accuracy (e.g., for ResNet) by cleaning data prior to training. These results are replicable using the open-source cleanlab release.",
                        "Citation Paper Authors": "Authors:Curtis G. Northcutt, Lu Jiang, Isaac L. Chuang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2201.10115v2": {
            "Paper Title": "Effects of Privacy-Inducing Noise on Welfare and Influence of Referendum\n  Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.11571v2": {
            "Paper Title": "TrustBoost: Boosting Trust among Interoperable Blockchains",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.06676v3": {
            "Paper Title": "A Tagging Solution to Discover IoT Devices in Apartments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.14071v2": {
            "Paper Title": "Verifiable Encodings for Secure Homomorphic Analytics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.08785v2": {
            "Paper Title": "Detecting AutoAttack Perturbations in the Frequency Domain",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.00038v2": {
            "Paper Title": "Differentially Private Optimization on Large Model at Small Cost",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.16242v3": {
            "Paper Title": "Differential Privacy has Bounded Impact on Fairness in Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.08976v2": {
            "Paper Title": "HMT: A Hardware-Centric Hybrid Bonsai Merkle Tree Algorithm for\n  High-Performance Authentication",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": "that is based\non the Repetitive Sequential Traversal (RST) access pattern,\ncommonly used in other recent benchmark suites such as\nShuhai ",
                    "Citation Text": "Zeke Wang, Hongjing Huang, Jie Zhang, and Gustavo Alonso.\nBenchmarking high bandwidth memory on fpgas. arXiv preprint\narXiv:2005.04324 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.04324",
                        "Citation Paper Title": "Title:Benchmarking High Bandwidth Memory on FPGAs",
                        "Citation Paper Abstract": "Abstract:FPGAs are starting to be enhanced with High Bandwidth Memory (HBM) as a way to reduce the memory bandwidth bottleneck encountered in some applications and to give the FPGA more capacity to deal with application state. However, the performance characteristics of HBM are still not well specified, especially in the context of FPGAs. In this paper, we bridge the gap between nominal specifications and actual performance by benchmarkingHBM on a state-of-the-art FPGA, i.e., a Xilinx Alveo U280 featuring a two-stack HBM subsystem. To this end, we propose Shuhai, a benchmarking tool that allows us to demystify all the underlying details of HBM on an FPGA. FPGA-based benchmarking should also provide a more accurate picture of HBM than doing so on CPUs/GPUs, since CPUs/GPUs are noisier systems due to their complex control logic and cache hierarchy. Since the memory itself is complex, leveraging custom hardware logic to benchmark inside an FPGA provides more details as well as accurate and deterministic measurements. We observe that 1) HBM is able to provide up to 425GB/s memory bandwidth, and 2) how HBM is used has a significant impact on performance, which in turn demonstrates the importance of unveiling the performance characteristics of HBM so as to select the best approach. As a yardstick, we also applyShuhaito DDR4to show the differences between HBM and DDR4.Shuhai can be easily generalized to other FPGA boards or other generations of memory, e.g., HBM3, and DDR3. We will makeShuhaiopen-source, benefiting the community",
                        "Citation Paper Authors": "Authors:Zeke Wang, Hongjing Huang, Jie Zhang, Gustavo Alonso"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2207.07444v2": {
            "Paper Title": "Federated Learning with Quantum Secure Aggregation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.13049v4": {
            "Paper Title": "TrojViT: Trojan Insertion in Vision Transformers",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": "to resolve the con-\nflicts between two gradients.\n4. Experimental Methodology\nWe present the details of our experimental methodology\nof TrojViT in this section.\nViT Models . We performed TrojViT attacks on multi-\nple pretrained ViT models including Deit ",
                    "Citation Text": "Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herve Jegou. Training\ndata-efficient image transformers & distillation through at-\ntention. In International Conference on Machine Learning ,\nvolume 139, pages 10347\u201310357, July 2021. 1, 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.12877",
                        "Citation Paper Title": "Title:Training data-efficient image transformers & distillation through attention",
                        "Citation Paper Abstract": "Abstract:Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption.\nIn this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data.\nMore importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.",
                        "Citation Paper Authors": "Authors:Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herv\u00e9 J\u00e9gou"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ". Therefore, attackers of TrojViT\nhave access to the ViTs model architecture, parameters, and\npatch size. Training a ViT model requires complex domain\nexpertise and costs huge amounts of GPU hours ",
                    "Citation Text": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In International Conference on Learning Representa-\ntions , 2021. 1, 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.11929",
                        "Citation Paper Title": "Title:An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
                        "Citation Paper Abstract": "Abstract:While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",
                        "Citation Paper Authors": "Authors:Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "2.1. Vision Transformer\nViT [7,15,23] has demonstrated better performance than\ntraditional CNNs in various computer vision tasks. A ViT\nbreaks down an input image as a series of patches which\ncan be viewed as words in a normal transformer ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Il-\nlia Polosukhin. Attention is all you need. In I. Guyon,\nU. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-\nwanathan, and R. Garnett, editors, Advances in Neural Infor-\nmation Processing Systems , volume 30. Curran Associates,\nInc., 2017. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.16070v2": {
            "Paper Title": "Ethereum Proof-of-Stake under Scrutiny",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": "presents an attack where nodes can reorganize the Tezos\u2019 Em my+ chain and then do a\ndouble-spend attack.Our work followsthe line of research f ocusingon \ufb02aws of Ethereum Proof-of-Stake [ 21,22,26].\nNeu et al. ",
                    "Citation Text": "Joachim Neu, Ertem Nusret Tas, and David Tse. 2021. Ebb- and-Flow Protocols: A Resolution of the Availability-Fina lity Dilemma. In 2021 IEEE\nSymposium onSecurity and Privacy (SP) . 446\u2013465.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.04987",
                        "Citation Paper Title": "Title:Ebb-and-Flow Protocols: A Resolution of the Availability-Finality Dilemma",
                        "Citation Paper Abstract": "Abstract:The CAP theorem says that no blockchain can be live under dynamic participation and safe under temporary network partitions. To resolve this availability-finality dilemma, we formulate a new class of flexible consensus protocols, ebb-and-flow protocols, which support a full dynamically available ledger in conjunction with a finalized prefix ledger. The finalized ledger falls behind the full ledger when the network partitions but catches up when the network heals. Gasper, the current candidate protocol for Ethereum 2.0's beacon chain, combines the finality gadget Casper FFG with the LMD GHOST fork choice rule and aims to achieve this property. However, we discovered an attack in the standard synchronous network model, highlighting a general difficulty with existing finality-gadget-based designs. We present a construction of provably secure ebb-and-flow protocols with optimal resilience. Nodes run an off-the-shelf dynamically available protocol, take snapshots of the growing available ledger, and input them into a separate off-the-shelf BFT protocol to finalize a prefix. We explore connections with flexible BFT and improve upon the state-of-the-art for that problem.",
                        "Citation Paper Authors": "Authors:Joachim Neu, Ertem Nusret Tas, David Tse"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "). Our work lies in this cate-\ngory of formalization by proposing a speci\ufb01cation of Ethere um Proof-of-Stake protocol\u2019sproperties and a high-level\ndescriptionthroughpseudo-code.Forwhatconcernsprotoc olformalizationofEthereumProof-of-Stake, ",
                    "Citation Text": "Vitalik Buterin, Diego Hernandez, Thor Kamphefner, Khi em Pham, Zhi Qiao, Danny Ryan, Juhyeok Sin, Ying Wang, and Yan X Zhang. 2020.\nCombiningGHOST and Casper.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.03052",
                        "Citation Paper Title": "Title:Combining GHOST and Casper",
                        "Citation Paper Abstract": "Abstract:We present \"Gasper,\" a proof-of-stake-based consensus protocol, which is an idealized version of the proposed Ethereum 2.0 beacon chain. The protocol combines Casper FFG, a finality tool, with LMD GHOST, a fork-choice rule. We prove safety, plausible liveness, and probabilistic liveness under different sets of assumptions.",
                        "Citation Paper Authors": "Authors:Vitalik Buterin, Diego Hernandez, Thor Kamphefner, Khiem Pham, Zhi Qiao, Danny Ryan, Juhyeok Sin, Ying Wang, Yan X Zhang"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "does the\n22Seesection4.1.4 for detailson justi\ufb01cation.\n23Seesection4.1.4 for detailson \ufb01nalization.\n21Pavlo\ufb00, Amoussou-Guenou, and Tucci-Piergiovanni.\nsame for the Tendermint\u2019s protocol(the consensus protocol of the Cosmos blockchain ",
                    "Citation Text": "Ethan Buchman,JaeKwon, and ZarkoMilosevic.2018. Thel atest gossiponBFT consensus.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.04938",
                        "Citation Paper Title": "Title:The latest gossip on BFT consensus",
                        "Citation Paper Abstract": "Abstract:The paper presents Tendermint, a new protocol for ordering events in a distributed network under adversarial conditions. More commonly known as Byzantine Fault Tolerant (BFT) consensus or atomic broadcast, the problem has attracted significant attention in recent years due to the widespread success of blockchain-based digital currencies, such as Bitcoin and Ethereum, which successfully solved the problem in a public setting without a central authority. Tendermint modernizes classic academic work on the subject and simplifies the design of the BFT algorithm by relying on a peer-to-peer gossip protocol among nodes.",
                        "Citation Paper Authors": "Authors:Ethan Buchman, Jae Kwon, Zarko Milosevic"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.12964v2": {
            "Paper Title": "Digital Security -- A Question of Perspective. A Large-Scale Telephone\n  Survey with Four At-Risk User Groups",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.05838v5": {
            "Paper Title": "DRAM Bender: An Extensible and Versatile FPGA-based Infrastructure to\n  Easily Test State-of-the-art DRAM Chips",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.03831v2": {
            "Paper Title": "How to Make Your Approximation Algorithm Private: A Black-Box\n  Differentially-Private Transformation for Tunable Approximation Algorithms of\n  Functions with Low Sensitivity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.05114v4": {
            "Paper Title": "Attack time analysis in dynamic attack trees via integer linear\n  programming",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.10133v3": {
            "Paper Title": "Efficient Privacy-Preserving Machine Learning with Lightweight Trusted\n  Hardware",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.04008v3": {
            "Paper Title": "Use of Cryptography in Malware Obfuscation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.04762v3": {
            "Paper Title": "Building Resilience in Cybersecurity -- An Artificial Lab Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.04521v2": {
            "Paper Title": "The Space of Adversarial Strategies",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.03968v2": {
            "Paper Title": "Optimal and Differentially Private Data Acquisition: Central and Local\n  Mechanisms",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.03505v3": {
            "Paper Title": "Sample-Efficient Personalization: Modeling User Parameters as Low Rank\n  Plus Sparse Components",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.11091v4": {
            "Paper Title": "E-DPNCT: An Enhanced Attack Resilient Differential Privacy Model For\n  Smart Grids Using Split Noise Cancellation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.12100v3": {
            "Paper Title": "zPROBE: Zero Peek Robustness Checks for Federated Learning",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": "for aggregation. However, BREA reveals\nthe pairwise distances of client updates to the server, i.e.,\neven one client\u2019s collusion with the server would reveal all\nupdates.\nSHARE ",
                    "Citation Text": "Raj Kiriti Velicheti, Derek Xia, and Oluwasanmi Koyejo.\nSecure byzantine-robust distributed learning via clustering.\narXiv preprint arXiv:2110.02940 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.02940",
                        "Citation Paper Title": "Title:Secure Byzantine-Robust Distributed Learning via Clustering",
                        "Citation Paper Abstract": "Abstract:Federated learning systems that jointly preserve Byzantine robustness and privacy have remained an open problem. Robust aggregation, the standard defense for Byzantine attacks, generally requires server access to individual updates or nonlinear computation -- thus is incompatible with privacy-preserving methods such as secure aggregation via multiparty computation. To this end, we propose SHARE (Secure Hierarchical Robust Aggregation), a distributed learning framework designed to cryptographically preserve client update privacy and robustness to Byzantine adversaries simultaneously. The key idea is to incorporate secure averaging among randomly clustered clients before filtering malicious updates through robust aggregation. Experiments show that SHARE has similar robustness guarantees as existing techniques while enhancing privacy.",
                        "Citation Paper Authors": "Authors:Raj Kiriti Velicheti, Derek Xia, Oluwasanmi Koyejo"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "iteratively scales\nthe accumulated gradients to ensure robust aggregation. The\naforesaid works focus on robust aggregation under IID data\nassumptions. ",
                    "Citation Text": "Sai Praneeth Karimireddy, Lie He, and Martin Jaggi.\nByzantine-robust learning on heterogeneous datasets via buck-\neting. In International Conference on Learning Representa-\ntions , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.09365",
                        "Citation Paper Title": "Title:Byzantine-Robust Learning on Heterogeneous Datasets via Bucketing",
                        "Citation Paper Abstract": "Abstract:In Byzantine robust distributed or federated learning, a central server wants to train a machine learning model over data distributed across multiple workers. However, a fraction of these workers may deviate from the prescribed algorithm and send arbitrary messages. While this problem has received significant attention recently, most current defenses assume that the workers have identical data. For realistic cases when the data across workers are heterogeneous (non-iid), we design new attacks which circumvent current defenses, leading to significant loss of performance. We then propose a simple bucketing scheme that adapts existing robust algorithms to heterogeneous datasets at a negligible computational cost. We also theoretically and experimentally validate our approach, showing that combining bucketing with existing robust algorithms is effective against challenging attacks. Our work is the first to establish guaranteed convergence for the non-iid Byzantine robust problem under realistic assumptions.",
                        "Citation Paper Authors": "Authors:Sai Praneeth Karimireddy, Lie He, Martin Jaggi"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "apply\nByzantine-resilient aggregation rules on a weighted aver-\nage of past gradients using a momentum term. In lieu of\nusing the median, centered clipping ",
                    "Citation Text": "Sai Praneeth Karimireddy, Lie He, and Martin Jaggi. Learning\nfrom history for byzantine robust optimization. In Interna-\ntional Conference on Machine Learning , pages 5311\u20135319.\nPMLR, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.10333",
                        "Citation Paper Title": "Title:Learning from History for Byzantine Robust Optimization",
                        "Citation Paper Abstract": "Abstract:Byzantine robustness has received significant attention recently given its importance for distributed and federated learning. In spite of this, we identify severe flaws in existing algorithms even when the data across the participants is identically distributed. First, we show realistic examples where current state of the art robust aggregation rules fail to converge even in the absence of any Byzantine attackers. Secondly, we prove that even if the aggregation rules may succeed in limiting the influence of the attackers in a single round, the attackers can couple their attacks across time eventually leading to divergence. To address these issues, we present two surprisingly simple strategies: a new robust iterative clipping procedure, and incorporating worker momentum to overcome time-coupled attacks. This is the first provably robust method for the standard stochastic optimization setting. Our code is open sourced at this https URL.",
                        "Citation Paper Authors": "Authors:Sai Praneeth Karimireddy, Lie He, Martin Jaggi"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "augments prior aggregation rules to ensure all coordinates\nare agreed upon by a majority of user gradients.\nSeveral works propose applying robust aggregation over\nan accumulated history of gradients, assuming IID data. ",
                    "Citation Text": "Zeyuan Allen-Zhu, Faeze Ebrahimianghazani, Jerry Li, and\nDan Alistarh. Byzantine-resilient non-convex stochastic gra-\ndient descent. In International Conference on Learning Rep-\nresentations , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.14368",
                        "Citation Paper Title": "Title:Byzantine-Resilient Non-Convex Stochastic Gradient Descent",
                        "Citation Paper Abstract": "Abstract:We study adversary-resilient stochastic distributed optimization, in which $m$ machines can independently compute stochastic gradients, and cooperate to jointly optimize over their local objective functions. However, an $\\alpha$-fraction of the machines are $\\textit{Byzantine}$, in that they may behave in arbitrary, adversarial ways. We consider a variant of this procedure in the challenging $\\textit{non-convex}$ case. Our main result is a new algorithm SafeguardSGD which can provably escape saddle points and find approximate local minima of the non-convex objective. The algorithm is based on a new concentration filtering technique, and its sample and time complexity bounds match the best known theoretical bounds in the stochastic, distributed setting when no Byzantine machines are present.\nOur algorithm is very practical: it improves upon the performance of all prior methods when training deep neural networks, it is relatively lightweight, and it is the first method to withstand two recently-proposed Byzantine attacks.",
                        "Citation Paper Authors": "Authors:Zeyuan Allen-Zhu, Faeze Ebrahimian, Jerry Li, Dan Alistarh"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.08529v3": {
            "Paper Title": "F3B: A Low-Overhead Blockchain Architecture with Per-Transaction\n  Front-Running Protection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.08316v2": {
            "Paper Title": "Boosting the Adversarial Transferability of Surrogate Models with Dark\n  Knowledge",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": ".\n\u2022We have also applied the proposed method to the problem\nof attacking face verification models. On the widely-used\nArcFace model ",
                    "Citation Text": "J. Deng, J. Guo, N. Xue, and S. Zafeiriou, \u201cArcface: Additive angular\nmargin loss for deep face recognition,\u201d in CVPR , 2019, pp. 4690\u20134699.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.07698",
                        "Citation Paper Title": "Title:ArcFace: Additive Angular Margin Loss for Deep Face Recognition",
                        "Citation Paper Abstract": "Abstract:Recently, a popular line of research in face recognition is adopting margins in the well-established softmax loss function to maximize class separability. In this paper, we first introduce an Additive Angular Margin Loss (ArcFace), which not only has a clear geometric interpretation but also significantly enhances the discriminative power. Since ArcFace is susceptible to the massive label noise, we further propose sub-center ArcFace, in which each class contains $K$ sub-centers and training samples only need to be close to any of the $K$ positive sub-centers. Sub-center ArcFace encourages one dominant sub-class that contains the majority of clean faces and non-dominant sub-classes that include hard or noisy faces. Based on this self-propelled isolation, we boost the performance through automatically purifying raw web faces under massive real-world noise. Besides discriminative feature embedding, we also explore the inverse problem, mapping feature vectors to face images. Without training any additional generator or discriminator, the pre-trained ArcFace model can generate identity-preserved face images for both subjects inside and outside the training data only by using the network gradient and Batch Normalization (BN) priors. Extensive experiments demonstrate that ArcFace can enhance the discriminative feature embedding as well as strengthen the generative face synthesis.",
                        "Citation Paper Authors": "Authors:Jiankang Deng, Jia Guo, Jing Yang, Niannan Xue, Irene Kotsia, Stefanos Zafeiriou"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": ".\nThe first three of them are normally trained models, includ-\ning Inception-v3 (Inc-v3) ",
                    "Citation Text": "C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \u201cRethinking\nthe inception architecture for computer vision,\u201d in CVPR , 2016, pp.\n2818\u20132826.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.00567",
                        "Citation Paper Title": "Title:Rethinking the Inception Architecture for Computer Vision",
                        "Citation Paper Abstract": "Abstract:Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.",
                        "Citation Paper Authors": "Authors:Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": ", a surrogate\nmodel is distilled using multiple teacher models. This method\nis inspired by previous works on ensemble attack ",
                    "Citation Text": "Y . Liu, X. Chen, C. Liu, and D. Song, \u201cDelving into transferable\nadversarial examples and black-box attacks,\u201d in ICLR , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.02770",
                        "Citation Paper Title": "Title:Delving into Transferable Adversarial Examples and Black-box Attacks",
                        "Citation Paper Abstract": "Abstract:An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack this http URL, which is a black-box image classification system.",
                        "Citation Paper Authors": "Authors:Yanpei Liu, Xinyun Chen, Chang Liu, Dawn Song"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "that augments the\ninputs with a pre-defined probability pt, and the translation-\ninvariant method (TIM) ",
                    "Citation Text": "Y . Dong, T. Pang, H. Su, and J. Zhu, \u201cEvading defenses to transferable\nadversarial examples by translation-invariant attacks,\u201d in CVPR , 2019,\npp. 4312\u20134321.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.02884",
                        "Citation Paper Title": "Title:Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks",
                        "Citation Paper Abstract": "Abstract:Deep neural networks are vulnerable to adversarial examples, which can mislead classifiers by adding imperceptible perturbations. An intriguing property of adversarial examples is their good transferability, making black-box attacks feasible in real-world applications. Due to the threat of adversarial attacks, many methods have been proposed to improve the robustness. Several state-of-the-art defenses are shown to be robust against transferable adversarial examples. In this paper, we propose a translation-invariant attack method to generate more transferable adversarial examples against the defense models. By optimizing a perturbation over an ensemble of translated images, the generated adversarial example is less sensitive to the white-box model being attacked and has better transferability. To improve the efficiency of attacks, we further show that our method can be implemented by convolving the gradient at the untranslated image with a pre-defined kernel. Our method is generally applicable to any gradient-based attack method. Extensive experiments on the ImageNet dataset validate the effectiveness of the proposed method. Our best attack fools eight state-of-the-art defenses at an 82% success rate on average based only on the transferability, demonstrating the insecurity of the current defense techniques.",
                        "Citation Paper Authors": "Authors:Yinpeng Dong, Tianyu Pang, Hang Su, Jun Zhu"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "that utilizes a momentum factor\n\u00b5, the diverse inputs method (DIM) ",
                    "Citation Text": "C. Xie, Z. Zhang, Y . Zhou, S. Bai, J. Wang, Z. Ren, and A. L. Yuille,\n\u201cImproving transferability of adversarial examples with input diversity,\u201d\ninCVPR , 2019, pp. 2730\u20132739.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.06978",
                        "Citation Paper Title": "Title:Improving Transferability of Adversarial Examples with Input Diversity",
                        "Citation Paper Abstract": "Abstract:Though CNNs have achieved the state-of-the-art performance on various vision tasks, they are vulnerable to adversarial examples --- crafted by adding human-imperceptible perturbations to clean images. However, most of the existing adversarial attacks only achieve relatively low success rates under the challenging black-box setting, where the attackers have no knowledge of the model structure and parameters. To this end, we propose to improve the transferability of adversarial examples by creating diverse input patterns. Instead of only using the original images to generate adversarial examples, our method applies random transformations to the input images at each iteration. Extensive experiments on ImageNet show that the proposed attack method can generate adversarial examples that transfer much better to different networks than existing baselines. By evaluating our method against top defense solutions and official baselines from NIPS 2017 adversarial competition, the enhanced attack reaches an average success rate of 73.0%, which outperforms the top-1 attack submission in the NIPS competition by a large margin of 6.6%. We hope that our proposed attack strategy can serve as a strong benchmark baseline for evaluating the robustness of networks to adversaries and the effectiveness of different defense methods in the future. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren, Alan Yuille"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.11054v2": {
            "Paper Title": "MLP-Hash: Protecting Face Templates via Hashing of Randomized\n  Multi-Layer Perceptron",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "Umut Uludag, Sharath Pankanti, Salil Prabhakar, and Anil K Jain,\n\u201cBiometric cryptosystems: issues and challenges,\u201d Proceedings of the\nIEEE , vol. 92, no. 6, pp. 948\u2013960, 2004. ",
                    "Citation Text": "Jiankang Deng, Jia Guo, Xue Niannan, and Stefanos Zafeiriou, \u201cArcface:\nAdditive angular margin loss for deep face recognition,\u201d in Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR) , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.07698",
                        "Citation Paper Title": "Title:ArcFace: Additive Angular Margin Loss for Deep Face Recognition",
                        "Citation Paper Abstract": "Abstract:Recently, a popular line of research in face recognition is adopting margins in the well-established softmax loss function to maximize class separability. In this paper, we first introduce an Additive Angular Margin Loss (ArcFace), which not only has a clear geometric interpretation but also significantly enhances the discriminative power. Since ArcFace is susceptible to the massive label noise, we further propose sub-center ArcFace, in which each class contains $K$ sub-centers and training samples only need to be close to any of the $K$ positive sub-centers. Sub-center ArcFace encourages one dominant sub-class that contains the majority of clean faces and non-dominant sub-classes that include hard or noisy faces. Based on this self-propelled isolation, we boost the performance through automatically purifying raw web faces under massive real-world noise. Besides discriminative feature embedding, we also explore the inverse problem, mapping feature vectors to face images. Without training any additional generator or discriminator, the pre-trained ArcFace model can generate identity-preserved face images for both subjects inside and outside the training data only by using the network gradient and Batch Normalization (BN) priors. Extensive experiments demonstrate that ArcFace can enhance the discriminative feature embedding as well as strengthen the generative face synthesis.",
                        "Citation Paper Authors": "Authors:Jiankang Deng, Jia Guo, Jing Yang, Niannan Xue, Irene Kotsia, Stefanos Zafeiriou"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.08295v2": {
            "Paper Title": "A Secure Federated Data-Driven Evolutionary Multi-objective Optimization\n  Algorithm",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": ", where the server aggregates the clients\u2019 updated\nmodels using weighted averaging.\nAlthough it is able to preserve data privacy to a certain\ndegree, the basic FL scheme may be vulnerable to various\nattacks, such as gradient leakage attacks ",
                    "Citation Text": "L. Zhu, Z. Liu, and S. Han, \u201cDeep leakage from gradients,\u201d Advances\nin Neural Information Processing Systems , vol. 32, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.08935",
                        "Citation Paper Title": "Title:Deep Leakage from Gradients",
                        "Citation Paper Abstract": "Abstract:Exchanging gradients is a widely used method in modern multi-node machine learning system (e.g., distributed training, collaborative learning). For a long time, people believed that gradients are safe to share: i.e., the training data will not be leaked by gradient exchange. However, we show that it is possible to obtain the private training data from the publicly shared gradients. We name this leakage as Deep Leakage from Gradient and empirically validate the effectiveness on both computer vision and natural language processing tasks. Experimental results show that our attack is much stronger than previous approaches: the recovery is pixel-wise accurate for images and token-wise matching for texts. We want to raise people's awareness to rethink the gradient's safety. Finally, we discuss several possible strategies to prevent such deep leakage. The most effective defense method is gradient pruning.",
                        "Citation Paper Authors": "Authors:Ligeng Zhu, Zhijian Liu, Song Han"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2207.10802v3": {
            "Paper Title": "Combing for Credentials: Active Pattern Extraction from Smart Reply",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": ".\nWhile much of the privacy focus has been on language\nmodels trained with auto-regressive objective, ",
                    "Citation Text": "Eric Lehman, Sarthak Jain, Karl Pichotta, Yoav Goldberg, and By-\nron C Wallace. Does bert pretrained on clinical notes reveal sensitive\ndata? arXiv:2104.07762 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.07762",
                        "Citation Paper Title": "Title:Does BERT Pretrained on Clinical Notes Reveal Sensitive Data?",
                        "Citation Paper Abstract": "Abstract:Large Transformers pretrained over clinical notes from Electronic Health Records (EHR) have afforded substantial gains in performance on predictive clinical tasks. The cost of training such models (and the necessity of data access to do so) coupled with their utility motivates parameter sharing, i.e., the release of pretrained models such as ClinicalBERT. While most efforts have used deidentified EHR, many researchers have access to large sets of sensitive, non-deidentified EHR with which they might train a BERT model (or similar). Would it be safe to release the weights of such a model if they did? In this work, we design a battery of approaches intended to recover Personal Health Information (PHI) from a trained BERT. Specifically, we attempt to recover patient names and conditions with which they are associated. We find that simple probing methods are not able to meaningfully extract sensitive information from BERT trained over the MIMIC-III corpus of EHR. However, more sophisticated \"attacks\" may succeed in doing so: To facilitate such research, we make our experimental setup and baseline probing models available at this https URL",
                        "Citation Paper Authors": "Authors:Eric Lehman, Sarthak Jain, Karl Pichotta, Yoav Goldberg, Byron C. Wallace"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": ".\nOn the other hand, large language models have been\nshown to leak information in different forms (e.g. training\ndata extraction ",
                    "Citation Text": "Nicholas Carlini, Florian Tram `er, Eric Wallace, Matthew Jagielski,\nAriel Herbert-V oss, Katherine Lee, Adam Roberts, Tom Brown, Dawn\nSong, \u00b4Ulfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting\ntraining data from large language models. In 30th USENIX Security\nSymposium , pages 2633\u20132650, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.07805",
                        "Citation Paper Title": "Title:Extracting Training Data from Large Language Models",
                        "Citation Paper Abstract": "Abstract:It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model.\nWe demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data.\nWe comprehensively evaluate our extraction attack to understand the factors that contribute to its success. Worryingly, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.",
                        "Citation Paper Authors": "Authors:Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, Colin Raffel"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "is orders of magnitudes higher than\nours for their attacks to be effective (0.3% vs our 0.005%).\nComparison with ",
                    "Citation Text": "Nils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas\nWutschitz, and Santiago Zanella-B \u00b4eguelin. Analyzing leak-\nage of personally identifiable information in language models.\narXiv:2302.00539 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.00539",
                        "Citation Paper Title": "Title:Analyzing Leakage of Personally Identifiable Information in Language Models",
                        "Citation Paper Abstract": "Abstract:Language Models (LMs) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks. Understanding the risk of LMs leaking Personally Identifiable Information (PII) has received less attention, which can be attributed to the false assumption that dataset curation techniques such as scrubbing are sufficient to prevent PII leakage. Scrubbing techniques reduce but do not prevent the risk of PII leakage: in practice scrubbing is imperfect and must balance the trade-off between minimizing disclosure and preserving the utility of the dataset. On the other hand, it is unclear to which extent algorithmic defenses such as differential privacy, designed to guarantee sentence- or user-level privacy, prevent PII disclosure. In this work, we introduce rigorous game-based definitions for three types of PII leakage via black-box extraction, inference, and reconstruction attacks with only API access to an LM. We empirically evaluate the attacks against GPT-2 models fine-tuned with and without defenses in three domains: case law, health care, and e-mails. Our main contributions are (i) novel attacks that can extract up to 10$\\times$ more PII sequences than existing attacks, (ii) showing that sentence-level differential privacy reduces the risk of PII disclosure but still leaks about 3% of PII sequences, and (iii) a subtle connection between record-level membership inference and PII reconstruction. Code to reproduce all experiments in the paper is available at this https URL.",
                        "Citation Paper Authors": "Authors:Nils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas Wutschitz, Santiago Zanella-B\u00e9guelin"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "have transformed the field of natural language\nprocessing. These pre-trained models can be fine-tuned on\na wide range of downstream tasks to provide impressive\nperformance and unprecedented abilities so far ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBERT: Pre-training of deep bidirectional transformers for language\nunderstanding. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers) ,\nNAACL-HLT \u201919, pages 4171\u20134186, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "investigates the trade-off between memorization\nand model size, data sample repetition and the context on\nwhich the extraction is aimed. Relevantly, recent work has\nshown that deduplicating training data mitigates privacy\nrisks ",
                    "Citation Text": "Nikhil Kandpal, Eric Wallace, and Colin Raffel. Deduplicat-\ning training data mitigates privacy risks in language models.\narXiv:2202.06539 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.06539",
                        "Citation Paper Title": "Title:Deduplicating Training Data Mitigates Privacy Risks in Language Models",
                        "Citation Paper Abstract": "Abstract:Past work has shown that large language models are susceptible to privacy attacks, where adversaries generate sequences from a trained model and detect which sequences are memorized from the training set. In this work, we show that the success of these attacks is largely due to duplication in commonly used web-scraped training sets. We first show that the rate at which language models regenerate training sequences is superlinearly related to a sequence's count in the training set. For instance, a sequence that is present 10 times in the training data is on average generated ~1000 times more often than a sequence that is present only once. We next show that existing methods for detecting memorized sequences have near-chance accuracy on non-duplicated training sequences. Finally, we find that after applying methods to deduplicate training data, language models are considerably more secure against these types of privacy attacks. Taken together, our results motivate an increased focus on deduplication in privacy-sensitive applications and a reevaluation of the practicality of existing privacy attacks.",
                        "Citation Paper Authors": "Authors:Nikhil Kandpal, Eric Wallace, Colin Raffel"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "demonstrates that Model API access to both pre-trained and\nfine-tuned versions of a language model can be exploited\nby an adversary to extract sensitive sequences from the\ntypically more sensitive fine-tuning dataset. ",
                    "Citation Text": "Huseyin A Inan, Osman Ramadan, Lukas Wutschitz, Daniel Jones,\nVictor R \u00a8uhle, James Withers, and Robert Sim. Training data leakage\nanalysis in language models. arXiv:2101.05405 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.05405",
                        "Citation Paper Title": "Title:Training Data Leakage Analysis in Language Models",
                        "Citation Paper Abstract": "Abstract:Recent advances in neural network based language models lead to successful deployments of such models, improving user experience in various applications. It has been demonstrated that strong performance of language models comes along with the ability to memorize rare training samples, which poses serious privacy threats in case the model is trained on confidential user content. In this work, we introduce a methodology that investigates identifying the user content in the training data that could be leaked under a strong and realistic threat model. We propose two metrics to quantify user-level data leakage by measuring a model's ability to produce unique sentence fragments within training data. Our metrics further enable comparing different models trained on the same data in terms of privacy. We demonstrate our approach through extensive numerical studies on both RNN and Transformer based models. We further illustrate how the proposed metrics can be utilized to investigate the efficacy of mitigations like differentially private training or API hardening.",
                        "Citation Paper Authors": "Authors:Huseyin A. Inan, Osman Ramadan, Lukas Wutschitz, Daniel Jones, Victor R\u00fchle, James Withers, Robert Sim"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.02617v6": {
            "Paper Title": "Individual Privacy Accounting for Differentially Private Stochastic\n  Gradient Descent",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.02339v2": {
            "Paper Title": "TransCAB: Transferable Clean-Annotation Backdoor to Object Detection\n  with Natural Trigger in Real-World",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": ", which are not immediately applicable for efficiently\nthwarting backdoor attacks on object detection. Here, we focus\non countering TransCAB from detecting the poisoned image\nor preventing its camouflage effect.\nAttack Image Detection. We apply the state-of-the-art de-\ntection countermeasure ",
                    "Citation Text": "B. Kim, A. Abuadbba, Y . Gao, Y . Zheng, M. E. Ahmed, S. Nepal, and\nH. Kim, \u201cDecamouflage: A framework to detect image-scaling attacks\non CNN,\u201d in Proc. DSN , pp. 63\u201374, IEEE, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.03735",
                        "Citation Paper Title": "Title:Decamouflage: A Framework to Detect Image-Scaling Attacks on Convolutional Neural Networks",
                        "Citation Paper Abstract": "Abstract:As an essential processing step in computer vision applications, image resizing or scaling, more specifically downsampling, has to be applied before feeding a normally large image into a convolutional neural network (CNN) model because CNN models typically take small fixed-size images as inputs. However, image scaling functions could be adversarially abused to perform a newly revealed attack called image-scaling attack, which can affect a wide range of computer vision applications building upon image-scaling functions.\nThis work presents an image-scaling attack detection framework, termed as Decamouflage. Decamouflage consists of three independent detection methods: (1) rescaling, (2) filtering/pooling, and (3) steganalysis. While each of these three methods is efficient standalone, they can work in an ensemble manner not only to improve the detection accuracy but also to harden potential adaptive attacks. Decamouflage has a pre-determined detection threshold that is generic. More precisely, as we have validated, the threshold determined from one dataset is also applicable to other different datasets. Extensive experiments show that Decamouflage achieves detection accuracy of 99.9\\% and 99.8\\% in the white-box (with the knowledge of attack algorithms) and the black-box (without the knowledge of attack algorithms) settings, respectively. To corroborate the efficiency of Decamouflage, we have also measured its run-time overhead on a personal PC with an i5 CPU and found that Decamouflage can detect image-scaling attacks in milliseconds. Overall, Decamouflage can accurately detect image scaling attacks in both white-box and black-box settings with acceptable run-time overhead.",
                        "Citation Paper Authors": "Authors:Bedeuro Kim, Alsharif Abuadbba, Yansong Gao, Yifeng Zheng, Muhammad Ejaz Ahmed, Hyoungshick Kim, Surya Nepal"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.12033v4": {
            "Paper Title": "Strategic Liquidity Provision in Uniswap v3",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.13063v3": {
            "Paper Title": "Scalable Program Clone Search Through Spectral Analysis",
            "Sentences": [
                {
                    "Sentence ID": 74,
                    "Sentence": "extract features such as\nthe number of arithmetic instructions. Gemini ",
                    "Citation Text": "Xiaojun Xu, Chang Liu, Qian Feng, Heng Yin, Le Song, and Dawn Song. 2017.\nNeural Network-based Graph Embedding for Cross-Platform Binary Code Simi-\nlarity Detection. In Proceedings of the 2017 ACM SIGSAC Conference on Computer\nand Communications Security . https://doi.org/10.1145/3133956.3134018",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.06525",
                        "Citation Paper Title": "Title:Neural Network-based Graph Embedding for Cross-Platform Binary Code Similarity Detection",
                        "Citation Paper Abstract": "Abstract:The problem of cross-platform binary code similarity detection aims at detecting whether two binary functions coming from different platforms are similar or not. It has many security applications, including plagiarism detection, malware detection, vulnerability search, etc. Existing approaches rely on approximate graph matching algorithms, which are inevitably slow and sometimes inaccurate, and hard to adapt to a new task. To address these issues, in this work, we propose a novel neural network-based approach to compute the embedding, i.e., a numeric vector, based on the control flow graph of each binary function, then the similarity detection can be done efficiently by measuring the distance between the embeddings for two functions. We implement a prototype called Gemini. Our extensive evaluation shows that Gemini outperforms the state-of-the-art approaches by large margins with respect to similarity detection accuracy. Further, Gemini can speed up prior art's embedding generation time by 3 to 4 orders of magnitude and reduce the required training time from more than 1 week down to 30 minutes to 10 hours. Our real world case studies demonstrate that Gemini can identify significantly more vulnerable firmware images than the state-of-the-art, i.e., Genius. Our research showcases a successful application of deep learning on computer security problems.",
                        "Citation Paper Authors": "Authors:Xiaojun Xu, Chang Liu, Qian Feng, Heng Yin, Le Song, Dawn Song"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.13230v3": {
            "Paper Title": "Active TLS Stack Fingerprinting: Characterizing TLS Server Deployments\n  at Scale",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.10533v3": {
            "Paper Title": "CGuard: Efficient Spatial Safety for C",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.01585v4": {
            "Paper Title": "Differentially Private Sampling from Rashomon Sets, and the Universality\n  of Langevin Diffusion for Convex Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.06228v2": {
            "Paper Title": "Unifying Gradients to Improve Real-world Robustness for Deep Networks",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": ". We here adopt two classical and practical steal-based attacks, eg. MAZE6 ",
                    "Citation Text": "Sanjay Kariyappa, Atul Prakash, and Moinuddin K Qureshi. 2021. Maze: Data-free model stealing attack using zeroth-\norder gradient estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .\n13814\u201313823.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.03161",
                        "Citation Paper Title": "Title:MAZE: Data-Free Model Stealing Attack Using Zeroth-Order Gradient Estimation",
                        "Citation Paper Abstract": "Abstract:Model Stealing (MS) attacks allow an adversary with black-box access to a Machine Learning model to replicate its functionality, compromising the confidentiality of the model. Such attacks train a clone model by using the predictions of the target model for different inputs. The effectiveness of such attacks relies heavily on the availability of data necessary to query the target model. Existing attacks either assume partial access to the dataset of the target model or availability of an alternate dataset with semantic similarities. This paper proposes MAZE -- a data-free model stealing attack using zeroth-order gradient estimation. In contrast to prior works, MAZE does not require any data and instead creates synthetic data using a generative model. Inspired by recent works in data-free Knowledge Distillation (KD), we train the generative model using a disagreement objective to produce inputs that maximize disagreement between the clone and the target model. However, unlike the white-box setting of KD, where the gradient information is available, training a generator for model stealing requires performing black-box optimization, as it involves accessing the target model under attack. MAZE relies on zeroth-order gradient estimation to perform this optimization and enables a highly accurate MS attack. Our evaluation with four datasets shows that MAZE provides a normalized clone accuracy in the range of 0.91x to 0.99x, and outperforms even the recent attacks that rely on partial data (JBDA, clone accuracy 0.13x to 0.69x) and surrogate data (KnockoffNets, clone accuracy 0.52x to 0.97x). We also study an extension of MAZE in the partial-data setting and develop MAZE-PD, which generates synthetic data closer to the target distribution. MAZE-PD further improves the clone accuracy (0.97x to 1.0x) and reduces the query required for the attack by 2x-24x.",
                        "Citation Paper Authors": "Authors:Sanjay Kariyappa, Atul Prakash, Moinuddin Qureshi"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": ".\nThe first one could utilize model outputs to estimate a surrogate model and then use the white-\nbox adversarial examples of the surrogate model to attack the original model based on attack\ntransferability ",
                    "Citation Text": "Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. 2017.\nPractical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia conference on computer\nand communications security . 506\u2013519.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.02697",
                        "Citation Paper Title": "Title:Practical Black-Box Attacks against Machine Learning",
                        "Citation Paper Abstract": "Abstract:Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19% and 88.94%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.",
                        "Citation Paper Authors": "Authors:Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik, Ananthram Swami"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": ")\napproaches. The first one is the most popular defense, the second to fourth ones belong to RI, the\nfifth one is a dynamic defense, and the rest are denoising methods. The AT models are obtained from\nRobustbench1 ",
                    "Citation Text": "Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang,\nPrateek Mittal, and Matthias Hein. 2021. RobustBench: a standardized adversarial robustness benchmark. In Thirty-fifth\nConference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2) .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.09670",
                        "Citation Paper Title": "Title:RobustBench: a standardized adversarial robustness benchmark",
                        "Citation Paper Abstract": "Abstract:As a research community, we are still lacking a systematic understanding of the progress on adversarial robustness which often makes it hard to identify the most promising ideas in training robust models. A key challenge in benchmarking robustness is that its evaluation is often error-prone leading to robustness overestimation. Our goal is to establish a standardized benchmark of adversarial robustness, which as accurately as possible reflects the robustness of the considered models within a reasonable computational budget. To this end, we start by considering the image classification task and introduce restrictions (possibly loosened in the future) on the allowed models. We evaluate adversarial robustness with AutoAttack, an ensemble of white- and black-box attacks, which was recently shown in a large-scale study to improve almost all robustness evaluations compared to the original publications. To prevent overadaptation of new defenses to AutoAttack, we welcome external evaluations based on adaptive attacks, especially where AutoAttack flags a potential overestimation of robustness. Our leaderboard, hosted at this https URL, contains evaluations of 120+ models and aims at reflecting the current state of the art in image classification on a set of well-defined tasks in $\\ell_\\infty$- and $\\ell_2$-threat models and on common corruptions, with possible extensions in the future. Additionally, we open-source the library this https URL that provides unified access to 80+ robust models to facilitate their downstream applications. Finally, based on the collected models, we analyze the impact of robustness on the performance on distribution shifts, calibration, out-of-distribution detection, fairness, privacy leakage, smoothness, and transferability.",
                        "Citation Paper Authors": "Authors:Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, Matthias Hein"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "proposes to use a pre-trained diffusion model as the input denoising network. ",
                    "Citation Text": "Motasem Alfarra, Juan C P\u00e9rez, Ali Thabet, Adel Bibi, Philip HS Torr, and Bernard Ghanem. 2022. Combating\nadversaries with anti-adversaries. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 36. 5992\u20136000.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.14347",
                        "Citation Paper Title": "Title:Combating Adversaries with Anti-Adversaries",
                        "Citation Paper Abstract": "Abstract:Deep neural networks are vulnerable to small input perturbations known as adversarial attacks. Inspired by the fact that these adversaries are constructed by iteratively minimizing the confidence of a network for the true class label, we propose the anti-adversary layer, aimed at countering this effect. In particular, our layer generates an input perturbation in the opposite direction of the adversarial one and feeds the classifier a perturbed version of the input. Our approach is training-free and theoretically supported. We verify the effectiveness of our approach by combining our layer with both nominally and robustly trained models and conduct large-scale experiments from black-box to adaptive attacks on CIFAR10, CIFAR100, and ImageNet. Our layer significantly enhances model robustness while coming at no cost on clean accuracy.",
                        "Citation Paper Authors": "Authors:Motasem Alfarra, Juan C. P\u00e9rez, Ali Thabet, Adel Bibi, Philip H. S. Torr, Bernard Ghanem"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "further reduces query times through utilizing data-dependent gradient prior information. Natural\nACM Trans. Intell. Syst. Technol., Vol. 1, No. 1, Article . Publication date: August 2023.4 Yingwen Wu, Sizhe Chen, Kun Fang, Xiaolin Huang\nEvolutionary Strategies (NES, ",
                    "Citation Text": "Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. 2018. Black-box Adversarial Attacks with Limited Queries\nand Information. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan,\nStockholm, Sweden, July 10-15, 2018 . PMLR, 2142\u20132151.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.08598",
                        "Citation Paper Title": "Title:Black-box Adversarial Attacks with Limited Queries and Information",
                        "Citation Paper Abstract": "Abstract:Current neural network-based classifiers are susceptible to adversarial examples even in the black-box setting, where the attacker only has query access to the model. In practice, the threat model for real-world systems is often more restrictive than the typical black-box model where the adversary can observe the full output of the network on arbitrarily many chosen inputs. We define three realistic threat models that more accurately characterize many real-world classifiers: the query-limited setting, the partial-information setting, and the label-only setting. We develop new attacks that fool classifiers under these more restrictive threat models, where previous methods would be impractical or ineffective. We demonstrate that our methods are effective against an ImageNet classifier under our proposed threat models. We also demonstrate a targeted black-box attack against a commercial classifier, overcoming the challenges of limited query access, partial information, and other practical issues to break the Google Cloud Vision API.",
                        "Citation Paper Authors": "Authors:Andrew Ilyas, Logan Engstrom, Anish Athalye, Jessy Lin"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "is the most popular approach in this category,\nwhich adds localized square-shape random noises into inputs. In addition to Sqaure, SimBA ",
                    "Citation Text": "Chuan Guo, Jacob R. Gardner, Yurong You, Andrew Gordon Wilson, and Kilian Q. Weinberger. 2019. Simple Black-box\nAdversarial Attacks. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019,\nLong Beach, California, USA , Vol. 97. PMLR, 2484\u20132493.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.07121",
                        "Citation Paper Title": "Title:Simple Black-box Adversarial Attacks",
                        "Citation Paper Abstract": "Abstract:We propose an intriguingly simple method for the construction of adversarial images in the black-box setting. In constrast to the white-box scenario, constructing black-box adversarial images has the additional constraint on query budget, and efficient attacks remain an open problem to date. With only the mild assumption of continuous-valued confidence scores, our highly query-efficient algorithm utilizes the following simple iterative principle: we randomly sample a vector from a predefined orthonormal basis and either add or subtract it to the target image. Despite its simplicity, the proposed method can be used for both untargeted and targeted attacks -- resulting in previously unprecedented query efficiency in both settings. We demonstrate the efficacy and efficiency of our algorithm on several real world settings including the Google Cloud Vision API. We argue that our proposed algorithm should serve as a strong baseline for future black-box attacks, in particular because it is extremely fast and its implementation requires less than 20 lines of PyTorch code.",
                        "Citation Paper Authors": "Authors:Chuan Guo, Jacob R. Gardner, Yurong You, Andrew Gordon Wilson, Kilian Q. Weinberger"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "constructively\nproposes to ignore gradient magnitudes and only focus on gradient signs to improve attack efficiency.\nFor the second type, attackers randomly choose an attack direction and adjust or directly abandon\nit according to the feedback from models. Square ",
                    "Citation Text": "Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. 2020. Square attack: a query-\nefficient black-box adversarial attack via random search. In European Conference on Computer Vision . Springer, 484\u2013501.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.00049",
                        "Citation Paper Title": "Title:Square Attack: a query-efficient black-box adversarial attack via random search",
                        "Citation Paper Abstract": "Abstract:We propose the Square Attack, a score-based black-box $l_2$- and $l_\\infty$-adversarial attack that does not rely on local gradient information and thus is not affected by gradient masking. Square Attack is based on a randomized search scheme which selects localized square-shaped updates at random positions so that at each iteration the perturbation is situated approximately at the boundary of the feasible set. Our method is significantly more query efficient and achieves a higher success rate compared to the state-of-the-art methods, especially in the untargeted setting. In particular, on ImageNet we improve the average query efficiency in the untargeted setting for various deep networks by a factor of at least $1.8$ and up to $3$ compared to the recent state-of-the-art $l_\\infty$-attack of Al-Dujaili & O'Reilly. Moreover, although our attack is black-box, it can also outperform gradient-based white-box attacks on the standard benchmarks achieving a new state-of-the-art in terms of the success rate. The code of our attack is available at this https URL.",
                        "Citation Paper Authors": "Authors:Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, Matthias Hein"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "are used as typical model architectures. On\nCIFAR10, PreResNet18 with vanilla training achieves 94.26%accuracy. On ImageNet, we directely\nuse the pre-trained model from torchvision package of PyTorch ",
                    "Citation Text": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zachary DeVito, Martin Raison,\nAlykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: An\nImperative Style, High-Performance Deep Learning Library. In Advances in Neural Information Processing Systems 32:\nAnnual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC,\nCanada . 8024\u20138035.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.01703",
                        "Citation Paper Title": "Title:PyTorch: An Imperative Style, High-Performance Deep Learning Library",
                        "Citation Paper Abstract": "Abstract:Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.\nIn this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.\nWe demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.",
                        "Citation Paper Authors": "Authors:Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.15596v2": {
            "Paper Title": "Individual Privacy Accounting with Gaussian Differential Privacy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.07026v2": {
            "Paper Title": "Comprehension from Chaos: Towards Informed Consent for Private\n  Computation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.15323v2": {
            "Paper Title": "Security Analysis of the Consumer Remote SIM Provisioning Protocol",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": "is a tool for modeling and automatic verification of cryptographic protocols. The protocol participants are\nmodeled as communicating processes in applied pi calculus ",
                    "Citation Text": "Mart\u00edn Abadi, Bruno Blanchet, and C\u00e9dric Fournet. 2018. The applied pi calculus: Mobile values, new names, and secure communication. Journal of\nthe ACM (JACM) 65, 1 (2018), 1.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.03003",
                        "Citation Paper Title": "Title:The Applied Pi Calculus: Mobile Values, New Names, and Secure Communication",
                        "Citation Paper Abstract": "Abstract:We study the interaction of the programming construct \"new\", which generates statically scoped names, with communication via messages on channels. This interaction is crucial in security protocols, which are the main motivating examples for our work, it also appears in other programming-language contexts.\nWe define the applied pi calculus, a simple, general extension of the pi calculus in which values can be formed from names via the application of built-in functions, subject to equations, and be sent as messages. (In contrast, the pure pi calculus lacks built-in functions, its only messages are atomic names.) We develop semantics and proof techniques for this extended language and apply them in reasoning about security protocols.\nThis paper essentially subsumes the conference paper that introduced the applied pi calculus in 2001. It fills gaps, incorporates improvements, and further explains and studies the applied pi calculus. Since 2001, the applied pi calculus has been the basis for much further work, described in many research publications and sometimes embodied in useful software, such as the tool ProVerif, which relies on the applied pi calculus to support the specification and automatic analysis of security protocols. Although this paper does not aim to be a complete review of the subject, it benefits from that further work and provides better foundations for some of it. In particular, the applied pi calculus has evolved through its implementation in ProVerif, and the present definition reflects that evolution.",
                        "Citation Paper Authors": "Authors:Mart\u00edn Abadi, Bruno Blanchet, C\u00e9dric Fournet"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.07714v3": {
            "Paper Title": "CrowdGuard: Federated Backdoor Detection in Federated Learning",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": ". It is common to introduce a\npeak non-IID rate of q\u2208[0,1]in sample counts for one label,\nthe so-called main label of the client ",
                    "Citation Text": "Xiaoyu Cao, Minghong Fang, Jia Liu, and Neil Zhenqiang Gong.\nFltrust: Byzantine-robust federated learning via trust bootstrapping. In\nNDSS , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.13995",
                        "Citation Paper Title": "Title:FLTrust: Byzantine-robust Federated Learning via Trust Bootstrapping",
                        "Citation Paper Abstract": "Abstract:Byzantine-robust federated learning aims to enable a service provider to learn an accurate global model when a bounded number of clients are malicious. The key idea of existing Byzantine-robust federated learning methods is that the service provider performs statistical analysis among the clients' local model updates and removes suspicious ones, before aggregating them to update the global model. However, malicious clients can still corrupt the global models in these methods via sending carefully crafted local model updates to the service provider. The fundamental reason is that there is no root of trust in existing federated learning methods.\nIn this work, we bridge the gap via proposing FLTrust, a new federated learning method in which the service provider itself bootstraps trust. In particular, the service provider itself collects a clean small training dataset (called root dataset) for the learning task and the service provider maintains a model (called server model) based on it to bootstrap trust. In each iteration, the service provider first assigns a trust score to each local model update from the clients, where a local model update has a lower trust score if its direction deviates more from the direction of the server model update. Then, the service provider normalizes the magnitudes of the local model updates such that they lie in the same hyper-sphere as the server model update in the vector space. Our normalization limits the impact of malicious local model updates with large magnitudes. Finally, the service provider computes the average of the normalized local model updates weighted by their trust scores as a global model update, which is used to update the global model. Our extensive evaluations on six datasets from different domains show that our FLTrust is secure against both existing attacks and strong adaptive attacks.",
                        "Citation Paper Authors": "Authors:Xiaoyu Cao, Minghong Fang, Jia Liu, Neil Zhenqiang Gong"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": ", i.e., all\nclients can have samples of all available labels, but the overall\ncount for each label differs ",
                    "Citation Text": "Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou.\nFedbn: Federated learning on non-iid features via local batch normal-\nization. In arXiv preprint arXiv:2102.07623 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.07623",
                        "Citation Paper Title": "Title:FedBN: Federated Learning on Non-IID Features via Local Batch Normalization",
                        "Citation Paper Abstract": "Abstract:The emerging paradigm of federated learning (FL) strives to enable collaborative training of deep models on the network edge without centrally aggregating raw data and hence improving data privacy. In most cases, the assumption of independent and identically distributed samples across local clients does not hold for federated learning setups. Under this setting, neural network training performance may vary significantly according to the data distribution and even hurt training convergence. Most of the previous work has focused on a difference in the distribution of labels or client shifts. Unlike those settings, we address an important problem of FL, e.g., different scanners/sensors in medical imaging, different scenery distribution in autonomous driving (highway vs. city), where local clients store examples with different distributions compared to other clients, which we denote as feature shift non-iid. In this work, we propose an effective method that uses local batch normalization to alleviate the feature shift before averaging models. The resulting scheme, called FedBN, outperforms both classical FedAvg, as well as the state-of-the-art for non-iid data (FedProx) on our extensive experiments. These empirical results are supported by a convergence analysis that shows in a simplified setting that FedBN has a faster convergence rate than FedAvg. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, Qi Dou"
                    }
                },
                {
                    "Sentence ID": 97,
                    "Sentence": ". Contrary to an independent and\nidentically distributed (IID) data scenario, a non-IID case\nnaturally delivers divergent trained models from each client.\nNon-IID can manifest with different severities ",
                    "Citation Text": "Hangyu Zhu, Jinjin Xu, Shiqing Liu, and Yaochu Jin. Federated learning\non non-iid data: A survey. In Neurocomputing , volume 465, pages 371\u2013\n390. Elsevier, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.06843",
                        "Citation Paper Title": "Title:Federated Learning on Non-IID Data: A Survey",
                        "Citation Paper Abstract": "Abstract:Federated learning is an emerging distributed machine learning framework for privacy preservation. However, models trained in federated learning usually have worse performance than those trained in the standard centralized learning mode, especially when the training data are not independent and identically distributed (Non-IID) on the local devices. In this survey, we pro-vide a detailed analysis of the influence of Non-IID data on both parametric and non-parametric machine learning models in both horizontal and vertical federated learning. In addition, cur-rent research work on handling challenges of Non-IID data in federated learning are reviewed, and both advantages and disadvantages of these approaches are discussed. Finally, we suggest several future research directions before concluding the paper.",
                        "Citation Paper Authors": "Authors:Hangyu Zhu, Jinjin Xu, Shiqing Liu, Yaochu Jin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.03591v2": {
            "Paper Title": "Quantum Local Differential Privacy and Quantum Statistical Query Model",
            "Sentences": [
                {
                    "Sentence ID": 52,
                    "Sentence": ", which extends hypothesis testing to the setting of\nrestricted measurements. Our result can also be regarded as a quantum version of the \u201cprivate\nChernoff-Stein lemma\u201d provided in ",
                    "Citation Text": "Shahab Asoodeh, Maryam Aliakbarpour, and Flavio P . Calm on. Local differential privacy\nis equivalent to contraction of an f-divergence. In 2021 IEEE International Symposium on\nInformation Theory (ISIT) , page 545\u2013550. IEEE Press, 2021. doi: 10.1109/ISIT45174.2 021.\n9517999. URL https://doi.org/10.1109/ISIT45174.2021.9517999 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.01258",
                        "Citation Paper Title": "Title:Local Differential Privacy Is Equivalent to Contraction of $E_\u03b3$-Divergence",
                        "Citation Paper Abstract": "Abstract:We investigate the local differential privacy (LDP) guarantees of a randomized privacy mechanism via its contraction properties. We first show that LDP constraints can be equivalently cast in terms of the contraction coefficient of the $E_\\gamma$-divergence. We then use this equivalent formula to express LDP guarantees of privacy mechanisms in terms of contraction coefficients of arbitrary $f$-divergences. When combined with standard estimation-theoretic tools (such as Le Cam's and Fano's converse methods), this result allows us to study the trade-off between privacy and utility in several testing and minimax and Bayesian estimation problems.",
                        "Citation Paper Authors": "Authors:Shahab Asoodeh, Maryam Aliakbarpour, Flavio P. Calmon"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "is de\ufb01ned as\nDmaxp\u03c1}\u03c3q \u201cinft\u03bb:\u03c1\u010fe\u03bb\u03c3u.\nThe quantum smooth max-relative entropy ",
                    "Citation Text": "Christoph Hirche, Cambyse Rouz\u00b4 e, and Daniel Stilck Fr anc \u00b8a. Quantum differential pri-\nvacy: An information theory perspective. IEEE Transactions on Information Theory , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.10717",
                        "Citation Paper Title": "Title:Quantum Differential Privacy: An Information Theory Perspective",
                        "Citation Paper Abstract": "Abstract:Differential privacy has been an exceptionally successful concept when it comes to providing provable security guarantees for classical computations. More recently, the concept was generalized to quantum computations. While classical computations are essentially noiseless and differential privacy is often achieved by artificially adding noise, near-term quantum computers are inherently noisy and it was observed that this leads to natural differential privacy as a feature.\nIn this work we discuss quantum differential privacy in an information theoretic framework by casting it as a quantum divergence. A main advantage of this approach is that differential privacy becomes a property solely based on the output states of the computation, without the need to check it for every measurement. This leads to simpler proofs and generalized statements of its properties as well as several new bounds for both, general and specific, noise models. In particular, these include common representations of quantum circuits and quantum machine learning concepts. Here, we focus on the difference in the amount of noise required to achieve certain levels of differential privacy versus the amount that would make any computation useless. Finally, we also generalize the classical concepts of local differential privacy, Renyi differential privacy and the hypothesis testing interpretation to the quantum setting, providing several new properties and insights.",
                        "Citation Paper Authors": "Authors:Christoph Hirche, Cambyse Rouz\u00e9, Daniel Stilck Fran\u00e7a"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.09975v2": {
            "Paper Title": "Risk of re-identification for shared clinical speech recordings",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.10240v2": {
            "Paper Title": "Differentially Private Partial Set Cover with Applications to Facility\n  Location",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.06936v2": {
            "Paper Title": "Decomposed Richelot isogenies of Jacobian varieties of hyperelliptic\n  curves and generalized Howe curves",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": ").\nRemark 2 In the case of curves of genus 3, decomplosed Richelot isogen ies are stud-\nied in Howe-Lepr\u00b4 evost-Poonen ",
                    "Citation Text": "Howe, E.W., Lepr\u00b4 evost, F., Poonen, B.: Large torsion s ubgroups of split jacobians\nof curves of genus two or three. Forum Math. 12, 315\u2013364 (2000)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:math/9809210",
                        "Citation Paper Title": "Title:Large torsion subgroups of split Jacobians of curves of genus two or three",
                        "Citation Paper Abstract": "Abstract:  We construct examples of families of curves of genus 2 or 3 over Q whose Jacobians split completely and have various large rational torsion subgroups. For example, the rational points on a certain elliptic surface over P^1 of positive rank parameterize a family of genus-2 curves over Q whose Jacobians each have 128 rational torsion points. Also, we find the genus-3 curve 15625(X^4 + Y^4 + Z^4) - 96914(X^2 Y^2 + X^2 Z^2 + Y^2 Z^2) = 0, whose Jacobian has 864 rational torsion points.\nThis paper has appeared in Forum Math. 12 (2000) 315-364.",
                        "Citation Paper Authors": "Authors:Everett W. Howe, Franck Leprevost, Bjorn Poonen"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.05225v6": {
            "Paper Title": "Compositional Vulnerability Detection with Insecurity Separation Logic\n  (Extended Version)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.10651v2": {
            "Paper Title": "Fant\u00f4mas: Understanding Face Anonymization Reversibility",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": "have proposed a\nsuper-resolution approach that removes Pixelation from face\nimages. A denoising and deblurring approach was proposed\nby Zamir et al. ",
                    "Citation Text": "S. W. Zamir, A. Arora, S. Khan, M. Hayat, F. S. Khan, M.-H.\nYang, and L. Shao, \u201cMulti-stage progressive image restoration,\u201d in\nProceedings of the IEEE/CVF conference on computer vision and\npattern recognition , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.02808",
                        "Citation Paper Title": "Title:Multi-Stage Progressive Image Restoration",
                        "Citation Paper Abstract": "Abstract:Image restoration tasks demand a complex balance between spatial details and high-level contextualized information while recovering images. In this paper, we propose a novel synergistic design that can optimally balance these competing goals. Our main proposal is a multi-stage architecture, that progressively learns restoration functions for the degraded inputs, thereby breaking down the overall recovery process into more manageable steps. Specifically, our model first learns the contextualized features using encoder-decoder architectures and later combines them with a high-resolution branch that retains local information. At each stage, we introduce a novel per-pixel adaptive design that leverages in-situ supervised attention to reweight the local features. A key ingredient in such a multi-stage architecture is the information exchange between different stages. To this end, we propose a two-faceted approach where the information is not only exchanged sequentially from early to late stages, but lateral connections between feature processing blocks also exist to avoid any loss of information. The resulting tightly interlinked multi-stage architecture, named as MPRNet, delivers strong performance gains on ten datasets across a range of tasks including image deraining, deblurring, and denoising. The source code and pre-trained models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, Ling Shao"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "train a convolutional neural network to\nreconstruct blurred faces. Lu et al. ",
                    "Citation Text": "C. Ma, Z. Jiang, Y . Rao, J. Lu, and J. Zhou, \u201cDeep face super-\nresolution with iterative collaboration between attentive recovery and\nlandmark estimation,\u201d in Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.13063",
                        "Citation Paper Title": "Title:Deep Face Super-Resolution with Iterative Collaboration between Attentive Recovery and Landmark Estimation",
                        "Citation Paper Abstract": "Abstract:Recent works based on deep learning and facial priors have succeeded in super-resolving severely degraded facial images. However, the prior knowledge is not fully exploited in existing methods, since facial priors such as landmark and component maps are always estimated by low-resolution or coarsely super-resolved images, which may be inaccurate and thus affect the recovery performance. In this paper, we propose a deep face super-resolution (FSR) method with iterative collaboration between two recurrent networks which focus on facial image recovery and landmark estimation respectively. In each recurrent step, the recovery branch utilizes the prior knowledge of landmarks to yield higher-quality images which facilitate more accurate landmark estimation in turn. Therefore, the iterative information interaction between two processes boosts the performance of each other progressively. Moreover, a new attentive fusion module is designed to strengthen the guidance of landmark maps, where facial components are generated individually and aggregated attentively for better restoration. Quantitative and qualitative experimental results show the proposed method significantly outperforms state-of-the-art FSR methods in recovering high-quality face images.",
                        "Citation Paper Authors": "Authors:Cheng Ma, Zhenyu Jiang, Yongming Rao, Jiwen Lu, Jie Zhou"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.13955v3": {
            "Paper Title": "MPCViT: Searching for Accurate and Efficient MPC-Friendly Vision\n  Transformer with Heterogeneous Attention",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.05125v2": {
            "Paper Title": "Cross-chain between a Parent Chain and Multiple Side Chains",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.05680v2": {
            "Paper Title": "REAP: A Large-Scale Realistic Adversarial Patch Benchmark",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "study patch\nadversarial training on classifiers). It is also known to be\na strong baseline and arguably the only effective defense\nacross other \u2113p-norms ",
                    "Citation Text": "Francesco Croce, Maksym Andriushchenko, Vikash Sehwag,\nNicolas Flammarion, Mung Chiang, Prateek Mittal, and\nMatthias Hein. RobustBench: A standardized adversarial\nrobustness benchmark. Technical report, 2020. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.09670",
                        "Citation Paper Title": "Title:RobustBench: a standardized adversarial robustness benchmark",
                        "Citation Paper Abstract": "Abstract:As a research community, we are still lacking a systematic understanding of the progress on adversarial robustness which often makes it hard to identify the most promising ideas in training robust models. A key challenge in benchmarking robustness is that its evaluation is often error-prone leading to robustness overestimation. Our goal is to establish a standardized benchmark of adversarial robustness, which as accurately as possible reflects the robustness of the considered models within a reasonable computational budget. To this end, we start by considering the image classification task and introduce restrictions (possibly loosened in the future) on the allowed models. We evaluate adversarial robustness with AutoAttack, an ensemble of white- and black-box attacks, which was recently shown in a large-scale study to improve almost all robustness evaluations compared to the original publications. To prevent overadaptation of new defenses to AutoAttack, we welcome external evaluations based on adaptive attacks, especially where AutoAttack flags a potential overestimation of robustness. Our leaderboard, hosted at this https URL, contains evaluations of 120+ models and aims at reflecting the current state of the art in image classification on a set of well-defined tasks in $\\ell_\\infty$- and $\\ell_2$-threat models and on common corruptions, with possible extensions in the future. Additionally, we open-source the library this https URL that provides unified access to 80+ robust models to facilitate their downstream applications. Finally, based on the collected models, we analyze the impact of robustness on the performance on distribution shifts, calibration, out-of-distribution detection, fairness, privacy leakage, smoothness, and transferability.",
                        "Citation Paper Authors": "Authors:Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, Matthias Hein"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.03909v2": {
            "Paper Title": "Dataset Obfuscation: Its Applications to and Impacts on Edge Machine\n  Learning",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": ", etc.\nThis paper focuses on protecting dataset privacy in tasks where sharing datasets is essential\nbetween edges or between edge and cloud. Authors of ",
                    "Citation Text": "Tianwei Zhang, Zecheng He, and Ruby B. Lee. Privacy-preserving machine learning through data obfuscation. CoRR ,\nabs/1807.01860, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.01860",
                        "Citation Paper Title": "Title:Privacy-preserving Machine Learning through Data Obfuscation",
                        "Citation Paper Abstract": "Abstract:As machine learning becomes a practice and commodity, numerous cloud-based services and frameworks are provided to help customers develop and deploy machine learning applications. While it is prevalent to outsource model training and serving tasks in the cloud, it is important to protect the privacy of sensitive samples in the training dataset and prevent information leakage to untrusted third parties. Past work have shown that a malicious machine learning service provider or end user can easily extract critical information about the training samples, from the model parameters or even just model outputs.\nIn this paper, we propose a novel and generic methodology to preserve the privacy of training data in machine learning applications. Specifically we introduce an obfuscate function and apply it to the training data before feeding them to the model training task. This function adds random noise to existing samples, or augments the dataset with new samples. By doing so sensitive information about the properties of individual samples, or statistical properties of a group of samples, is hidden. Meanwhile the model trained from the obfuscated dataset can still achieve high accuracy. With this approach, the customers can safely disclose the data or models to third-party providers or end users without the need to worry about data privacy. Our experiments show that this approach can effective defeat four existing types of machine learning privacy attacks at negligible accuracy cost.",
                        "Citation Paper Authors": "Authors:Tianwei Zhang, Zecheng He, Ruby B. Lee"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.00636v2": {
            "Paper Title": "Differentially Private Community Detection for Stochastic Block Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.04490v2": {
            "Paper Title": "\"Sign in with ... Privacy'': Timely Disclosure of Privacy Differences\n  among Web SSO Login Options",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": "used formal analysis to evaluate security properties of OAuth 2.0. To test compliance with security best practices, Rahat\net al. built Cerberus ",
                    "Citation Text": "T. A. Rahat, Y. Feng, and Y. Tian. Cerberus: Query-driven Scalable Security Checking for OAuth Service Provider Implementations. In ACM CCS ,\n2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.01005",
                        "Citation Paper Title": "Title:Cerberus: Query-driven Scalable Vulnerability Detection in OAuth Service Provider Implementations",
                        "Citation Paper Abstract": "Abstract:OAuth protocols have been widely adopted to simplify user authentication and service authorization for third-party applications. However, little effort has been devoted to automatically checking the security of the libraries that service providers widely use. In this paper, we formalize the OAuth specifications and security best practices, and design Cerberus, an automated static analyzer, to find logical flaws and identify vulnerabilities in the implementation of OAuth service provider libraries. To efficiently detect security violations in a large codebase of service provider implementation, Cerberus employs a query-driven algorithm for answering queries about OAuth specifications. We demonstrate the effectiveness of Cerberus by evaluating it on datasets of popular OAuth libraries with millions of downloads. Among these high-profile libraries, Cerberus has identified 47 vulnerabilities from ten classes of logical flaws, 24 of which were previously unknown. We got acknowledged by the developers of eight libraries and had three accepted CVEs.",
                        "Citation Paper Authors": "Authors:Tamjid Al Rahat, Yu Feng, Yuan Tian"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "categorized SSO testing approaches used by previous research pointing out\nbenefits and limitations, and built PrOfESSOS, a security analysis tool for testing OIDC implementations. Fett et al. ",
                    "Citation Text": "D. Fett, R. K\u00fcsters, and G. Schmitz. A Comprehensive Formal Security Analysis of OAuth 2.0. In ACM CCS , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1601.01229",
                        "Citation Paper Title": "Title:A Comprehensive Formal Security Analysis of OAuth 2.0",
                        "Citation Paper Abstract": "Abstract:The OAuth 2.0 protocol is one of the most widely deployed authorization/single sign-on (SSO) protocols and also serves as the foundation for the new SSO standard OpenID Connect. Despite the popularity of OAuth, so far analysis efforts were mostly targeted at finding bugs in specific implementations and were based on formal models which abstract from many web features or did not provide a formal treatment at all.\nIn this paper, we carry out the first extensive formal analysis of the OAuth 2.0 standard in an expressive web model. Our analysis aims at establishing strong authorization, authentication, and session integrity guarantees, for which we provide formal definitions. In our formal analysis, all four OAuth grant types (authorization code grant, implicit grant, resource owner password credentials grant, and the client credentials grant) are covered. They may even run simultaneously in the same and different relying parties and identity providers, where malicious relying parties, identity providers, and browsers are considered as well. Our modeling and analysis of the OAuth 2.0 standard assumes that security recommendations and best practices are followed, in order to avoid obvious and known attacks.\nWhen proving the security of OAuth in our model, we discovered four attacks which break the security of OAuth. The vulnerabilities can be exploited in practice and are present also in OpenID Connect.\nWe propose fixes for the identified vulnerabilities, and then, for the first time, actually prove the security of OAuth in an expressive web model. In particular, we show that the fixed version of OAuth (with security recommendations and best practices in place) provides the authorization, authentication, and session integrity properties we specify.",
                        "Citation Paper Authors": "Authors:Daniel Fett, Ralf Kuesters, Guido Schmitz"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "proposed ROAuth as an extension to OAuth 2.0 and built a Firefox browser extension to allow\nFacebook users to configure (and possibly limit) requested permissions in SSO logins with Facebook. Li et al. ",
                    "Citation Text": "W. Li, C. J. Mitchell, and T. Chen. OAuthGuard: Protecting User Security and Privacy with OAuth 2.0 and OpenID Connect. In Workshop on Security\nStandardisation Research , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.08960",
                        "Citation Paper Title": "Title:OAuthGuard: Protecting User Security and Privacy with OAuth 2.0 and OpenID Connect",
                        "Citation Paper Abstract": "Abstract:Millions of users routinely use Google to log in to websites supporting OAuth 2.0 or OpenID Connect; the security of OAuth 2.0 and OpenID Connect is therefore of critical importance. As revealed in previous studies, in practice RPs often implement OAuth 2.0 incorrectly, and so many real-world OAuth 2.0 and OpenID Connect systems are vulnerable to attack. However, users of such flawed systems are typically unaware of these issues, and so are at risk of attacks which could result in unauthorised access to the victim user's account at an RP. In order to address this threat, we have developed OAuthGuard, an OAuth 2.0 and OpenID Connect vulnerability scanner and protector, that works with RPs using Google OAuth 2.0 and OpenID Connect services. It protects user security and privacy even when RPs do not implement OAuth 2.0 or OpenID Connect correctly. We used OAuthGuard to survey the 1000 top-ranked websites supporting Google sign-in for the possible presence of five OAuth 2.0 or OpenID Connect security and privacy vulnerabilities, of which one has not previously been described in the literature. Of the 137 sites in our study that employ Google Sign-in, 69 were found to suffer from at least one serious vulnerability. OAuthGuard was able to protect user security and privacy for 56 of these 69 RPs, and for the other 13 was able to warn users that they were using an insecure implementation.",
                        "Citation Paper Authors": "Authors:Wanpeng Li, Chris J Mitchell, Thomas Chen"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "proposed PseudoID to enable users to login to RPs using pseudonyms. Xu et al. ",
                    "Citation Text": "R. Xu, S. Yang, F. Zhang, and Z. Fang. MISO: Legacy-compatible Privacy-preserving Single Sign-on using Trusted Execution Environments. IEEE\nEuroS&P , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2305.06833",
                        "Citation Paper Title": "Title:MISO: Legacy-compatible Privacy-preserving Single Sign-on using Trusted Execution Environments",
                        "Citation Paper Abstract": "Abstract:Single sign-on (SSO) allows users to authenticate to third-party applications through a central identity provider. Despite their wide adoption, deployed SSO systems suffer from privacy problems such as user tracking by the identity provider. While numerous solutions have been proposed by academic papers, none were adopted because they require modifying identity providers, a significant adoption barrier in practice. Solutions do get deployed, however, fail to eliminate major privacy issues. Leveraging Trusted Execution Environments (TEEs), we propose MISO, the first privacy-preserving SSO system that is completely compatible with existing identity providers (such as Google and Facebook). This means MISO can be easily integrated into existing SSO ecosystem today and benefit end users. MISO also enables new functionality that standard SSO cannot offer: MISO allows users to leverage multiple identity providers in a single SSO workflow, potentially in a threshold fashion, to better protect user accounts. We fully implemented MISO based on Intel SGX. Our evaluation shows that MISO can handle high user concurrency with practical performance.",
                        "Citation Paper Authors": "Authors:Rongwu Xu, Sen Yang, Fan Zhang, Zhixuan Fang"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "developed a Selenium-based tool to track differences in RP18 Morkonda, Chiasson, and van Oorschot\npermission requests and IdP usage over a nine-year period. Westers et al. ",
                    "Citation Text": "M. Westers, T. Wich, L. Jannett, V. Mladenov, C. Mainka, and A. Mayer. SSO-Monitor: Fully-Automatic Large-Scale Landscape, Security, and Privacy\nAnalyses of Single Sign-On in the Wild. arXiv:2302.01024 [cs] , Feb 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.01024",
                        "Citation Paper Title": "Title:SSO-Monitor: Fully-Automatic Large-Scale Landscape, Security, and Privacy Analyses of Single Sign-On in the Wild",
                        "Citation Paper Abstract": "Abstract:Single Sign-On (SSO) shifts the crucial authentication process on a website to to the underlying SSO protocols and their correct implementation. To strengthen SSO security, organizations, such as IETF and W3C, maintain advisories to address known threats. One could assume that these security best practices are widely deployed on websites. We show that this assumption is a fallacy. We present SSO-MONITOR, an open-source fully-automatic large-scale SSO landscape, security, and privacy analysis tool. In contrast to all previous work, SSO-MONITOR uses a highly extensible, fully automated workflow with novel visual-based SSO detection techniques, enhanced security and privacy analyses, and continuously updated monitoring results. It receives a list of domains as input to discover the login pages, recognize the supported Identity Providers (IdPs), and execute the SSO. It further reveals the current security level of SSO in the wild compared to the security best practices on paper. With SSO-MONITOR, we automatically identified 1,632 websites with 3,020 Apple, Facebook, or Google logins within the Tranco 10k. Our continuous monitoring also revealed how quickly these numbers change over time. SSO-MONITOR can automatically login to each SSO website. It records the logins by tracing HTTP and in-browser communication to detect widespread security and privacy issues automatically. We introduce a novel deep-level inspection of HTTP parameters that we call SMARTPARMS. Using SMARTPARMS for security analyses, we uncovered URL parameters in 5 Client Application (Client) secret leakages and 337 cases with weak CSRF protection. We additionally identified 447 cases with no CSRF protection, 342 insecure SSO flows and 9 cases with nested URL parameters, leading to an open redirect in one case. SSO-MONITOR reveals privacy leakages that deanonymize users in 200 cases.",
                        "Citation Paper Authors": "Authors:Maximilian Westers, Tobias Wich, Louis Jannett, Vladislav Mladenov, Christian Mainka, Andreas Mayer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.01179v2": {
            "Paper Title": "Tournesol: Permissionless Collaborative Algorithmic Governance with\n  Security Guarantees",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.10963v4": {
            "Paper Title": "Quotable Signatures for Authenticating Shared Quotes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.07346v2": {
            "Paper Title": "An Embarrassingly Simple Backdoor Attack on Self-supervised Learning",
            "Sentences": [
                {
                    "Sentence ID": 37,
                    "Sentence": "injects\nbackdoors into pre-trained encoders and releases backdoored\nmodels to victims in downstream tasks; SSLBackdoor ",
                    "Citation Text": "Aniruddha Saha, Ajinkya Tejankar, Soroush Abbasi\nKoohpayegani, and Hamed Pirsiavash. Backdoor at-\ntacks on self-supervised learning. ArXiv e-prints , 2021.\n1, 3, 4, 9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.10123",
                        "Citation Paper Title": "Title:Backdoor Attacks on Self-Supervised Learning",
                        "Citation Paper Abstract": "Abstract:Large-scale unlabeled data has spurred recent progress in self-supervised learning methods that learn rich visual representations. State-of-the-art self-supervised methods for learning representations from images (e.g., MoCo, BYOL, MSF) use an inductive bias that random augmentations (e.g., random crops) of an image should produce similar embeddings. We show that such methods are vulnerable to backdoor attacks - where an attacker poisons a small part of the unlabeled data by adding a trigger (image patch chosen by the attacker) to the images. The model performance is good on clean test images, but the attacker can manipulate the decision of the model by showing the trigger at test time. Backdoor attacks have been studied extensively in supervised learning and to the best of our knowledge, we are the first to study them for self-supervised learning. Backdoor attacks are more practical in self-supervised learning, since the use of large unlabeled data makes data inspection to remove poisons prohibitive. We show that in our targeted attack, the attacker can produce many false positives for the target category by using the trigger at test time. We also propose a defense method based on knowledge distillation that succeeds in neutralizing the attack. Our code is available here: this https URL .",
                        "Citation Paper Authors": "Authors:Aniruddha Saha, Ajinkya Tejankar, Soroush Abbasi Koohpayegani, Hamed Pirsiavash"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": ". Their accuracy on the benchmark datasets\nis summarized in Table 9 in Appendix \u00a7 D.\nModels \u2013 By default, we use an encoder with ResNet-\n18 ",
                    "Citation Text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. Deep Residual Learning for Image Recognition.\nInProceedings of IEEE Conference on Computer Vi-\nsion and Pattern Recognition (CVPR) , 2016. 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1502.01852",
                        "Citation Paper Title": "Title:Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
                        "Citation Paper Abstract": "Abstract:Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.",
                        "Citation Paper Authors": "Authors:Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": ", (ii) attack vectors \u2013 polluting training\ndata [ 30,52], searching vulnerable architecture ",
                    "Citation Text": "Ren Pang, Changjiang Li, Zhaohan Xi, Shouling Ji,\nand Ting Wang. The dark side of automl: Towards\narchitectural backdoor search. In The Eleventh Interna-\ntional Conference on Learning Representations , 2022.\n3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.12179",
                        "Citation Paper Title": "Title:Neural Architectural Backdoors",
                        "Citation Paper Abstract": "Abstract:This paper asks the intriguing question: is it possible to exploit neural architecture search (NAS) as a new attack vector to launch previously improbable attacks? Specifically, we present EVAS, a new attack that leverages NAS to find neural architectures with inherent backdoors and exploits such vulnerability using input-aware triggers. Compared with existing attacks, EVAS demonstrates many interesting properties: (i) it does not require polluting training data or perturbing model parameters; (ii) it is agnostic to downstream fine-tuning or even re-training from scratch; (iii) it naturally evades defenses that rely on inspecting model parameters or training data. With extensive evaluation on benchmark datasets, we show that EVAS features high evasiveness, transferability, and robustness, thereby expanding the adversary's design spectrum. We further characterize the mechanisms underlying EVAS, which are possibly explainable by architecture-level ``shortcuts'' that recognize trigger patterns. This work raises concerns about the current practice of NAS and points to potential directions to develop effective countermeasures.",
                        "Citation Paper Authors": "Authors:Ren Pang, Changjiang Li, Zhaohan Xi, Shouling Ji, Ting Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.09803v4": {
            "Paper Title": "GUARD: Graph Universal Adversarial Defense",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": ". Therefore, a simple similarity-based prepro-\ncessing method achieves is able to achieve good defensive results.\n(ii)Insertion is a more powerful attack operation compared\nto compared with deletion. In ",
                    "Citation Text": "Liang Chen, Jintang Li, Qibiao Peng, Yang Liu, Zibin Zheng, and Carl Yang. 2021.\nUnderstanding Structural Vulnerability in Graph Convolutional Networks. In\nIJCAI , Zhi-Hua Zhou (Ed.). 2249\u20132255.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.06280",
                        "Citation Paper Title": "Title:Understanding Structural Vulnerability in Graph Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:Recent studies have shown that Graph Convolutional Networks (GCNs) are vulnerable to adversarial attacks on the graph structure. Although multiple works have been proposed to improve their robustness against such structural adversarial attacks, the reasons for the success of the attacks remain unclear. In this work, we theoretically and empirically demonstrate that structural adversarial examples can be attributed to the non-robust aggregation scheme (i.e., the weighted mean) of GCNs. Specifically, our analysis takes advantage of the breakdown point which can quantitatively measure the robustness of aggregation schemes. The key insight is that weighted mean, as the basic design of GCNs, has a low breakdown point and its output can be dramatically changed by injecting a single edge. We show that adopting the aggregation scheme with a high breakdown point (e.g., median or trimmed mean) could significantly enhance the robustness of GCNs against structural attacks. Extensive experiments on four real-world datasets demonstrate that such a simple but effective method achieves the best robustness performance compared to state-of-the-art models.",
                        "Citation Paper Authors": "Authors:Liang Chen, Jintang Li, Qibiao Peng, Yang Liu, Zibin Zheng, Carl Yang"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": ".\nAdversarial attack on graphs. Literature is rich on attacking\nGCNs with adversarial examples. The most widely used solution\nfor crafting adversarial examples on graphs is to utilize a locally\ntrained surrogate model (typically a GCN) ",
                    "Citation Text": "Daniel Z\u00fcgner, Amir Akbarnejad, and Stephan G\u00fcnnemann. 2018. Adversarial\nAttacks on Neural Networks for Graph Data. In KDD . 2847\u20132856.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.07984",
                        "Citation Paper Title": "Title:Adversarial Attacks on Neural Networks for Graph Data",
                        "Citation Paper Abstract": "Abstract:Deep learning models for graphs have achieved strong performance for the task of node classification. Despite their proliferation, currently there is no study of their robustness to adversarial attacks. Yet, in domains where they are likely to be used, e.g. the web, adversaries are common. Can deep learning models for graphs be easily fooled? In this work, we introduce the first study of adversarial attacks on attributed graphs, specifically focusing on models exploiting ideas of graph convolutions. In addition to attacks at test time, we tackle the more challenging class of poisoning/causative attacks, which focus on the training phase of a machine learning model. We generate adversarial perturbations targeting the node's features and the graph structure, thus, taking the dependencies between instances in account. Moreover, we ensure that the perturbations remain unnoticeable by preserving important data characteristics. To cope with the underlying discrete domain we propose an efficient algorithm Nettack exploiting incremental computations. Our experimental study shows that accuracy of node classification significantly drops even when performing only few perturbations. Even more, our attacks are transferable: the learned attacks generalize to other state-of-the-art node classification models and unsupervised approaches, and likewise are successful even when only limited knowledge about the graph is given.",
                        "Citation Paper Authors": "Authors:Daniel Z\u00fcgner, Amir Akbarnejad, Stephan G\u00fcnnemann"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": ", differs from that appli-\ncable to any image in vision research ",
                    "Citation Text": "Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal\nFrossard. 2017. Universal Adversarial Perturbations. In CVPR . IEEE Computer\nSociety, 86\u201394.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.08401",
                        "Citation Paper Title": "Title:Universal adversarial perturbations",
                        "Citation Paper Abstract": "Abstract:Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images.",
                        "Citation Paper Authors": "Authors:Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal Frossard"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2207.04129v3": {
            "Paper Title": "How many perturbations break this model? Evaluating robustness beyond\n  adversarial accuracy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.04087v3": {
            "Paper Title": "Symmetry Defense Against CNN Adversarial Perturbation Attacks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.13662v2": {
            "Paper Title": "Analyzing Privacy Leakage in Machine Learning via Multiple Hypothesis\n  Testing: A Lesson From Fano",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.03942v3": {
            "Paper Title": "Privacy-Aware Compression for Federated Learning Through Numerical\n  Mechanism Design",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.02408v3": {
            "Paper Title": "Rickrolling the Artist: Injecting Backdoors into Text Encoders for\n  Text-to-Image Synthesis",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": "dataset to inject the backdoors. For\nour evaluation, we took the 40,504 samples from the MS-\nCOCO ",
                    "Citation Text": "Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll \u00b4ar, and\nC. Lawrence Zitnick. Microsoft coco: Common objects\nin context. In European Conference on Computer Vision\n(ECCV) , pages 740\u2013755, 2014. 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1405.0312",
                        "Citation Paper Title": "Title:Microsoft COCO: Common Objects in Context",
                        "Citation Paper Abstract": "Abstract:We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
                        "Citation Paper Authors": "Authors:Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Doll\u00e1r"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "proposed to integrate the backdoors into early lay-\ners of a neural network. Qi et al. ",
                    "Citation Text": "Fanchao Qi, Yuan Yao, Sophia Xu, Zhiyuan Liu, and\nMaosong Sun. Turn the combination lock: Learnable textual\nbackdoor attacks via word substitution. In Annual Meeting of\nthe Association for Computational Linguistics and the Inter-\nnational Joint Conference on Natural Language Processing\n(ACL/IJCNLP) , pages 4873\u20134883, 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.06361",
                        "Citation Paper Title": "Title:Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word Substitution",
                        "Citation Paper Abstract": "Abstract:Recent studies show that neural natural language processing (NLP) models are vulnerable to backdoor attacks. Injected with backdoors, models perform normally on benign examples but produce attacker-specified predictions when the backdoor is activated, presenting serious security threats to real-world applications. Since existing textual backdoor attacks pay little attention to the invisibility of backdoors, they can be easily detected and blocked. In this work, we present invisible backdoors that are activated by a learnable combination of word substitution. We show that NLP models can be injected with backdoors that lead to a nearly 100% attack success rate, whereas being highly invisible to existing defense strategies and even human inspections. The results raise a serious alarm to the security of NLP models, which requires further research to be resolved. All the data and code of this paper are released at this https URL.",
                        "Citation Paper Authors": "Authors:Fanchao Qi, Yuan Yao, Sophia Xu, Zhiyuan Liu, Maosong Sun"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "penalized the negative dot-products\nbetween the fine-tuning and poisoning loss gradients, and Li\net al. ",
                    "Citation Text": "Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian\nMa, and Xipeng Qiu. Backdoor attacks on pre-trained mod-\nels by layerwise weight poisoning. In Conference on Em-\npirical Methods in Natural Language Processing (EMNLP) ,\npages 3023\u20133032, 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.13888",
                        "Citation Paper Title": "Title:Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning",
                        "Citation Paper Abstract": "Abstract:\\textbf{P}re-\\textbf{T}rained \\textbf{M}odel\\textbf{s} have been widely applied and recently proved vulnerable under backdoor attacks: the released pre-trained weights can be maliciously poisoned with certain triggers. When the triggers are activated, even the fine-tuned model will predict pre-defined labels, causing a security threat. These backdoors generated by the poisoning methods can be erased by changing hyper-parameters during fine-tuning or detected by finding the triggers. In this paper, we propose a stronger weight-poisoning attack method that introduces a layerwise weight poisoning strategy to plant deeper backdoors; we also introduce a combinatorial trigger that cannot be easily detected. The experiments on text classification tasks show that previous defense methods cannot resist our weight-poisoning method, which indicates that our method can be widely applied and may provide hints for future model robustness studies.",
                        "Citation Paper Authors": "Authors:Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma, Xipeng Qiu"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "introduced invisibly rendered zero-width\nUnicode characters as triggers to attack sentimental analysis\nmodels. To make backdoor attacks more robust against fine-\ntuning, Kurita et al. ",
                    "Citation Text": "Keita Kurita, Paul Michel, and Graham Neubig. Weight poi-\nsoning attacks on pretrained models. In Annual Meeting of\nthe Association for Computational Linguistics (ACL) , pages\n2793\u20132806, 2020. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.06660",
                        "Citation Paper Title": "Title:Weight Poisoning Attacks on Pre-trained Models",
                        "Citation Paper Abstract": "Abstract:Recently, NLP has seen a surge in the usage of large pre-trained models. Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice. This raises the question of whether downloading untrusted pre-trained weights can pose a security threat. In this paper, we show that it is possible to construct ``weight poisoning'' attacks where pre-trained weights are injected with vulnerabilities that expose ``backdoors'' after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword. We show that by applying a regularization method, which we call RIPPLe, and an initialization procedure, which we call Embedding Surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure. Our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat. Finally, we outline practical defenses against such attacks. Code to reproduce our experiments is available at this https URL.",
                        "Citation Paper Authors": "Authors:Keita Kurita, Paul Michel, Graham Neubig"
                    }
                },
                {
                    "Sentence ID": 60,
                    "Sentence": "describes a class of security attacks\nagainst machine learning models that manipulates the train-\ning data of a model before or during its training process.\nThis distinguishes it from adversarial examples ",
                    "Citation Text": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan\nBruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus.\nIntriguing properties of neural networks. In International\nConference on Learning Representations (ICLR) , 2014. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1312.6199",
                        "Citation Paper Title": "Title:Intriguing properties of neural networks",
                        "Citation Paper Abstract": "Abstract:Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.\nFirst, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks.\nSecond, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
                        "Citation Paper Authors": "Authors:Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus"
                    }
                },
                {
                    "Sentence ID": 64,
                    "Sentence": "for a comprehensive introduction to diffusion models.\nA domain encoder maps the text embeddings zto an in-\ntermediate representation. This representation is then fed\ninto the U-Net by cross-attention layers ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Il-\nlia Polosukhin. Attention is all you need. In Conference\non Neural Information Processing Systems (NeurIPS) , pages\n5998\u20136008, 2017. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.11577v4": {
            "Paper Title": "Machine Unlearning of Features and Labels",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": "increase the accuracy of influence functions by using\nhigh-order approximations, Barshan et al. ",
                    "Citation Text": "E. Barshan, M. Brunet, and G. Dziugaite, \u201cRelatif: Identi-\nfying explanatory training examples via relative influence,\u201d\ninInternational Conference on Artificial Intelligence and\nStatistics (AISTATS) , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.11630",
                        "Citation Paper Title": "Title:RelatIF: Identifying Explanatory Training Examples via Relative Influence",
                        "Citation Paper Abstract": "Abstract:In this work, we focus on the use of influence functions to identify relevant training examples that one might hope \"explain\" the predictions of a machine learning model. One shortcoming of influence functions is that the training examples deemed most \"influential\" are often outliers or mislabelled, making them poor choices for explanation. In order to address this shortcoming, we separate the role of global versus local influence. We introduce RelatIF, a new class of criteria for choosing relevant training examples by way of an optimization objective that places a constraint on global influence. RelatIF considers the local influence that an explanatory example has on a prediction relative to its global effects on the model. In empirical evaluations, we find that the examples returned by RelatIF are more intuitive when compared to those found using influence functions.",
                        "Citation Paper Authors": "Authors:Elnaz Barshan, Marc-Etienne Brunet, Gintare Karolina Dziugaite"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "address this problem and propose a\nstrategy for unlearning data instances from general classification\nmodels. Similarly, Ginart et al. ",
                    "Citation Text": "A. Ginart, M. Y . Guan, G. Valiant, and J. Zou, \u201cMaking\nAI forget you: Data deletion in machine learning,\u201d in\nAdvances in Neural Information Processing Systems\n(NIPS) , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.05012",
                        "Citation Paper Title": "Title:Making AI Forget You: Data Deletion in Machine Learning",
                        "Citation Paper Abstract": "Abstract:Intense recent discussions have focused on how to provide individuals with control over when their data can and cannot be used --- the EU's Right To Be Forgotten regulation is an example of this effort. In this paper we initiate a framework studying what to do when it is no longer permissible to deploy models derivative from specific user data. In particular, we formulate the problem of efficiently deleting individual data points from trained machine learning models. For many standard ML models, the only way to completely remove an individual's data is to retrain the whole model from scratch on the remaining data, which is often not computationally practical. We investigate algorithmic principles that enable efficient data deletion in ML. For the specific setting of k-means clustering, we propose two provably efficient deletion algorithms which achieve an average of over 100X improvement in deletion efficiency across 6 datasets, while producing clusters of comparable statistical quality to a canonical k-means++ baseline.",
                        "Citation Paper Authors": "Authors:Antonio Ginart, Melody Y. Guan, Gregory Valiant, James Zou"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.06900v2": {
            "Paper Title": "MM-BD: Post-Training Detection of Backdoor Attacks with Arbitrary\n  Backdoor Pattern Types Using a Maximum Margin Statistic",
            "Sentences": [
                {
                    "Sentence ID": 100,
                    "Sentence": "to evaluate our\ndetection method. The backdoor pattern is a small set of\ninserted points (e.g.) to mimic real objects, such as a ball\ncarried by a pedestrian. 10 good and 10 attacked DGCNN ",
                    "Citation Text": "Y . Wang, Y . Sun, Z. Liu, S. Sarma, M. Bronstein, and J. Solomon,\n\u201cDynamic graph cnn for learning on point clouds,\u201d Acm Transac-\ntions On Graphics (tog) , vol. 38, no. 5, pp. 1\u201312, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.07829",
                        "Citation Paper Title": "Title:Dynamic Graph CNN for Learning on Point Clouds",
                        "Citation Paper Abstract": "Abstract:Point clouds provide a flexible geometric representation suitable for countless applications in computer graphics; they also comprise the raw output of most 3D data acquisition devices. While hand-designed features on point clouds have long been proposed in graphics and vision, however, the recent overwhelming success of convolutional neural networks (CNNs) for image analysis suggests the value of adapting insight from CNN to the point cloud world. Point clouds inherently lack topological information so designing a model to recover topology can enrich the representation power of point clouds. To this end, we propose a new neural network module dubbed EdgeConv suitable for CNN-based high-level tasks on point clouds including classification and segmentation. EdgeConv acts on graphs dynamically computed in each layer of the network. It is differentiable and can be plugged into existing architectures. Compared to existing modules operating in extrinsic space or treating each point independently, EdgeConv has several appealing properties: It incorporates local neighborhood information; it can be stacked applied to learn global shape properties; and in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding. We show the performance of our model on standard benchmarks including ModelNet40, ShapeNetPart, and S3DIS.",
                        "Citation Paper Authors": "Authors:Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, Justin M. Solomon"
                    }
                },
                {
                    "Sentence ID": 98,
                    "Sentence": ". Here, we show the effectiveness of MM-BD\nagainst backdoor attacks on speeches and point clouds for\nexample.\n6.5.1. Speech Command Classification. We evaluate MM-\nBD on a speech domain using 10 clean and 10 back-\ndoor ensembles of M5 ",
                    "Citation Text": "W. Dai, C. Dai, S. Qu, J. Li, and S. Das, \u201cVery deep convolutional\nneural networks for raw waveforms,\u201d in ICASSP , 2017, pp. 421\u2013425.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.00087",
                        "Citation Paper Title": "Title:Very Deep Convolutional Neural Networks for Raw Waveforms",
                        "Citation Paper Abstract": "Abstract:Learning acoustic models directly from the raw waveform data with minimal processing is challenging. Current waveform-based models have generally used very few (~2) convolutional layers, which might be insufficient for building high-level discriminative features. In this work, we propose very deep convolutional neural networks (CNNs) that directly use time-domain waveforms as inputs. Our CNNs, with up to 34 weight layers, are efficient to optimize over very long sequences (e.g., vector of size 32000), necessary for processing acoustic waveforms. This is achieved through batch normalization, residual learning, and a careful design of down-sampling in the initial layers. Our networks are fully convolutional, without the use of fully connected layers and dropout, to maximize representation learning. We use a large receptive field in the first convolutional layer to mimic bandpass filters, but very small receptive fields subsequently to control the model capacity. We demonstrate the performance gains with the deeper models. Our evaluation shows that the CNN with 18 weight layers outperform the CNN with 3 weight layers by over 15% in absolute accuracy for an environmental sound recognition task and matches the performance of models using log-mel features.",
                        "Citation Paper Authors": "Authors:Wei Dai, Chia Dai, Shuhui Qu, Juncheng Li, Samarjit Das"
                    }
                },
                {
                    "Sentence ID": 74,
                    "Sentence": ") by the victim\n2. Some recently proposed advanced backdoor attacks use backdoor\npatterns that are sample-specific in the input space ",
                    "Citation Text": "Y . Li, Y . Li, B. Wu, L. Li, R. He, and S. Lyu, \u201cInvisible backdoor\nattack with sample-specific triggers,\u201d in ICCV , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.03816",
                        "Citation Paper Title": "Title:Invisible Backdoor Attack with Sample-Specific Triggers",
                        "Citation Paper Abstract": "Abstract:Recently, backdoor attacks pose a new security threat to the training process of deep neural networks (DNNs). Attackers intend to inject hidden backdoors into DNNs, such that the attacked model performs well on benign samples, whereas its prediction will be maliciously changed if hidden backdoors are activated by the attacker-defined trigger. Existing backdoor attacks usually adopt the setting that triggers are sample-agnostic, $i.e.,$ different poisoned samples contain the same trigger, resulting in that the attacks could be easily mitigated by current backdoor defenses. In this work, we explore a novel attack paradigm, where backdoor triggers are sample-specific. In our attack, we only need to modify certain training samples with invisible perturbation, while not need to manipulate other training components ($e.g.$, training loss, and model structure) as required in many existing attacks. Specifically, inspired by the recent advance in DNN-based image steganography, we generate sample-specific invisible additive noises as backdoor triggers by encoding an attacker-specified string into benign images through an encoder-decoder network. The mapping from the string to the target label will be generated when DNNs are trained on the poisoned dataset. Extensive experiments on benchmark datasets verify the effectiveness of our method in attacking models with or without defenses.",
                        "Citation Paper Authors": "Authors:Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, Siwei Lyu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.09049v3": {
            "Paper Title": "Perfectly Covert Communication with a Reflective Panel",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": "L. Lv, Q. Wu, Z. Li, Z. Ding, N. Al-Dhahir, and J. Chen, \u201cCovert communication in intelligent reflecting surface-assisted\nnoma systems: Design, analysis, and optimization,\u201d IEEE Transactions on Wireless Communications , vol. 21, no. 3, pp.\n1735\u20131750, 2021. ",
                    "Citation Text": "X. Zhou, S. Yan, Q. Wu, F. Shu, and D. W. K. Ng, \u201cIntelligent reflecting surface (IRS)-aided covert wireless communications\nwith delay constraint,\u201d IEEE Transactions on Wireless Communications , vol. 21, no. 1, pp. 532\u2013547, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.03726",
                        "Citation Paper Title": "Title:Intelligent Reflecting Surface (IRS)-Aided Covert Wireless Communications with Delay Constraint",
                        "Citation Paper Abstract": "Abstract:This work examines the performance gain achieved by deploying an intelligent reflecting surface (IRS) in covert communications. To this end, we formulate the joint design of the transmit power and the IRS reflection coefficients by taking into account the communication covertness for the cases with global channel state information (CSI) and without a warden's instantaneous CSI. For the case of global CSI, we first prove that perfect covertness is achievable with the aid of the IRS even for a single-antenna transmitter, which is impossible without an IRS. Then, we develop a penalty successive convex approximation (PSCA) algorithm to tackle the design problem. Considering the high complexity of the PSCA algorithm, we further propose a low-complexity two-stage algorithm, where analytical expressions for the transmit power and the IRS's reflection coefficients are derived. For the case without the warden's instantaneous CSI, we first derive the covertness constraint analytically facilitating the optimal phase shift design. Then, we consider three hardware-related constraints on the IRS's reflection amplitudes and determine their optimal designs together with the optimal transmit power. Our examination shows that significant performance gain can be achieved by deploying an IRS into covert communications.",
                        "Citation Paper Authors": "Authors:Xiaobo Zhou, Shihao Yan, Qingqing Wu, Feng Shu, Derrick Wing Kwan Ng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.02281v5": {
            "Paper Title": "FLAME: Taming Backdoors in Federated Learning (Extended Version 1)",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": ". Previous works often\nemployed equal weights ( si=1/n) for the contributions of all\nclients [9,56,64]. We adopt this approach in this paper, i.e., we\nsetGt=\u03a3n\ni=1Wi/n. Further, other state-of-the-art aggregation\nrules, e.g., Krum ",
                    "Citation Text": "Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and\nJulien Stainer. Machine Learning with Adversaries: Byzantine\nTolerant Gradient Descent. In NIPS , 2017.\n15",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.02757",
                        "Citation Paper Title": "Title:Byzantine-Tolerant Machine Learning",
                        "Citation Paper Abstract": "Abstract:The growth of data, the need for scalability and the complexity of models used in modern machine learning calls for distributed implementations. Yet, as of today, distributed machine learning frameworks have largely ignored the possibility of arbitrary (i.e., Byzantine) failures. In this paper, we study the robustness to Byzantine failures at the fundamental level of stochastic gradient descent (SGD), the heart of most machine learning algorithms. Assuming a set of $n$ workers, up to $f$ of them being Byzantine, we ask how robust can SGD be, without limiting the dimension, nor the size of the parameter space.\nWe first show that no gradient descent update rule based on a linear combination of the vectors proposed by the workers (i.e, current approaches) tolerates a single Byzantine failure. We then formulate a resilience property of the update rule capturing the basic requirements to guarantee convergence despite $f$ Byzantine workers. We finally propose Krum, an update rule that satisfies the resilience property aforementioned. For a $d$-dimensional learning problem, the time complexity of Krum is $O(n^2 \\cdot (d + \\log n))$.",
                        "Citation Paper Authors": "Authors:Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, Julien Stainer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.04256v6": {
            "Paper Title": "SoK: Blockchain Decentralization",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": ". In the analysis of DeFi on blockchains, Werner et\nal. (2021) ",
                    "Citation Text": "S. M. Werner, D. Perez, L. Gudgeon, A. Klages-Mundt, D. Harz,\nand W. J. Knottenbelt, \u201cSok: Decentralized finance (defi),\u201d\narXiv:2101.08778 [cs, econ, q-fin] , 04 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.08778",
                        "Citation Paper Title": "Title:SoK: Decentralized Finance (DeFi)",
                        "Citation Paper Abstract": "Abstract:Decentralized Finance (DeFi), a blockchain powered peer-to-peer financial system, is mushrooming. Two years ago the total value locked in DeFi systems was approximately 700m USD, now, as of April 2022, it stands at around 150bn USD. The frenetic evolution of the ecosystem has created challenges in understanding the basic principles of these systems and their security risks. In this Systematization of Knowledge (SoK) we delineate the DeFi ecosystem along the following axes: its primitives, its operational protocol types and its security. We provide a distinction between technical security, which has a healthy literature, and economic security, which is largely unexplored, connecting the latter with new models and thereby synthesizing insights from computer science, economics and finance. Finally, we outline the open research challenges in the ecosystem across these security types.",
                        "Citation Paper Authors": "Authors:Sam M. Werner, Daniel Perez, Lewis Gudgeon, Ariah Klages-Mundt, Dominik Harz, William J. Knottenbelt"
                    }
                },
                {
                    "Sentence ID": 122,
                    "Sentence": ". Along-\nside blockchain attacks, DeFi applications have also seen\nthreats ",
                    "Citation Text": "L. Zhou, X. Xiong, J. Ernstberger, S. Chaliasos, Z. Wang, Y . Wang,\nK. Qin, R. Wattenhofer, D. Song, and A. Gervais, \u201cSok: Decentral-\nized finance (defi) incidents,\u201d arXiv:2208.13035 [cs] , 08 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2208.13035",
                        "Citation Paper Title": "Title:SoK: Decentralized Finance (DeFi) Attacks",
                        "Citation Paper Abstract": "Abstract:Within just four years, the blockchain-based Decentralized Finance (DeFi) ecosystem has accumulated a peak total value locked (TVL) of more than 253 billion USD. This surge in DeFi's popularity has, unfortunately, been accompanied by many impactful incidents. According to our data, users, liquidity providers, speculators, and protocol operators suffered a total loss of at least 3.24 billion USD from Apr 30, 2018 to Apr 30, 2022. Given the blockchain's transparency and increasing incident frequency, two questions arise: How can we systematically measure, evaluate, and compare DeFi incidents? How can we learn from past attacks to strengthen DeFi security?\nIn this paper, we introduce a common reference frame to systematically evaluate and compare DeFi incidents, including both attacks and accidents. We investigate 77 academic papers, 30 audit reports, and 181 real-world incidents. Our data reveals several gaps between academia and the practitioners' community. For example, few academic papers address \"price oracle attacks\" and \"permissonless interactions\", while our data suggests that they are the two most frequent incident types (15% and 10.5% correspondingly). We also investigate potential defenses, and find that: (i) 103 (56%) of the attacks are not executed atomically, granting a rescue time frame for defenders; (ii) SoTA bytecode similarity analysis can at least detect 31 vulnerable/23 adversarial contracts; and (iii) 33 (15.3%) of the adversaries leak potentially identifiable information by interacting with centralized exchanges.",
                        "Citation Paper Authors": "Authors:Liyi Zhou, Xihan Xiong, Jens Ernstberger, Stefanos Chaliasos, Zhipeng Wang, Ye Wang, Kaihua Qin, Roger Wattenhofer, Dawn Song, Arthur Gervais"
                    }
                },
                {
                    "Sentence ID": 119,
                    "Sentence": ", which fo-\ncuses on blockchain network auditability. The decentralized\nnature of blockchains has led to challenges like Front-\nrunning ",
                    "Citation Text": "S. Eskandari, S. Moosavi, and J. Clark, \u201cSok: Transparent dishon-\nesty: front-running attacks on blockchain,\u201d arXiv:1902.05164 [cs] ,\n04 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.05164",
                        "Citation Paper Title": "Title:SoK: Transparent Dishonesty: front-running attacks on Blockchain",
                        "Citation Paper Abstract": "Abstract:We consider front-running to be a course of action where an entity benefits from prior access to privileged market information about upcoming transactions and trades. Front-running has been an issue in financial instrument markets since the 1970s. With the advent of the blockchain technology, front-running has resurfaced in new forms we explore here, instigated by blockchains decentralized and transparent nature. In this paper, we draw from a scattered body of knowledge and instances of front-running across the top 25 most active decentral applications (DApps) deployed on Ethereum blockchain. Additionally, we carry out a detailed analysis of this http URL initial coin offering (ICO) and show evidence of abnormal miners behavior indicative of front-running token purchases. Finally, we map the proposed solutions to front-running into useful categories.",
                        "Citation Paper Authors": "Authors:Shayan Eskandari, Seyedehmahsa Moosavi, Jeremy Clark"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "T\u2192 -Preferential attachment: transaction value\n-Probability distribution: transaction volume-A\u221eETH/ethereum\nLINK\nUSDT\nBNB ",
                    "Citation Text": "C. Campajola, R. Cristodaro, F. M. De Collibus, T. Yan, N. Val-\nlarano, and C. J. Tessone, \u201cThe evolution of centralisation on cryp-\ntocurrency platforms,\u201d arXiv:2206.05081 [physics, q-fin] , 06 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.05081",
                        "Citation Paper Title": "Title:The Evolution Of Centralisation on Cryptocurrency Platforms",
                        "Citation Paper Abstract": "Abstract:More than ten years ago the blockchain was acclaimed as the solution to overcome centralised trusted third parties for online payments. Through the years the crypto-movement changed and evolved, although decentralisation remained the core ideology and the necessary feature every new crypto-project should provide. In this paper we study the concept of centralisation in cryptocurrencies using a wide array of methodologies from the complex systems literature, on a comparative collection of blockchains, in order to define the many different levels a blockchain system may display (de-)centralisation and to question whether the present state of cryptocurrencies is, in a technological and economical sense, actually decentralised.",
                        "Citation Paper Authors": "Authors:Carlo Campajola, Raffaele Cristodaro, Francesco Maria De Collibus, Tao Yan, Nicolo' Vallarano, Claudio J. Tessone"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.03276v3": {
            "Paper Title": "Modeling Self-Propagating Malware with Epidemiological Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.07944v4": {
            "Paper Title": "Distributed Online Private Learning of Convex Nondecomposable Objectives",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.05877v2": {
            "Paper Title": "Fabricated Flips: Poisoning Federated Learning without Data",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": ". In this paper, we focus on training-time\n2Attacks LIE ",
                    "Citation Text": "Gilad Baruch, Moran Baruch, and Yoav Goldberg. A little is enough:\nCircumventing defenses for distributed learning. In Conference on Neural\nInformation Processing Systems , pages 8632\u20138642, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.06156",
                        "Citation Paper Title": "Title:A Little Is Enough: Circumventing Defenses For Distributed Learning",
                        "Citation Paper Abstract": "Abstract:Distributed learning is central for large-scale training of deep-learning models. However, they are exposed to a security threat in which Byzantine participants can interrupt or control the learning process. Previous attack models and their corresponding defenses assume that the rogue participants are (a) omniscient (know the data of all other participants), and (b) introduce large change to the parameters. We show that small but well-crafted changes are sufficient, leading to a novel non-omniscient attack on distributed learning that go undetected by all existing defenses. We demonstrate our attack method works not only for preventing convergence but also for repurposing of the model behavior (backdooring). We show that 20% of corrupt workers are sufficient to degrade a CIFAR10 model accuracy by 50%, as well as to introduce backdoors into MNIST and CIFAR10 models without hurting their accuracy",
                        "Citation Paper Authors": "Authors:Moran Baruch, Gilad Baruch, Yoav Goldberg"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": ".\nDatasets and networks. In this work, we consider three\ndatasets. Fashion-MNIST ",
                    "Citation Text": "Han Xiao, Kashif Rasul, and Roland V ollgraf. Fashion-mnist: a novel\nimage dataset for benchmarking machine learning algorithms. arXiv\npreprint arXiv:1708.07747 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.07747",
                        "Citation Paper Title": "Title:Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms",
                        "Citation Paper Abstract": "Abstract:We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at this https URL",
                        "Citation Paper Authors": "Authors:Han Xiao, Kashif Rasul, Roland Vollgraf"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": ", we require a=b\u00d7(St+1)\u22122P+J.\nThe attack can be extended to other tasks, e.g., text processing,\nby replacing the filter model and using a Seq2Seq model ",
                    "Citation Text": "Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence\n13learning with neural networks. Advances in neural information processing\nsystems , 27, 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.3215",
                        "Citation Paper Title": "Title:Sequence to Sequence Learning with Neural Networks",
                        "Citation Paper Abstract": "Abstract:Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",
                        "Citation Paper Authors": "Authors:Ilya Sutskever, Oriol Vinyals, Quoc V. Le"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "identifies Sybils based on the diversity\nof client contributions using cosine similarity of client updates.\nii) Statistic defenses curate the aggregated model by computing\nthe statistics of every parameter across multiple updates.\nMedian ",
                    "Citation Text": "Dong Yin, Yudong Chen, Kannan Ramchandran, and Peter L. Bartlett.\nByzantine-robust distributed learning: Towards optimal statistical rates.\nInProceedings of the International Conference on Machine Learning ,\nvolume 80, pages 5636\u20135645, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.01498",
                        "Citation Paper Title": "Title:Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates",
                        "Citation Paper Abstract": "Abstract:In large-scale distributed learning, security issues have become increasingly important. Particularly in a decentralized environment, some computing units may behave abnormally, or even exhibit Byzantine failures -- arbitrary and potentially adversarial behavior. In this paper, we develop distributed learning algorithms that are provably robust against such failures, with a focus on achieving optimal statistical performance. A main result of this work is a sharp analysis of two robust distributed gradient descent algorithms based on median and trimmed mean operations, respectively. We prove statistical error rates for three kinds of population loss functions: strongly convex, non-strongly convex, and smooth non-convex. In particular, these algorithms are shown to achieve order-optimal statistical error rates for strongly convex losses. To achieve better communication efficiency, we further propose a median-based distributed algorithm that is provably robust, and uses only one communication round. For strongly convex quadratic loss, we show that this algorithm achieves the same optimal error rate as the robust distributed gradient descent algorithms.",
                        "Citation Paper Authors": "Authors:Dong Yin, Yudong Chen, Kannan Ramchandran, Peter Bartlett"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": ". They may even reconstruct the private\nlocal training data ",
                    "Citation Text": "Hongxu Yin, Arun Mallya, Arash Vahdat, Jose M. Alvarez, Jan Kautz,\nand Pavlo Molchanov. See through gradients: Image batch recovery\nvia gradinversion. In IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2021 , pages 16337\u201316346. Computer Vision\nFoundation / IEEE, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.07586",
                        "Citation Paper Title": "Title:See through Gradients: Image Batch Recovery via GradInversion",
                        "Citation Paper Abstract": "Abstract:Training deep neural networks requires gradient estimation from data batches to update parameters. Gradients per parameter are averaged over a set of data and this has been presumed to be safe for privacy-preserving training in joint, collaborative, and federated learning applications. Prior work only showed the possibility of recovering input data given gradients under very restrictive conditions - a single input point, or a network with no non-linearities, or a small 32x32 px input batch. Therefore, averaging gradients over larger batches was thought to be safe. In this work, we introduce GradInversion, using which input images from a larger batch (8 - 48 images) can also be recovered for large networks such as ResNets (50 layers), on complex datasets such as ImageNet (1000 classes, 224x224 px). We formulate an optimization task that converts random noise into natural images, matching gradients while regularizing image fidelity. We also propose an algorithm for target class label recovery given gradients. We further propose a group consistency regularization framework, where multiple agents starting from different random seeds work together to find an enhanced reconstruction of original data batch. We show that gradients encode a surprisingly large amount of information, such that all the individual images can be recovered with high fidelity via GradInversion, even for complex datasets, deep networks, and large batch sizes.",
                        "Citation Paper Authors": "Authors:Hongxu Yin, Arun Mallya, Arash Vahdat, Jose M. Alvarez, Jan Kautz, Pavlo Molchanov"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.11236v2": {
            "Paper Title": "Boosting the Transferability of Adversarial Attacks with Global Momentum\n  Initialization",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": ". In addition, we adopt nine ad-\nvanced defense methods to test the attack performance,\ne.g. HGD ",
                    "Citation Text": "Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang,\nXiaolin Hu, and Jun Zhu. Defense against adversarial at-\ntacks using high-level representation guided denoiser. In\n2018 IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2018, Salt Lake City, UT, USA, June 18-\n22, 2018 , pages 1778\u20131787. Computer Vision Foundation /\nIEEE Computer Society, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.02976",
                        "Citation Paper Title": "Title:Defense against Adversarial Attacks Using High-Level Representation Guided Denoiser",
                        "Citation Paper Abstract": "Abstract:Neural networks are vulnerable to adversarial examples, which poses a threat to their application in security sensitive systems. We propose high-level representation guided denoiser (HGD) as a defense for image classification. Standard denoiser suffers from the error amplification effect, in which small residual adversarial noise is progressively amplified and leads to wrong classifications. HGD overcomes this problem by using a loss function defined as the difference between the target model's outputs activated by the clean image and denoised image. Compared with ensemble adversarial training which is the state-of-the-art defending method on large images, HGD has three advantages. First, with HGD as a defense, the target model is more robust to either white-box or black-box adversarial attacks. Second, HGD can be trained on a small subset of the images and generalizes well to other images and unseen classes. Third, HGD can be transferred to defend models other than the one guiding it. In NIPS competition on defense against adversarial attacks, our HGD solution won the first place and outperformed other models by a large margin.",
                        "Citation Paper Authors": "Authors:Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Xiaolin Hu, Jun Zhu"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": ". FGSM gen-\nerates an adversarial example for only one step with the aim\nof maximizing the loss function:\nxadv=xclean+\u03f5\u00b7sign(\u2207xJ(xclean, y)),\nwhere sign(\u00b7) represents the sign function.\nIterative Fast Gradient Sign Method (I-FGSM) ",
                    "Citation Text": "Alexey Kurakin, Ian J Goodfellow, and Samy Bengio. Ad-\nversarial examples in the physical world. In Artificial in-\ntelligence safety and security , pages 99\u2013112. Chapman and\nHall/CRC, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.02533",
                        "Citation Paper Title": "Title:Adversarial examples in the physical world",
                        "Citation Paper Abstract": "Abstract:Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.",
                        "Citation Paper Authors": "Authors:Alexey Kurakin, Ian Goodfellow, Samy Bengio"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": ", we\nrandomly select 1,000 images from the ILSVRC 2012 vali-\ndation set ",
                    "Citation Text": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael S. Bernstein, Alexander C. Berg,\nand Fei-Fei Li. Imagenet large scale visual recognition chal-\nlenge. Int. J. Comput. Vis. , 115(3):211\u2013252, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.0575",
                        "Citation Paper Title": "Title:ImageNet Large Scale Visual Recognition Challenge",
                        "Citation Paper Abstract": "Abstract:The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions.\nThis paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.",
                        "Citation Paper Authors": "Authors:Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, Li Fei-Fei"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": ". DIM performs data\naugmentation using random scaling and padding operations\nwith a certain probability to alleviate overfitting in adver-\nsarial attacks, which inspires one of the main perspectives\nof transferability enhancement.\nTranslation-Invariant Method (TIM) ",
                    "Citation Text": "Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu.\nEvading defenses to transferable adversarial examples\nby translation-invariant attacks. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 4312\u20134321, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.02884",
                        "Citation Paper Title": "Title:Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks",
                        "Citation Paper Abstract": "Abstract:Deep neural networks are vulnerable to adversarial examples, which can mislead classifiers by adding imperceptible perturbations. An intriguing property of adversarial examples is their good transferability, making black-box attacks feasible in real-world applications. Due to the threat of adversarial attacks, many methods have been proposed to improve the robustness. Several state-of-the-art defenses are shown to be robust against transferable adversarial examples. In this paper, we propose a translation-invariant attack method to generate more transferable adversarial examples against the defense models. By optimizing a perturbation over an ensemble of translated images, the generated adversarial example is less sensitive to the white-box model being attacked and has better transferability. To improve the efficiency of attacks, we further show that our method can be implemented by convolving the gradient at the untranslated image with a pre-defined kernel. Our method is generally applicable to any gradient-based attack method. Extensive experiments on the ImageNet dataset validate the effectiveness of the proposed method. Our best attack fools eight state-of-the-art defenses at an 82% success rate on average based only on the transferability, demonstrating the insecurity of the current defense techniques.",
                        "Citation Paper Authors": "Authors:Yinpeng Dong, Tianyu Pang, Hang Su, Jun Zhu"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "In this section, we present some gradient-based attack\nmethods. Details of the defense methods can be found in\ntheSupplementary Material 2 .\n2.1. Gradient Optimization Attacks\nFast Gradient Sign Method (FGSM) ",
                    "Citation Text": "Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy.\nExplaining and harnessing adversarial examples. In ICLR ,\n2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.6572",
                        "Citation Paper Title": "Title:Explaining and Harnessing Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.13499v4": {
            "Paper Title": "FieldFuzz: In Situ Blackbox Fuzzing of Proprietary Industrial Automation\n  Runtimes via the Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.04123v2": {
            "Paper Title": "Nitriding: A tool kit for building scalable, networked, secure enclaves",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.06039v5": {
            "Paper Title": "Reactive Synthesis of Smart Contract Control Flows",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.06970v5": {
            "Paper Title": "Computing a Group Action from the Class Field Theory of Imaginary\n  Hyperelliptic Function Fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.03328v2": {
            "Paper Title": "Securing Secure Aggregation: Mitigating Multi-Round Privacy Leakage in\n  Federated Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.00394v2": {
            "Paper Title": "Wiretap Secret Key Agreement Via Secure Omniscience",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.10848v2": {
            "Paper Title": "Robust Quantity-Aware Aggregation for Federated Learning",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": "propose Norm-bound that clips the \ud835\udc3f2norm of\nreceived updates to a predefined threshold. Pillutla et al . ",
                    "Citation Text": "Krishna Pillutla, Sham M Kakade, and Zaid Harchaoui. 2019. Robust aggregation\nfor federated learning. arXiv preprint arXiv:1912.13445 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.13445",
                        "Citation Paper Title": "Title:Robust Aggregation for Federated Learning",
                        "Citation Paper Abstract": "Abstract:Federated learning is the centralized training of statistical models from decentralized data on mobile devices while preserving the privacy of each device. We present a robust aggregation approach to make federated learning robust to settings when a fraction of the devices may be sending corrupted updates to the server. The approach relies on a robust aggregation oracle based on the geometric median, which returns a robust aggregate using a constant number of iterations of a regular non-robust averaging oracle. The robust aggregation oracle is privacy-preserving, similar to the non-robust secure average oracle it builds upon. We establish its convergence for least squares estimation of additive models. We provide experimental results with linear models and deep networks for three tasks in computer vision and natural language processing. The robust aggregation approach is agnostic to the level of corruption; it outperforms the classical aggregation approach in terms of robustness when the level of corruption is high, while being competitive in the regime of low corruption. Two variants, a faster one with one-step robust aggregation and another one with on-device personalization, round off the paper.",
                        "Citation Paper Authors": "Authors:Krishna Pillutla, Sham M. Kakade, Zaid Harchaoui"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "propose Krum and mKrum that compute square-\ndistance-based scores to select and average the updates closest to\na subset of neighboring updates. Bulyan ",
                    "Citation Text": "El Mahdi El Mhamdi, Rachid Guerraoui, and S\u00e9bastien Rouault. 2018. The Hidden\nVulnerability of Distributed Learning in Byzantium. In ICML , Vol. 80. 3521\u20133530.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.07927",
                        "Citation Paper Title": "Title:The Hidden Vulnerability of Distributed Learning in Byzantium",
                        "Citation Paper Abstract": "Abstract:While machine learning is going through an era of celebrated success, concerns have been raised about the vulnerability of its backbone: stochastic gradient descent (SGD). Recent approaches have been proposed to ensure the robustness of distributed SGD against adversarial (Byzantine) workers sending poisoned gradients during the training phase. Some of these approaches have been proven Byzantine-resilient: they ensure the convergence of SGD despite the presence of a minority of adversarial workers.\nWe show in this paper that convergence is not enough. In high dimension $d \\gg 1$, an adver\\-sary can build on the loss function's non-convexity to make SGD converge to ineffective models. More precisely, we bring to light that existing Byzantine-resilient schemes leave a margin of poisoning of $\\Omega\\left(f(d)\\right)$, where $f(d)$ increases at least like $\\sqrt{d~}$. Based on this leeway, we build a simple attack, and experimentally show its strong to utmost effectivity on CIFAR-10 and MNIST.\nWe introduce Bulyan, and prove it significantly reduces the attackers leeway to a narrow $O( \\frac{1}{\\sqrt{d~}})$ bound. We empirically show that Bulyan does not suffer the fragility of existing aggregation rules and, at a reasonable cost in terms of required batch size, achieves convergence as if only non-Byzantine gradients had been used to update the model.",
                        "Citation Paper Authors": "Authors:El Mahdi El Mhamdi, Rachid Guerraoui, S\u00e9bastien Rouault"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "pro-\npose Median and Trimean that apply coordinate-wise median and\ntrimmed-mean, respectively, to filter out malicious updates. Blan-\nchard et al . ",
                    "Citation Text": "Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. 2017.\nMachine learning with adversaries: Byzantine tolerant gradient descent. NIPS 30\n(2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.02757",
                        "Citation Paper Title": "Title:Byzantine-Tolerant Machine Learning",
                        "Citation Paper Abstract": "Abstract:The growth of data, the need for scalability and the complexity of models used in modern machine learning calls for distributed implementations. Yet, as of today, distributed machine learning frameworks have largely ignored the possibility of arbitrary (i.e., Byzantine) failures. In this paper, we study the robustness to Byzantine failures at the fundamental level of stochastic gradient descent (SGD), the heart of most machine learning algorithms. Assuming a set of $n$ workers, up to $f$ of them being Byzantine, we ask how robust can SGD be, without limiting the dimension, nor the size of the parameter space.\nWe first show that no gradient descent update rule based on a linear combination of the vectors proposed by the workers (i.e, current approaches) tolerates a single Byzantine failure. We then formulate a resilience property of the update rule capturing the basic requirements to guarantee convergence despite $f$ Byzantine workers. We finally propose Krum, an update rule that satisfies the resilience property aforementioned. For a $d$-dimensional learning problem, the time complexity of Krum is $O(n^2 \\cdot (d + \\log n))$.",
                        "Citation Paper Authors": "Authors:Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, Julien Stainer"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": ", updates are weight-averaged\naccording to the quantity of each client\u2019s training samples. Reddi\net al. ",
                    "Citation Text": "Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,\nJakub Kone\u010dn\u00fd, Sanjiv Kumar, and Hugh Brendan McMahan. 2021. Adaptive\nFederated Optimization. In ICLR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.00295",
                        "Citation Paper Title": "Title:Adaptive Federated Optimization",
                        "Citation Paper Abstract": "Abstract:Federated learning is a distributed machine learning paradigm in which a large number of clients coordinate with a central server to learn a model without sharing their own training data. Standard federated optimization methods such as Federated Averaging (FedAvg) are often difficult to tune and exhibit unfavorable convergence behavior. In non-federated settings, adaptive optimization methods have had notable success in combating such issues. In this work, we propose federated versions of adaptive optimizers, including Adagrad, Adam, and Yogi, and analyze their convergence in the presence of heterogeneous data for general non-convex settings. Our results highlight the interplay between client heterogeneity and communication efficiency. We also perform extensive experiments on these methods and show that the use of adaptive optimizers can significantly improve the performance of federated learning.",
                        "Citation Paper Authors": "Authors:Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Kone\u010dn\u00fd, Sanjiv Kumar, H. Brendan McMahan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.05796v3": {
            "Paper Title": "Generalizing DP-SGD with Shuffling and Batch Clipping",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.07160v2": {
            "Paper Title": "FedTracker: Furnishing Ownership Verification and Traceability for\n  Federated Learning Model",
            "Sentences": [
                {
                    "Sentence ID": 53,
                    "Sentence": "Z. Wang, K. Mei, H. Ding, J. Zhai, and S. Ma, \u201cRethinking\nthe reverse-engineering of trojan triggers,\u201d in Advances in Neural\nInformation Processing Systems , 2022. ",
                    "Citation Text": "M. Yurochkin, M. Agarwal, S. Ghosh, K. Greenewald, N. Hoang,\nand Y. Khazaeni, \u201cBayesian nonparametric federated learning of\nneural networks,\u201d in Proceedings of 2019 International Conference on\nMachine Learning , 2019, pp. 7252\u20137261.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.12022",
                        "Citation Paper Title": "Title:Bayesian Nonparametric Federated Learning of Neural Networks",
                        "Citation Paper Abstract": "Abstract:In federated learning problems, data is scattered across different servers and exchanging or pooling it is often impractical or prohibited. We develop a Bayesian nonparametric framework for federated learning with neural networks. Each data server is assumed to provide local neural network weights, which are modeled through our framework. We then develop an inference approach that allows us to synthesize a more expressive global network without additional supervision, data pooling and with as few as a single communication round. We then demonstrate the efficacy of our approach on federated learning problems simulated from two popular image classification datasets.",
                        "Citation Paper Authors": "Authors:Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Trong Nghia Hoang, Yasaman Khazaeni"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.14017v3": {
            "Paper Title": "BASALISC: Programmable Hardware Accelerator for BGV Fully Homomorphic\n  Encryption",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.09495v2": {
            "Paper Title": "ROI: A method for identifying organizations receiving personal data",
            "Sentences": [
                {
                    "Sentence ID": 46,
                    "Sentence": "revealed that SVM outperforms Logistic Regression  and \nConvolutional Neural Networks for categorization of privacy practices.  Relying on \nour previous experience building these kinds of classifiers ",
                    "Citation Text": "Guam\u00e1n, D. S., Rodriguez, D., del Alamo, J. M., & Such, J. (2023). Automated\nGDPR compliance assessment for cross -border personal data transfers in\nandroid applications. Computers & Security , 130, 103262.\nhttps://doi.org/10.1016/J.COSE.2023.103262",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.07297",
                        "Citation Paper Title": "Title:Automating the GDPR Compliance Assessment for Cross-border Personal Data Transfers in Android Applications",
                        "Citation Paper Abstract": "Abstract:The General Data Protection Regulation (GDPR) aims to ensure that all personal data processing activities are fair and transparent for the European Union (EU) citizens, regardless of whether these are carried out within the EU or anywhere else. To this end, it sets strict requirements to transfer personal data outside the EU. However, checking these requirements is a daunting task for supervisory authorities, particularly in the mobile app domain due to the huge number of apps available and their dynamic nature. In this paper, we propose a fully automated method to assess the compliance of mobile apps with the GDPR requirements for cross-border personal data transfers. We have applied the method to the top-free 10,080 apps from the Google Play Store. The results reveal that there is still a very significant gap between what app providers and third-party recipients do in practice and what is intended by the GDPR. A substantial 56% of analysed apps are potentially non-compliant with the GDPR cross-border transfer requirements.",
                        "Citation Paper Authors": "Authors:Danny S. Guam\u00e1n, Xavier Ferrer, Jose M. del Alamo, Jose Such"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.03075v3": {
            "Paper Title": "Systematic Assessment of Fuzzers using Mutation Analysis",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": ". The binaries were especially created for the CGC. Hence,\nchallenge binaries necessarily have a bias to be used in the\nCGC and consist mostly of command line tools.\nA recent benchmarking approach is Magma ",
                    "Citation Text": "Ahmad Hazimeh, Adrian Herrera, and Mathias Payer. \u201cMagma: A\nGround-Truth Fuzzing Benchmark\u201d. Proceedings of the ACM on\nMeasurement and Analysis of Computing Systems 4 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.01120",
                        "Citation Paper Title": "Title:Magma: A Ground-Truth Fuzzing Benchmark",
                        "Citation Paper Abstract": "Abstract:High scalability and low running costs have made fuzz testing the de facto standard for discovering software bugs. Fuzzing techniques are constantly being improved in a race to build the ultimate bug-finding tool. However, while fuzzing excels at finding bugs in the wild, evaluating and comparing fuzzer performance is challenging due to the lack of metrics and benchmarks. For example, crash count, perhaps the most commonly-used performance metric, is inaccurate due to imperfections in deduplication techniques. Additionally, the lack of a unified set of targets results in ad hoc evaluations that hinder fair comparison.\nWe tackle these problems by developing Magma, a ground-truth fuzzing benchmark that enables uniform fuzzer evaluation and comparison. By introducing real bugs into real software, Magma allows for the realistic evaluation of fuzzers against a broad set of targets. By instrumenting these bugs, Magma also enables the collection of bug-centric performance metrics independent of the fuzzer. Magma is an open benchmark consisting of seven targets that perform a variety of input manipulations and complex computations, presenting a challenge to state-of-the-art fuzzers.\nWe evaluate seven widely-used mutation-based fuzzers (AFL, AFLFast, AFL++, FairFuzz, MOpt-AFL, honggfuzz, and SymCC-AFL) against Magma over 200,000 CPU-hours. Based on the number of bugs reached, triggered, and detected, we draw conclusions about the fuzzers' exploration and detection capabilities. This provides insight into fuzzer performance evaluation, highlighting the importance of ground truth in performing more accurate and meaningful evaluations.",
                        "Citation Paper Authors": "Authors:Ahmad Hazimeh, Adrian Herrera, Mathias Payer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.16993v2": {
            "Paper Title": "Post-Quantum $\u03ba$-to-1 Trapdoor Claw-free Functions from\n  Extrapolated Dihedral Cosets",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": ", i.e. 2O(n2). Recently,\nBrakerski et al. showed that under quantum polynomial-time reductions, the LWE is\nequivalent to the extrapolated dihedral coset problem (EDCP) ",
                    "Citation Text": "Brakerski, Z., Kirshanova, E., Stehl\u00e9, D., Wen, W.: Learning with errors\nand extrapolated dihedral cosets. In: IACR International Workshop on Pub-\nlic Key Cryptography. pp. 702\u2013727. Springer (2018) https://doi.org/10.1007/\n978-3-319-76581-5_24.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.08223",
                        "Citation Paper Title": "Title:Learning With Errors and Extrapolated Dihedral Cosets",
                        "Citation Paper Abstract": "Abstract:The hardness of the learning with errors (LWE) problem is one of the most fruitful resources of modern cryptography. In particular, it is one of the most prominent candidates for secure post-quantum cryptography. Understanding its quantum complexity is therefore an important goal. We show that under quantum polynomial time reductions, LWE is equivalent to a relaxed version of the dihedral coset problem (DCP), which we call extrapolated DCP (eDCP). The extent of extrapolation varies with the LWE noise rate. By considering different extents of extrapolation, our result generalizes Regev's famous proof that if DCP is in BQP (quantum poly-time) then so is LWE (FOCS'02). We also discuss a connection between eDCP and Childs and Van Dam's algorithm for generalized hidden shift problems (SODA'07). Our result implies that a BQP solution for LWE might not require the full power of solving DCP, but rather only a solution for its relaxed version, eDCP, which could be easier.",
                        "Citation Paper Authors": "Authors:Zvika Brakerski, Elena Kirshanova, Damien Stehl\u00e9, Weiqiang Wen"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": ". They pointed that it is sufficient to solve the DCP by a lin-\near number of coset samples, but post-processing requires exponentially more time.\nBesides, Bacon et al. ",
                    "Citation Text": "Bacon, D., Childs, A.M., van Dam, W.: Optimal measurements for the dihedral\nhidden subgroup problem. arXiv preprint quant-ph/0501044 (2005) https://doi.\norg/10.4086/cjtcs.2006.002.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:quant-ph/0504083",
                        "Citation Paper Title": "Title:From optimal measurement to efficient quantum algorithms for the hidden subgroup problem over semidirect product groups",
                        "Citation Paper Abstract": "Abstract:  We approach the hidden subgroup problem by performing the so-called pretty good measurement on hidden subgroup states. For various groups that can be expressed as the semidirect product of an abelian group and a cyclic group, we show that the pretty good measurement is optimal and that its probability of success and unitary implementation are closely related to an average-case algebraic problem. By solving this problem, we find efficient quantum algorithms for a number of nonabelian hidden subgroup problems, including some for which no efficient algorithm was previously known: certain metacyclic groups as well as all groups of the form (Z_p)^r X| Z_p for fixed r (including the Heisenberg group, r=2). In particular, our results show that entangled measurements across multiple copies of hidden subgroup states can be useful for efficiently solving the nonabelian HSP.",
                        "Citation Paper Authors": "Authors:Dave Bacon, Andrew M. Childs, Wim van Dam"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.03297v2": {
            "Paper Title": "Preprocessors Matter! Realistic Decision-Based Attacks on Machine\n  Learning Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.12900v4": {
            "Paper Title": "Pre-trained Perceptual Features Improve Differentially Private Image\n  Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.00250v2": {
            "Paper Title": "Split Learning without Local Weight Sharing to Enhance Client-side Data\n  Privacy",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": ". That is, the smashed data exposed by clients\nmay be exploited to recover the raw input data. Therefore,\nprivacy protection techniques in SL typically aim to minimize\ndata leakage from the smashed data. For example, the noise\ndefense approach ",
                    "Citation Text": "S. Abuadbba, K. Kim, M. Kim, C. Thapa, S. A. Camtepe, Y . Gao,\nH. Kim, and S. Nepal, \u201cCan We Use Split Learning on 1D CNN Models\nfor Privacy Preserving Training?\u201d in Proceedings of the 15th ACM Asia\nConference on Computer and Communications Security , 2020, pp. 305\u2013\n318.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.12365",
                        "Citation Paper Title": "Title:Can We Use Split Learning on 1D CNN Models for Privacy Preserving Training?",
                        "Citation Paper Abstract": "Abstract:A new collaborative learning, called split learning, was recently introduced, aiming to protect user data privacy without revealing raw input data to a server. It collaboratively runs a deep neural network model where the model is split into two parts, one for the client and the other for the server. Therefore, the server has no direct access to raw data processed at the client. Until now, the split learning is believed to be a promising approach to protect the client's raw data; for example, the client's data was protected in healthcare image applications using 2D convolutional neural network (CNN) models. However, it is still unclear whether the split learning can be applied to other deep learning models, in particular, 1D CNN.\nIn this paper, we examine whether split learning can be used to perform privacy-preserving training for 1D CNN models. To answer this, we first design and implement an 1D CNN model under split learning and validate its efficacy in detecting heart abnormalities using medical ECG data. We observed that the 1D CNN model under split learning can achieve the same accuracy of 98.9\\% like the original (non-split) model. However, our evaluation demonstrates that split learning may fail to protect the raw data privacy on 1D CNN models. To address the observed privacy leakage in split learning, we adopt two privacy leakage mitigation techniques: 1) adding more hidden layers to the client side and 2) applying differential privacy. Although those mitigation techniques are helpful in reducing privacy leakage, they have a significant impact on model accuracy. Hence, based on those results, we conclude that split learning alone would not be sufficient to maintain the confidentiality of raw sequential data in 1D CNN models.",
                        "Citation Paper Authors": "Authors:Sharif Abuadbba, Kyuyeon Kim, Minki Kim, Chandra Thapa, Seyit A. Camtepe, Yansong Gao, Hyoungshick Kim, Surya Nepal"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "propose client-based privacy protection, which employs two\ndifferent loss functions computed on the client and server\nsides. In line with this approach, ",
                    "Citation Text": "J. Li, A. S. Rakin, X. Chen, Z. He, D. Fan, and C. Chakrabarti, \u201cResSFL:\nA Resistance Transfer Framework for Defending Model Inversion At-\ntack in Split Federated Learning,\u201d in 2022 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2022, pp. 10 184\u201310 192.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.04007",
                        "Citation Paper Title": "Title:ResSFL: A Resistance Transfer Framework for Defending Model Inversion Attack in Split Federated Learning",
                        "Citation Paper Abstract": "Abstract:This work aims to tackle Model Inversion (MI) attack on Split Federated Learning (SFL). SFL is a recent distributed training scheme where multiple clients send intermediate activations (i.e., feature map), instead of raw data, to a central server. While such a scheme helps reduce the computational load at the client end, it opens itself to reconstruction of raw data from intermediate activation by the server. Existing works on protecting SFL only consider inference and do not handle attacks during training. So we propose ResSFL, a Split Federated Learning Framework that is designed to be MI-resistant during training. It is based on deriving a resistant feature extractor via attacker-aware training, and using this extractor to initialize the client-side model prior to standard SFL training. Such a method helps in reducing the computational complexity due to use of strong inversion model in client-side adversarial training as well as vulnerability of attacks launched in early training epochs. On CIFAR-100 dataset, our proposed framework successfully mitigates MI attack on a VGG-11 model with a high reconstruction Mean-Square-Error of 0.050 compared to 0.005 obtained by the baseline system. The framework achieves 67.5% accuracy (only 1% accuracy drop) with very low computation overhead. Code is released at: this https URL.",
                        "Citation Paper Authors": "Authors:Jingtao Li, Adnan Siraj Rakin, Xing Chen, Zhezhi He, Deliang Fan, Chaitali Chakrabarti"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "to reduce the correlation\nbetween smashed and input data. However, these mechanisms\nrequire efforts to mitigate the impact of noise perturbation on\nmodel accuracy ",
                    "Citation Text": "J. Wang, J. Zhang, W. Bao, X. Zhu, B. Cao, and P. S. Yu, \u201cNot\nJust Privacy: Improving Performance of Private Deep Learning in\nMobile Cloud,\u201d in Proceedings of the 24th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining , 2018, pp. 2407\u2013\n2416.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.03428",
                        "Citation Paper Title": "Title:Not Just Privacy: Improving Performance of Private Deep Learning in Mobile Cloud",
                        "Citation Paper Abstract": "Abstract:The increasing demand for on-device deep learning services calls for a highly efficient manner to deploy deep neural networks (DNNs) on mobile devices with limited capacity. The cloud-based solution is a promising approach to enabling deep learning applications on mobile devices where the large portions of a DNN are offloaded to the cloud. However, revealing data to the cloud leads to potential privacy risk. To benefit from the cloud data center without the privacy risk, we design, evaluate, and implement a cloud-based framework ARDEN which partitions the DNN across mobile devices and cloud data centers. A simple data transformation is performed on the mobile device, while the resource-hungry training and the complex inference rely on the cloud data center. To protect the sensitive information, a lightweight privacy-preserving mechanism consisting of arbitrary data nullification and random noise addition is introduced, which provides strong privacy guarantee. A rigorous privacy budget analysis is given. Nonetheless, the private perturbation to the original data inevitably has a negative impact on the performance of further inference on the cloud side. To mitigate this influence, we propose a noisy training method to enhance the cloud-side network robustness to perturbed data. Through the sophisticated design, ARDEN can not only preserve privacy but also improve the inference performance. To validate the proposed ARDEN, a series of experiments based on three image datasets and a real mobile application are conducted. The experimental results demonstrate the effectiveness of ARDEN. Finally, we implement ARDEN on a demo system to verify its practicality.",
                        "Citation Paper Authors": "Authors:Ji Wang, Jianguo Zhang, Weidong Bao, Xiaomin Zhu, Bokai Cao, Philip S. Yu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.10224v4": {
            "Paper Title": "Friendly Noise against Adversarial Noise: A Powerful Defense against\n  Data Poisoning Attacks",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": ", and from 100% to30% for the Poison Frogs attack ",
                    "Citation Text": "Ali Shafahi, W. Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor\nDumitras, and Tom Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural\nnetworks, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.00792",
                        "Citation Paper Title": "Title:Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks",
                        "Citation Paper Abstract": "Abstract:Data poisoning is an attack on machine learning models wherein the attacker adds examples to the training set to manipulate the behavior of the model at test time. This paper explores poisoning attacks on neural nets. The proposed attacks use \"clean-labels\"; they don't require the attacker to have any control over the labeling of training data. They are also targeted; they control the behavior of the classifier on a $\\textit{specific}$ test instance without degrading overall classifier performance. For example, an attacker could add a seemingly innocuous image (that is properly labeled) to a training set for a face recognition engine, and control the identity of a chosen person at test time. Because the attacker does not need to control the labeling function, poisons could be entered into the training set simply by leaving them on the web and waiting for them to be scraped by a data collection bot.\nWe present an optimization-based method for crafting poisons, and show that just one single poison image can control classifier behavior when transfer learning is used. For full end-to-end training, we present a \"watermarking\" strategy that makes poisoning reliable using multiple ($\\approx$50) poisoned training instances. We demonstrate our method by generating poisoned frog images from the CIFAR dataset and using them to manipulate image classifiers.",
                        "Citation Paper Authors": "Authors:Ali Shafahi, W. Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, Tom Goldstein"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": ". Other methods impose constraints on gradient magnitudes and directions ",
                    "Citation Text": "Sanghyun Hong, Varun Chandrasekaran, Yi \u02d8gitcan Kaya, Tudor Dumitra\u00b8 s, and Nicolas Papernot.\nOn the effectiveness of mitigating data poisoning attacks with gradient shaping. arXiv preprint\narXiv:2002.11497 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.11497",
                        "Citation Paper Title": "Title:On the Effectiveness of Mitigating Data Poisoning Attacks with Gradient Shaping",
                        "Citation Paper Abstract": "Abstract:Machine learning algorithms are vulnerable to data poisoning attacks. Prior taxonomies that focus on specific scenarios, e.g., indiscriminate or targeted, have enabled defenses for the corresponding subset of known attacks. Yet, this introduces an inevitable arms race between adversaries and defenders. In this work, we study the feasibility of an attack-agnostic defense relying on artifacts that are common to all poisoning attacks. Specifically, we focus on a common element between all attacks: they modify gradients computed to train the model. We identify two main artifacts of gradients computed in the presence of poison: (1) their $\\ell_2$ norms have significantly higher magnitudes than those of clean gradients, and (2) their orientation differs from clean gradients. Based on these observations, we propose the prerequisite for a generic poisoning defense: it must bound gradient magnitudes and minimize differences in orientation. We call this gradient shaping. As an exemplar tool to evaluate the feasibility of gradient shaping, we use differentially private stochastic gradient descent (DP-SGD), which clips and perturbs individual gradients during training to obtain privacy guarantees. We find that DP-SGD, even in configurations that do not result in meaningful privacy guarantees, increases the model's robustness to indiscriminate attacks. It also mitigates worst-case targeted attacks and increases the adversary's cost in multi-poison scenarios. The only attack we find DP-SGD to be ineffective against is a strong, yet unrealistic, indiscriminate attack. Our results suggest that, while we currently lack a generic poisoning defense, gradient shaping is a promising direction for future research.",
                        "Citation Paper Authors": "Authors:Sanghyun Hong, Varun Chandrasekaran, Yi\u011fitcan Kaya, Tudor Dumitra\u015f, Nicolas Papernot"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "used Generative Adversarial\nNetworks (GANs) to generate adversarial perturbations, and ",
                    "Citation Text": "Divyam Madaan, Jinwoo Shin, and Sung Ju Hwang. Learning to generate noise for multi-attack\nrobustness. In International Conference on Machine Learning , pages 7279\u20137289. PMLR, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.12135",
                        "Citation Paper Title": "Title:Learning to Generate Noise for Multi-Attack Robustness",
                        "Citation Paper Abstract": "Abstract:Adversarial learning has emerged as one of the successful techniques to circumvent the susceptibility of existing methods against adversarial perturbations. However, the majority of existing defense methods are tailored to defend against a single category of adversarial perturbation (e.g. $\\ell_\\infty$-attack). In safety-critical applications, this makes these methods extraneous as the attacker can adopt diverse adversaries to deceive the system. Moreover, training on multiple perturbations simultaneously significantly increases the computational overhead during training. To address these challenges, we propose a novel meta-learning framework that explicitly learns to generate noise to improve the model's robustness against multiple types of attacks. Its key component is Meta Noise Generator (MNG) that outputs optimal noise to stochastically perturb a given sample, such that it helps lower the error on diverse adversarial perturbations. By utilizing samples generated by MNG, we train a model by enforcing the label consistency across multiple perturbations. We validate the robustness of models trained by our scheme on various datasets and against a wide variety of perturbations, demonstrating that it significantly outperforms the baselines across multiple perturbations with a marginal computational cost.",
                        "Citation Paper Authors": "Authors:Divyam Madaan, Jinwoo Shin, Sung Ju Hwang"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "showed that additive augmentations of Gaussian or Speckle noise is a\nsimple yet very strong baseline for robustness against image corruptions. The application of optimized\nnoise has been mainly studied in the context of adversarial training. ",
                    "Citation Text": "Yuanhao Cai, Xiaowan Hu, Haoqian Wang, Yulun Zhang, Hanspeter Pfister, and Donglai Wei.\nLearning to generate realistic noisy images via pixel-level noise-aware adversarial training.\nAdvances in Neural Information Processing Systems , 34, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.02844",
                        "Citation Paper Title": "Title:Learning to Generate Realistic Noisy Images via Pixel-level Noise-aware Adversarial Training",
                        "Citation Paper Abstract": "Abstract:Existing deep learning real denoising methods require a large amount of noisy-clean image pairs for supervision. Nonetheless, capturing a real noisy-clean dataset is an unacceptable expensive and cumbersome procedure. To alleviate this problem, this work investigates how to generate realistic noisy images. Firstly, we formulate a simple yet reasonable noise model that treats each real noisy pixel as a random variable. This model splits the noisy image generation problem into two sub-problems: image domain alignment and noise domain alignment. Subsequently, we propose a novel framework, namely Pixel-level Noise-aware Generative Adversarial Network (PNGAN). PNGAN employs a pre-trained real denoiser to map the fake and real noisy images into a nearly noise-free solution space to perform image domain alignment. Simultaneously, PNGAN establishes a pixel-level adversarial training to conduct noise domain alignment. Additionally, for better noise fitting, we present an efficient architecture Simple Multi-scale Network (SMNet) as the generator. Qualitative validation shows that noise generated by PNGAN is highly similar to real noise in terms of intensity and distribution. Quantitative experiments demonstrate that a series of denoisers trained with the generated noisy images achieve state-of-the-art (SOTA) results on four real denoising benchmarks. Part of codes, pre-trained models, and results are available at this https URL for comparisons.",
                        "Citation Paper Authors": "Authors:Yuanhao Cai, Xiaowan Hu, Haoqian Wang, Yulun Zhang, Hanspeter Pfister, Donglai Wei"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "used Gaussian noise to defend against query-based\nblack box attacks, and ",
                    "Citation Text": "Evgenia Rusak, Lukas Schott, Roland S Zimmermann, Julian Bitterwolf, Oliver Bringmann,\nMatthias Bethge, and Wieland Brendel. A simple way to make neural networks robust against\ndiverse image corruptions. In European Conference on Computer Vision , pages 53\u201369. Springer,\n2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2001.06057",
                        "Citation Paper Title": "Title:A simple way to make neural networks robust against diverse image corruptions",
                        "Citation Paper Abstract": "Abstract:The human visual system is remarkably robust against a wide range of naturally occurring variations and corruptions like rain or snow. In contrast, the performance of modern image recognition models strongly degrades when evaluated on previously unseen corruptions. Here, we demonstrate that a simple but properly tuned training with additive Gaussian and Speckle noise generalizes surprisingly well to unseen corruptions, easily reaching the previous state of the art on the corruption benchmark ImageNet-C (with ResNet50) and on MNIST-C. We build on top of these strong baseline results and show that an adversarial training of the recognition model against uncorrelated worst-case noise distributions leads to an additional increase in performance. This regularization can be combined with previously proposed defense methods for further improvement.",
                        "Citation Paper Authors": "Authors:Evgenia Rusak, Lukas Schott, Roland S. Zimmermann, Julian Bitterwolf, Oliver Bringmann, Matthias Bethge, Wieland Brendel"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": ". Hence, the application of random and adversarial noise has been\nstudied in various domains. In particular, ",
                    "Citation Text": "Zeyu Qin, Yanbo Fan, Hongyuan Zha, and Baoyuan Wu. Random noise defense against\nquery-based black-box attacks. Advances in Neural Information Processing Systems , 34, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.11470",
                        "Citation Paper Title": "Title:Random Noise Defense Against Query-Based Black-Box Attacks",
                        "Citation Paper Abstract": "Abstract:The query-based black-box attacks have raised serious threats to machine learning models in many real applications. In this work, we study a lightweight defense method, dubbed Random Noise Defense (RND), which adds proper Gaussian noise to each query. We conduct the theoretical analysis about the effectiveness of RND against query-based black-box attacks and the corresponding adaptive attacks. Our theoretical results reveal that the defense performance of RND is determined by the magnitude ratio between the noise induced by RND and the noise added by the attackers for gradient estimation or local search. The large magnitude ratio leads to the stronger defense performance of RND, and it's also critical for mitigating adaptive attacks. Based on our analysis, we further propose to combine RND with a plausible Gaussian augmentation Fine-tuning (RND-GF). It enables RND to add larger noise to each query while maintaining the clean accuracy to obtain a better trade-off between clean accuracy and defense performance. Additionally, RND can be flexibly combined with the existing defense methods to further boost the adversarial robustness, such as adversarial training (AT). Extensive experiments on CIFAR-10 and ImageNet verify our theoretical findings and the effectiveness of RND and RND-GF.",
                        "Citation Paper Authors": "Authors:Zeyu Qin, Yanbo Fan, Hongyuan Zha, Baoyuan Wu"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": ".\nDefense Strategies. Existing defenses against data poisoning can be divided into filtering and robust\ntraining methods. Filtering methods detect outliers in feature space using thresholding ",
                    "Citation Text": "Jacob Steinhardt, Pang Wei Koh, and Percy Liang. Certified defenses for data poisoning attacks,\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03691",
                        "Citation Paper Title": "Title:Certified Defenses for Data Poisoning Attacks",
                        "Citation Paper Abstract": "Abstract:Machine learning systems trained on user-provided data are susceptible to data poisoning attacks, whereby malicious users inject false training data with the aim of corrupting the learned model. While recent work has proposed a number of attacks and defenses, little is understood about the worst-case loss of a defense in the face of a determined attacker. We address this by constructing approximate upper bounds on the loss across a broad family of attacks, for defenders that first perform outlier removal followed by empirical risk minimization. Our approximation relies on two assumptions: (1) that the dataset is large enough for statistical concentration between train and test error to hold, and (2) that outliers within the clean (non-poisoned) data do not have a strong effect on the model. Our bound comes paired with a candidate attack that often nearly matches the upper bound, giving us a powerful tool for quickly assessing defenses on a given dataset. Empirically, we find that even under a simple defense, the MNIST-1-7 and Dogfish datasets are resilient to attack, while in contrast the IMDB sentiment dataset can be driven from 12% to 23% test error by adding only 3% poisoned data.",
                        "Citation Paper Authors": "Authors:Jacob Steinhardt, Pang Wei Koh, Percy Liang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.01834v2": {
            "Paper Title": "Invariant Aggregator for Defending against Federated Backdoor Attacks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.02248v3": {
            "Paper Title": "LNGate$^2$: Secure Bidirectional IoT Micro-payments using Bitcoin's\n  Lightning Network and Threshold Cryptography",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.14348v3": {
            "Paper Title": "Synthetic Text Generation with Differential Privacy: A Simple and\n  Practical Recipe",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.02405v2": {
            "Paper Title": "Inverting Cryptographic Hash Functions via Cube-and-Conquer",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.13104v2": {
            "Paper Title": "Towards Secrecy-Aware Attacks Against Trust Prediction in Signed Social\n  Networks",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": "adopts a\nspectral clustering algorithm based on the signed Laplacian\nmatrix to construct the embeddings for each node, from which\nthe similarities are computed. Huang et al. ",
                    "Citation Text": "Z. Huang, A. Silva, and A. Singh, \u201cPole: Polarized\nembedding for signed networks,\u201d in WSDM , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.09899",
                        "Citation Paper Title": "Title:POLE: Polarized Embedding for Signed Networks",
                        "Citation Paper Abstract": "Abstract:From the 2016 U.S. presidential election to the 2021 Capitol riots to the spread of misinformation related to COVID-19, many have blamed social media for today's deeply divided society. Recent advances in machine learning for signed networks hold the promise to guide small interventions with the goal of reducing polarization in social media. However, existing models are especially ineffective in predicting conflicts (or negative links) among users. This is due to a strong correlation between link signs and the network structure, where negative links between polarized communities are too sparse to be predicted even by state-of-the-art approaches. To address this problem, we first design a partition-agnostic polarization measure for signed graphs based on the signed random-walk and show that many real-world graphs are highly polarized. Then, we propose POLE (POLarized Embedding for signed networks), a signed embedding method for polarized graphs that captures both topological and signed similarities jointly via signed autocovariance. Through extensive experiments, we show that POLE significantly outperforms state-of-the-art methods in signed link prediction, particularly for negative links with gains of up to one order of magnitude.",
                        "Citation Paper Authors": "Authors:Zexi Huang, Arlei Silva, Ambuj Singh"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "defines\ntwo important metrics: goodness and fairness to measure the\ntrustworthiness of individual nodes.3\nB. Adversarial Graph Analysis\nRecently, there is a surge of research efforts on attacking\nvarious graph analytic tasks, such as node classification [9, 15],\nlink prediction ",
                    "Citation Text": "K. Zhou, T. P. Michalak, M. Waniek, T. Rahwan, and\nY . V orobeychik, Attacking Similarity-Based Link Predic-\ntion in Social Networks . Richland, SC: International\nFoundation for Autonomous Agents and Multiagent Sys-\ntems, 2019, p. 305\u2013313.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.08368",
                        "Citation Paper Title": "Title:Attacking Similarity-Based Link Prediction in Social Networks",
                        "Citation Paper Abstract": "Abstract:Link prediction is one of the fundamental problems in computational social science. A particularly common means to predict existence of unobserved links is via structural similarity metrics, such as the number of common neighbors; node pairs with higher similarity are thus deemed more likely to be linked. However, a number of applications of link prediction, such as predicting links in gang or terrorist networks, are adversarial, with another party incentivized to minimize its effectiveness by manipulating observed information about the network. We offer a comprehensive algorithmic investigation of the problem of attacking similarity-based link prediction through link deletion, focusing on two broad classes of such approaches, one which uses only local information about target links, and another which uses global network information. While we show several variations of the general problem to be NP-Hard for both local and global metrics, we exhibit a number of well-motivated special cases which are tractable. Additionally, we provide principled and empirically effective algorithms for the intractable cases, in some cases proving worst-case approximation guarantees.",
                        "Citation Paper Authors": "Authors:Kai Zhou, Tomasz P. Michalak, Talal Rahwan, Marcin Waniek, Yevgeniy Vorobeychik"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.08260v2": {
            "Paper Title": "From Bi-Level to One-Level: A Framework for Structural Attacks to Graph\n  Anomaly Detection",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": ". It adopts GCN for fraudulent detection in\nthe online APP review system by utilizing important char-\nacteristics like similarity, special symbols, timestamps,\ndevice and login status.\n\u2022GEM ",
                    "Citation Text": "Z. Liu, C. Chen, X. Yang, J. Zhou, X. Li, and L. Song, \u201cHeterogeneous\ngraph neural networks for malicious account detection,\u201d in Proceedings\nof the 27th ACM International Conference on Information and Knowl-\nedge Management , pp. 2077\u20132085, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.12307",
                        "Citation Paper Title": "Title:Heterogeneous Graph Neural Networks for Malicious Account Detection",
                        "Citation Paper Abstract": "Abstract:We present, GEM, the first heterogeneous graph neural network approach for detecting malicious accounts at Alipay, one of the world's leading mobile cashless payment platform. Our approach, inspired from a connected subgraph approach, adaptively learns discriminative embeddings from heterogeneous account-device graphs based on two fundamental weaknesses of attackers, i.e. device aggregation and activity aggregation. For the heterogeneous graph consists of various types of nodes, we propose an attention mechanism to learn the importance of different types of nodes, while using the sum operator for modeling the aggregation patterns of nodes in each type. Experiments show that our approaches consistently perform promising results compared with competitive methods over time.",
                        "Citation Paper Authors": "Authors:Ziqi Liu, Chaochao Chen, Xinxing Yang, Jun Zhou, Xiaolong Li, Le Song"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.13815v2": {
            "Paper Title": "FocusedCleaner: Sanitizing Poisoned Graphs for Robust GNN-based Node\n  Classification",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": "constructed a kNN graph based on node attributes and\nincorporate the kNN graph into GNN to ensure attribute\nconsistency. ",
                    "Citation Text": "L. Chen, J. Li, Q. Peng, Y. Liu, Z. Zheng, and C. Yang, \u201cUn-\nderstanding structural vulnerability in graph convolutional net-\nworks,\u201d in Proceedings of the Thirtieth International Joint Conference\non Artificial Intelligence, IJCAI-21 (Z.-H. Zhou, ed.), pp. 2249\u20132255,\nInternational Joint Conferences on Artificial Intelligence Organi-\nzation, 8 2021. Main Track.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.06280",
                        "Citation Paper Title": "Title:Understanding Structural Vulnerability in Graph Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:Recent studies have shown that Graph Convolutional Networks (GCNs) are vulnerable to adversarial attacks on the graph structure. Although multiple works have been proposed to improve their robustness against such structural adversarial attacks, the reasons for the success of the attacks remain unclear. In this work, we theoretically and empirically demonstrate that structural adversarial examples can be attributed to the non-robust aggregation scheme (i.e., the weighted mean) of GCNs. Specifically, our analysis takes advantage of the breakdown point which can quantitatively measure the robustness of aggregation schemes. The key insight is that weighted mean, as the basic design of GCNs, has a low breakdown point and its output can be dramatically changed by injecting a single edge. We show that adopting the aggregation scheme with a high breakdown point (e.g., median or trimmed mean) could significantly enhance the robustness of GCNs against structural attacks. Extensive experiments on four real-world datasets demonstrate that such a simple but effective method achieves the best robustness performance compared to state-of-the-art models.",
                        "Citation Paper Authors": "Authors:Liang Chen, Jintang Li, Qibiao Peng, Yang Liu, Zibin Zheng, Carl Yang"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "(DGMM) based on carefully designed features.\nFirst, we define the soft class probability ",
                    "Citation Text": "G. E. Hinton, O. Vinyals, and J. Dean, \u201cDistilling the knowledge\nin a neural network,\u201d ArXiv , vol. abs/1503.02531, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1503.02531",
                        "Citation Paper Title": "Title:Distilling the Knowledge in a Neural Network",
                        "Citation Paper Abstract": "Abstract:A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.",
                        "Citation Paper Authors": "Authors:Geoffrey Hinton, Oriol Vinyals, Jeff Dean"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.06821v2": {
            "Paper Title": "Factoring using multiplicative relations modulo $n$: a subexponential\n  algorithm inspired by the index calculus",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.10886v3": {
            "Paper Title": "Backdoor Attack and Defense in Federated Generative Adversarial\n  Network-based Medical Image Synthesis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.14842v3": {
            "Paper Title": "Efficient Reward Poisoning Attacks on Online Deep Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.08287v3": {
            "Paper Title": "Bankrupting DoS Attackers",
            "Sentences": [
                {
                    "Sentence ID": 84,
                    "Sentence": ", interactive communication [23, 3], the Sybil attack [38, 36], and bridge\nassignment in Tor ",
                    "Citation Text": "Mahdi Zamani, Jared Saia, and Jedidiah Crandall. TorBricks: Blocking-Resistant Tor Bridge\nDistribution. In International Symposium on Stabilization, Safety, and Security of Distributed\nSystems (SSS) , pages 426\u2013440. Springer, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1608.00509",
                        "Citation Paper Title": "Title:TorBricks: Blocking-Resistant Tor Bridge Distribution",
                        "Citation Paper Abstract": "Abstract:Tor is currently the most popular network for anonymous Internet access. It critically relies on volunteer nodes called bridges for relaying Internet traffic when a user's ISP blocks connections to Tor. Unfortunately, current methods for distributing bridges are vulnerable to malicious users who obtain and block bridge addresses. In this paper, we propose TorBricks, a protocol for distributing Tor bridges to n users, even when an unknown number t < n of these users are controlled by a malicious adversary. TorBricks distributes O(tlog(n)) bridges and guarantees that all honest users can connect to Tor with high probability after O(log(t)) rounds of communication with the distributor.\nWe also extend our algorithm to perform privacy-preserving bridge distribution when run among multiple untrusted distributors. This not only prevents the distributors from learning bridge addresses and bridge assignment information, but also provides resistance against malicious attacks from a m/3 fraction of the distributors, where m is the number of distributors.",
                        "Citation Paper Authors": "Authors:Mahdi Zamani, Jared Saia, Jedidiah Crandall"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.12319v2": {
            "Paper Title": "Privacy-preserving machine learning with tensor networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.04188v2": {
            "Paper Title": "Differentially Private Stochastic Gradient Descent with Low-Noise",
            "Sentences": [
                {
                    "Sentence ID": 44,
                    "Sentence": "developed a DP-SGD algo-\nrithm with gradient perturbation which improved the gradient comple xity toO(n2) for non-smooth losses. The\nwork most related to our paper is ",
                    "Citation Text": "Puyu Wang, Yunwen Lei, Yiming Ying, and Hai Zhang. Di\ufb00erentially p rivate sgd with non-smooth losses.\nApplied and Computational Harmonic Analysis , 56:306\u2013336, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.08925",
                        "Citation Paper Title": "Title:Differentially Private SGD with Non-Smooth Losses",
                        "Citation Paper Abstract": "Abstract:In this paper, we are concerned with differentially private {stochastic gradient descent (SGD)} algorithms in the setting of stochastic convex optimization (SCO). Most of the existing work requires the loss to be Lipschitz continuous and strongly smooth, and the model parameter to be uniformly bounded. However, these assumptions are restrictive as many popular losses violate these conditions including the hinge loss for SVM, the absolute loss in robust regression, and even the least square loss in an unbounded domain. We significantly relax these restrictive assumptions and establish privacy and generalization (utility) guarantees for private SGD algorithms using output and gradient perturbations associated with non-smooth convex losses. Specifically, the loss function is relaxed to have an $\\alpha$-H\u00f6lder continuous gradient (referred to as $\\alpha$-H\u00f6lder smoothness) which instantiates the Lipschitz continuity ($\\alpha=0$) and the strong smoothness ($\\alpha=1$). We prove that noisy SGD with $\\alpha$-H\u00f6lder smooth losses using gradient perturbation can guarantee $(\\epsilon,\\delta)$-differential privacy (DP) and attain optimal excess population risk $\\mathcal{O}\\Big(\\frac{\\sqrt{d\\log(1/\\delta)}}{n\\epsilon}+\\frac{1}{\\sqrt{n}}\\Big)$, up to logarithmic terms, with the gradient complexity $ \\mathcal{O}( n^{2-\\alpha\\over 1+\\alpha}+ n).$ This shows an important trade-off between $\\alpha$-H\u00f6lder smoothness of the loss and the computational complexity for private SGD with statistically optimal performance. In particular, our results indicate that $\\alpha$-H\u00f6lder smoothness with $\\alpha\\ge {1/2}$ is sufficient to guarantee $(\\epsilon,\\delta)$-DP of noisy SGD algorithms while achieving optimal excess risk with the linear gradient complexity $\\mathcal{O}(n).$",
                        "Citation Paper Authors": "Authors:Puyu Wang, Yunwen Lei, Yiming Ying, Hai Zhang"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": ". However, their algorit hms have a large gradient complexity (mea-\nsured by the total number of computing the gradient). Speci\ufb01cally , their analysis establishes gradient complexity\nO(n1.5\u221a\u01eb+ (n\u01eb)2.5(dlog(1/\u03b4))\u22121) andO(n4.5\u221a\u01eb+ (n)6.5\u01eb4.5(dlog(1/\u03b4))\u22122) for strongly smooth and non-smooth\nlosses, respectively. ",
                    "Citation Text": "Vitaly Feldman, Tomer Koren, and Kunal Talwar. Private stocha stic convex optimization: optimal rates in\nlinear time. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theor y of Computing , pages\n439\u2013449, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.04763",
                        "Citation Paper Title": "Title:Private Stochastic Convex Optimization: Optimal Rates in Linear Time",
                        "Citation Paper Abstract": "Abstract:We study differentially private (DP) algorithms for stochastic convex optimization: the problem of minimizing the population loss given i.i.d. samples from a distribution over convex loss functions. A recent work of Bassily et al. (2019) has established the optimal bound on the excess population loss achievable given $n$ samples. Unfortunately, their algorithm achieving this bound is relatively inefficient: it requires $O(\\min\\{n^{3/2}, n^{5/2}/d\\})$ gradient computations, where $d$ is the dimension of the optimization problem.\nWe describe two new techniques for deriving DP convex optimization algorithms both achieving the optimal bound on excess loss and using $O(\\min\\{n, n^2/d\\})$ gradient computations. In particular, the algorithms match the running time of the optimal non-private algorithms. The first approach relies on the use of variable batch sizes and is analyzed using the privacy amplification by iteration technique of Feldman et al. (2018). The second approach is based on a general reduction to the problem of localizing an approximately optimal solution with differential privacy. Such localization, in turn, can be achieved using existing (non-private) uniformly stable optimization algorithms. As in the earlier work, our algorithms require a mild smoothness assumption. We also give a linear-time algorithm achieving the optimal bound on the excess loss for the strongly convex case, as well as a faster algorithm for the non-smooth case.",
                        "Citation Paper Authors": "Authors:Vitaly Feldman, Tomer Koren, Kunal Talwar"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "established the excess population risk bou nds in the order of O/parenleftbig1\u221an+1\nn\u01eb/radicalbig\ndlog(1/\u03b4)/parenrightbig\nfor (\u01eb,\u03b4)-di\ufb00erentially private stochastic convex optimization algorithms fo r both strongly smooth and non-smooth\nlosses, which match the lower bound given in ",
                    "Citation Text": "Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private emp irical risk minimization: E\ufb03cient algo-\nrithms and tight error bounds. In 2014 IEEE 55th Annual Symposium on Foundations of Computer S cience ,\npages 464\u2013473. IEEE, 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1405.7085",
                        "Citation Paper Title": "Title:Differentially Private Empirical Risk Minimization: Efficient Algorithms and Tight Error Bounds",
                        "Citation Paper Abstract": "Abstract:In this paper, we initiate a systematic investigation of differentially private algorithms for convex empirical risk minimization. Various instantiations of this problem have been studied before. We provide new algorithms and matching lower bounds for private ERM assuming only that each data point's contribution to the loss function is Lipschitz bounded and that the domain of optimization is bounded. We provide a separate set of algorithms and matching lower bounds for the setting in which the loss functions are known to also be strongly convex.\nOur algorithms run in polynomial time, and in some cases even match the optimal non-private running time (as measured by oracle complexity). We give separate algorithms (and lower bounds) for $(\\epsilon,0)$- and $(\\epsilon,\\delta)$-differential privacy; perhaps surprisingly, the techniques used for designing optimal algorithms in the two cases are completely different.\nOur lower bounds apply even to very simple, smooth function families, such as linear and quadratic functions. This implies that algorithms from previous work can be used to obtain optimal error rates, under the additional assumption that the contributions of each data point to the loss function is smooth. We show that simple approaches to smoothing arbitrary loss functions (in order to apply previous techniques) do not yield optimal error rates. In particular, optimal algorithms were not previously known for problems such as training support vector machines and the high-dimensional median.",
                        "Citation Paper Authors": "Authors:Raef Bassily, Adam Smith, Abhradeep Thakurta"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.00241v4": {
            "Paper Title": "Adversarial Policies Beat Superhuman Go AIs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.07791v6": {
            "Paper Title": "A Robust Dynamic Average Consensus Algorithm that Ensures both\n  Differential Privacy and Accurate Convergence",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": ", which\nhas recently been applied to distributed optimization (see ",
                    "Citation Text": "Z. Huang, S. Mitra, and N. Vaidya, \u201cDifferentially priv ate distributed\noptimization,\u201d in Proceedings of the 2015 International Conference on\nDistributed Computing and Networking , New York, NY , USA, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1401.2596",
                        "Citation Paper Title": "Title:Differentially Private Distributed Optimization",
                        "Citation Paper Abstract": "Abstract:In distributed optimization and iterative consensus literature, a standard problem is for $N$ agents to minimize a function $f$ over a subset of Euclidean space, where the cost function is expressed as a sum $\\sum f_i$. In this paper, we study the private distributed optimization (PDOP) problem with the additional requirement that the cost function of the individual agents should remain differentially private. The adversary attempts to infer information about the private cost functions from the messages that the agents exchange. Achieving differential privacy requires that any change of an individual's cost function only results in unsubstantial changes in the statistics of the messages. We propose a class of iterative algorithms for solving PDOP, which achieves differential privacy and convergence to the optimal value. Our analysis reveals the dependence of the achieved accuracy and the privacy levels on the the parameters of the algorithm. We observe that to achieve $\\epsilon$-differential privacy the accuracy of the algorithm has the order of $O(\\frac{1}{\\epsilon^2})$.",
                        "Citation Paper Authors": "Authors:Zhenqi Huang, Sayan Mitra, Nitin Vaidya"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": ", plenty of results have emer ged\non differentially-private average consensus (see, e.g. [2 8], ",
                    "Citation Text": "Z. Huang, S. Mitra, and G. Dullerud, \u201cDifferentially pr ivate iterative\nsynchronous consensus,\u201d in Proceedings of the 2012 ACM workshop on\nPrivacy in the Electronic Society . ACM, 2012, pp. 81\u201390.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1207.4262",
                        "Citation Paper Title": "Title:Differentially Private Iterative Synchronous Consensus",
                        "Citation Paper Abstract": "Abstract:The iterative consensus problem requires a set of processes or agents with different initial values, to interact and update their states to eventually converge to a common value. Protocols solving iterative consensus serve as building blocks in a variety of systems where distributed coordination is required for load balancing, data aggregation, sensor fusion, filtering, clock synchronization and platooning of autonomous vehicles. In this paper, we introduce the private iterative consensus problem where agents are required to converge while protecting the privacy of their initial values from honest but curious adversaries. Protecting the initial states, in many applications, suffice to protect all subsequent states of the individual participants.\nFirst, we adapt the notion of differential privacy in this setting of iterative computation. Next, we present a server-based and a completely distributed randomized mechanism for solving private iterative consensus with adversaries who can observe the messages as well as the internal states of the server and a subset of the clients. Finally, we establish the tradeoff between privacy and the accuracy of the proposed randomized mechanism.",
                        "Citation Paper Authors": "Authors:Zhenqi Huang, Sayan Mitra, Geir Dullerud"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.01753v2": {
            "Paper Title": "Looking Beyond IoCs: Automatically Extracting Attack Patterns from\n  External CTI",
            "Sentences": [
                {
                    "Sentence ID": 54,
                    "Sentence": "is a cybersecurity KG populated from\nmultiple heterogeneous cybersecurity data sources and frequently\nupdated. An Extraction, Transformation, and Loading (ETL) peri-\nodically checks and updates the KG as new security information\nbecomes available. EXTRACTOR ",
                    "Citation Text": "Kiavash Satvat, Rigel Gjomemo, and VN Venkatakrishnan. 2021. EXTRACTOR:\nExtracting attack behavior from threat reports. In 2021 IEEE European Symposium\non Security and Privacy (EuroS&P) . IEEE, 598\u2013615.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.08618",
                        "Citation Paper Title": "Title:EXTRACTOR: Extracting Attack Behavior from Threat Reports",
                        "Citation Paper Abstract": "Abstract:The knowledge on attacks contained in Cyber Threat Intelligence (CTI) reports is very important to effectively identify and quickly respond to cyber threats. However, this knowledge is often embedded in large amounts of text, and therefore difficult to use effectively. To address this challenge, we propose a novel approach and tool called EXTRACTOR that allows precise automatic extraction of concise attack behaviors from CTI reports. EXTRACTOR makes no strong assumptions about the text and is capable of extracting attack behaviors as provenance graphs from unstructured text. We evaluate EXTRACTOR using real-world incident reports from various sources as well as reports of DARPA adversarial engagements that involve several attack campaigns on various OS platforms of Windows, Linux, and FreeBSD. Our evaluation results show that EXTRACTOR can extract concise provenance graphs from CTI reports and show that these graphs can successfully be used by cyber-analytics tools in threat-hunting.",
                        "Citation Paper Authors": "Authors:Kiavash Satvat, Rigel Gjomemo, V.N. Venkatakrishnan"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": "used semi-supervised learning with a bootstrapping\nalgorithm for extracting the relation between security entities. They\nachieved an average F1-score of 82% on a dataset containing eight\ndifferent relation types. The work in ",
                    "Citation Text": "Aditya Pingle, Aritran Piplai, Sudip Mittal, Anupam Joshi, James Holt, and Richard\nZak. 2019. RelExt: Relation Extraction using Deep Learning approaches for Cy-\nbersecurity Knowledge Graph Improvement. In Proceedings of the 2019 IEEE/ACM\nInternational Conference on Advances in Social Networks Analysis and Mining .\n879\u2013886.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.02497",
                        "Citation Paper Title": "Title:RelExt: Relation Extraction using Deep Learning approaches for Cybersecurity Knowledge Graph Improvement",
                        "Citation Paper Abstract": "Abstract:Security Analysts that work in a `Security Operations Center' (SoC) play a major role in ensuring the security of the organization. The amount of background knowledge they have about the evolving and new attacks makes a significant difference in their ability to detect attacks. Open source threat intelligence sources, like text descriptions about cyber-attacks, can be stored in a structured fashion in a cybersecurity knowledge graph. A cybersecurity knowledge graph can be paramount in aiding a security analyst to detect cyber threats because it stores a vast range of cyber threat information in the form of semantic triples which can be queried. A semantic triple contains two cybersecurity entities with a relationship between them. In this work, we propose a system to create semantic triples over cybersecurity text, using deep learning approaches to extract possible relationships. We use the set of semantic triples generated through our system to assert in a cybersecurity knowledge graph. Security Analysts can retrieve this data from the knowledge graph, and use this information to form a decision about a cyber-attack.",
                        "Citation Paper Authors": "Authors:Aditya Pingle, Aritran Piplai, Sudip Mittal, Anupam Joshi, James Holt, Richard Zak"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "compares different neural networks, including CNN, LSTM,\nBERT, and CRF, for cybersecurity NER.\nRelation Extraction: To the best of our knowledge, there is\nno open-source dataset for cybersecurity relation extraction. Early\nwork in ",
                    "Citation Text": "Corinne L Jones, Robert A Bridges, Kelly MT Huffer, and John R Goodall. 2015.\nTowards a relation extraction framework for cyber-security concepts. In Pro-\nceedings of the 10th Annual Cyber and Information Security Research Conference .\n1\u20134.RAID \u201923, October 16\u201318, 2023, Hong Kong, Hong Kong Md Tanvirul Alam, Dipkamal Bhusal, Youngja Park, and Nidhi Rastogi",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1504.04317",
                        "Citation Paper Title": "Title:Towards a relation extraction framework for cyber-security concepts",
                        "Citation Paper Abstract": "Abstract:In order to assist security analysts in obtaining information pertaining to their network, such as novel vulnerabilities, exploits, or patches, information retrieval methods tailored to the security domain are needed. As labeled text data is scarce and expensive, we follow developments in semi-supervised Natural Language Processing and implement a bootstrapping algorithm for extracting security entities and their relationships from text. The algorithm requires little input data, specifically, a few relations or patterns (heuristics for identifying relations), and incorporates an active learning component which queries the user on the most important decisions to prevent drifting from the desired relations. Preliminary testing on a small corpus shows promising results, obtaining precision of .82.",
                        "Citation Paper Authors": "Authors:Corinne L. Jones, Robert A. Bridges, Kelly Huffer, John Goodall"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.10258v2": {
            "Paper Title": "In and Out-of-Domain Text Adversarial Robustness via Label Smoothing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.11030v4": {
            "Paper Title": "Adversarial Cheap Talk",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.03781v3": {
            "Paper Title": "IoT-REX: A Secure Remote-Control System for IoT Devices from Centralized\n  Multi-Designated Verifier Signatures",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.06154v2": {
            "Paper Title": "On the Robustness of Bayesian Neural Networks to Adversarial Attacks",
            "Sentences": [
                {
                    "Sentence ID": 69,
                    "Sentence": ". However,\napproximate Bayesian inference techniques for deep learning is\nan active area of research, including recent developments, e.g.,\nSWAG ",
                    "Citation Text": "W. J. Maddox, P. Izmailov, T. Garipov, D. P. Vetrov, and\nA. G. Wilson, \u201cA simple baseline for bayesian uncer-\ntainty in deep learning,\u201d Advances in neural information\nprocessing systems , vol. 32, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.02476",
                        "Citation Paper Title": "Title:A Simple Baseline for Bayesian Uncertainty in Deep Learning",
                        "Citation Paper Abstract": "Abstract:We propose SWA-Gaussian (SWAG), a simple, scalable, and general purpose approach for uncertainty representation and calibration in deep learning. Stochastic Weight Averaging (SWA), which computes the first moment of stochastic gradient descent (SGD) iterates with a modified learning rate schedule, has recently been shown to improve generalization in deep learning. With SWAG, we fit a Gaussian using the SWA solution as the first moment and a low rank plus diagonal covariance also derived from the SGD iterates, forming an approximate posterior distribution over neural network weights; we then sample from this Gaussian distribution to perform Bayesian model averaging. We empirically find that SWAG approximates the shape of the true posterior, in accordance with results describing the stationary distribution of SGD iterates. Moreover, we demonstrate that SWAG performs well on a wide variety of tasks, including out of sample detection, calibration, and transfer learning, in comparison to many popular alternatives including MC dropout, KFAC Laplace, SGLD, and temperature scaling.",
                        "Citation Paper Authors": "Authors:Wesley Maddox, Timur Garipov, Pavel Izmailov, Dmitry Vetrov, Andrew Gordon Wilson"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": ",\nwhile approximate samples can be obtained more cheaply via\nVariational Inference (VI) ",
                    "Citation Text": "C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wier-\nstra, \u201cWeight uncertainty in neural networks,\u201d arXiv\npreprint arXiv:1505.05424 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1505.05424",
                        "Citation Paper Title": "Title:Weight Uncertainty in Neural Networks",
                        "Citation Paper Abstract": "Abstract:We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.",
                        "Citation Paper Authors": "Authors:Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": ". One such attacks is the Fast Gradient Sign Method (FGSM) ",
                    "Citation Text": "I. J. Goodfellow, J. Shlens, and C. Szegedy, \u201cExplaining\nand harnessing adversarial examples,\u201d arXiv preprint\narXiv:1412.6572 , 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.6572",
                        "Citation Paper Title": "Title:Explaining and Harnessing Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": "that adversarial perturbations often arise in directions normal\nto the data manifold. The suggestion that lower-dimensional\ndata structures might be ubiquitous in NN problems is also cor-\nroborated by recent results ",
                    "Citation Text": "S. Goldt, M. M\u00e9zard, F. Krzakala, and L. Zdeborov\u00e1,\n\u201cModelling the influence of data structure on learning in\nneural networks,\u201d arXiv preprint arXiv:1909.11500 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.11500",
                        "Citation Paper Title": "Title:Modelling the influence of data structure on learning in neural networks: the hidden manifold model",
                        "Citation Paper Abstract": "Abstract:Understanding the reasons for the success of deep neural networks trained using stochastic gradient-based methods is a key open problem for the nascent theory of deep learning. The types of data where these networks are most successful, such as images or sequences of speech, are characterised by intricate correlations. Yet, most theoretical work on neural networks does not explicitly model training data, or assumes that elements of each data sample are drawn independently from some factorised probability distribution. These approaches are thus by construction blind to the correlation structure of real-world data sets and their impact on learning in neural networks. Here, we introduce a generative model for structured data sets that we call the hidden manifold model (HMM). The idea is to construct high-dimensional inputs that lie on a lower-dimensional manifold, with labels that depend only on their position within this manifold, akin to a single layer decoder or generator in a generative adversarial network. We demonstrate that learning of the hidden manifold model is amenable to an analytical treatment by proving a \"Gaussian Equivalence Property\" (GEP), and we use the GEP to show how the dynamics of two-layer neural networks trained using one-pass stochastic gradient descent is captured by a set of integro-differential equations that track the performance of the network at all times. This permits us to analyse in detail how a neural network learns functions of increasing complexity during training, how its performance depends on its size and how it is impacted by parameters such as the learning rate or the dimension of the hidden manifold.",
                        "Citation Paper Authors": "Authors:Sebastian Goldt, Marc M\u00e9zard, Florent Krzakala, Lenka Zdeborov\u00e1"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ". Adversarial\nexamples have been found to be so widespread in state-of-the-\nart deterministic NNs that they have even been hypothesized to\n1The code for the experiments can be found at https://github.com/ginevracoal/\nrobustBNNs.be an intrinsic property of certain models ",
                    "Citation Text": "A. Ilyas, S. Santurkar, D. Tsipras, L. Engstrom, B. Tran,\nand A. Madry, \u201cAdversarial examples are not bugs, they\nare features,\u201d Advances in neural information processing\nsystems , vol. 32, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.02175",
                        "Citation Paper Title": "Title:Adversarial Examples Are Not Bugs, They Are Features",
                        "Citation Paper Abstract": "Abstract:Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data.",
                        "Citation Paper Authors": "Authors:Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, Aleksander Madry"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.06951v5": {
            "Paper Title": "AI Ethics on Blockchain: Topic Analysis on Twitter Data for Blockchain\n  Security",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.05632v5": {
            "Paper Title": "Blockchain Network Analysis: A Comparative Study of Decentralized Banks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.06573v2": {
            "Paper Title": "On the Evolution of (Hateful) Memes by Means of Multimodal Contrastive\n  Learning",
            "Sentences": [
                {
                    "Sentence ID": 33,
                    "Sentence": ", they investigate the predictability of suc-\ncessful memes using historical patterns. Dubey et al. ",
                    "Citation Text": "Abhimanyu Dubey, Esteban Moro, Manuel Cebri\u00e1n, and Iyad\nRahwan. MemeSequencer: Sparse Matching for Embedding\nImage Macros. In The Web Conference (WWW) , pages 1225\u2013\n1235. ACM, 2018. 12, 13",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.04936",
                        "Citation Paper Title": "Title:MemeSequencer: Sparse Matching for Embedding Image Macros",
                        "Citation Paper Abstract": "Abstract:The analysis of the creation, mutation, and propagation of social media content on the Internet is an essential problem in computational social science, affecting areas ranging from marketing to political mobilization. A first step towards understanding the evolution of images online is the analysis of rapidly modifying and propagating memetic imagery or `memes'. However, a pitfall in proceeding with such an investigation is the current incapability to produce a robust semantic space for such imagery, capable of understanding differences in Image Macros. In this study, we provide a first step in the systematic study of image evolution on the Internet, by proposing an algorithm based on sparse representations and deep learning to decouple various types of content in such images and produce a rich semantic embedding. We demonstrate the benefits of our approach on a variety of tasks pertaining to memes and Image Macros, such as image clustering, image retrieval, topic prediction and virality prediction, surpassing the existing methods on each. In addition to its utility on quantitative tasks, our method opens up the possibility of obtaining the first large-scale understanding of the evolution and propagation of memetic imagery.",
                        "Citation Paper Authors": "Authors:Abhimanyu Dubey, Esteban Moro, Manuel Cebrian, Iyad Rahwan"
                    }
                },
                {
                    "Sentence ID": 58,
                    "Sentence": "pro-\nvide a quantitative approach to studying online antisemitism.\nThey study the antisemitic language by studying the dynamic\ndistances between word embeddings over time. Fatemeh et\nal. ",
                    "Citation Text": "Fatemeh Tahmasbi, Leonard Schild, Chen Ling, Jeremy\nBlackburn, Gianluca Stringhini, Yang Zhang, and Savvas Zan-\nnettou. \u201cGo eat a bat, Chang!\u201d: On the Emergence of Sino-\nphobic Behavior on Web Communities in the Face of COVID-\n19. In The Web Conference (WWW) , pages 1122\u20131133. ACM,\n2021. 12",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.04046",
                        "Citation Paper Title": "Title:\"Go eat a bat, Chang!\": On the Emergence of Sinophobic Behavior on Web Communities in the Face of COVID-19",
                        "Citation Paper Abstract": "Abstract:The outbreak of the COVID-19 pandemic has changed our lives in unprecedented ways. In the face of the projected catastrophic consequences, many countries have enacted social distancing measures in an attempt to limit the spread of the virus. Under these conditions, the Web has become an indispensable medium for information acquisition, communication, and entertainment. At the same time, unfortunately, the Web is being exploited for the dissemination of potentially harmful and disturbing content, such as the spread of conspiracy theories and hateful speech towards specific ethnic groups, in particular towards Chinese people since COVID-19 is believed to have originated from China. In this paper, we make a first attempt to study the emergence of Sinophobic behavior on the Web during the outbreak of the COVID-19 pandemic. We collect two large-scale datasets from Twitter and 4chan's Politically Incorrect board (/pol/) over a time period of approximately five months and analyze them to investigate whether there is a rise or important differences with regard to the dissemination of Sinophobic content. We find that COVID-19 indeed drives the rise of Sinophobia on the Web and that the dissemination of Sinophobic content is a cross-platform phenomenon: it exists on fringe Web communities like \\dspol, and to a lesser extent on mainstream ones like Twitter. Also, using word embeddings over time, we characterize the evolution and emergence of new Sinophobic slurs on both Twitter and /pol/. Finally, we find interesting differences in the context in which words related to Chinese people are used on the Web before and after the COVID-19 outbreak: on Twitter we observe a shift towards blaming China for the situation, while on /pol/ we find a shift towards using more (and new) Sinophobic slurs.",
                        "Citation Paper Authors": "Authors:Fatemeh Tahmasbi, Leonard Schild, Chen Ling, Jeremy Blackburn, Gianluca Stringhini, Yang Zhang, Savvas Zannettou"
                    }
                },
                {
                    "Sentence ID": 62,
                    "Sentence": ". Many re-\nsearch efforts focus on hate speech detection. Zahrah et\nal. ",
                    "Citation Text": "Fatima Zahrah, Jason R. C. Nurse, and Michael Goldsmith.\nA Comparison of Online Hate on Reddit and 4chan: A Case\nStudy of the 2020 US Election. In ACM Symposium on Ap-\nplied Computing (SAC) , pages 1797\u20131800. ACM, 2022. 12",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.01302",
                        "Citation Paper Title": "Title:A Comparison of Online Hate on Reddit and 4chan: A Case Study of the 2020 US Election",
                        "Citation Paper Abstract": "Abstract:The rapid integration of the Internet into our daily lives has led to many benefits but also to a number of new, wide-spread threats such as online hate, trolling, bullying, and generally aggressive behaviours. While research has traditionally explored online hate, in particular, on one platform, the reality is that such hate is a phenomenon that often makes use of multiple online networks. In this article, we seek to advance the discussion into online hate by harnessing a comparative approach, where we make use of various Natural Language Processing (NLP) techniques to computationally analyse hateful content from Reddit and 4chan relating to the 2020 US Presidential Elections. Our findings show how content and posting activity can differ depending on the platform being used. Through this, we provide initial comparison into the platform-specific behaviours of online hate, and how different platforms can serve specific purposes. We further provide several avenues for future research utilising a cross-platform approach so as to gain a more comprehensive understanding of the global hate ecosystem.",
                        "Citation Paper Authors": "Authors:Fatima Zahrah, Jason R. C. Nurse, Michael Goldsmith"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.01943v2": {
            "Paper Title": "When and How to Fool Explainable Models (and Humans) with Adversarial\n  Examples",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.01785v2": {
            "Paper Title": "TabLeak: Tabular Data Leakage in Federated Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.06454v2": {
            "Paper Title": "AGChain: A Blockchain-based Gateway for Trustworthy App Delegation from\n  Mobile App Markets",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": "leveraged control- and data-flow analysis of smart contracts\u2019\nbytecode to detect the gas-related vulnerabilities, including unbounded mass\noperations, non-isolated external calls, and integer overflows. Additionally,\nGASOL ",
                    "Citation Text": "E. Albert, J. Correas, P. Gordillo, G. Rom\u00b4 an-D\u00b4 \u0131ez, A. Rubio, GASOL:\nGas Analysis and Optimization for Ethereum Smart Contracts, in: Proc.\nSpringer TACAS, 2020.\n30",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.11929",
                        "Citation Paper Title": "Title:GASOL: Gas Analysis and Optimization for Ethereum Smart Contracts",
                        "Citation Paper Abstract": "Abstract:We present the main concepts, components, and usage of GASOL, a Gas AnalysiS and Optimization tooL for Ethereum smart contracts. GASOL offers a wide variety of cost models that allow inferring the gas consumption associated to selected types of EVM instructions and/or inferring the number of times that such types of bytecode instructions are executed. Among others, we have cost models to measure only storage opcodes, to measure a selected family of gas-consumption opcodes following the Ethereum's classification, to estimate the cost of a selected program line, etc. After choosing the desired cost model and the function of interest, GASOL returns to the user an upper bound of the cost for this function. As the gas consumption is often dominated by the instructions that access the storage, GASOL uses the gas analysis to detect under-optimized storage patterns, and includes an (optional) automatic optimization of the selected function. Our tool can be used within an Eclipse plugin for Solidity which displays the gas and instructions bounds and, when applicable, the gas-optimized Solidity function.",
                        "Citation Paper Authors": "Authors:Elvira Albert, Jes\u00fas Correas, Pablo Gordillo, Guillermo Rom\u00e1n-D\u00edez, Albert Rubio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.04750v2": {
            "Paper Title": "Errorless Robust JPEG Steganography using Outputs of JPEG Coders",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.13235v2": {
            "Paper Title": "Chaos Theory and Adversarial Robustness",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.09657v2": {
            "Paper Title": "Grafting Laplace and Gaussian distributions: A new noise mechanism for\n  differential privacy",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "have suggested the u se\nof Subbotin noise for differential privacy.\nIn ",
                    "Citation Text": "C. L. Canonne, G. Kamath, and T. Steinke, \u201cThe discrete G aussian for\ndifferential privacy,\u201d in Proc. Adv. Neural Inf. Process. Syst. , vol. 33.\nPMLR, 2020, pp. 15 676\u201315 688.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.00010",
                        "Citation Paper Title": "Title:The Discrete Gaussian for Differential Privacy",
                        "Citation Paper Abstract": "Abstract:A key tool for building differentially private systems is adding Gaussian noise to the output of a function evaluated on a sensitive dataset. Unfortunately, using a continuous distribution presents several practical challenges. First and foremost, finite computers cannot exactly represent samples from continuous distributions, and previous work has demonstrated that seemingly innocuous numerical errors can entirely destroy privacy. Moreover, when the underlying data is itself discrete (e.g., population counts), adding continuous noise makes the result less interpretable.\nWith these shortcomings in mind, we introduce and analyze the discrete Gaussian in the context of differential privacy. Specifically, we theoretically and experimentally show that adding discrete Gaussian noise provides essentially the same privacy and accuracy guarantees as the addition of continuous Gaussian noise. We also present an simple and efficient algorithm for exact sampling from this distribution. This demonstrates its applicability for privately answering counting queries, or more generally, low-sensitivity integer-valued queries.",
                        "Citation Paper Authors": "Authors:Cl\u00e9ment L. Canonne, Gautam Kamath, Thomas Steinke"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.11076v3": {
            "Paper Title": "Collaborative Learning for Cyberattack Detection in Blockchain Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.16987v2": {
            "Paper Title": "Secure Software Development Methodologies: A Multivocal Literature\n  Review",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.03151v2": {
            "Paper Title": "Privacy Amplification via Shuffled Check-Ins",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.15875v2": {
            "Paper Title": "Data Poisoning Attack Aiming the Vulnerability of Continual Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.11660v3": {
            "Paper Title": "A Black-box NLP Classifier Attacker",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.03216v2": {
            "Paper Title": "Unlearning Graph Classifiers with Limited Data Resources",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": "proposed to hide\nthe real gradient residual by adding a linear noise term b\ud835\udc47wto the\ntraining loss, a technique known as loss perturbation ",
                    "Citation Text": "Kamalika Chaudhuri, Claire Monteleoni, and Anand D Sarwate. 2011. Differen-\ntially private empirical risk minimization. Journal of Machine Learning Research\n12, 3 (2011).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:0912.0071",
                        "Citation Paper Title": "Title:Differentially Private Empirical Risk Minimization",
                        "Citation Paper Abstract": "Abstract:Privacy-preserving machine learning algorithms are crucial for the increasingly common setting in which personal data, such as medical or financial records, are analyzed. We provide general techniques to produce privacy-preserving approximations of classifiers learned via (regularized) empirical risk minimization (ERM). These algorithms are private under the $\\epsilon$-differential privacy definition due to Dwork et al. (2006). First we apply the output perturbation ideas of Dwork et al. (2006), to ERM classification. Then we propose a new method, objective perturbation, for privacy-preserving machine learning algorithm design. This method entails perturbing the objective function before optimizing over classifiers. If the loss and regularizer satisfy certain convexity and differentiability criteria, we prove theoretical results showing that our algorithms preserve privacy, and provide generalization bounds for linear and nonlinear kernels. We further present a privacy-preserving technique for tuning the parameters in general machine learning algorithms, thereby providing end-to-end privacy guarantees for the training process. We apply these results to produce privacy-preserving analogues of regularized logistic regression and support vector machines. We obtain encouraging results from evaluating their performance on real demographic and benchmark data sets. Our results show that both theoretically and empirically, objective perturbation is superior to the previous state-of-the-art, output perturbation, in managing the inherent tradeoff between privacy and learning performance.",
                        "Citation Paper Authors": "Authors:Kamalika Chaudhuri, Claire Monteleoni, Anand D. Sarwate"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": "due to the adoption of nonlinearities.\nFew-shot learning. Few-shot learning ",
                    "Citation Text": "Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. 2020. Generalizing\nfrom a few examples: A survey on few-shot learning. ACM computing surveys\n(csur) 53, 3 (2020), 1\u201334.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.05046",
                        "Citation Paper Title": "Title:Generalizing from a Few Examples: A Survey on Few-Shot Learning",
                        "Citation Paper Abstract": "Abstract:Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-Shot Learning (FSL) is proposed to tackle this problem. Using prior knowledge, FSL can rapidly generalize to new tasks containing only a few samples with supervised information. In this paper, we conduct a thorough survey to fully understand FSL. Starting from a formal definition of FSL, we distinguish FSL from several relevant machine learning problems. We then point out that the core issue in FSL is that the empirical risk minimized is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize FSL methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the FSL problem setups, techniques, applications and theories, are also proposed to provide insights for future research.",
                        "Citation Paper Authors": "Authors:Yaqing Wang, Quanming Yao, James Kwok, Lionel M. Ni"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": ". Sim-\nilar observations regarding the relationship between DP and ap-\nproximate unlearning were made in the context of unstructured\nunlearning [ 28,50]. Only one recent line of work considered dif-\nferential privacy for graph classification ",
                    "Citation Text": "Tamara T Mueller, Johannes C Paetzold, Chinmay Prabhakar, Dmitrii Usynin,\nDaniel Rueckert, and Georgios Kaissis. 2022. Differentially Private Graph Classi-\nfication with GNNs. arXiv preprint arXiv:2202.02575 (2022).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.02575",
                        "Citation Paper Title": "Title:Differentially Private Graph Classification with GNNs",
                        "Citation Paper Abstract": "Abstract:Graph Neural Networks (GNNs) have established themselves as the state-of-the-art models for many machine learning applications such as the analysis of social networks, protein interactions and molecules. Several among these datasets contain privacy-sensitive data. Machine learning with differential privacy is a promising technique to allow deriving insight from sensitive data while offering formal guarantees of privacy protection. However, the differentially private training of GNNs has so far remained under-explored due to the challenges presented by the intrinsic structural connectivity of graphs. In this work, we introduce differential privacy for graph-level classification, one of the key applications of machine learning on graphs. Our method is applicable to deep learning on multi-graph datasets and relies on differentially private stochastic gradient descent (DP-SGD). We show results on a variety of synthetic and public datasets and evaluate the impact of different GNN architectures and training hyperparameters on model performance for differentially private graph classification. Finally, we apply explainability techniques to assess whether similar representations are learned in the private and non-private settings and establish robust baselines for future work in this area.",
                        "Citation Paper Authors": "Authors:Tamara T. Mueller, Johannes C. Paetzold, Chinmay Prabhakar, Dmitrii Usynin, Daniel Rueckert, Georgios Kaissis"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "performs a theoretical study of gener-\nalizations of unlearning and ",
                    "Citation Text": "Aditya Golatkar, Alessandro Achille, and Stefano Soatto. 2020. Eternal sunshine\nof the spotless net: Selective forgetting in deep networks. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition . 9304\u20139312.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.04933",
                        "Citation Paper Title": "Title:Eternal Sunshine of the Spotless Net: Selective Forgetting in Deep Networks",
                        "Citation Paper Abstract": "Abstract:We explore the problem of selectively forgetting a particular subset of the data used for training a deep neural network. While the effects of the data to be forgotten can be hidden from the output of the network, insights may still be gleaned by probing deep into its weights. We propose a method for \"scrubbing'\" the weights clean of information about a particular set of training data. The method does not require retraining from scratch, nor access to the data originally used for training. Instead, the weights are modified so that any probing function of the weights is indistinguishable from the same function applied to the weights of a network trained without the data to be forgotten. This condition is a generalized and weaker form of Differential Privacy. Exploiting ideas related to the stability of stochastic gradient descent, we introduce an upper-bound on the amount of information remaining in the weights, which can be estimated efficiently even for deep neural networks.",
                        "Citation Paper Authors": "Authors:Aditya Golatkar, Alessandro Achille, Stefano Soatto"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.16424v2": {
            "Paper Title": "Machine Unlearning of Federated Clusters",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.03008v2": {
            "Paper Title": "Algorithms for bounding contribution for histogram estimation under\n  user-level privacy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.12044v4": {
            "Paper Title": "Backdoor Cleansing with Unlabeled Data",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": ". These triggers\nare not stealthy since they can be perceived by human eyes.\nMore complex triggers are developed such as a sinusoidal\nstrip ",
                    "Citation Text": "Mauro Barni, Kassem Kallas, and Benedetta Tondi. A new\nbackdoor attack in cnns by training set corruption without\nlabel poisoning. In 2019 IEEE International Conference on\nImage Processing (ICIP) , pages 101\u2013105. IEEE, 2019. 2, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.11237",
                        "Citation Paper Title": "Title:A new Backdoor Attack in CNNs by training set corruption without label poisoning",
                        "Citation Paper Abstract": "Abstract:Backdoor attacks against CNNs represent a new threat against deep learning systems, due to the possibility of corrupting the training set so to induce an incorrect behaviour at test time. To avoid that the trainer recognises the presence of the corrupted samples, the corruption of the training set must be as stealthy as possible. Previous works have focused on the stealthiness of the perturbation injected into the training samples, however they all assume that the labels of the corrupted samples are also poisoned. This greatly reduces the stealthiness of the attack, since samples whose content does not agree with the label can be identified by visual inspection of the training set or by running a pre-classification step. In this paper we present a new backdoor attack without label poisoning Since the attack works by corrupting only samples of the target class, it has the additional advantage that it does not need to identify beforehand the class of the samples to be attacked at test time. Results obtained on the MNIST digits recognition task and the traffic signs classification task show that backdoor attacks without label poisoning are indeed possible, thus raising a new alarm regarding the use of deep learning in security-critical applications.",
                        "Citation Paper Authors": "Authors:Mauro Barni, Kassem Kallas, Benedetta Tondi"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": ". Tiny-ImageNet++ contains 20,000 images dis-\ntributed evenly in 1000 classes. Its image resolution is the\nsame as Tiny-ImageNet. ResNet-18 ",
                    "Citation Text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition , pages 770\u2013778, 2016. 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.03385",
                        "Citation Paper Title": "Title:Deep Residual Learning for Image Recognition",
                        "Citation Paper Abstract": "Abstract:Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
                        "Citation Paper Authors": "Authors:Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "decompose end-to-end training process\ninto three stages including self-supervised feature learning,\nclassifier learning and finetuning whole classifier with fil-\ntered samples. Based on the characteristics of backdoor\nmodel training, Li et al. ",
                    "Citation Text": "Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li,\nand Xingjun Ma. Anti-backdoor learning: Training clean\nmodels on poisoned data. Advances in Neural Information\nProcessing Systems , 34:14900\u201314912, 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.11571",
                        "Citation Paper Title": "Title:Anti-Backdoor Learning: Training Clean Models on Poisoned Data",
                        "Citation Paper Abstract": "Abstract:Backdoor attack has emerged as a major security threat to deep neural networks (DNNs). While existing defense methods have demonstrated promising results on detecting or erasing backdoors, it is still not clear whether robust training methods can be devised to prevent the backdoor triggers being injected into the trained model in the first place. In this paper, we introduce the concept of \\emph{anti-backdoor learning}, aiming to train \\emph{clean} models given backdoor-poisoned data. We frame the overall learning process as a dual-task of learning the \\emph{clean} and the \\emph{backdoor} portions of data. From this view, we identify two inherent characteristics of backdoor attacks as their weaknesses: 1) the models learn backdoored data much faster than learning with clean data, and the stronger the attack the faster the model converges on backdoored data; 2) the backdoor task is tied to a specific class (the backdoor target class). Based on these two weaknesses, we propose a general learning scheme, Anti-Backdoor Learning (ABL), to automatically prevent backdoor attacks during training. ABL introduces a two-stage \\emph{gradient ascent} mechanism for standard training to 1) help isolate backdoor examples at an early training stage, and 2) break the correlation between backdoor examples and the target class at a later training stage. Through extensive experiments on multiple benchmark datasets against 10 state-of-the-art attacks, we empirically show that ABL-trained models on backdoor-poisoned data achieve the same performance as they were trained on purely clean data. Code is available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, Xingjun Ma"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "employs\nShapley estimation to synthesize triggers and then detect\nsensitive neurons to synthesized triggers. Chen et al. ",
                    "Citation Text": "Tianlong Chen, Zhenyu Zhang, Yihua Zhang, Shiyu Chang,\nSijia Liu, and Zhangyang Wang. Quarantine: Sparsity can\nuncover the trojan attack trigger for free. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 598\u2013609, 2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.11819",
                        "Citation Paper Title": "Title:Quarantine: Sparsity Can Uncover the Trojan Attack Trigger for Free",
                        "Citation Paper Abstract": "Abstract:Trojan attacks threaten deep neural networks (DNNs) by poisoning them to behave normally on most samples, yet to produce manipulated results for inputs attached with a particular trigger. Several works attempt to detect whether a given DNN has been injected with a specific trigger during the training. In a parallel line of research, the lottery ticket hypothesis reveals the existence of sparse subnetworks which are capable of reaching competitive performance as the dense network after independent training. Connecting these two dots, we investigate the problem of Trojan DNN detection from the brand new lens of sparsity, even when no clean training data is available. Our crucial observation is that the Trojan features are significantly more stable to network pruning than benign features. Leveraging that, we propose a novel Trojan network detection regime: first locating a \"winning Trojan lottery ticket\" which preserves nearly full Trojan information yet only chance-level performance on clean inputs; then recovering the trigger embedded in this already isolated subnetwork. Extensive experiments on various datasets, i.e., CIFAR-10, CIFAR-100, and ImageNet, with different network architectures, i.e., VGG-16, ResNet-18, ResNet-20s, and DenseNet-100 demonstrate the effectiveness of our proposal. Codes are available at this https URL.",
                        "Citation Paper Authors": "Authors:Tianlong Chen, Zhenyu Zhang, Yihua Zhang, Shiyu Chang, Sijia Liu, Zhangyang Wang"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": ", other meth-\nods [3,4,10,46] are proposed to improve the quality of syn-\nthesized triggers. For example, ShapPruning ",
                    "Citation Text": "Jiyang Guan, Zhuozhuo Tu, Ran He, and Dacheng Tao. Few-\nshot backdoor defense using shapley estimation. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pages 13358\u201313367, 2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.14889",
                        "Citation Paper Title": "Title:Few-shot Backdoor Defense Using Shapley Estimation",
                        "Citation Paper Abstract": "Abstract:Deep neural networks have achieved impressive performance in a variety of tasks over the last decade, such as autonomous driving, face recognition, and medical diagnosis. However, prior works show that deep neural networks are easily manipulated into specific, attacker-decided behaviors in the inference stage by backdoor attacks which inject malicious small hidden triggers into model training, raising serious security threats. To determine the triggered neurons and protect against backdoor attacks, we exploit Shapley value and develop a new approach called Shapley Pruning (ShapPruning) that successfully mitigates backdoor attacks from models in a data-insufficient situation (1 image per class or even free of data). Considering the interaction between neurons, ShapPruning identifies the few infected neurons (under 1% of all neurons) and manages to protect the model's structure and accuracy after pruning as many infected neurons as possible. To accelerate ShapPruning, we further propose discarding threshold and $\\epsilon$-greedy strategy to accelerate Shapley estimation, making it possible to repair poisoned models with only several minutes. Experiments demonstrate the effectiveness and robustness of our method against various attacks and tasks compared to existing methods.",
                        "Citation Paper Authors": "Authors:Jiyang Guan, Zhuozhuo Tu, Ran He, Dacheng Tao"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "jointly optimizes\ntrigger injection function and classification loss function to\nget stealthy triggers. BppAttack ",
                    "Citation Text": "Zhenting Wang, Juan Zhai, and Shiqing Ma. Bppattack:\nStealthy and efficient trojan attacks against deep neural net-\nworks via image quantization and contrastive adversarial\nlearning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 15074\u2013\n15084, 2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.13383",
                        "Citation Paper Title": "Title:BppAttack: Stealthy and Efficient Trojan Attacks against Deep Neural Networks via Image Quantization and Contrastive Adversarial Learning",
                        "Citation Paper Abstract": "Abstract:Deep neural networks are vulnerable to Trojan attacks. Existing attacks use visible patterns (e.g., a patch or image transformations) as triggers, which are vulnerable to human inspection. In this paper, we propose stealthy and efficient Trojan attacks, BppAttack. Based on existing biology literature on human visual systems, we propose to use image quantization and dithering as the Trojan trigger, making imperceptible changes. It is a stealthy and efficient attack without training auxiliary models. Due to the small changes made to images, it is hard to inject such triggers during training. To alleviate this problem, we propose a contrastive learning based approach that leverages adversarial attacks to generate negative sample pairs so that the learned trigger is precise and accurate. The proposed method achieves high attack success rates on four benchmark datasets, including MNIST, CIFAR-10, GTSRB, and CelebA. It also effectively bypasses existing Trojan defenses and human inspection. Our code can be found in this https URL.",
                        "Citation Paper Authors": "Authors:Zhenting Wang, Juan Zhai, Shiqing Ma"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": ", an input-aware dynamic pattern [26, 29], etc. Re-\ncent works [7, 20, 25, 36] design more imperceptible trig-\ngers. Refool ",
                    "Citation Text": "Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Reflec-\ntion backdoor: A natural backdoor attack on deep neural net-\nworks. In European Conference on Computer Vision , pages\n182\u2013199. Springer, 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.02343",
                        "Citation Paper Title": "Title:Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:Recent studies have shown that DNNs can be compromised by backdoor attacks crafted at training time. A backdoor attack installs a backdoor into the victim model by injecting a backdoor pattern into a small proportion of the training data. At test time, the victim model behaves normally on clean test data, yet consistently predicts a specific (likely incorrect) target class whenever the backdoor pattern is present in a test example. While existing backdoor attacks are effective, they are not stealthy. The modifications made on training data or labels are often suspicious and can be easily detected by simple data filtering or human inspection. In this paper, we present a new type of backdoor attack inspired by an important natural phenomenon: reflection. Using mathematical modeling of physical reflection models, we propose reflection backdoor (Refool) to plant reflections as backdoor into a victim model. We demonstrate on 3 computer vision tasks and 5 datasets that, Refool can attack state-of-the-art DNNs with high success rate, and is resistant to state-of-the-art backdoor defenses.",
                        "Citation Paper Authors": "Authors:Yunfei Liu, Xingjun Ma, James Bailey, Feng Lu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.13537v3": {
            "Paper Title": "Private Online Prediction from Experts: Separations and Faster Rates",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.01579v2": {
            "Paper Title": "Data-free Defense of Black Box Models Against Adversarial Attacks",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": "proposed an alternate\ntraining mechanism between generator and surrogate model, where gen-\nerator is trained to produce synthetic samples to maximize the discrep-\nancy between the predictions of the surrogate and the black box model.\nTruong et al. ",
                    "Citation Text": "J.-B. Truong, P. Maini, R. J. Walls, N. Papernot, Data-free model extrac-\ntion, in: Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2021, pp. 4771\u20134780.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.14779",
                        "Citation Paper Title": "Title:Data-Free Model Extraction",
                        "Citation Paper Abstract": "Abstract:Current model extraction attacks assume that the adversary has access to a surrogate dataset with characteristics similar to the proprietary data used to train the victim model. This requirement precludes the use of existing model extraction techniques on valuable models, such as those trained on rare or hard to acquire datasets. In contrast, we propose data-free model extraction methods that do not require a surrogate dataset. Our approach adapts techniques from the area of data-free knowledge transfer for model extraction. As part of our study, we identify that the choice of loss is critical to ensuring that the extracted model is an accurate replica of the victim model. Furthermore, we address difficulties arising from the adversary's limited access to the victim model in a black-box setting. For example, we recover the model's logits from its probability predictions to approximate gradients. We find that the proposed data-free model extraction approach achieves high-accuracy with reasonable query complexity -- 0.99x and 0.92x the victim model accuracy on SVHN and CIFAR-10 datasets given 2M and 20M queries respectively.",
                        "Citation Paper Authors": "Authors:Jean-Baptiste Truong, Pratyush Maini, Robert J. Walls, Nicolas Papernot"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "query the\nblack box model on the natural images using adaptive strategy via\nreinforcement learning to get output predictions and use them to repli-\ncate the functionality of the black box model. Barbalau et al. ",
                    "Citation Text": "A. Barbalau, A. Cosma, R. T. Ionescu, M. Popescu, Black-box ripper: Copy-\ning black-box models using generative evolutionary algorithms, Advances in\nNeural Information Processing Systems 33 (2020) 20120\u201320129.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.11158",
                        "Citation Paper Title": "Title:Black-Box Ripper: Copying black-box models using generative evolutionary algorithms",
                        "Citation Paper Abstract": "Abstract:We study the task of replicating the functionality of black-box neural models, for which we only know the output class probabilities provided for a set of input images. We assume back-propagation through the black-box model is not possible and its training images are not available, e.g. the model could be exposed only through an API. In this context, we present a teacher-student framework that can distill the black-box (teacher) model into a student model with minimal accuracy loss. To generate useful data samples for training the student, our framework (i) learns to generate images on a proxy data set (with images and classes different from those used to train the black-box) and (ii) applies an evolutionary strategy to make sure that each generated data sample exhibits a high response for a specific class when given as input to the black box. Our framework is compared with several baseline and state-of-the-art methods on three benchmark data sets. The empirical evidence indicates that our model is superior to the considered baselines. Although our method does not back-propagate through the black-box network, it generally surpasses state-of-the-art methods that regard the teacher as a glass-box model. Our code is available at: this https URL.",
                        "Citation Paper Authors": "Authors:Antonio Barbalau, Adrian Cosma, Radu Tudor Ionescu, Marius Popescu"
                    }
                },
                {
                    "Sentence ID": 62,
                    "Sentence": ", on which we evaluate the clean and the adversarial accuracy\nagainst three different adversarial attacks (i.e., BIM ",
                    "Citation Text": "A. Kurakin, I. J. Goodfellow, S. Bengio, Adversarial examples in the physical\nworld, in: Artificial intelligence safety and security, Chapman and Hall/CRC,\n2018, pp. 99\u2013112.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.02533",
                        "Citation Paper Title": "Title:Adversarial examples in the physical world",
                        "Citation Paper Abstract": "Abstract:Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.",
                        "Citation Paper Authors": "Authors:Alexey Kurakin, Ian Goodfellow, Samy Bengio"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "used pixel\ndeflection technique followed by adaptive thresholding on the wavelets coef-\nficients for denoising. Unlike these works, our setup is more challenging due\nto no access to both the training data and the model weights. Mustafa et\nal. ",
                    "Citation Text": "A. Mustafa, S. H. Khan, M. Hayat, J. Shen, L. Shao, Image super-resolution as\na defense against adversarial attacks, IEEE Transactions on Image Processing\n29 (2019) 1711\u20131724.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.01677",
                        "Citation Paper Title": "Title:Image Super-Resolution as a Defense Against Adversarial Attacks",
                        "Citation Paper Abstract": "Abstract:Convolutional Neural Networks have achieved significant success across multiple computer vision tasks. However, they are vulnerable to carefully crafted, human-imperceptible adversarial noise patterns which constrain their deployment in critical security-sensitive systems. This paper proposes a computationally efficient image enhancement approach that provides a strong defense mechanism to effectively mitigate the effect of such adversarial perturbations. We show that deep image restoration networks learn mapping functions that can bring off-the-manifold adversarial samples onto the natural image manifold, thus restoring classification towards correct classes. A distinguishing feature of our approach is that, in addition to providing robustness against attacks, it simultaneously enhances image quality and retains models performance on clean images. Furthermore, the proposed method does not modify the classifier or requires a separate mechanism to detect adversarial images. The effectiveness of the scheme has been demonstrated through extensive experiments, where it has proven a strong defense in gray-box settings. The proposed scheme is simple and has the following advantages: (1) it does not require any model training or parameter optimization, (2) it complements other existing defense mechanisms, (3) it is agnostic to the attacked model and attack type and (4) it provides superior performance across all popular attack algorithms. Our codes are publicly available at this https URL.",
                        "Citation Paper Authors": "Authors:Aamir Mustafa, Salman H. Khan, Munawar Hayat, Jianbing Shen, Ling Shao"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": ", uses Generative\nAdversarial Networks to learn the distribution of clean images and generate\nsamples similar to clean images from inputs corrupted with adversarial noise.\nSun et al. ",
                    "Citation Text": "B. Sun, N.-h. Tsai, F. Liu, R. Yu, H. Su, Adversarial defense by stratified\nconvolutional sparse coding, in: Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2019, pp. 11447\u201311456.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.00037",
                        "Citation Paper Title": "Title:Adversarial Defense by Stratified Convolutional Sparse Coding",
                        "Citation Paper Abstract": "Abstract:We propose an adversarial defense method that achieves state-of-the-art performance among attack-agnostic adversarial defense methods while also maintaining robustness to input resolution, scale of adversarial perturbation, and scale of dataset size. Based on convolutional sparse coding, we construct a stratified low-dimensional quasi-natural image space that faithfully approximates the natural image space while also removing adversarial perturbations. We introduce a novel Sparse Transformation Layer (STL) in between the input image and the first layer of the neural network to efficiently project images into our quasi-natural image space. Our experiments show state-of-the-art performance of our method compared to other attack-agnostic adversarial defense methods in various adversarial settings.",
                        "Citation Paper Authors": "Authors:Bo Sun, Nian-hsuan Tsai, Fangchen Liu, Ronald Yu, Hao Su"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": ", first classifies inputs as clean or adversarial\nand then transforms the adversarial inputs closer to the clean image manifold.\nAnother approach, Defense-GAN by Samangouei et al. ",
                    "Citation Text": "P. Samangouei, M. Kabkab, R. Chellappa, Defense-gan: Protecting clas-\nsifiers against adversarial attacks using generative models, arXiv preprint\narXiv:1805.06605 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.06605",
                        "Citation Paper Title": "Title:Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural network approaches have been widely adopted for machine learning tasks, including classification. However, they were shown to be vulnerable to adversarial perturbations: carefully crafted small perturbations can cause misclassification of legitimate images. We propose Defense-GAN, a new framework leveraging the expressive capability of generative models to defend deep neural networks against such attacks. Defense-GAN is trained to model the distribution of unperturbed images. At inference time, it finds a close output to a given image which does not contain the adversarial changes. This output is then fed to the classifier. Our proposed method can be used with any classification model and does not modify the classifier structure or training procedure. It can also be used as a defense against any attack as it does not assume knowledge of the process for generating the adversarial examples. We empirically show that Defense-GAN is consistently effective against different attack methods and improves on existing defense strategies. Our code has been made publicly available at this https URL",
                        "Citation Paper Authors": "Authors:Pouya Samangouei, Maya Kabkab, Rama Chellappa"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "offer faster training\nbut lack robustness against a wide range of attacks and perform inadequately\nagainst stronger attacks ",
                    "Citation Text": "W. He, J. Wei, X. Chen, N. Carlini, D. Song, Adversarial example de-\nfenses: ensembles of weak defenses are not strong, in: Proceedings of the\n11th USENIX Conference on Offensive Technologies, 2017, pp. 15\u201315.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.04701",
                        "Citation Paper Title": "Title:Adversarial Example Defenses: Ensembles of Weak Defenses are not Strong",
                        "Citation Paper Abstract": "Abstract:Ongoing research has proposed several methods to defend neural networks against adversarial examples, many of which researchers have shown to be ineffective. We ask whether a strong defense can be created by combining multiple (possibly weak) defenses. To answer this question, we study three defenses that follow this approach. Two of these are recently proposed defenses that intentionally combine components designed to work well together. A third defense combines three independent defenses. For all the components of these defenses and the combined defenses themselves, we show that an adaptive adversary can create adversarial examples successfully with low distortion. Thus, our work implies that ensemble of weak defenses is not sufficient to provide strong defense against adversarial examples.",
                        "Citation Paper Authors": "Authors:Warren He, James Wei, Xinyun Chen, Nicholas Carlini, Dawn Song"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": ". On the other hand, Non-AT-\nbased methods like JARN ",
                    "Citation Text": "A. Chan, Y. Tay, Y. S. Ong, J. Fu, Jacobian adversarially regularized networks\nfor robustness, in: International Conference on Learning Representations,\n2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.10185",
                        "Citation Paper Title": "Title:Jacobian Adversarially Regularized Networks for Robustness",
                        "Citation Paper Abstract": "Abstract:Adversarial examples are crafted with imperceptible perturbations with the intent to fool neural networks. Against such attacks, adversarial training and its variants stand as the strongest defense to date. Previous studies have pointed out that robust models that have undergone adversarial training tend to produce more salient and interpretable Jacobian matrices than their non-robust counterparts. A natural question is whether a model trained with an objective to produce salient Jacobian can result in better robustness. This paper answers this question with affirmative empirical results. We propose Jacobian Adversarially Regularized Networks (JARN) as a method to optimize the saliency of a classifier's Jacobian by adversarially regularizing the model's Jacobian to resemble natural training images. Image classifiers trained with JARN show improved robust accuracy compared to standard models on the MNIST, SVHN and CIFAR-10 datasets, uncovering a new angle to boost robustness without using adversarial training examples.",
                        "Citation Paper Authors": "Authors:Alvin Chan, Yi Tay, Yew Soon Ong, Jie Fu"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "use wavelets for\ndetecting adversarially perturbed iris images. Prakash et al. ",
                    "Citation Text": "A. Prakash, N. Moran, S. Garber, A. DiLillo, J. Storer, Deflecting adversar-\nial attacks with pixel deflection, in: Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2018, pp. 8571\u20138580.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.08926",
                        "Citation Paper Title": "Title:Deflecting Adversarial Attacks with Pixel Deflection",
                        "Citation Paper Abstract": "Abstract:CNNs are poised to become integral parts of many critical systems. Despite their robustness to natural variations, image pixel values can be manipulated, via small, carefully crafted, imperceptible perturbations, to cause a model to misclassify images. We present an algorithm to process an image so that classification accuracy is significantly preserved in the presence of such adversarial manipulations. Image classifiers tend to be robust to natural noise, and adversarial attacks tend to be agnostic to object location. These observations motivate our strategy, which leverages model robustness to defend against adversarial perturbations by forcing the image to match natural image statistics. Our algorithm locally corrupts the image by redistributing pixel values via a process we term pixel deflection. A subsequent wavelet-based denoising operation softens this corruption, as well as some of the adversarial changes. We demonstrate experimentally that the combination of these techniques enables the effective recovery of the true class, against a variety of robust attacks. Our results compare favorably with current state-of-the-art defenses, without requiring retraining or modifying the CNN.",
                        "Citation Paper Authors": "Authors:Aaditya Prakash, Nick Moran, Solomon Garber, Antonella DiLillo, James Storer"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "use the GAN framework with a\nproxy dataset composed of either related/unrelated data or synthetic\ndata.\n5\u2022Without Proxy data - Kariyappa et al. ",
                    "Citation Text": "S. Kariyappa, A. Prakash, M. K. Qureshi, Maze: Data-free model steal-\ning attack using zeroth-order gradient estimation, in: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021,\npp. 13814\u201313823.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.03161",
                        "Citation Paper Title": "Title:MAZE: Data-Free Model Stealing Attack Using Zeroth-Order Gradient Estimation",
                        "Citation Paper Abstract": "Abstract:Model Stealing (MS) attacks allow an adversary with black-box access to a Machine Learning model to replicate its functionality, compromising the confidentiality of the model. Such attacks train a clone model by using the predictions of the target model for different inputs. The effectiveness of such attacks relies heavily on the availability of data necessary to query the target model. Existing attacks either assume partial access to the dataset of the target model or availability of an alternate dataset with semantic similarities. This paper proposes MAZE -- a data-free model stealing attack using zeroth-order gradient estimation. In contrast to prior works, MAZE does not require any data and instead creates synthetic data using a generative model. Inspired by recent works in data-free Knowledge Distillation (KD), we train the generative model using a disagreement objective to produce inputs that maximize disagreement between the clone and the target model. However, unlike the white-box setting of KD, where the gradient information is available, training a generator for model stealing requires performing black-box optimization, as it involves accessing the target model under attack. MAZE relies on zeroth-order gradient estimation to perform this optimization and enables a highly accurate MS attack. Our evaluation with four datasets shows that MAZE provides a normalized clone accuracy in the range of 0.91x to 0.99x, and outperforms even the recent attacks that rely on partial data (JBDA, clone accuracy 0.13x to 0.69x) and surrogate data (KnockoffNets, clone accuracy 0.52x to 0.97x). We also study an extension of MAZE in the partial-data setting and develop MAZE-PD, which generates synthetic data closer to the target distribution. MAZE-PD further improves the clone accuracy (0.97x to 1.0x) and reduces the query required for the attack by 2x-24x.",
                        "Citation Paper Authors": "Authors:Sanjay Kariyappa, Atul Prakash, Moinuddin Qureshi"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "is\nused to extract knowledge using soft labels obtained from the black\nbox model. With few training samples, Papernot et al. ",
                    "Citation Text": "N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, A. Swami,\nPractical black-box attacks against machine learning, in: Proceedings of the\n2017 ACM on Asia conference on computer and communications security,\n2017, pp. 506\u2013519.\n34",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.02697",
                        "Citation Paper Title": "Title:Practical Black-Box Attacks against Machine Learning",
                        "Citation Paper Abstract": "Abstract:Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19% and 88.94%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.",
                        "Citation Paper Authors": "Authors:Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik, Ananthram Swami"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.05206v2": {
            "Paper Title": "It's TEEtime: A New Architecture Bringing Sovereignty to Smartphones",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.00845v3": {
            "Paper Title": "Improving Differentially Private SGD via Randomly Sparsified Gradients",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.10095v2": {
            "Paper Title": "Improving Robustness of TCM-based Robust Steganography with Variable\n  Robustness",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.06735v3": {
            "Paper Title": "Private Non-Convex Federated Learning Without a Trusted Server",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.04918v2": {
            "Paper Title": "Detection of Sparse Anomalies in High-Dimensional Network Telescope\n  Signals",
            "Sentences": [
                {
                    "Sentence ID": 37,
                    "Sentence": ". The form that most closely resembles the set-up of\nour problem is Zhou et.al. ",
                    "Citation Text": "Z. Zhou, X. Li, J. Wright, E. Candes, and Y . Ma, \u201cStable principal\ncomponent pursuit,\u201d in Information Theory Proceedings (ISIT), 2010\nIEEE International Symposium on . IEEE, 2010, pp. 1518\u20131522.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1001.2363",
                        "Citation Paper Title": "Title:Stable Principal Component Pursuit",
                        "Citation Paper Abstract": "Abstract:  In this paper, we study the problem of recovering a low-rank matrix (the principal components) from a high-dimensional data matrix despite both small entry-wise noise and gross sparse errors. Recently, it has been shown that a convex program, named Principal Component Pursuit (PCP), can recover the low-rank matrix when the data matrix is corrupted by gross sparse errors. We further prove that the solution to a related convex program (a relaxed PCP) gives an estimate of the low-rank matrix that is simultaneously stable to small entrywise noise and robust to gross sparse errors. More precisely, our result shows that the proposed convex program recovers the low-rank matrix even though a positive fraction of its entries are arbitrarily corrupted, with an error bound proportional to the noise level. We present simulation results to support our result and demonstrate that the new convex program accurately recovers the principal components (the low-rank matrix) under quite broad conditions. To our knowledge, this is the first result that shows the classical Principal Component Analysis (PCA), optimal for small i.i.d. noise, can be made robust to gross sparse errors; or the first that shows the newly proposed PCP can be made stable to small entry-wise perturbations.",
                        "Citation Paper Authors": "Authors:Zihan Zhou, Xiaodong Li, John Wright, Emmanuel Candes, Yi Ma"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.06854v2": {
            "Paper Title": "On the explainable properties of 1-Lipschitz Neural Networks: An Optimal\n  Transport Perspective",
            "Sentences": [
                {
                    "Sentence ID": 22,
                    "Sentence": ",\nand called Harmonized ResNet50 . This finding is interesting as it indicates OTNNs are less prone to\nrelying on spurious correlations ",
                    "Citation Text": "R. Geirhos, J.-H. Jacobsen, C. Michaelis, R. Zemel, W. Brendel, M. Bethge, and F. A. Wichmann.\nShortcut learning in deep neural networks. Nature Machine Intelligence , 2(11):665\u2013673, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.07780",
                        "Citation Paper Title": "Title:Shortcut Learning in Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:Deep learning has triggered the current rise of artificial intelligence and is the workhorse of today's machine intelligence. Numerous success stories have rapidly spread all over science, industry and society, but its limitations have only recently come into focus. In this perspective we seek to distill how many of deep learning's problems can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in Comparative Psychology, Education and Linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike. Based on these observations, we develop a set of recommendations for model interpretation and benchmarking, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications.",
                        "Citation Paper Authors": "Authors:Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, Felix A. Wichmann"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ",\nusing the ClickMe dataset, we compute the human feature alignment of OTNN Saliency Maps and\ncompare with the others models tested in ",
                    "Citation Text": "T. Fel, I. Felipe, D. Linsley, and T. Serre. Harmonizing the object recognition strategies of deep\nneural networks with humans. Advances in Neural Information Processing Systems (NeurIPS) ,\n2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2211.04533",
                        "Citation Paper Title": "Title:Harmonizing the object recognition strategies of deep neural networks with humans",
                        "Citation Paper Abstract": "Abstract:The many successes of deep neural networks (DNNs) over the past decade have largely been driven by computational scale rather than insights from biological intelligence. Here, we explore if these trends have also carried concomitant improvements in explaining the visual strategies humans rely on for object recognition. We do this by comparing two related but distinct properties of visual strategies in humans and DNNs: where they believe important visual features are in images and how they use those features to categorize objects. Across 84 different DNNs trained on ImageNet and three independent datasets measuring the where and the how of human visual strategies for object recognition on those images, we find a systematic trade-off between DNN categorization accuracy and alignment with human visual strategies for object recognition. State-of-the-art DNNs are progressively becoming less aligned with humans as their accuracy improves. We rectify this growing issue with our neural harmonizer: a general-purpose training routine that both aligns DNN and human visual strategies and improves categorization accuracy. Our work represents the first demonstration that the scaling laws that are guiding the design of DNNs today have also produced worse models of human vision. We release our code and data at this https URL to help the field build more human-like DNNs.",
                        "Citation Paper Authors": "Authors:Thomas Fel, Ivan Felipe, Drew Linsley, Thomas Serre"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": ", or stability [ 68,8] of explanation methods.\nA recent approach ",
                    "Citation Text": "D. Linsley, S. Eberhardt, T. Sharma, P. Gupta, and T. Serre. What are the visual features\nunderlying human versus machine vision? In Proceedings of the IEEE International Conference\non Computer Vision Workshops , pages 2706\u20132714, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1701.02704",
                        "Citation Paper Title": "Title:What are the visual features underlying human versus machine vision?",
                        "Citation Paper Abstract": "Abstract:Although Deep Convolutional Networks (DCNs) are approaching the accuracy of human observers at object recognition, it is unknown whether they leverage similar visual representations to achieve this performance. To address this, we introduce Clicktionary, a web-based game for identifying visual features used by human observers during object recognition. Importance maps derived from the game are consistent across participants and uncorrelated with image saliency measures. These results suggest that Clicktionary identifies image regions that are meaningful and diagnostic for object recognition but different than those driving eye movements. Surprisingly, Clicktionary importance maps are only weakly correlated with relevance maps derived from DCNs trained for object recognition. Our study demonstrates that the narrowing gap between the object recognition accuracy of human observers and DCNs obscures distinct visual strategies used by each to achieve this performance.",
                        "Citation Paper Authors": "Authors:Drew Linsley, Sven Eberhardt, Tarun Sharma, Pankaj Gupta, Thomas Serre"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ". Details on architectures and parameters are given in Appendix A.2.\nClassification performance: OTNN models achieve comparable results to unconstrained ones,\nconfirming claims of ",
                    "Citation Text": "L. B\u00e9thune, A. Gonz\u00e1lez-Sanz, F. Mamalet, and M. Serrurier. The many faces of 1-lipschitz\nneural networks. CoRR , abs/2104.05097, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.05097",
                        "Citation Paper Title": "Title:Pay attention to your loss: understanding misconceptions about 1-Lipschitz neural networks",
                        "Citation Paper Abstract": "Abstract:Lipschitz constrained networks have gathered considerable attention in the deep learning community, with usages ranging from Wasserstein distance estimation to the training of certifiably robust classifiers. However they remain commonly considered as less accurate, and their properties in learning are still not fully understood. In this paper we clarify the matter: when it comes to classification 1-Lipschitz neural networks enjoy several advantages over their unconstrained counterpart. First, we show that these networks are as accurate as classical ones, and can fit arbitrarily difficult boundaries. Then, relying on a robustness metric that reflects operational needs we characterize the most robust classifier: the WGAN discriminator. Next, we show that 1-Lipschitz neural networks generalize well under milder assumptions. Finally, we show that hyper-parameters of the loss are crucial for controlling the accuracy-robustness trade-off. We conclude that they exhibit appealing properties to pave the way toward provably accurate, and provably robust neural networks.",
                        "Citation Paper Authors": "Authors:Louis B\u00e9thune, Thibaut Boissin, Mathieu Serrurier, Franck Mamalet, Corentin Friedrich, Alberto Gonz\u00e1lez-Sanz"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.12348v2": {
            "Paper Title": "SNAP: Efficient Extraction of Private Properties with Poisoning",
            "Sentences": [
                {
                    "Sentence ID": 22,
                    "Sentence": ". The best existing\nmembership inference attacks train multiple models to analyze the distribution of loss ",
                    "Citation Text": "J. Ye, A. Maddi, S. K. Murakonda, and R. Shokri, \u201cEnhanced membership inference attacks against machine learning models.\u201d\narXiv, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.09679",
                        "Citation Paper Title": "Title:Enhanced Membership Inference Attacks against Machine Learning Models",
                        "Citation Paper Abstract": "Abstract:How much does a machine learning algorithm leak about its training data, and why? Membership inference attacks are used as an auditing tool to quantify this leakage. In this paper, we present a comprehensive \\textit{hypothesis testing framework} that enables us not only to formally express the prior work in a consistent way, but also to design new membership inference attacks that use reference models to achieve a significantly higher power (true positive rate) for any (false positive rate) error. More importantly, we explain \\textit{why} different attacks perform differently. We present a template for indistinguishability games, and provide an interpretation of attack success rate across different instances of the game. We discuss various uncertainties of attackers that arise from the formulation of the problem, and show how our approach tries to minimize the attack uncertainty to the one bit secret about the presence or absence of a data point in the training set. We perform a \\textit{differential analysis} between all types of attacks, explain the gap between them, and show what causes data points to be vulnerable to an attack (as the reasons vary due to different granularities of memorization, from overfitting to conditional memorization). Our auditing framework is openly accessible as part of the \\textit{Privacy Meter} software tool.",
                        "Citation Paper Authors": "Authors:Jiayuan Ye, Aadyaa Maddi, Sasi Kumar Murakonda, Vincent Bindschaedler, Reza Shokri"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.00068v2": {
            "Paper Title": "Differentially Private Enhanced Permissioned Blockchain for Private Data\n  Sharing in Industrial IoT",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.06746v2": {
            "Paper Title": "PoliGraph: Automated Privacy Policy Analysis using Knowledge Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.11708v3": {
            "Paper Title": "Towards an Improved Understanding of Software Vulnerability Assessment\n  Using Data-Driven Approaches",
            "Sentences": [
                {
                    "Sentence ID": 415,
                    "Sentence": ". The difference is mainly because these new\nSVs do not only emerge from configurations/code as in traditional systems, but also from\ntraining data and/or trained models ",
                    "Citation Text": "I. Rosenberg, A. Shabtai, Y. Elovici, and L. Rokach, \u201cAdversarial machine learning\nattacksanddefensemethodsinthecybersecuritydomain,\u201d ACM Computing Surveys\n(CSUR), vol. 54, no. 5, pp. 1\u201336, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.02407",
                        "Citation Paper Title": "Title:Adversarial Machine Learning Attacks and Defense Methods in the Cyber Security Domain",
                        "Citation Paper Abstract": "Abstract:In recent years machine learning algorithms, and more specifically deep learning algorithms, have been widely used in many fields, including cyber security. However, machine learning systems are vulnerable to adversarial attacks, and this limits the application of machine learning, especially in non-stationary, adversarial environments, such as the cyber security domain, where actual adversaries (e.g., malware developers) exist. This paper comprehensively summarizes the latest research on adversarial attacks against security solutions based on machine learning techniques and illuminates the risks they pose. First, the adversarial attack methods are characterized based on their stage of occurrence, and the attacker's goals and capabilities. Then, we categorize the applications of adversarial attack and defense methods in the cyber security domain. Finally, we highlight some characteristics identified in recent research and discuss the impact of recent advancements in other adversarial learning domains on future research directions in the cyber security domain. This paper is the first to discuss the unique challenges of implementing end-to-end adversarial attacks in the cyber security domain, map them in a unified taxonomy, and use the taxonomy to highlight future research directions.",
                        "Citation Paper Authors": "Authors:Ihai Rosenberg, Asaf Shabtai, Yuval Elovici, Lior Rokach"
                    }
                },
                {
                    "Sentence ID": 276,
                    "Sentence": ", domain-agnostic yet specific to a model type\n(graph neural network ",
                    "Citation Text": "Y. Li, S. Wang, and T. N. Nguyen, \u201cVulnerability detection with fine-grained inter-\npretations,\u201d in the 22nd ACM SIGSOFT International Symposium on Foundations\nof Software Engineering , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.10478",
                        "Citation Paper Title": "Title:Vulnerability Detection with Fine-grained Interpretations",
                        "Citation Paper Abstract": "Abstract:Despite the successes of machine learning (ML) and deep learning (DL) based vulnerability detectors (VD), they are limited to providing only the decision on whether a given code is vulnerable or not, without details on what part of the code is relevant to the detected vulnerability. We present IVDetect an interpretable vulnerability detector with the philosophy of using Artificial Intelligence (AI) to detect vulnerabilities, while using Intelligence Assistant (IA) via providing VD interpretations in terms of vulnerable statements.\nFor vulnerability detection, we separately consider the vulnerable statements and their surrounding contexts via data and control dependencies. This allows our model better discriminate vulnerable statements than using the mixture of vulnerable code and~contextual code as in existing approaches. In addition to the coarse-grained vulnerability detection result, we leverage interpretable AI to provide users with fine-grained interpretations that include the sub-graph in the Program Dependency Graph (PDG) with the crucial statements that are relevant to the detected vulnerability. Our empirical evaluation on vulnerability databases shows that IVDetect outperforms the existing DL-based approaches by 43%--84% and 105%--255% in top-10 nDCG and MAP ranking scores. IVDetect correctly points out the vulnerable statements relevant to the vulnerability via its interpretation~in 67% of the cases with a top-5 ranked list. It improves over baseline interpretation models by 12.3%--400% and 9%--400% in accuracy.",
                        "Citation Paper Authors": "Authors:Yi Li, Shaohua Wang, Tien N. Nguyen"
                    }
                },
                {
                    "Sentence ID": 410,
                    "Sentence": ". In the first approach, prior studies successfully used the feature activation\nmaps in a CNN model ",
                    "Citation Text": "R. Russell, L. Kim, L. Hamilton, T. Lazovich, J. Harer, O. Ozdemir, P. Ellingwood,\nand M. McConley, \u201cAutomated vulnerability detection in source code using deep\nrepresentation learning,\u201d in 2018 17th IEEE international conference on machine\nlearning and applications (ICMLA) . IEEE, 2018, pp. 757\u2013762.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.04320",
                        "Citation Paper Title": "Title:Automated Vulnerability Detection in Source Code Using Deep Representation Learning",
                        "Citation Paper Abstract": "Abstract:Increasing numbers of software vulnerabilities are discovered every year whether they are reported publicly or discovered internally in proprietary code. These vulnerabilities can pose serious risk of exploit and result in system compromise, information leaks, or denial of service. We leveraged the wealth of C and C++ open-source code available to develop a large-scale function-level vulnerability detection system using machine learning. To supplement existing labeled vulnerability datasets, we compiled a vast dataset of millions of open-source functions and labeled it with carefully-selected findings from three different static analyzers that indicate potential exploits. The labeled dataset is available at: this https URL. Using these datasets, we developed a fast and scalable vulnerability detection tool based on deep feature representation learning that directly interprets lexed source code. We evaluated our tool on code from both real software packages and the NIST SATE IV benchmark dataset. Our results demonstrate that deep feature representation learning on source code is a promising approach for automated software vulnerability detection.",
                        "Citation Paper Authors": "Authors:Rebecca L. Russell, Louis Kim, Lei H. Hamilton, Tomo Lazovich, Jacob A. Harer, Onur Ozdemir, Paul M. Ellingwood, Marc W. McConley"
                    }
                },
                {
                    "Sentence ID": 202,
                    "Sentence": ") and\nrecent sub-word embeddings (e.g., fastText [196, 246]) for SV assessment. fastText is an\nextension of Word2Vec ",
                    "Citation Text": "T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, \u201cDistributed rep-\nresentations of words and phrases and their compositionality,\u201d arXiv preprint\narXiv:1310.4546 , 2013.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1310.4546",
                        "Citation Paper Title": "Title:Distributed Representations of Words and Phrases and their Compositionality",
                        "Citation Paper Abstract": "Abstract:The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
                        "Citation Paper Authors": "Authors:Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "has been most commonly used for\nbuilding SV assessment models ",
                    "Citation Text": "T. H. M. Le, H. Chen, and M. A. Babar, \u201cA survey on data-driven software vulner-\nability assessment and prioritization,\u201d ACM Computing Surveys (CSUR) , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.08364",
                        "Citation Paper Title": "Title:A Survey on Data-driven Software Vulnerability Assessment and Prioritization",
                        "Citation Paper Abstract": "Abstract:Software Vulnerabilities (SVs) are increasing in complexity and scale, posing great security risks to many software systems. Given the limited resources in practice, SV assessment and prioritization help practitioners devise optimal SV mitigation plans based on various SV characteristics. The surges in SV data sources and data-driven techniques such as Machine Learning and Deep Learning have taken SV assessment and prioritization to the next level. Our survey provides a taxonomy of the past research efforts and highlights the best practices for data-driven SV assessment and prioritization. We also discuss the current limitations and propose potential solutions to address such issues.",
                        "Citation Paper Authors": "Authors:Triet H. M. Le, Huaming Chen, M. Ali Babar"
                    }
                },
                {
                    "Sentence ID": 385,
                    "Sentence": ") each from SO and SSE to categorize\nthe answer types. Stratification ensured the proportion of each topic was maintained.\nFollowing ",
                    "Citation Text": "Z. Chen, Y. Cao, Y. Liu, H. Wang, T. Xie, and X. Liu, \u201cA comprehensive study on\nchallengesindeployingdeeplearningbasedsoftware,\u201d in the 28th ACM Joint Meeting\non European Software Engineering Conference and Symposium on the Foundations\nof Software Engineering , 2020, pp. 750\u2013762.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.00760",
                        "Citation Paper Title": "Title:A Comprehensive Study on Challenges in Deploying Deep Learning Based Software",
                        "Citation Paper Abstract": "Abstract:Deep learning (DL) becomes increasingly pervasive, being used in a wide range of software applications. These software applications, named as DL based software (in short as DL software), integrate DL models trained using a large data corpus with DL programs written based on DL frameworks such as TensorFlow and Keras. A DL program encodes the network structure of a desirable DL model and the process by which the model is trained using the training data. To help developers of DL software meet the new challenges posed by DL, enormous research efforts in software engineering have been devoted. Existing studies focus on the development of DL software and extensively analyze faults in DL programs. However, the deployment of DL software has not been comprehensively studied. To fill this knowledge gap, this paper presents a comprehensive study on understanding challenges in deploying DL software. We mine and analyze 3,023 relevant posts from Stack Overflow, a popular Q&A website for developers, and show the increasing popularity and high difficulty of DL software deployment among developers. We build a taxonomy of specific challenges encountered by developers in the process of DL software deployment through manual inspection of 769 sampled posts and report a series of actionable implications for researchers, developers, and DL framework vendors.",
                        "Citation Paper Authors": "Authors:Zhenpeng Chen, Yanbin Cao, Yuanqiang Liu, Haoyu Wang, Tao Xie, Xuanzhe Liu"
                    }
                },
                {
                    "Sentence ID": 196,
                    "Sentence": ". Table 4.1 lists different values for\nthe window and vector sizes of Word2vec used for tuning the performance of learning-based\nSV assessment models.\nfastText .fastText ",
                    "Citation Text": "P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, \u201cEnriching word vectors with\nsubword information,\u201d Transactions of the Association for Computational Linguis-\ntics, vol. 5, pp. 135\u2013146, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.04606",
                        "Citation Paper Title": "Title:Enriching Word Vectors with Subword Information",
                        "Citation Paper Abstract": "Abstract:Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character $n$-grams. A vector representation is associated to each character $n$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",
                        "Citation Paper Authors": "Authors:Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov"
                    }
                },
                {
                    "Sentence ID": 270,
                    "Sentence": ". The discrepancy is because NVD reports\nare vetted by security experts, while ITS reports may be contributed by users/developers\nwith limited security knowledge ",
                    "Citation Text": "R. Croft, A. Babar, and L. Li, \u201cAn investigation into inconsistency of software vul-\nnerability severity across data sources,\u201d in 2022 29th IEEE International Conference\non Software Analysis, Evolution and Reengineering (SANER) , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.10356",
                        "Citation Paper Title": "Title:An Investigation into Inconsistency of Software Vulnerability Severity across Data Sources",
                        "Citation Paper Abstract": "Abstract:Software Vulnerability (SV) severity assessment is a vital task for informing SV remediation and triage. Ranking of SV severity scores is often used to advise prioritization of patching efforts. However, severity assessment is a difficult and subjective manual task that relies on expertise, knowledge, and standardized reporting schemes. Consequently, different data sources that perform independent analysis may provide conflicting severity rankings. Inconsistency across these data sources affects the reliability of severity assessment data, and can consequently impact SV prioritization and fixing. In this study, we investigate severity ranking inconsistencies over the SV reporting lifecycle. Our analysis helps characterize the nature of this problem, identify correlated factors, and determine the impacts of inconsistency on downstream tasks. Our findings observe that SV severity often lacks consideration or is underestimated during initial reporting, and such SVs consequently receive lower prioritization. We identify six potential attributes that are correlated to this misjudgment, and show that inconsistency in severity reporting schemes can severely degrade the performance of downstream severity prediction by up to 77%. Our findings help raise awareness of SV severity data inconsistencies and draw attention to this data quality problem. These insights can help developers better consider SV severity data sources, and improve the reliability of consequent SV prioritization. Furthermore, we encourage researchers to provide more attention to SV severity data selection.",
                        "Citation Paper Authors": "Authors:Roland Croft, M. Ali Babar, Li Li"
                    }
                },
                {
                    "Sentence ID": 241,
                    "Sentence": ". We use the following\ntext preprocessing techniques: ( i) removal of stop words and punctuations, ( ii) conversion\nto lowercase and ( iii) stemming. The stop words are combined from the default lists of\nthescikit-learn ",
                    "Citation Text": "F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blon-\ndel, P. Prettenhofer, R. Weiss, V. Dubourg et al., \u201cScikit-learn: Machine learning in\npython,\u201d the Journal of machine Learning research , vol. 12, pp. 2825\u20132830, 2011.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1201.0490",
                        "Citation Paper Title": "Title:Scikit-learn: Machine Learning in Python",
                        "Citation Paper Abstract": "Abstract:Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from this http URL.",
                        "Citation Paper Authors": "Authors:Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Andreas M\u00fcller, Joel Nothman, Gilles Louppe, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, \u00c9douard Duchesnay"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.07073v2": {
            "Paper Title": "DP-Image: Differential Privacy for Image Data in Feature Space",
            "Sentences": [
                {
                    "Sentence ID": 50,
                    "Sentence": ". It can be used to quantify\nthe extent that the perturbation is invisible to human eyes. A high score means that two images are structurally\nsimilar.\n5. FID ",
                    "Citation Text": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, B ernhard Nessler, and Sepp Hochreiter. Gans trained by\na two time-scale update rule converge to a local nash equilib rium. In Advances in neural information processing\nsystems , pages 6626\u20136637, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.08500",
                        "Citation Paper Title": "Title:GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
                        "Citation Paper Abstract": "Abstract:Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the \"Fr\u00e9chet Inception Distance\" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.",
                        "Citation Paper Authors": "Authors:Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "can help to explore variations on d ata you already have. Moreover, there have also been\nresearches to combine V AEs and GANs together, which can gene rate compelling results for complex datasets such\nas images ",
                    "Citation Text": "Lars Mescheder, Sebastian Nowozin, and Andreas Geiger . Adversarial variational bayes: Unifying variational\nautoencoders and generative adversarial networks. In Proceedings of the 34th International Conference on Ma-\nchine Learning-Volume 70 , pages 2391\u20132400. JMLR. org, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1701.04722",
                        "Citation Paper Title": "Title:Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.",
                        "Citation Paper Authors": "Authors:Lars Mescheder, Sebastian Nowozin, Andreas Geiger"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "in 2014. It int roduced an Inception module, which signi\ufb01cantly\nreduced the number of parameters in the network (4M, compare d to 60 million for AlexNet). GoogLeNet also has\nmultiple subsequent versions, such as Inception-v3 ",
                    "Citation Text": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, J on Shlens, and Zbigniew Wojna. Rethinking the inception\narchitecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision and Pa ttern\nRecognition , pages 2818\u20132826, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.00567",
                        "Citation Paper Title": "Title:Rethinking the Inception Architecture for Computer Vision",
                        "Citation Paper Abstract": "Abstract:Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.",
                        "Citation Paper Authors": "Authors:Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": ". It is this work that popularized CNN in computer vision.\nSzegedy et al. Google proposed GoogLeNet ",
                    "Citation Text": "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Serma net, Scott Reed, Dragomir Anguelov, Dumitru Erhan,\nVincent Vanhoucke, Andrew Rabinovich, et al. Going deeper w ith convolutions. Cvpr, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.4842",
                        "Citation Paper Title": "Title:Going Deeper with Convolutions",
                        "Citation Paper Abstract": "Abstract:We propose a deep convolutional neural network architecture codenamed \"Inception\", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.",
                        "Citation Paper Authors": "Authors:Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.07165v5": {
            "Paper Title": "OLIVE: Oblivious Federated Learning on Trusted Execution Environment\n  against the risk of sparsification",
            "Sentences": [
                {
                    "Sentence ID": 58,
                    "Sentence": ", which are outside the threat\nmodel of the Olive , as well as TEE and are difficult to prevent.\nSecurity of SGX. Finally, we discuss the use of SGX as a secu-\nrity primitive against known attacks. According to ",
                    "Citation Text": "Alexander Nilsson, Pegah Nikbakht Bideh, and Joakim Brorsson. 2020. A survey\nof published attacks on Intel SGX. arXiv preprint arXiv:2006.13598 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.13598",
                        "Citation Paper Title": "Title:A Survey of Published Attacks on Intel SGX",
                        "Citation Paper Abstract": "Abstract:Intel Software Guard Extensions (SGX) provides a trusted execution environment (TEE) to run code and operate sensitive data. SGX provides runtime hardware protection where both code and data are protected even if other code components are malicious. However, recently many attacks targeting SGX have been identified and introduced that can thwart the hardware defence provided by SGX. In this paper we present a survey of all attacks specifically targeting Intel SGX that are known to the authors, to date. We categorized the attacks based on their implementation details into 7 different categories. We also look into the available defence mechanisms against identified attacks and categorize the available types of mitigations for each presented attack.",
                        "Citation Paper Authors": "Authors:Alexander Nilsson, Pegah Nikbakht Bideh, Joakim Brorsson"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.03458v4": {
            "Paper Title": "PAC Privacy: Automatic Privacy Measurement and Control of Data\n  Processing",
            "Sentences": [
                {
                    "Sentence ID": 61,
                    "Sentence": "empirically st udied the actual privacy guarantee provided\nby DP-SGD and showed that there is a substantial gap between t he best theoretical bound we can claim\nso far and the practical distinguishing advantage of the adv ersary. ",
                    "Citation Text": "Stock, P., Shilov, I., Mironov, I., Sablayrolles, A.: D efending against reconstruction attacks with r \\\u2019enyi\ndi\ufb00erential privacy. arXiv preprint arXiv:2202.07623 (20 22)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.07623",
                        "Citation Paper Title": "Title:Defending against Reconstruction Attacks with R\u00e9nyi Differential Privacy",
                        "Citation Paper Abstract": "Abstract:Reconstruction attacks allow an adversary to regenerate data samples of the training set using access to only a trained model. It has been recently shown that simple heuristics can reconstruct data samples from language models, making this threat scenario an important aspect of model release. Differential privacy is a known solution to such attacks, but is often used with a relatively large privacy budget (epsilon > 8) which does not translate to meaningful guarantees. In this paper we show that, for a same mechanism, we can derive privacy guarantees for reconstruction attacks that are better than the traditional ones from the literature. In particular, we show that larger privacy budgets do not protect against membership inference, but can still protect extraction of rare secrets. We show experimentally that our guarantees hold against various language models, including GPT-2 finetuned on Wikitext-103.",
                        "Citation Paper Authors": "Authors:Pierre Stock, Igor Shilov, Ilya Mironov, Alexandre Sablayrolles"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": "suggest that without proper privacy preservation, man y popular machine learning algorithms could have\nsevere privacy risks. On the other hand, ",
                    "Citation Text": "Nasr, M., Songi, S., Thakurta, A., Papemoti, N., Carlin , N.: Adversary instantiation: Lower bounds for\ndi\ufb00erentially private machine learning. In: 2021 IEEE Symp osium on Security and Privacy (SP). pp.\n866\u2013882. IEEE (2021)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.04535",
                        "Citation Paper Title": "Title:Adversary Instantiation: Lower Bounds for Differentially Private Machine Learning",
                        "Citation Paper Abstract": "Abstract:Differentially private (DP) machine learning allows us to train models on private data while limiting data leakage. DP formalizes this data leakage through a cryptographic game, where an adversary must predict if a model was trained on a dataset D, or a dataset D' that differs in just one example.If observing the training algorithm does not meaningfully increase the adversary's odds of successfully guessing which dataset the model was trained on, then the algorithm is said to be differentially private. Hence, the purpose of privacy analysis is to upper bound the probability that any adversary could successfully guess which dataset the model was trained this http URL our paper, we instantiate this hypothetical adversary in order to establish lower bounds on the probability that this distinguishing game can be won. We use this adversary to evaluate the importance of the adversary capabilities allowed in the privacy analysis of DP training algorithms.For DP-SGD, the most common method for training neural networks with differential privacy, our lower bounds are tight and match the theoretical upper bound. This implies that in order to prove better upper bounds, it will be necessary to make use of additional assumptions. Fortunately, we find that our attacks are significantly weaker when additional (realistic)restrictions are put in place on the adversary's capabilities.Thus, in the practical setting common to many real-world deployments, there is a gap between our lower bounds and the upper bounds provided by the analysis: differential privacy is conservative and adversaries may not be able to leak as much information as suggested by the theoretical bound.",
                        "Citation Paper Authors": "Authors:Milad Nasr, Shuang Song, Abhradeep Thakurta, Nicolas Papernot, Nicholas Carlini"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": ".\n5An upper bound on individual sensitivity of Empirical Risk M inimization is only known for strongly-convex optimizatio n ",
                    "Citation Text": "Chaudhuri, K., Monteleoni, C., Sarwate, A.D.: Di\ufb00eren tially private empirical risk minimization. Jour-\nnal of Machine Learning Research 12(3) (2011)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:0912.0071",
                        "Citation Paper Title": "Title:Differentially Private Empirical Risk Minimization",
                        "Citation Paper Abstract": "Abstract:Privacy-preserving machine learning algorithms are crucial for the increasingly common setting in which personal data, such as medical or financial records, are analyzed. We provide general techniques to produce privacy-preserving approximations of classifiers learned via (regularized) empirical risk minimization (ERM). These algorithms are private under the $\\epsilon$-differential privacy definition due to Dwork et al. (2006). First we apply the output perturbation ideas of Dwork et al. (2006), to ERM classification. Then we propose a new method, objective perturbation, for privacy-preserving machine learning algorithm design. This method entails perturbing the objective function before optimizing over classifiers. If the loss and regularizer satisfy certain convexity and differentiability criteria, we prove theoretical results showing that our algorithms preserve privacy, and provide generalization bounds for linear and nonlinear kernels. We further present a privacy-preserving technique for tuning the parameters in general machine learning algorithms, thereby providing end-to-end privacy guarantees for the training process. We apply these results to produce privacy-preserving analogues of regularized logistic regression and support vector machines. We obtain encouraging results from evaluating their performance on real demographic and benchmark data sets. Our results show that both theoretically and empirically, objective perturbation is superior to the previous state-of-the-art, output perturbation, in managing the inherent tradeoff between privacy and learning performance.",
                        "Citation Paper Authors": "Authors:Kamalika Chaudhuri, Claire Monteleoni, Anand D. Sarwate"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.13631v3": {
            "Paper Title": "On the Robustness of Dataset Inference",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.11049v2": {
            "Paper Title": "How Does a Deep Learning Model Architecture Impact Its Privacy? A\n  Comprehensive Study of Privacy Attacks on CNNs and Transformers",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": ",\nwere the early attacks to employ an optimization-based tech-\nnique to reconstruct the training samples. Later research like\nInverting Gradients ",
                    "Citation Text": "Jonas Geiping, Hartmut Bauermeister, Hannah Dr\u00f6ge, and\nMichael Moeller. Inverting Gradients - How easy is it to break\nprivacy in federated learning? Advances in Neural Information\nProcessing Systems , 33:16937\u201316947, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.14053",
                        "Citation Paper Title": "Title:Inverting Gradients -- How easy is it to break privacy in federated learning?",
                        "Citation Paper Abstract": "Abstract:The idea of federated learning is to collaboratively train a neural network on a server. Each user receives the current weights of the network and in turns sends parameter updates (gradients) based on local data. This protocol has been designed not only to train neural networks data-efficiently, but also to provide privacy benefits for users, as their input data remains on device and only parameter gradients are shared. But how secure is sharing parameter gradients? Previous attacks have provided a false sense of security, by succeeding only in contrived settings - even for a single image. However, by exploiting a magnitude-invariant loss along with optimization strategies based on adversarial attacks, we show that is is actually possible to faithfully reconstruct images at high resolution from the knowledge of their parameter gradients, and demonstrate that such a break of privacy is possible even for trained deep networks. We analyze the effects of architecture as well as parameters on the difficulty of reconstructing an input image and prove that any input to a fully connected layer can be reconstructed analytically independent of the remaining architecture. Finally we discuss settings encountered in practice and show that even averaging gradients over several iterations or several images does not protect the user's privacy in federated learning applications in computer vision.",
                        "Citation Paper Authors": "Authors:Jonas Geiping, Hartmut Bauermeister, Hannah Dr\u00f6ge, Michael Moeller"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "further\ndeveloped the attack methods to extract sensitive information\nfrom Transformers. The use of Generative Adversarial Net-\nworks (GANs) in some gradient inversion attack methods ",
                    "Citation Text": "Zhuohang Li, Jiaxin Zhang, Luyang Liu, and Jian Liu. Au-\nditing Privacy Defenses in Federated Learning via Generative\nGradient Leakage. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , pages\n10132\u201310142, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.15696",
                        "Citation Paper Title": "Title:Auditing Privacy Defenses in Federated Learning via Generative Gradient Leakage",
                        "Citation Paper Abstract": "Abstract:Federated Learning (FL) framework brings privacy benefits to distributed learning systems by allowing multiple clients to participate in a learning task under the coordination of a central server without exchanging their private data. However, recent studies have revealed that private information can still be leaked through shared gradient information. To further protect user's privacy, several defense mechanisms have been proposed to prevent privacy leakage via gradient information degradation methods, such as using additive noise or gradient compression before sharing it with the server. In this work, we validate that the private training data can still be leaked under certain defense settings with a new type of leakage, i.e., Generative Gradient Leakage (GGL). Unlike existing methods that only rely on gradient information to reconstruct data, our method leverages the latent space of generative adversarial networks (GAN) learned from public image datasets as a prior to compensate for the informational loss during gradient degradation. To address the nonlinearity caused by the gradient operator and the GAN model, we explore various gradient-free optimization methods (e.g., evolution strategies and Bayesian optimization) and empirically show their superiority in reconstructing high-quality images from gradients compared to gradient-based optimizers. We hope the proposed method can serve as a tool for empirically measuring the amount of privacy leakage to facilitate the design of more robust defense mechanisms.",
                        "Citation Paper Authors": "Authors:Zhuohang Li, Jiaxin Zhang, Luyang Liu, Jian Liu"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "improved the\nattack performance by incorporating regularizations into the\noptimization process. APRIL ",
                    "Citation Text": "Jiahao Lu, Xi Sheryl Zhang, Tianli Zhao, Xiangyu He, and\nJian Cheng. APRIL: Finding the Achilles\u2019 Heel on Privacy for\nVision Transformers. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , pages\n10051\u201310060, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.14087",
                        "Citation Paper Title": "Title:APRIL: Finding the Achilles' Heel on Privacy for Vision Transformers",
                        "Citation Paper Abstract": "Abstract:Federated learning frameworks typically require collaborators to share their local gradient updates of a common model instead of sharing training data to preserve privacy. However, prior works on Gradient Leakage Attacks showed that private training data can be revealed from gradients. So far almost all relevant works base their attacks on fully-connected or convolutional neural networks. Given the recent overwhelmingly rising trend of adapting Transformers to solve multifarious vision tasks, it is highly valuable to investigate the privacy risk of vision transformers. In this paper, we analyse the gradient leakage risk of self-attention based mechanism in both theoretical and practical manners. Particularly, we propose APRIL - Attention PRIvacy Leakage, which poses a strong threat to self-attention inspired models such as ViT. Showing how vision Transformers are at the risk of privacy leakage via gradients, we urge the significance of designing privacy-safer Transformer models and defending schemes.",
                        "Citation Paper Authors": "Authors:Jiahao Lu, Xi Sheryl Zhang, Tianli Zhao, Xiangyu He, Jian Cheng"
                    }
                },
                {
                    "Sentence ID": 71,
                    "Sentence": "later claimed that\nthe overlearning feature of deep learning models caused the\nexecution of the attacks. Attributes could also be inferred\nthrough a relaxed notion ",
                    "Citation Text": "Benjamin Zi Hao Zhao, Aviral Agrawal, Catisha Coburn,\nHassan Jameel Asghar, Raghav Bhaskar, et al. On the\n(In)Feasibility of Attribute Inference Attacks on Machine\nLearning Models. In 2021 IEEE European Symposium on\nSecurity and Privacy (EuroS&P) , pages 232\u2013251, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.07101",
                        "Citation Paper Title": "Title:On the (In)Feasibility of Attribute Inference Attacks on Machine Learning Models",
                        "Citation Paper Abstract": "Abstract:With an increase in low-cost machine learning APIs, advanced machine learning models may be trained on private datasets and monetized by providing them as a service. However, privacy researchers have demonstrated that these models may leak information about records in the training dataset via membership inference attacks. In this paper, we take a closer look at another inference attack reported in literature, called attribute inference, whereby an attacker tries to infer missing attributes of a partially known record used in the training dataset by accessing the machine learning model as an API. We show that even if a classification model succumbs to membership inference attacks, it is unlikely to be susceptible to attribute inference attacks. We demonstrate that this is because membership inference attacks fail to distinguish a member from a nearby non-member. We call the ability of an attacker to distinguish the two (similar) vectors as strong membership inference. We show that membership inference attacks cannot infer membership in this strong setting, and hence inferring attributes is infeasible. However, under a relaxed notion of attribute inference, called approximate attribute inference, we show that it is possible to infer attributes close to the true attributes. We verify our results on three publicly available datasets, five membership, and three attribute inference attacks reported in literature.",
                        "Citation Paper Authors": "Authors:Benjamin Zi Hao Zhao, Aviral Agrawal, Catisha Coburn, Hassan Jameel Asghar, Raghav Bhaskar, Mohamed Ali Kaafar, Darren Webb, Peter Dickinson"
                    }
                },
                {
                    "Sentence ID": 66,
                    "Sentence": ". Research by [10, 53] has shown\nthat ViTs can surpass CNNs in various downstream tasks.\nLater research has focused on numerous improvements of\nViTs, including tokenization ",
                    "Citation Text": "Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nZihang Jiang, et al. Tokens-to-Token ViT: Training Vision\nTransformers from Scratch on ImageNet. In 2021 IEEE/CVF\nInternational Conference on Computer Vision (ICCV) , pages\n538\u2013547, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.11986",
                        "Citation Paper Title": "Title:Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet",
                        "Citation Paper Abstract": "Abstract:Transformers, which are popular for language modeling, have been explored for solving vision tasks recently, e.g., the Vision Transformer (ViT) for image classification. The ViT model splits each image into a sequence of tokens with fixed length and then applies multiple Transformer layers to model their global relation for classification. However, ViT achieves inferior performance to CNNs when trained from scratch on a midsize dataset like ImageNet. We find it is because: 1) the simple tokenization of input images fails to model the important local structure such as edges and lines among neighboring pixels, leading to low training sample efficiency; 2) the redundant attention backbone design of ViT leads to limited feature richness for fixed computation budgets and limited training samples. To overcome such limitations, we propose a new Tokens-To-Token Vision Transformer (T2T-ViT), which incorporates 1) a layer-wise Tokens-to-Token (T2T) transformation to progressively structurize the image to tokens by recursively aggregating neighboring Tokens into one Token (Tokens-to-Token), such that local structure represented by surrounding tokens can be modeled and tokens length can be reduced; 2) an efficient backbone with a deep-narrow structure for vision transformer motivated by CNN architecture design after empirical study. Notably, T2T-ViT reduces the parameter count and MACs of vanilla ViT by half, while achieving more than 3.0\\% improvement when trained from scratch on ImageNet. It also outperforms ResNets and achieves comparable performance with MobileNets by directly training on ImageNet. For example, T2T-ViT with comparable size to ResNet50 (21.5M parameters) can achieve 83.3\\% top1 accuracy in image resolution 384$\\times$384 on ImageNet. (Code: this https URL)",
                        "Citation Paper Authors": "Authors:Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, Shuicheng Yan"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": ". The attention mechanism has sig-\nnificantly contributed to the advancement of natural language\nprocessing [3, 9, 63], subsequently leading to the introduc-\ntion of Transformers in the field of computer vision as Vision\nTransformers (ViT) ",
                    "Citation Text": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, et al. An\nImage is Worth 16x16 Words: Transformers for Image Recog-\nnition at Scale. In International Conference on Learning Rep-\nresentations , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.11929",
                        "Citation Paper Title": "Title:An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
                        "Citation Paper Abstract": "Abstract:While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",
                        "Citation Paper Authors": "Authors:Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby"
                    }
                },
                {
                    "Sentence ID": 59,
                    "Sentence": ".\n2.2 Vision Transformers\nOriginating from natural language processing, Vision Trans-\nformers divide the input image into multiple patches, forming\na one-dimensional sequence of token embeddings. Their ex-\nceptional performance can be attributed to the multi-head\nself-attention modules ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\nLlion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polo-\nsukhin. Attention is All you Need. In Advances in Neural\nInformation Processing Systems , volume 30, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.13616v3": {
            "Paper Title": "Towards A Proactive ML Approach for Detecting Backdoor Poison Samples",
            "Sentences": [
                {
                    "Sentence ID": 59,
                    "Sentence": "introduce input preprocessing\nto suppress the effectiveness of backdoor triggers during the\ninference stage. There are also some certified defenses ",
                    "Citation Text": "Binghui Wang, Xiaoyu Cao, Neil Zhenqiang Gong,\net al. On certifying robustness against backdoor\nattacks via randomized smoothing. arXiv preprint\narXiv:2002.11750 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.11750",
                        "Citation Paper Title": "Title:On Certifying Robustness against Backdoor Attacks via Randomized Smoothing",
                        "Citation Paper Abstract": "Abstract:Backdoor attack is a severe security threat to deep neural networks (DNNs). We envision that, like adversarial examples, there will be a cat-and-mouse game for backdoor attacks, i.e., new empirical defenses are developed to defend against backdoor attacks but they are soon broken by strong adaptive backdoor attacks. To prevent such cat-and-mouse game, we take the first step towards certified defenses against backdoor attacks. Specifically, in this work, we study the feasibility and effectiveness of certifying robustness against backdoor attacks using a recent technique called randomized smoothing. Randomized smoothing was originally developed to certify robustness against adversarial examples. We generalize randomized smoothing to defend against backdoor attacks. Our results show the theoretical feasibility of using randomized smoothing to certify robustness against backdoor attacks. However, we also find that existing randomized smoothing methods have limited effectiveness at defending against backdoor attacks, which highlight the needs of new theory and methods to certify robustness against backdoor attacks.",
                        "Citation Paper Authors": "Authors:Binghui Wang, Xiaoyu Cao, Jinyuan jia, Neil Zhenqiang Gong"
                    }
                },
                {
                    "Sentence ID": 58,
                    "Sentence": "attempt to elimi-\nnate the model backdoor via finetuning the model on a small\nclean dataset. Udeshi et al. ",
                    "Citation Text": "Sakshi Udeshi, Shanshan Peng, Gerald Woo, Lionell\nLoh, Louth Rawshan, and Sudipta Chattopadhyay.\nModel agnostic defence against backdoor attacks in ma-\nchine learning. arXiv preprint arXiv:1908.02203 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.02203",
                        "Citation Paper Title": "Title:Model Agnostic Defence against Backdoor Attacks in Machine Learning",
                        "Citation Paper Abstract": "Abstract:Machine Learning (ML) has automated a multitude of our day-to-day decision making domains such as education, employment and driving automation. The continued success of ML largely depends on our ability to trust the model we are using. Recently, a new class of attacks called Backdoor Attacks have been developed. These attacks undermine the user's trust in ML models. In this work, we present NEO, a model agnostic framework to detect and mitigate such backdoor attacks in image classification ML models. For a given image classification model, our approach analyses the inputs it receives and determines if the model is backdoored. In addition to this feature, we also mitigate these attacks by determining the correct predictions of the poisoned images. An appealing feature of NEO is that it can, for the first time, isolate and reconstruct the backdoor trigger. NEO is also the first defence methodology, to the best of our knowledge that is completely blackbox.\nWe have implemented NEO and evaluated it against three state of the art poisoned models. These models include highly critical applications such as traffic sign detection (USTS) and facial detection. In our evaluation, we show that NEO can detect $\\approx$88% of the poisoned inputs on average and it is as fast as 4.4 ms per input image. We also reconstruct the poisoned input for the user to effectively test their systems.",
                        "Citation Paper Authors": "Authors:Sakshi Udeshi, Shanshan Peng, Gerald Woo, Lionell Loh, Louth Rawshan, Sudipta Chattopadhyay"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "directly train meta classifiers to predict whether a model is\nbackdoored. Liu et al. ",
                    "Citation Text": "Yuntao Liu, Yang Xie, and Ankur Srivastava. Neural\ntrojans. In 2017 IEEE International Conference on\nComputer Design (ICCD) , pages 45\u201348. IEEE, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.00942",
                        "Citation Paper Title": "Title:Neural Trojans",
                        "Citation Paper Abstract": "Abstract:While neural networks demonstrate stronger capabilities in pattern recognition nowadays, they are also becoming larger and deeper. As a result, the effort needed to train a network also increases dramatically. In many cases, it is more practical to use a neural network intellectual property (IP) that an IP vendor has already trained. As we do not know about the training process, there can be security threats in the neural IP: the IP vendor (attacker) may embed hidden malicious functionality, i.e. neural Trojans, into the neural IP. We show that this is an effective attack and provide three mitigation techniques: input anomaly detection, re-training, and input preprocessing. All the techniques are proven effective. The input anomaly detection approach is able to detect 99.8% of Trojan triggers although with 12.2% false positive. The re-training approach is able to prevent 94.1% of Trojan triggers from triggering the Trojan although it requires that the neural IP be reconfigurable. In the input preprocessing approach, 90.2% of Trojan triggers are rendered ineffective and no assumption about the neural IP is needed.",
                        "Citation Paper Authors": "Authors:Yuntao Liu, Yang Xie, Ankur Srivastava"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "sug-\ngests eliminating the model backdoor via pruning. Du et al. ",
                    "Citation Text": "Min Du, Ruoxi Jia, and Dawn Song. Robust anomaly\ndetection and backdoor attack detection via differential\nprivacy. arXiv preprint arXiv:1911.07116 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.07116",
                        "Citation Paper Title": "Title:Robust Anomaly Detection and Backdoor Attack Detection Via Differential Privacy",
                        "Citation Paper Abstract": "Abstract:Outlier detection and novelty detection are two important topics for anomaly detection. Suppose the majority of a dataset are drawn from a certain distribution, outlier detection and novelty detection both aim to detect data samples that do not fit the distribution. Outliers refer to data samples within this dataset, while novelties refer to new samples. In the meantime, backdoor poisoning attacks for machine learning models are achieved through injecting poisoning samples into the training dataset, which could be regarded as \"outliers\" that are intentionally added by attackers. Differential privacy has been proposed to avoid leaking any individual's information, when aggregated analysis is performed on a given dataset. It is typically achieved by adding random noise, either directly to the input dataset, or to intermediate results of the aggregation mechanism. In this paper, we demonstrate that applying differential privacy can improve the utility of outlier detection and novelty detection, with an extension to detect poisoning samples in backdoor attacks. We first present a theoretical analysis on how differential privacy helps with the detection, and then conduct extensive experiments to validate the effectiveness of differential privacy in improving outlier detection, novelty detection, and backdoor attack detection.",
                        "Citation Paper Authors": "Authors:Min Du, Ruoxi Jia, Dawn Song"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.08136v2": {
            "Paper Title": "A Utility-Preserving Obfuscation Approach for YouTube Recommendations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.03755v4": {
            "Paper Title": "Fact-Saboteurs: A Taxonomy of Evidence Manipulation Attacks against\n  Fact-Verification Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.03617v2": {
            "Paper Title": "Subject Granular Differential Privacy in Federated Learning",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": ", that does not enforce any privacy guarantees. All our\n9algorithms are implemented in our distributed FL framework built\non distributed PyTorch.\nWe focus our evaluation on Cross-Silo FL ",
                    "Citation Text": "Kairouz, Peter, McMahan, H. Brendan, Avent, Brendan, Bellet, Aur\u00e9lien, Bennis,\nMehdi, Bhagoji, Arjun Nitin, Bonawitz, Keith, Charles, Zachary, Cormode, Graham,\nCummings, Rachel, D\u2019Oliveira, Rafael G. L., Rouayheb, Salim El, Evans, David,\nGardner, Josh, Garrett, Zachary, Gasc\u00f3n, Adri\u00e0, Ghazi, Badih, Gibbons, Phillip B.,\nGruteser, Marco, Harchaoui, Za\u00efd, He, Chaoyang, He, Lie, Huo, Zhouyuan, Hutchin-\nson, Ben, Hsu, Justin, Jaggi, Martin, Javidi, Tara, Joshi, Gauri, Khodak, Mikhail,\nKonecn\u00fd, Jakub, Korolova, Aleksandra, Koushanfar, Farinaz, Koyejo, Sanmi, Lepoint,\nTancr\u00e8de, Liu, Yang, Mittal, Prateek, Mohri, Mehryar, Nock, Richard, \u00d6zg\u00fcr, Ayfer,\nPagh, Rasmus, Raykova, Mariana, Qi, Hang, Ramage, Daniel, Raskar, Ramesh, Song,\nDawn, Song, Weikang, Stich, Sebastian U., Sun, Ziteng, Suresh, Ananda Theertha,\nTram\u00e8r, Florian, Vepakomma, Praneeth, Wang, Jianyu, Xiong, Li, Xu, Zheng, Yang,\nQiang, Yu, Felix X., Yu, Han, & Zhao, Sen. 2019. Advances and Open Problems in\nFederated Learning. CoRR ,abs/1912.04977 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.04977",
                        "Citation Paper Title": "Title:Advances and Open Problems in Federated Learning",
                        "Citation Paper Abstract": "Abstract:Federated learning (FL) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this paper discusses recent advances and presents an extensive collection of open problems and challenges.",
                        "Citation Paper Authors": "Authors:Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G.L. D'Oliveira, Hubert Eichner, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adri\u00e0 Gasc\u00f3n, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Kone\u010dn\u00fd, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancr\u00e8de Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer \u00d6zg\u00fcr, Rasmus Pagh, Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tram\u00e8r, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, Sen Zhao"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.14884v2": {
            "Paper Title": "Do Software Security Practices Yield Fewer Vulnerabilities?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.09779v8": {
            "Paper Title": "Private Federated Learning Without a Trusted Server: Optimal Algorithms\n  for Convex Losses",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.04502v2": {
            "Paper Title": "A Close Look at a Systematic Method for Analyzing Sets of Security\n  Advice",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.08728v3": {
            "Paper Title": "Nonparametric extensions of randomized response for private confidence\n  sets",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.15396v2": {
            "Paper Title": "Quantum security of subset cover problems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.11990v2": {
            "Paper Title": "An Empirical Study on Real Bug Fixes from Solidity Smart Contract\n  Projects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.03301v3": {
            "Paper Title": "EvadeDroid: A Practical Evasion Attack on Machine Learning for Black-box\n  Android Malware Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.15128v3": {
            "Paper Title": "Level Up with RealAEs: Leveraging Domain Constraints in Feature Space to\n  Strengthen Robustness of Android Malware Detection",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": "fail to execute.\nTo address the above limitation, several studies [ 13,14,28\u201331]\nhave instead generated RealAEs. Specifically, they rely on problem-\nspace transformations that satisfy domain constraints. Labaca-Castro\net al. ",
                    "Citation Text": "Raphael Labaca-Castro, Luis Mu\u00f1oz-Gonz\u00e1lez, Feargus Pendlebury, Gabi Dreo\nRodosek, Fabio Pierazzi, and Lorenzo Cavallaro. Realizable universal adversarial\nperturbations for malware. arXiv preprint arXiv:2102.06747 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.06747",
                        "Citation Paper Title": "Title:Realizable Universal Adversarial Perturbations for Malware",
                        "Citation Paper Abstract": "Abstract:Machine learning classifiers are vulnerable to adversarial examples -- input-specific perturbations that manipulate models' output. Universal Adversarial Perturbations (UAPs), which identify noisy patterns that generalize across the input space, allow the attacker to greatly scale up the generation of such examples. Although UAPs have been explored in application domains beyond computer vision, little is known about their properties and implications in the specific context of realizable attacks, such as malware, where attackers must satisfy challenging problem-space constraints.\nIn this paper we explore the challenges and strengths of UAPs in the context of malware classification. We generate sequences of problem-space transformations that induce UAPs in the corresponding feature-space embedding and evaluate their effectiveness across different malware domains. Additionally, we propose adversarial training-based mitigations using knowledge derived from the problem-space transformations, and compare against alternative feature-space defenses.\nOur experiments limit the effectiveness of a white box Android evasion attack to ~20% at the cost of ~3% TPR at 1% FPR. We additionally show how our method can be adapted to more restrictive domains such as Windows malware.\nWe observe that while adversarial training in the feature space must deal with large and often unconstrained regions, UAPs in the problem space identify specific vulnerabilities that allow us to harden a classifier more effectively, shifting the challenges and associated cost of identifying new universal adversarial transformations back to the attacker.",
                        "Citation Paper Authors": "Authors:Raphael Labaca-Castro, Luis Mu\u00f1oz-Gonz\u00e1lez, Feargus Pendlebury, Gabi Dreo Rodosek, Fabio Pierazzi, Lorenzo Cavallaro"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "introduced a generic constraint language to define feature\ndependencies for botnet and credit risk detection. Erdemir et al. ",
                    "Citation Text": "Ecenaz Erdemir, Jeffrey Bickford, Luca Melis, and Sergul Aydore. Adversarial\nrobustness with non-uniform perturbations. Advances in Neural Information\nProcessing Systems (NeurIPS) , 34:19147\u201319159, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.12002",
                        "Citation Paper Title": "Title:Adversarial Robustness with Non-uniform Perturbations",
                        "Citation Paper Abstract": "Abstract:Robustness of machine learning models is critical for security related applications, where real-world adversaries are uniquely focused on evading neural network based detectors. Prior work mainly focus on crafting adversarial examples (AEs) with small uniform norm-bounded perturbations across features to maintain the requirement of imperceptibility. However, uniform perturbations do not result in realistic AEs in domains such as malware, finance, and social networks. For these types of applications, features typically have some semantically meaningful dependencies. The key idea of our proposed approach is to enable non-uniform perturbations that can adequately represent these feature dependencies during adversarial training. We propose using characteristics of the empirical data distribution, both on correlations between the features and the importance of the features themselves. Using experimental datasets for malware classification, credit risk prediction, and spam detection, we show that our approach is more robust to real-world attacks. Finally, we present robustness certification utilizing non-uniform perturbation bounds, and show that non-uniform bounds achieve better certification.",
                        "Citation Paper Authors": "Authors:Ecenaz Erdemir, Jeffrey Bickford, Luca Melis, Sergul Aydore"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "improved the adversarial robustness of DNN-based models used for\nmalware, spam, and credit risk detection by using non-uniformed\nperturbations based on the PGD attack ",
                    "Citation Text": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and\nAdrian Vladu. Towards deep learning models resistant to adversarial attacks. In\n2018 International Conference on Learning Representations (ICLR) , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06083",
                        "Citation Paper Title": "Title:Towards Deep Learning Models Resistant to Adversarial Attacks",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL.",
                        "Citation Paper Authors": "Authors:Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "employed domain knowl-\nedge to group flow-based features in NIDSs. Sheatsley et al. ",
                    "Citation Text": "Ryan Sheatsley, Nicolas Papernot, Michael Weisman, Gunjan Verma, and Patrick\nMcDaniel. Adversarial examples in constrained domains. arXiv preprint\narXiv:2011.01183 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.01183",
                        "Citation Paper Title": "Title:Adversarial Examples in Constrained Domains",
                        "Citation Paper Abstract": "Abstract:Machine learning algorithms have been shown to be vulnerable to adversarial manipulation through systematic modification of inputs (e.g., adversarial examples) in domains such as image recognition. Under the default threat model, the adversary exploits the unconstrained nature of images; each feature (pixel) is fully under control of the adversary. However, it is not clear how these attacks translate to constrained domains that limit which and how features can be modified by the adversary (e.g., network intrusion detection). In this paper, we explore whether constrained domains are less vulnerable than unconstrained domains to adversarial example generation algorithms. We create an algorithm for generating adversarial sketches: targeted universal perturbation vectors which encode feature saliency within the envelope of domain constraints. To assess how these algorithms perform, we evaluate them in constrained (e.g., network intrusion detection) and unconstrained (e.g., image recognition) domains. The results demonstrate that our approaches generate misclassification rates in constrained domains that were comparable to those of unconstrained domains (greater than 95%). Our investigation shows that the narrow attack surface exposed by constrained domains is still sufficiently large to craft successful adversarial examples; and thus, constraints do not appear to make a domain robust. Indeed, with as little as five randomly selected features, one can still generate adversarial examples.",
                        "Citation Paper Authors": "Authors:Ryan Sheatsley, Nicolas Papernot, Michael Weisman, Gunjan Verma, Patrick McDaniel"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "proposed an evasion attack to generate\nreal-world adversarial Android apps through problem-space trans-\nformations guided by feature-space perturbations. Chen et al. ",
                    "Citation Text": "Xiao Chen, Chaoran Li, Derui Wang, Sheng Wen, Jun Zhang, Surya Nepal, Yang\nXiang, and Kui Ren. Android hiv: A study of repackaging malware for evading\nmachine-learning detection. IEEE Transactions on Information Forensics and\nSecurity , 15:987\u20131001, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.04218",
                        "Citation Paper Title": "Title:Android HIV: A Study of Repackaging Malware for Evading Machine-Learning Detection",
                        "Citation Paper Abstract": "Abstract:Machine learning based solutions have been successfully employed for automatic detection of malware on Android. However, machine learning models lack robustness to adversarial examples, which are crafted by adding carefully chosen perturbations to the normal inputs. So far, the adversarial examples can only deceive detectors that rely on syntactic features (e.g., requested permissions, API calls, etc), and the perturbations can only be implemented by simply modifying application's manifest. While recent Android malware detectors rely more on semantic features from Dalvik bytecode rather than manifest, existing attacking/defending methods are no longer effective. In this paper, we introduce a new attacking method that generates adversarial examples of Android malware and evades being detected by the current models. To this end, we propose a method of applying optimal perturbations onto Android APK that can successfully deceive the machine learning detectors. We develop an automated tool to generate the adversarial examples without human intervention. In contrast to existing works, the adversarial examples crafted by our method can also deceive recent machine learning based detectors that rely on semantic features such as control-flow-graph. The perturbations can also be implemented directly onto APK's Dalvik bytecode rather than Android manifest to evade from recent detectors. We demonstrate our attack on two state-of-the-art Android malware detection schemes, MaMaDroid and Drebin. Our results show that the malware detection rates decreased from 96% to 0% in MaMaDroid, and from 97% to 0% in Drebin, with just a small number of codes to be inserted into the APK.",
                        "Citation Paper Authors": "Authors:Xiao Chen, Chaoran Li, Derui Wang, Sheng Wen, Jun Zhang, Surya Nepal, Yang Xiang, Kui Ren"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "proposed a feature-space evasion attack to generate An-\ndroid AEs by changing the features that seem important for the\nSVM classifier. Grosse et al. ",
                    "Citation Text": "Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and\nPatrick McDaniel. Adversarial examples for malware detection. In European\nsymposium on research in computer security , pages 62\u201379. Springer, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.06280",
                        "Citation Paper Title": "Title:On the (Statistical) Detection of Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:Machine Learning (ML) models are applied in a variety of tasks such as network intrusion detection or Malware classification. Yet, these models are vulnerable to a class of malicious inputs known as adversarial examples. These are slightly perturbed inputs that are classified incorrectly by the ML model. The mitigation of these adversarial inputs remains an open problem. As a step towards understanding adversarial examples, we show that they are not drawn from the same distribution than the original data, and can thus be detected using statistical tests. Using thus knowledge, we introduce a complimentary approach to identify specific inputs that are adversarial. Specifically, we augment our ML model with an additional output, in which the model is trained to classify all adversarial inputs. We evaluate our approach on multiple adversarial example crafting methods (including the fast gradient sign and saliency map methods) with several datasets. The statistical test flags sample sets containing adversarial inputs confidently at sample sizes between 10 and 100 data points. Furthermore, our augmented model either detects adversarial examples as outliers with high accuracy (> 80%) or increases the adversary's cost - the perturbation added - by more than 150%. In this way, we show that statistical properties of adversarial examples are essential to their detection.",
                        "Citation Paper Authors": "Authors:Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, Patrick McDaniel"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.14597v2": {
            "Paper Title": "Defense Against Adversarial Attacks on Audio DeepFake Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.03714v3": {
            "Paper Title": "Reconstructing Training Data from Model Gradient, Provably",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.08891v2": {
            "Paper Title": "Wink: Deniable Secure Messaging",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.04157v3": {
            "Paper Title": "Extractors: Low Entropy Requirements Colliding With Non-Malleability",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": "could only obtain\nseeded non-malleable extractors for entropy rate above 1 /2.\nInyetanotherbreakthroughresult, ",
                    "Citation Text": "Chattopadhyay, E., Goyal, V., Li, X.: Non-malleable extractors a nd codes, with their many tampered exten-\nsions. In: Proceedings of the forty-eighth annual ACM symposium on Theory of Computing. pp. 285\u2013298.\nACM (2016)\nare no longer fully uniform conditioned on the event that Eve let them through. This is not a problem, by Lemma 14, this only doubles\nextraction epsilons.\n11Constant 0 .999 is just a placeholder for any constant less then 1. By",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1505.00107",
                        "Citation Paper Title": "Title:Non-Malleable Extractors and Codes, with their Many Tampered Extensions",
                        "Citation Paper Abstract": "Abstract:Randomness extractors and error correcting codes are fundamental objects in computer science. Recently, there have been several natural generalizations of these objects, in the context and study of tamper resilient cryptography. These are seeded non-malleable extractors, introduced in [DW09]; seedless non-malleable extractors, introduced in [CG14b]; and non-malleable codes, introduced in [DPW10].\nHowever, explicit constructions of non-malleable extractors appear to be hard, and the known constructions are far behind their non-tampered counterparts.\nIn this paper we make progress towards solving the above problems. Our contributions are as follows.\n(1) We construct an explicit seeded non-malleable extractor for min-entropy $k \\geq \\log^2 n$. This dramatically improves all previous results and gives a simpler 2-round privacy amplification protocol with optimal entropy loss, matching the best known result in [Li15b].\n(2) We construct the first explicit non-malleable two-source extractor for min-entropy $k \\geq n-n^{\\Omega(1)}$, with output size $n^{\\Omega(1)}$ and error $2^{-n^{\\Omega(1)}}$.\n(3) We initiate the study of two natural generalizations of seedless non-malleable extractors and non-malleable codes, where the sources or the codeword may be tampered many times. We construct the first explicit non-malleable two-source extractor with tampering degree $t$ up to $n^{\\Omega(1)}$, which works for min-entropy $k \\geq n-n^{\\Omega(1)}$, with output size $n^{\\Omega(1)}$ and error $2^{-n^{\\Omega(1)}}$. We show that we can efficiently sample uniformly from any pre-image. By the connection in [CG14b], we also obtain the first explicit non-malleable codes with tampering degree $t$ up to $n^{\\Omega(1)}$, relative rate $n^{\\Omega(1)}/n$, and error $2^{-n^{\\Omega(1)}}$.",
                        "Citation Paper Authors": "Authors:Eshan Chattopadhyay, Vipul Goyal, Xin Li"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": ", and used these to construct privacy ampli\ufb01cation protocols.\nNon-malleable two-source extractors. A natural strengthening of both seeded non-malleable extractor s,\nand two-source extractors are two-source non-malleable extractors. Two-source non-malleable extractors were\nintroduced by Cheraghchi and Guruswami ",
                    "Citation Text": "Cheraghchi, M., Guruswami, V.: Non-malleable coding against bit-w ise and split-state tampering. Journal of\nCryptology 30(1), 191\u2013241 (Jan 2017)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1309.1151",
                        "Citation Paper Title": "Title:Non-Malleable Coding Against Bit-wise and Split-State Tampering",
                        "Citation Paper Abstract": "Abstract:Non-malleable coding, introduced by Dziembowski, Pietrzak and Wichs (ICS 2010), aims for protecting the integrity of information against tampering attacks in situations where error-detection is impossible. Intuitively, information encoded by a non-malleable code either decodes to the original message or, in presence of any tampering, to an unrelated message. Dziembowski et al. show existence of non-malleable codes for any class of tampering functions of bounded size.\nWe consider constructions of coding schemes against two well-studied classes of tampering functions: bit-wise tampering functions (where the adversary tampers each bit of the encoding independently) and split-state adversaries (where two independent adversaries arbitrarily tamper each half of the encoded sequence).\n1. For bit-tampering, we obtain explicit and efficiently encodable and decodable codes of length $n$ achieving rate $1-o(1)$ and error (security) $\\exp(-\\tilde{\\Omega}(n^{1/7}))$. We improve the error to $\\exp(-\\tilde{\\Omega}(n))$ at the cost of making the construction Monte Carlo with success probability $1-\\exp(-\\Omega(n))$. Previously, the best known construction of bit-tampering codes was the Monte Carlo construction of Dziembowski et al. (ICS 2010) achieving rate ~.1887.\n2. We initiate the study of seedless non-malleable extractors as a variation of non-malleable extractors introduced by Dodis and Wichs (STOC 2009). We show that construction of non-malleable codes for the split-state model reduces to construction of non-malleable two-source extractors. We prove existence of such extractors, which implies that codes obtained from our reduction can achieve rates arbitrarily close to 1/5 and exponentially small error. Currently, the best known explicit construction of split-state coding schemes is due to Aggarwal, Dodis and Lovett (ECCC TR13-081) which only achieves vanishing (polynomially small) rate.",
                        "Citation Paper Authors": "Authors:Mahdi Cheraghchi, Venkatesan Guruswami"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.10880v2": {
            "Paper Title": "Learning to Invert: Simple Adaptive Attacks for Gradient Inversion in\n  Federated Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.15444v3": {
            "Paper Title": "10 Security and Privacy Problems in Large Foundation Models",
            "Sentences": [
                {
                    "Sentence ID": 82,
                    "Sentence": "proposed backdoor\nattacks against LSTM-based text classification via poisoning its training dataset. In\nparticular, they showed that the backdoored classifier classifies any text embedded\nwith a pre-defined, attacker-chosen trigger into the attacker-chosen target class.\nZhang et al. ",
                    "Citation Text": "Zaixi Zhang, Jinyuan Jia, Binghui Wang, and Neil Zhenqiang Gong. Backdoor\nattacks to graph neural networks. In SACMAT , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.11165",
                        "Citation Paper Title": "Title:Backdoor Attacks to Graph Neural Networks",
                        "Citation Paper Abstract": "Abstract:In this work, we propose the first backdoor attack to graph neural networks (GNN). Specifically, we propose a \\emph{subgraph based backdoor attack} to GNN for graph classification. In our backdoor attack, a GNN classifier predicts an attacker-chosen target label for a testing graph once a predefined subgraph is injected to the testing graph. Our empirical results on three real-world graph datasets show that our backdoor attacks are effective with a small impact on a GNN's prediction accuracy for clean testing graphs. Moreover, we generalize a randomized smoothing based certified defense to defend against our backdoor attacks. Our empirical results show that the defense is effective in some cases but ineffective in other cases, highlighting the needs of new defenses for our backdoor attacks.",
                        "Citation Paper Authors": "Authors:Zaixi Zhang, Jinyuan Jia, Binghui Wang, Neil Zhenqiang Gong"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2207.04396v4": {
            "Paper Title": "Graph Generative Model for Benchmarking Graph Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.05400v3": {
            "Paper Title": "How to Backdoor Diffusion Models?",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": "optimizerwith learning rate 2 e-4 and 8 e-5 for CIFAR10 and CelebA-\nHQ ",
                    "Citation Text": "Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.\nDeep learning face attributes in the wild. In ICCV , 2015.\n2, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1411.7766",
                        "Citation Paper Title": "Title:Deep Learning Face Attributes in the Wild",
                        "Citation Paper Abstract": "Abstract:Predicting face attributes in the wild is challenging due to complex face variations. We propose a novel deep learning framework for attribute prediction in the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointly with attribute tags, but pre-trained differently. LNet is pre-trained by massive general object categories for face localization, while ANet is pre-trained by massive face identities for attribute prediction. This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation.\n(1) It shows how the performances of face localization (LNet) and attribute prediction (ANet) can be improved by different pre-training strategies.\n(2) It reveals that although the filters of LNet are fine-tuned only with image-level attribute tags, their response maps over entire images have strong indication of face locations. This fact enables training LNet for face localization with only image-level annotations, but without face bounding boxes or landmarks, which are required by all attribute recognition works.\n(3) It also demonstrates that the high-level hidden neurons of ANet automatically discover semantic concepts after pre-training with massive face identities, and such concepts are significantly enriched after fine-tuning with attribute tags. Each attribute can be well explained with a sparse linear combination of these concepts.",
                        "Citation Paper Authors": "Authors:Ziwei Liu, Ping Luo, Xiaogang Wang, Xiaoou Tang"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "that was fine-tuned from the released stable dif-\nfusion model ",
                    "Citation Text": "Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj \u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR , 2021. 1, 3\n9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.10752",
                        "Citation Paper Title": "Title:High-Resolution Image Synthesis with Latent Diffusion Models",
                        "Citation Paper Abstract": "Abstract:By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at this https URL .",
                        "Citation Paper Authors": "Authors:Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": ".\n2.3. Backdoor Attack on Generative Models\nVery recently, several works begin to explore backdoor\nattacks on some generative models like generative adver-\nsarial nets (GANs) [11, 32]. The work in ",
                    "Citation Text": "Ambrish Rawat, Killian Levacher, and Mathieu Sinn. The\ndevil is in the GAN: backdoor attacks and defenses in deep\ngenerative models. In ESORICS , 2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.01644",
                        "Citation Paper Title": "Title:The Devil is in the GAN: Backdoor Attacks and Defenses in Deep Generative Models",
                        "Citation Paper Abstract": "Abstract:Deep Generative Models (DGMs) are a popular class of deep learning models which find widespread use because of their ability to synthesize data from complex, high-dimensional manifolds. However, even with their increasing industrial adoption, they haven't been subject to rigorous security and privacy analysis. In this work we examine one such aspect, namely backdoor attacks on DGMs which can significantly limit the applicability of pre-trained models within a model supply chain and at the very least cause massive reputation damage for companies outsourcing DGMs form third parties.\nWhile similar attacks scenarios have been studied in the context of classical prediction models, their manifestation in DGMs hasn't received the same attention. To this end we propose novel training-time attacks which result in corrupted DGMs that synthesize regular data under normal operations and designated target outputs for inputs sampled from a trigger distribution. These attacks are based on an adversarial loss function that combines the dual objectives of attack stealth and fidelity. We systematically analyze these attacks, and show their effectiveness for a variety of approaches like Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as well as different data domains including images and audio. Our experiments show that - even for large-scale industry-grade DGMs (like StyleGAN) - our attacks can be mounted with only modest computational effort. We also motivate suitable defenses based on static/dynamic model and output inspections, demonstrate their usefulness, and prescribe a practical and comprehensive defense strategy that paves the way for safe usage of DGMs.",
                        "Citation Paper Authors": "Authors:Ambrish Rawat, Killian Levacher, Mathieu Sinn"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": ". In general, diffusion models\nregard sample generation as a diffusion process modeled\nby stochastic differential equations (SDEs) ",
                    "Citation Text": "Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. In ICLR , 2021. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.13456",
                        "Citation Paper Title": "Title:Score-Based Generative Modeling through Stochastic Differential Equations",
                        "Citation Paper Abstract": "Abstract:Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\\aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.",
                        "Citation Paper Authors": "Authors:Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": ", image synthesis [1, 8, 16, 17, 33, 36, 37, 40],\nand audio generation ",
                    "Citation Text": "Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and\nBryan Catanzaro. Diffwave: A versatile diffusion model for\naudio synthesis. In ICLR , 2021. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.09761",
                        "Citation Paper Title": "Title:DiffWave: A Versatile Diffusion Model for Audio Synthesis",
                        "Citation Paper Abstract": "Abstract:In this work, we propose DiffWave, a versatile diffusion probabilistic model for conditional and unconditional waveform generation. The model is non-autoregressive, and converts the white noise signal into structured waveform through a Markov chain with a constant number of steps at synthesis. It is efficiently trained by optimizing a variant of variational bound on the data likelihood. DiffWave produces high-fidelity audios in different waveform generation tasks, including neural vocoding conditioned on mel spectrogram, class-conditional generation, and unconditional generation. We demonstrate that DiffWave matches a strong WaveNet vocoder in terms of speech quality (MOS: 4.44 versus 4.43), while synthesizing orders of magnitude faster. In particular, it significantly outperforms autoregressive and GAN-based waveform models in the challenging unconditional generation task in terms of audio quality and sample diversity from various automatic and human evaluations.",
                        "Citation Paper Authors": "Authors:Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, Bryan Catanzaro"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "2.1. Diffusion Models\nDiffusion models have recently achieved significant ad-\nvances in several tasks and domains, such as density esti-\nmation ",
                    "Citation Text": "Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan\nHo. Variational diffusion models. 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.00630",
                        "Citation Paper Title": "Title:Variational Diffusion Models",
                        "Citation Paper Abstract": "Abstract:Diffusion-based generative models have demonstrated a capacity for perceptually impressive synthesis, but can they also be great likelihood-based models? We answer this in the affirmative, and introduce a family of diffusion-based generative models that obtain state-of-the-art likelihoods on standard image density estimation benchmarks. Unlike other diffusion-based models, our method allows for efficient optimization of the noise schedule jointly with the rest of the model. We show that the variational lower bound (VLB) simplifies to a remarkably short expression in terms of the signal-to-noise ratio of the diffused data, thereby improving our theoretical understanding of this model class. Using this insight, we prove an equivalence between several models proposed in the literature. In addition, we show that the continuous-time VLB is invariant to the noise schedule, except for the signal-to-noise ratio at its endpoints. This enables us to learn a noise schedule that minimizes the variance of the resulting VLB estimator, leading to faster optimization. Combining these advances with architectural improvements, we obtain state-of-the-art likelihoods on image density estimation benchmarks, outperforming autoregressive models that have dominated these benchmarks for many years, with often significantly faster optimization. In addition, we show how to use the model as part of a bits-back compression scheme, and demonstrate lossless compression rates close to the theoretical optimum. Code is available at this https URL .",
                        "Citation Paper Authors": "Authors:Diederik P. Kingma, Tim Salimans, Ben Poole, Jonathan Ho"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.06530v2": {
            "Paper Title": "Multi-Epoch Matrix Factorization Mechanisms for Private Machine Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.15317v4": {
            "Paper Title": "Bridge the Gap Between CV and NLP! A Gradient-based Textual Adversarial\n  Attack Framework",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.00309v2": {
            "Paper Title": "Differentially Private Adaptive Optimization with Delayed\n  Preconditioners",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.08374v2": {
            "Paper Title": "Beyond the Surface: Investigating Malicious CVE Proof of Concept\n  Exploits on GitHub",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "have reached similar conclu-\nsions, as they found that CVE-related information is being\ndisseminated in GitHub discussions even prior to a vulnera-\nbility being officially published. Neil, Mittal and Jishi ",
                    "Citation Text": "Lorenzo Neil, Sudip Mittal, and Anupam Joshi. Min-\ning threat intelligence about open-source projects and\nlibraries from code repository issues and bug reports.\nIn2018 IEEE International Conference on Intelligence\nand Security Informatics (ISI) , pages 7\u201312. IEEE, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.04673",
                        "Citation Paper Title": "Title:Mining Threat Intelligence about Open-Source Projects and Libraries from Code Repository Issues and Bug Reports",
                        "Citation Paper Abstract": "Abstract:Open-Source Projects and Libraries are being used in software development while also bearing multiple security vulnerabilities. This use of third party ecosystem creates a new kind of attack surface for a product in development. An intelligent attacker can attack a product by exploiting one of the vulnerabilities present in linked projects and libraries.\nIn this paper, we mine threat intelligence about open source projects and libraries from bugs and issues reported on public code repositories. We also track library and project dependencies for installed software on a client machine. We represent and store this threat intelligence, along with the software dependencies in a security knowledge graph. Security analysts and developers can then query and receive alerts from the knowledge graph if any threat intelligence is found about linked libraries and projects, utilized in their products.",
                        "Citation Paper Authors": "Authors:Lorenzo Neil, Sudip Mittal, Anupam Joshi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.08371v2": {
            "Paper Title": "Sketching for First Order Method: Efficient Algorithm for Low-Bandwidth\n  Channel and Vulnerability",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.02766v2": {
            "Paper Title": "Quantum Measurement Adversary",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.10143v2": {
            "Paper Title": "Lattice-Based Quantum Advantage from Rotated Measurements",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.10858v2": {
            "Paper Title": "Robust Universal Adversarial Perturbations",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": "introduce\nExpectation over Transformation (EOT) and use it to print real-world objects which are adversarial\ngiven a range of physical and environmental conditions.\nRobust Adversarial Perturbations. Li et al. ",
                    "Citation Text": "Juncheng Li, Shuhui Qu, Xinjian Li, Joseph Szurley, J. Zico Kolter, and Florian Metze. Adver-\nsarial music: Real world audio adversary against wake-word detection system. In Proc. Neural\nInformation Processing Systems (NeurIPS) , pages 11908\u201311918, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.00126",
                        "Citation Paper Title": "Title:Adversarial Music: Real World Audio Adversary Against Wake-word Detection System",
                        "Citation Paper Abstract": "Abstract:Voice Assistants (VAs) such as Amazon Alexa or Google Assistant rely on wake-word detection to respond to people's commands, which could potentially be vulnerable to audio adversarial examples. In this work, we target our attack on the wake-word detection system, jamming the model with some inconspicuous background music to deactivate the VAs while our audio adversary is present. We implemented an emulated wake-word detection system of Amazon Alexa based on recent publications. We validated our models against the real Alexa in terms of wake-word detection accuracy. Then we computed our audio adversaries with consideration of expectation over transform and we implemented our audio adversary with a differentiable synthesizer. Next, we verified our audio adversaries digitally on hundreds of samples of utterances collected from the real world. Our experiments show that we can effectively reduce the recognition F1 score of our emulated model from 93.4% to 11.0%. Finally, we tested our audio adversary over the air, and verified it works effectively against Alexa, reducing its F1 score from 92.5% to 11.0%.; We also verified that non-adversarial music does not disable Alexa as effectively as our music at the same sound level. To the best of our knowledge, this is the first real-world adversarial attack against a commercial-grade VA wake-word detection system. Our code and demo videos can be accessed at \\url{this https URL}",
                        "Citation Paper Authors": "Authors:Juncheng B. Li, Shuhui Qu, Xinjian Li, Joseph Szurley, J. Zico Kolter, Florian Metze"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "proposes Robust Physical Perturbations ( RP2) in order to show that adding graffiti on a stop sign\ncan cause it to be misclassified in both simulations and in the real world. Athalye et al. ",
                    "Citation Text": "Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adver-\nsarial examples. In International conference on machine learning , pages 284\u2013293. PMLR,\n2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1707.07397",
                        "Citation Paper Title": "Title:Synthesizing Robust Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:Standard methods for generating adversarial examples for neural networks do not consistently fool neural network classifiers in the physical world due to a combination of viewpoint shifts, camera noise, and other natural transformations, limiting their relevance to real-world systems. We demonstrate the existence of robust 3D adversarial objects, and we present the first algorithm for synthesizing examples that are adversarial over a chosen distribution of transformations. We synthesize two-dimensional adversarial images that are robust to noise, distortion, and affine transformation. We apply our algorithm to complex three-dimensional objects, using 3D-printing to manufacture the first physical adversarial objects. Our results demonstrate the existence of 3D adversarial objects in the physical world.",
                        "Citation Paper Authors": "Authors:Anish Athalye, Logan Engstrom, Andrew Ilyas, Kevin Kwok"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "UAP Algorithms. Most works focusing on UAPs [ 36,37,48,28,3,23,49] generate singular vectors\nand do not consider perturbation robustness. Bahramali et al. ",
                    "Citation Text": "Alireza Bahramali, Milad Nasr, Amir Houmansadr, Dennis Goeckel, and Don Towsley. Ro-\nbust adversarial attacks against dnn-based wireless communication systems. arXiv preprint\narXiv:2102.00918 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.00918",
                        "Citation Paper Title": "Title:Robust Adversarial Attacks Against DNN-Based Wireless Communication Systems",
                        "Citation Paper Abstract": "Abstract:Deep Neural Networks (DNNs) have become prevalent in wireless communication systems due to their promising performance. However, similar to other DNN-based applications, they are vulnerable to adversarial examples. In this work, we propose an input-agnostic, undetectable, and robust adversarial attack against DNN-based wireless communication systems in both white-box and black-box scenarios. We design tailored Universal Adversarial Perturbations (UAPs) to perform the attack. We also use a Generative Adversarial Network (GAN) to enforce an undetectability constraint for our attack. Furthermore, we investigate the robustness of our attack against countermeasures. We show that in the presence of defense mechanisms deployed by the communicating parties, our attack performs significantly better compared to existing attacks against DNN-based wireless systems. In particular, the results demonstrate that even when employing well-considered defenses, DNN-based wireless communications are vulnerable to adversarial attacks.",
                        "Citation Paper Authors": "Authors:Alireza Bahramali, Milad Nasr, Amir Houmansadr, Dennis Goeckel, Don Towsley"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.15266v2": {
            "Paper Title": "Data Poisoning Attacks Against Multimodal Encoders",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": ", which is the most representative and\nwidely used multimodal application. We leverage the pre-\ntrained CLIP2as the starting point, where the image encoder\nis Vision Transformer ViT-B/32 architecture ",
                    "Citation Text": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is\nWorth 16x16 Words: Transformers for Image Recognition at\nScale. In International Conference on Learning Representa-\ntions (ICLR) , 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.11929",
                        "Citation Paper Title": "Title:An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
                        "Citation Paper Abstract": "Abstract:While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",
                        "Citation Paper Authors": "Authors:Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": ".Image search engine. The task of an image search engine\nis also known as a text-image retrieval task. It is designed\nfor scenarios where the queries are from one modality, and\nthe retrieval galleries are from another ",
                    "Citation Text": "Min Cao, Shiping Li, Juntao Li, Liqiang Nie, and Min Zhang.\nImage-text Retrieval: A Survey on Recent Research and De-\nvelopment. In International Joint Conferences on Artifical In-\ntelligence (IJCAI) , pages 5410\u20135417. IJCAI, 2022. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.14713",
                        "Citation Paper Title": "Title:Image-text Retrieval: A Survey on Recent Research and Development",
                        "Citation Paper Abstract": "Abstract:In the past few years, cross-modal image-text retrieval (ITR) has experienced increased interest in the research community due to its excellent research value and broad real-world application. It is designed for the scenarios where the queries are from one modality and the retrieval galleries from another modality. This paper presents a comprehensive and up-to-date survey on the ITR approaches from four perspectives. By dissecting an ITR system into two processes: feature extraction and feature alignment, we summarize the recent advance of the ITR approaches from these two perspectives. On top of this, the efficiency-focused study on the ITR system is introduced as the third perspective. To keep pace with the times, we also provide a pioneering overview of the cross-modal pre-training ITR approaches as the fourth perspective. Finally, we outline the common benchmark datasets and valuation metric for ITR, and conduct the accuracy comparison among the representative ITR approaches. Some critical yet less studied issues are discussed at the end of the paper.",
                        "Citation Paper Authors": "Authors:Min Cao, Shiping Li, Juntao Li, Liqiang Nie, Min Zhang"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ". The\nlearned visual-linguistic representations also help image gen-\neration [19, 24], image captioning ",
                    "Citation Text": "Ron Mokady, Amir Hertz, and Amit H. Bermano. ClipCap:\nCLIP Prefix for Image Captioning. CoRR abs/2111.09734 ,\n2021. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.09734",
                        "Citation Paper Title": "Title:ClipCap: CLIP Prefix for Image Captioning",
                        "Citation Paper Abstract": "Abstract:Image captioning is a fundamental task in vision-language understanding, where the model predicts a textual informative caption to a given input image. In this paper, we present a simple approach to address this task. We use CLIP encoding as a prefix to the caption, by employing a simple mapping network, and then fine-tunes a language model to generate the image captions. The recently proposed CLIP model contains rich semantic features which were trained with textual context, making it best for vision-language perception. Our key idea is that together with a pre-trained language model (GPT2), we obtain a wide understanding of both visual and textual data. Hence, our approach only requires rather quick training to produce a competent captioning model. Without additional annotations or pre-training, it efficiently generates meaningful captions for large-scale and diverse datasets. Surprisingly, our method works well even when only the mapping network is trained, while both CLIP and the language model remain frozen, allowing a lighter architecture with less trainable parameters. Through quantitative evaluation, we demonstrate our model achieves comparable results to state-of-the-art methods on the challenging Conceptual Captions and nocaps datasets, while it is simpler, faster, and lighter. Our code is available in this https URL.",
                        "Citation Paper Authors": "Authors:Ron Mokady, Amir Hertz, Amit H. Bermano"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.08781v2": {
            "Paper Title": "Stochastic Differentially Private and Fair Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.14736v2": {
            "Paper Title": "PRISM: Privacy Preserving Healthcare Internet of Things Security\n  Management",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.03317v3": {
            "Paper Title": "Subject Membership Inference Attacks in Federated Learning",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": "that are highly relevant to our work.\nWhile these advanced attacks are worth investigating to\nimprove subject-level membership inference, they are over-\nwhelmingly focused on membership inference of particular data\npoints ",
                    "Citation Text": "T. Nguyen, P. Lai, K. Tran, N. Phan, and M. T. Thai, \u201cActive membership\ninference attack under local differential privacy in federated learning,\u201d in\nInternational Conference on Artificial Intelligence and Statistics . PMLR,\n2023, pp. 5714\u20135730.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.12685",
                        "Citation Paper Title": "Title:Active Membership Inference Attack under Local Differential Privacy in Federated Learning",
                        "Citation Paper Abstract": "Abstract:Federated learning (FL) was originally regarded as a framework for collaborative learning among clients with data privacy protection through a coordinating server. In this paper, we propose a new active membership inference (AMI) attack carried out by a dishonest server in FL. In AMI attacks, the server crafts and embeds malicious parameters into global models to effectively infer whether a target data sample is included in a client's private training data or not. By exploiting the correlation among data features through a non-linear decision boundary, AMI attacks with a certified guarantee of success can achieve severely high success rates under rigorous local differential privacy (LDP) protection; thereby exposing clients' training data to significant privacy risk. Theoretical and experimental results on several benchmark datasets show that adding sufficient privacy-preserving noise to prevent our attack would significantly damage FL's model utility.",
                        "Citation Paper Authors": "Authors:Truc Nguyen, Phung Lai, Khang Tran, NhatHai Phan, My T. Thai"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": ". White-box attacks are quiteplausible if the adversary is posing as a legitimate user in\nfederated learning, and this opens up new avenues of risk ",
                    "Citation Text": "A. Wainakh, F. Ventola, T. M \u00a8u\u00dfig, J. Keim, C. G. Cordero, E. Zimmer,\nT. Grube, K. Kersting, and M. M \u00a8uhlh\u00a8auser, \u201cUser Label Leakage from\nGradients in Federated Learning,\u201d arXiv preprint arXiv:2105.09369 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.09369",
                        "Citation Paper Title": "Title:User-Level Label Leakage from Gradients in Federated Learning",
                        "Citation Paper Abstract": "Abstract:Federated learning enables multiple users to build a joint model by sharing their model updates (gradients), while their raw data remains local on their devices. In contrast to the common belief that this provides privacy benefits, we here add to the very recent results on privacy risks when sharing gradients. Specifically, we investigate Label Leakage from Gradients (LLG), a novel attack to extract the labels of the users' training data from their shared gradients. The attack exploits the direction and magnitude of gradients to determine the presence or absence of any label. LLG is simple yet effective, capable of leaking potential sensitive information represented by labels, and scales well to arbitrary batch sizes and multiple classes. We mathematically and empirically demonstrate the validity of the attack under different settings. Moreover, empirical results show that LLG successfully extracts labels with high accuracy at the early stages of model training. We also discuss different defense mechanisms against such leakage. Our findings suggest that gradient compression is a practical technique to mitigate the attack.",
                        "Citation Paper Authors": "Authors:Aidmar Wainakh, Fabrizio Ventola, Till M\u00fc\u00dfig, Jens Keim, Carlos Garcia Cordero, Ephraim Zimmer, Tim Grube, Kristian Kersting, Max M\u00fchlh\u00e4user"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": ". In particular, white-box attacks\nthat rely on the gradients sent during training show that these\ngradients reveal a lot of detail about the training data ",
                    "Citation Text": "L. Zhu, Z. Liu, and S. Han, \u201cDeep Leakage from Gradients,\u201d Advances\nin Neural Information Processing Systems , vol. 32, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.08935",
                        "Citation Paper Title": "Title:Deep Leakage from Gradients",
                        "Citation Paper Abstract": "Abstract:Exchanging gradients is a widely used method in modern multi-node machine learning system (e.g., distributed training, collaborative learning). For a long time, people believed that gradients are safe to share: i.e., the training data will not be leaked by gradient exchange. However, we show that it is possible to obtain the private training data from the publicly shared gradients. We name this leakage as Deep Leakage from Gradient and empirically validate the effectiveness on both computer vision and natural language processing tasks. Experimental results show that our attack is much stronger than previous approaches: the recovery is pixel-wise accurate for images and token-wise matching for texts. We want to raise people's awareness to rethink the gradient's safety. Finally, we discuss several possible strategies to prevent such deep leakage. The most effective defense method is gradient pruning.",
                        "Citation Paper Authors": "Authors:Ligeng Zhu, Zhijian Liu, Song Han"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.16205v2": {
            "Paper Title": "Local Model Reconstruction Attacks in Federated Learning and their Uses",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.02341v2": {
            "Paper Title": "From Malware Samples to Fractal Images: A New Paradigm for\n  Classification. (Version 2.0, Previous version paper name: Have you ever seen\n  malware?)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.08018v2": {
            "Paper Title": "Privately Estimating a Gaussian: Efficient, Robust and Optimal",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.10750v2": {
            "Paper Title": "Canary in a Coalmine: Better Membership Inference with Ensembled\n  Adversarial Queries",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.12700v3": {
            "Paper Title": "Epistemic Parity: Reproducibility as an Evaluation Metric for\n  Differential Privacy",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": "studied privacy-utility trade-offs for ML tasks and found\nthat commonly-used \ud835\udf16values and implementations in practice may\nbe ineffective: either unacceptable privacy leakage or unacceptable\nutility tends to occur. Hay et al . ",
                    "Citation Text": "Michael Hay, Ashwin Machanavajjhala, Gerome Miklau, Yan Chen, and Dan\nZhang. 2016. Principled evaluation of differentially private algorithms using\ndpbench. In Proceedings of the 2016 International Conference on Management of\nData . 139\u2013154.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.04817",
                        "Citation Paper Title": "Title:Principled Evaluation of Differentially Private Algorithms using DPBench",
                        "Citation Paper Abstract": "Abstract:Differential privacy has become the dominant standard in the research community for strong privacy protection. There has been a flood of research into query answering algorithms that meet this standard. Algorithms are becoming increasingly complex, and in particular, the performance of many emerging algorithms is {\\em data dependent}, meaning the distribution of the noise added to query answers may change depending on the input data. Theoretical analysis typically only considers the worst case, making empirical study of average case performance increasingly important.\nIn this paper we propose a set of evaluation principles which we argue are essential for sound evaluation. Based on these principles we propose DPBench, a novel evaluation framework for standardized evaluation of privacy algorithms. We then apply our benchmark to evaluate algorithms for answering 1- and 2-dimensional range queries. The result is a thorough empirical study of 15 published algorithms on a total of 27 datasets that offers new insights into algorithm behavior---in particular the influence of dataset scale and shape---and a more complete characterization of the state of the art. Our methodology is able to resolve inconsistencies in prior empirical studies and place algorithm performance in context through comparison to simple baselines. Finally, we pose open research questions which we hope will guide future algorithm design.",
                        "Citation Paper Authors": "Authors:Michael Hay, Ashwin Machanavajjhala, Gerome Miklau, Yan Chen, Dan Zhang"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": ". However, unlike MST, AIM is\nworkload aware : It parameterizes a private synthetic distribution\nthrough an iterative process akin to that of the fundamental DP\nsynthesis Multiplicative Weights Exponential Mechanism (MWEM)\nalgorithm ",
                    "Citation Text": "Moritz Hardt, Katrina Ligett, and Frank McSherry. 2010. A simple and practical\nalgorithm for differentially private data release. arXiv preprint arXiv:1012.4763\n(2010).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1012.4763",
                        "Citation Paper Title": "Title:A simple and practical algorithm for differentially private data release",
                        "Citation Paper Abstract": "Abstract:We present new theoretical results on differentially private data release useful with respect to any target class of counting queries, coupled with experimental results on a variety of real world data sets.\nSpecifically, we study a simple combination of the multiplicative weights approach of [Hardt and Rothblum, 2010] with the exponential mechanism of [McSherry and Talwar, 2007]. The multiplicative weights framework allows us to maintain and improve a distribution approximating a given data set with respect to a set of counting queries. We use the exponential mechanism to select those queries most incorrectly tracked by the current distribution. Combing the two, we quickly approach a distribution that agrees with the data set on the given set of queries up to small error.\nThe resulting algorithm and its analysis is simple, but nevertheless improves upon previous work in terms of both error and running time. We also empirically demonstrate the practicality of our approach on several data sets commonly used in the statistical community for contingency table release.",
                        "Citation Paper Authors": "Authors:Moritz Hardt, Katrina Ligett, Frank McSherry"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.06516v2": {
            "Paper Title": "How to Sift Out a Clean Data Subset in the Presence of Data Poisoning?",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": "on the model trained on the\nentire dataset. Model inversion is a type of privacy attack\naimed at reconstructing the representative points for each\nclass from the trained model.\n\u2022Spectral Filtering\u2019s Least Scores (SF-Least ) ",
                    "Citation Text": "B. Tran, J. Li, and A. Madry, \u201cSpectral signatures in\nbackdoor attacks,\u201d in NeuIPS , 2018, pp. 8000\u20138010.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.00636",
                        "Citation Paper Title": "Title:Spectral Signatures in Backdoor Attacks",
                        "Citation Paper Abstract": "Abstract:A recent line of work has uncovered a new form of data poisoning: so-called \\emph{backdoor} attacks. These attacks are particularly dangerous because they do not affect a network's behavior on typical, benign data. Rather, the network only deviates from its expected output when triggered by a perturbation planted by an adversary.\nIn this paper, we identify a new property of all known backdoor attacks, which we call \\emph{spectral signatures}. This property allows us to utilize tools from robust statistics to thwart the attacks. We demonstrate the efficacy of these signatures in detecting and removing poisoned examples on real image sets and state of the art neural network architectures. We believe that understanding spectral signatures is a crucial first step towards designing ML systems secure against such backdoor attacks",
                        "Citation Paper Authors": "Authors:Brandon Tran, Jerry Li, Aleksander Madry"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": ", we implement each gradient-sampler as a two-\nlayer fully-connected neural network. The activation units\nin the first and second layer are PReLU ",
                    "Citation Text": "K. He, X. Zhang, S. Ren, and J. Sun, \u201cDelving deep\ninto rectifiers: Surpassing human-level performance on\nimagenet classification,\u201d in ICCV , 2015, pp. 1026\u20131034.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1502.01852",
                        "Citation Paper Title": "Title:Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
                        "Citation Paper Abstract": "Abstract:Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.",
                        "Citation Paper Authors": "Authors:Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2201.12675v2": {
            "Paper Title": "Decepticons: Corrupted Transformers Breach Privacy in Federated Learning\n  for Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.14772v2": {
            "Paper Title": "IBP Regularization for Verified Adversarial Robustness via\n  Branch-and-Bound",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.00846v2": {
            "Paper Title": "Faster Rates of Convergence to Stationary Points in Differentially\n  Private Optimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.07341v3": {
            "Paper Title": "Does CLIP Know My Face?",
            "Sentences": [
                {
                    "Sentence ID": 57,
                    "Sentence": "have even shown that isolated training images can\nbe re-generated by large diffusion models, i.e., Stable Dif-\nfusion ",
                    "Citation Text": "Robin Rombach, Andreas Blattmann, Dominik\nLorenz, Patrick Esser, and Bj \u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion mod-\nels. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR) ,\npages 10684\u201310695, June 2022.\n15",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.10752",
                        "Citation Paper Title": "Title:High-Resolution Image Synthesis with Latent Diffusion Models",
                        "Citation Paper Abstract": "Abstract:By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at this https URL .",
                        "Citation Paper Authors": "Authors:Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "also noted that the model can be used to classify\ncelebrities from the CelebA dataset ",
                    "Citation Text": "Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learn-\ning face attributes in the wild. In 2015 IEEE Interna-\ntional Conference on Computer Vision (ICCV) , pages\n3730\u20133738, Los Alamitos, CA, USA, dec 2015. IEEE\nComputer Society.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1411.7766",
                        "Citation Paper Title": "Title:Deep Learning Face Attributes in the Wild",
                        "Citation Paper Abstract": "Abstract:Predicting face attributes in the wild is challenging due to complex face variations. We propose a novel deep learning framework for attribute prediction in the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointly with attribute tags, but pre-trained differently. LNet is pre-trained by massive general object categories for face localization, while ANet is pre-trained by massive face identities for attribute prediction. This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation.\n(1) It shows how the performances of face localization (LNet) and attribute prediction (ANet) can be improved by different pre-training strategies.\n(2) It reveals that although the filters of LNet are fine-tuned only with image-level attribute tags, their response maps over entire images have strong indication of face locations. This fact enables training LNet for face localization with only image-level annotations, but without face bounding boxes or landmarks, which are required by all attribute recognition works.\n(3) It also demonstrates that the high-level hidden neurons of ANet automatically discover semantic concepts after pre-training with massive face identities, and such concepts are significantly enriched after fine-tuning with attribute tags. Each attribute can be well explained with a sparse linear combination of these concepts.",
                        "Citation Paper Authors": "Authors:Ziwei Liu, Ping Luo, Xiaogang Wang, Xiaoou Tang"
                    }
                },
                {
                    "Sentence ID": 77,
                    "Sentence": "exploited the uniqueness of the data samples of a person to\ncheck whether their data was used for training. For gener-\native adversarial networks (GANs) Webster et al. ",
                    "Citation Text": "Ryan Webster, Julien Rabin, Lo \u00a8\u0131c Simon, and Fr \u00b4ed\u00b4eric\nJurie. This person (probably) exists. identity mem-\nbership attacks against GAN generated faces. arXiv ,\narXiv:2107.06018, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.06018",
                        "Citation Paper Title": "Title:This Person (Probably) Exists. Identity Membership Attacks Against GAN Generated Faces",
                        "Citation Paper Abstract": "Abstract:Recently, generative adversarial networks (GANs) have achieved stunning realism, fooling even human observers. Indeed, the popular tongue-in-cheek website {\\small \\url{ this http URL}}, taunts users with GAN generated images that seem too real to believe. On the other hand, GANs do leak information about their training data, as evidenced by membership attacks recently demonstrated in the literature. In this work, we challenge the assumption that GAN faces really are novel creations, by constructing a successful membership attack of a new kind. Unlike previous works, our attack can accurately discern samples sharing the same identity as training samples without being the same samples. We demonstrate the interest of our attack across several popular face datasets and GAN training procedures. Notably, we show that even in the presence of significant dataset diversity, an over represented person can pose a privacy concern.",
                        "Citation Paper Authors": "Authors:Ryan Webster, Julien Rabin, Loic Simon, Frederic Jurie"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "have criticized that these attacks\noften have a high false-positive rate and thus their signifi-\ncance remains limited.\nMoving beyond MIAs but staying uni-modal, Li et al. ",
                    "Citation Text": "Guoyao Li, Shahbaz Rezaei, and Xin Liu. User-\nlevel membership inference attack against metric em-\nbedding learning. In ICLR 2022 Workshop on\nPAIR\u02c62Struct: Privacy, Accountability, Interpretabil-\nity, Robustness, Reasoning on Structured Data , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.02077",
                        "Citation Paper Title": "Title:User-Level Membership Inference Attack against Metric Embedding Learning",
                        "Citation Paper Abstract": "Abstract:Membership inference (MI) determines if a sample was part of a victim model training set. Recent development of MI attacks focus on record-level membership inference which limits their application in many real-world scenarios. For example, in the person re-identification task, the attacker (or investigator) is interested in determining if a user's images have been used during training or not. However, the exact training images might not be accessible to the attacker. In this paper, we develop a user-level MI attack where the goal is to find if any sample from the target user has been used during training even when no exact training sample is available to the attacker. We focus on metric embedding learning due to its dominance in person re-identification, where user-level MI attack is more sensible. We conduct an extensive evaluation on several datasets and show that our approach achieves high accuracy on user-level MI task.",
                        "Citation Paper Authors": "Authors:Guoyao Li, Shahbaz Rezaei, Xin Liu"
                    }
                },
                {
                    "Sentence ID": 84,
                    "Sentence": ". Their goal is to recon-\nstruct specific training samples ",
                    "Citation Text": "Y . Zhang, R. Jia, H. Pei, W. Wang, B. Li, and\nD. Song. The secret revealer: Generative model-\ninversion attacks against deep neural networks. In\n2020 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR) , pages 250\u2013258, Los\nAlamitos, CA, USA, jun 2020. IEEE Computer So-\nciety.\n17A Hard- and Software Details\nThe experiments conducted in this work were run on a NVIDIA DGX machines with NVIDIA DGX Server Version 5.1.0\nand Ubuntu 20.04.4 LTS. The machines have NVIDIA A100-SXM4-40GB GPUs, AMD EPYC 7742 64-Core processors\nand 1.9TB of RAM. The experiments were run with Python 3.8, CUDA 11.3 and PyTorch 1.12.1 with TorchVision 0.13.1.\nB Prompt Templates and Generation of Possible Names\nThe prompt templates used for our IDIA can be seen in Table 1. In each of the prompt templates, Xwas substituted with\npossible names. The possible names were generated by combining the most popular male and female first names in the US\nfrom 1880-2008",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.07135",
                        "Citation Paper Title": "Title:The Secret Revealer: Generative Model-Inversion Attacks Against Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:This paper studies model-inversion attacks, in which the access to a model is abused to infer information about the training data. Since its first introduction, such attacks have raised serious concerns given that training data usually contain privacy-sensitive information. Thus far, successful model-inversion attacks have only been demonstrated on simple models, such as linear regression and logistic regression. Previous attempts to invert neural networks, even the ones with simple architectures, have failed to produce convincing results. We present a novel attack method, termed the generative model-inversion attack, which can invert deep neural networks with high success rates. Rather than reconstructing private training data from scratch, we leverage partial public information, which can be very generic, to learn a distributional prior via generative adversarial networks (GANs) and use it to guide the inversion process. Moreover, we theoretically prove that a model's predictive power and its vulnerability to inversion attacks are indeed two sides of the same coin---highly predictive models are able to establish a strong correlation between features and labels, which coincides exactly with what an adversary exploits to mount the attacks. Our extensive experiments demonstrate that the proposed attack improves identification accuracy over the existing work by about 75\\% for reconstructing face images from a state-of-the-art face recognition classifier. We also show that differential privacy, in its canonical form, is of little avail to defend against our attacks.",
                        "Citation Paper Authors": "Authors:Yuheng Zhang, Ruoxi Jia, Hengzhi Pei, Wenxiao Wang, Bo Li, Dawn Song"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.06963v2": {
            "Paper Title": "Privacy-Preserving Decentralized Inference with Graph Neural Networks in\n  Wireless Networks",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": ". Most of them have employed\nGNNs for wireless resource allocation, such as link schedul ing ",
                    "Citation Text": "M. Lee, G. Yu, and G. Y . Li, \u201cGraph embedding based wireles s link\nscheduling with few training samples,\u201d IEEE Trans. Wireless Commun. ,\nvol. 20, no. 4, pp. 2282\u20132294, Apr. 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.02871",
                        "Citation Paper Title": "Title:Graph Embedding based Wireless Link Scheduling with Few Training Samples",
                        "Citation Paper Abstract": "Abstract:Link scheduling in device-to-device (D2D) networks is usually formulated as a non-convex combinatorial problem, which is generally NP-hard and difficult to get the optimal solution. Traditional methods to solve this problem are mainly based on mathematical optimization techniques, where accurate channel state information (CSI), usually obtained through channel estimation and feedback, is needed. To overcome the high computational complexity of the traditional methods and eliminate the costly channel estimation stage, machine leaning (ML) has been introduced recently to address the wireless link scheduling problems. In this paper, we propose a novel graph embedding based method for link scheduling in D2D networks. We first construct a fully-connected directed graph for the D2D network, where each D2D pair is a node while interference links among D2D pairs are the edges. Then we compute a low-dimensional feature vector for each node in the graph. The graph embedding process is based on the distances of both communication and interference links, therefore without requiring the accurate CSI. By utilizing a multi-layer classifier, a scheduling strategy can be learned in a supervised manner based on the graph embedding results for each node. We also propose an unsupervised manner to train the graph embedding based method to further reinforce the scalability and generalizability and develop a K-nearest neighbor graph representation method to reduce the computational complexity. Extensive simulation demonstrates that the proposed method is near-optimal compared with the existing state-of-art methods but is with only hundreds of training samples. It is also competitive in terms of scalability and generalizability to more complicated scenarios.",
                        "Citation Paper Authors": "Authors:Mengyuan Lee, Guanding Yu, Geoffrey Ye Li"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "; (ii) modifying existing training algo-\nrithms, such as adding noise to the gradients generated by th e\nSGD/Adam algorithm, for preserving the training privacy of\nGNNs ",
                    "Citation Text": "T. T. Mueller, J. C. Paetzold, C. Prabhakar, D. Usynin, D . Rueckert, and\nG. Kaissis, \u201cDifferentially private graph classi\ufb01cation w ith GNNs,\u201d arXiv\npreprint arXiv:2202.02575 , Feb. 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.02575",
                        "Citation Paper Title": "Title:Differentially Private Graph Classification with GNNs",
                        "Citation Paper Abstract": "Abstract:Graph Neural Networks (GNNs) have established themselves as the state-of-the-art models for many machine learning applications such as the analysis of social networks, protein interactions and molecules. Several among these datasets contain privacy-sensitive data. Machine learning with differential privacy is a promising technique to allow deriving insight from sensitive data while offering formal guarantees of privacy protection. However, the differentially private training of GNNs has so far remained under-explored due to the challenges presented by the intrinsic structural connectivity of graphs. In this work, we introduce differential privacy for graph-level classification, one of the key applications of machine learning on graphs. Our method is applicable to deep learning on multi-graph datasets and relies on differentially private stochastic gradient descent (DP-SGD). We show results on a variety of synthetic and public datasets and evaluate the impact of different GNN architectures and training hyperparameters on model performance for differentially private graph classification. Finally, we apply explainability techniques to assess whether similar representations are learned in the private and non-private settings and establish robust baselines for future work in this area.",
                        "Citation Paper Authors": "Authors:Tamara T. Mueller, Johannes C. Paetzold, Chinmay Prabhakar, Dmitrii Usynin, Daniel Rueckert, Georgios Kaissis"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "1) GNNs in Wireless Networks: Recently, there are some\nexisting works about incorporating GNNs into wireless ap-\nplications ",
                    "Citation Text": "M. Lee, G. Yu, H. Dai, and G. Y . Li, \u201cGraph neural networks m eet\nwireless communications: Motivation, applications, and f uture directions,\u201d\nIEEE Wirel. Commun. , vol. 29, no.5, pp. 12\u201319, Oct. 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.04047",
                        "Citation Paper Title": "Title:Graph Neural Networks Meet Wireless Communications: Motivation, Applications, and Future Directions",
                        "Citation Paper Abstract": "Abstract:As an efficient graph analytical tool, graph neural networks (GNNs) have special properties that are particularly fit for the characteristics and requirements of wireless communications, exhibiting good potential for the advancement of next-generation wireless communications. This article aims to provide a comprehensive overview of the interplay between GNNs and wireless communications, including GNNs for wireless communications (GNN4Com) and wireless communications for GNNs (Com4GNN). In particular, we discuss GNN4Com based on how graphical models are constructed and introduce Com4GNN with corresponding incentives. We also highlight potential research directions to promote future research endeavors for GNNs in wireless communications.",
                        "Citation Paper Authors": "Authors:Mengyuan Lee, Guanding Yu, Huaiyu Dai, Geoffrey Ye Li"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2207.08336v2": {
            "Paper Title": "When Fairness Meets Privacy: Fair Classification with Semi-Private\n  Sensitive Attributes",
            "Sentences": [
                {
                    "Sentence ID": 49,
                    "Sentence": ". The majority of existing debiasing techniques have\nbeen applied at different stages of a machine learning model ",
                    "Citation Text": "Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram\nGalstyan. 2021. A survey on bias and fairness in machine learning. ACM Com-\nputing Surveys (CSUR) 54, 6 (2021), 1\u201335.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.09635",
                        "Citation Paper Title": "Title:A Survey on Bias and Fairness in Machine Learning",
                        "Citation Paper Abstract": "Abstract:With the widespread use of AI systems and applications in our everyday lives, it is important to take fairness issues into consideration while designing and engineering these types of systems. Such systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in machine learning, natural language processing, and deep learning that addresses such challenges in different subdomains. With the commercialization of these systems, researchers are becoming aware of the biases that these applications can contain and have attempted to address them. In this survey we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined in order to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and how they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.",
                        "Citation Paper Authors": "Authors:Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, Aram Galstyan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.11760v3": {
            "Paper Title": "Fingerprinting Generative Adversarial Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.13710v2": {
            "Paper Title": "Motif-Backdoor: Rethinking the Backdoor Attack on Graph Neural Networks\n  via Motifs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.09186v3": {
            "Paper Title": "pvCNN: Privacy-Preserving and Verifiable Convolutional Neural Network\n  Testing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.02751v2": {
            "Paper Title": "Tubes Among Us: Analog Attack on Automatic Speaker Identification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.12551v3": {
            "Paper Title": "Masked Jigsaw Puzzle: A Versatile Position Embedding for Vision\n  Transformers",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": "22 79.8 16.21 64.3\nDeiT-S + MJP 22 80.5 8.96 82.9\nSwin-T ",
                    "Citation Text": "Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV) , 2021. 1, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.14030",
                        "Citation Paper Title": "Title:Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
                        "Citation Paper Abstract": "Abstract:This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \\textbf{S}hifted \\textbf{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at~\\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "29 81.3 15.49 41.5\nSwin-T + MJP 29 81.3 12.36 66.9\n5. Experiments\nWe follow the typical supervised pre-training procedure,\nwhere all the compared models are trained on ImageNet-\n1K ",
                    "Citation Text": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al. Imagenet large\nscale visual recognition challenge. International Journal of\nComputer Vision , 115(3):211\u2013252, 2015. 2, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.0575",
                        "Citation Paper Title": "Title:ImageNet Large Scale Visual Recognition Challenge",
                        "Citation Paper Abstract": "Abstract:The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions.\nThis paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.",
                        "Citation Paper Authors": "Authors:Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, Li Fei-Fei"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "proposed correlating\nthe PEs with their local neighborhood of the input sequence.\nLiuet al. ",
                    "Citation Text": "Yahui Liu, Enver Sangineto, Wei Bi, Nicu Sebe, Bruno Lepri,\nand Marco Nadai. Efficient training of visual transformers\nwith small datasets. Advances in Neural Information Process-\ning Systems (NeurIPS) , 2021. 3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.03746",
                        "Citation Paper Title": "Title:Efficient Training of Visual Transformers with Small Datasets",
                        "Citation Paper Abstract": "Abstract:Visual Transformers (VTs) are emerging as an architectural paradigm alternative to Convolutional networks (CNNs). Differently from CNNs, VTs can capture global relations between image elements and they potentially have a larger representation capacity. However, the lack of the typical convolutional inductive bias makes these models more data-hungry than common CNNs. In fact, some local properties of the visual domain which are embedded in the CNN architectural design, in VTs should be learned from samples. In this paper, we empirically analyse different VTs, comparing their robustness in a small training-set regime, and we show that, despite having a comparable accuracy when trained on ImageNet, their performance on smaller datasets can be largely different. Moreover, we propose a self-supervised task which can extract additional information from images with only a negligible computational overhead. This task encourages the VTs to learn spatial relations within an image and makes the VT training much more robust when training data are scarce. Our task is used jointly with the standard (supervised) training and it does not depend on specific architectural choices, thus it can be easily plugged in the existing VTs. Using an extensive evaluation with different VTs and datasets, we show that our method can improve (sometimes dramatically) the final accuracy of the VTs. Our code is available at: this https URL.",
                        "Citation Paper Authors": "Authors:Yahui Liu, Enver Sangineto, Wei Bi, Nicu Sebe, Bruno Lepri, Marco De Nadai"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "have achieved superior performance than their\ncounterpart CNNs on image classification and various other\n2downstream tasks ( e.g., object detection [3, 57], object re-\nidentification ",
                    "Citation Text": "Shuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li,\nand Wei Jiang. Transreid: Transformer-based object re-\nidentification. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV) , 2021. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.04378",
                        "Citation Paper Title": "Title:TransReID: Transformer-based Object Re-Identification",
                        "Citation Paper Abstract": "Abstract:Extracting robust feature representation is one of the key challenges in object re-identification (ReID). Although convolution neural network (CNN)-based methods have achieved great success, they only process one local neighborhood at a time and suffer from information loss on details caused by convolution and downsampling operators (e.g. pooling and strided convolution). To overcome these limitations, we propose a pure transformer-based object ReID framework named TransReID. Specifically, we first encode an image as a sequence of patches and build a transformer-based strong baseline with a few critical improvements, which achieves competitive results on several ReID benchmarks with CNN-based methods. To further enhance the robust feature learning in the context of transformers, two novel modules are carefully designed. (i) The jigsaw patch module (JPM) is proposed to rearrange the patch embeddings via shift and patch shuffle operations which generates robust features with improved discrimination ability and more diversified coverage. (ii) The side information embeddings (SIE) is introduced to mitigate feature bias towards camera/view variations by plugging in learnable embeddings to incorporate these non-visual clues. To the best of our knowledge, this is the first work to adopt a pure transformer for ReID research. Experimental results of TransReID are superior promising, which achieve state-of-the-art performance on both person and vehicle ReID benchmarks.",
                        "Citation Paper Authors": "Authors:Shuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li, Wei Jiang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.15160v3": {
            "Paper Title": "Mitigating Adversarial Attacks by Distributing Different Copies to\n  Different Users",
            "Sentences": [
                {
                    "Sentence ID": 47,
                    "Sentence": "derives the\nsearch direction from the training loss function\u2019s gradient. The\nattractors are from a proposed method ",
                    "Citation Text": "Jiyi Zhang, Ee Chien Chang, and Hwee Kuan Lee. 2022. Confusing and Detecting\nML Adversarial Attacks with Injected Attractors. In ASIA CCS . 322\u2013336.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.02732",
                        "Citation Paper Title": "Title:Confusing and Detecting ML Adversarial Attacks with Injected Attractors",
                        "Citation Paper Abstract": "Abstract:Many machine learning adversarial attacks find adversarial samples of a victim model ${\\mathcal M}$ by following the gradient of some attack objective functions, either explicitly or implicitly. To confuse and detect such attacks, we take the proactive approach that modifies those functions with the goal of misleading the attacks to some local minimals, or to some designated regions that can be easily picked up by an analyzer. To achieve this goal, we propose adding a large number of artifacts, which we called $attractors$, onto the otherwise smooth function. An attractor is a point in the input space, where samples in its neighborhood have gradient pointing toward it. We observe that decoders of watermarking schemes exhibit properties of attractors and give a generic method that injects attractors from a watermark decoder into the victim model ${\\mathcal M}$. This principled approach allows us to leverage on known watermarking schemes for scalability and robustness and provides explainability of the outcomes. Experimental studies show that our method has competitive performance. For instance, for un-targeted attacks on CIFAR-10 dataset, we can reduce the overall attack success rate of DeepFool to 1.9%, whereas known defense LID, FS and MagNet can reduce the rate to 90.8%, 98.5% and 78.5% respectively.",
                        "Citation Paper Authors": "Authors:Jiyi Zhang, Ee-Chien Chang, Hwee Kuan Lee"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": ". An approach\nin mitigating watermark collusion attacks is via anti-collusion fin-\ngerprinting code ",
                    "Citation Text": "Boris Skoric, T. U. Vladimirova, Mehmet Utku Celik, and Joop Talstra. 2008.\nTardos Fingerprinting is Better Than We Thought. IEEE Trans. Inf. Theory 54, 8\n(2008), 3663\u20133676.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:cs/0607131",
                        "Citation Paper Title": "Title:Tardos fingerprinting is better than we thought",
                        "Citation Paper Abstract": "Abstract:  We review the fingerprinting scheme by Tardos and show that it has a much better performance than suggested by the proofs in Tardos' original paper. In particular, the length of the codewords can be significantly reduced.\nFirst we generalize the proofs of the false positive and false negative error probabilities with the following modifications: (1) we replace Tardos' hard-coded numbers by variables and (2) we allow for independently chosen false positive and false negative error rates. It turns out that all the collusion-resistance properties can still be proven when the code length is reduced by a factor of more than 2.\nSecond, we study the statistical properties of the fingerprinting scheme, in particular the average and variance of the accusations. We identify which colluder strategy forces the content owner to employ the longest code. Using a gaussian approximation for the probability density functions of the accusations, we show that the required false negative and false positive error rate can be achieved with codes that are a factor 2 shorter than required for rigid proofs.\nCombining the results of these two approaches, we show that the Tardos scheme can be used with a code length approximately 5 times shorter than in the original construction.",
                        "Citation Paper Authors": "Authors:B. Skoric, T.U. Vladimirova, M. Celik, J.C. Talstra"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": ", extends the FGSM from a one-step attack into an iterative\nprocess. The search process starts at a random point within the\nnorm ball. The DeepFool ",
                    "Citation Text": "Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. 2016.\nDeepFool: A Simple and Accurate Method to Fool Deep Neural Networks. In\nCVPR . 2574\u20132582.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.04599",
                        "Citation Paper Title": "Title:DeepFool: a simple and accurate method to fool deep neural networks",
                        "Citation Paper Abstract": "Abstract:State-of-the-art deep neural networks have achieved impressive results on many image classification tasks. However, these same architectures have been shown to be unstable to small, well sought, perturbations of the images. Despite the importance of this phenomenon, no effective methods have been proposed to accurately compute the robustness of state-of-the-art deep classifiers to such perturbations on large-scale datasets. In this paper, we fill this gap and propose the DeepFool algorithm to efficiently compute perturbations that fool deep networks, and thus reliably quantify the robustness of these classifiers. Extensive experimental results show that our approach outperforms recent methods in the task of computing adversarial perturbations and making classifiers more robust.",
                        "Citation Paper Authors": "Authors:Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Pascal Frossard"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": ". It moves a fixed small step \ud835\udf16\nin the direction that maximally changes the prediction result. The\nadversarial sample is:\nx\u2032=x+\ud835\udf16\u00b7sign\u0000\u2207x\ud835\udc3d\u0000\ud835\udf03,x,y\ud835\udc61\ud835\udc5f\ud835\udc62\ud835\udc52\u0001\u0001\nwhere y\ud835\udc61\ud835\udc5f\ud835\udc62\ud835\udc52 is the one-hot vector of the true label of the input x.\nPGD ",
                    "Citation Text": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and\nAdrian Vladu. 2018. Towards Deep Learning Models Resistant to Adversarial\nAttacks. In ICLR .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06083",
                        "Citation Paper Title": "Title:Towards Deep Learning Models Resistant to Adversarial Attacks",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL.",
                        "Citation Paper Authors": "Authors:Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.05056v3": {
            "Paper Title": "Testing Human Ability To Detect Deepfake Images of Human Faces",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.12776v2": {
            "Paper Title": "To Signal or Not to Signal? Layering Traffic Analysis Resistance on\n  Secure Instant Messaging",
            "Sentences": [
                {
                    "Sentence ID": 50,
                    "Sentence": "is focused on\nscalability for DC-nets. Since the hybrid model does not use\nround-based communication, Atom\u2019s scalability solutions are\nnot applicable.\nVerdict ",
                    "Citation Text": "H. Corrigan-Gibbs, D. I. Wolinsky, and B. Ford, \u201cProactively Ac-\ncountable Anonymous Messaging in Verdict,\u201d in USENIX , 2013.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1209.4819",
                        "Citation Paper Title": "Title:Proactively Accountable Anonymous Messaging in Verdict",
                        "Citation Paper Abstract": "Abstract:The DC-nets approach to anonymity has long held attraction for its strength against traffic analysis, but practical implementations remain vulnerable to internal disruption or \"jamming\" attacks requiring time-consuming tracing procedures to address. We present Verdict, the first practical anonymous group communication system built using proactively verifiable DC-nets: participants use public key cryptography to construct DC-net ciphertexts, and knowledge proofs to detect and detect and exclude misbehavior before disruption. We compare three alternative constructions for verifiable DC-nets, one using bilinear maps and two based on simpler ElGamal encryption. While verifiable DC-nets incurs higher computation overheads due to the public-key cryptography involved, our experiments suggest Verdict is practical for anonymous group messaging or microblogging applications, supporting groups of 100 clients at 1 second per round or 1000 clients at 10 seconds per round. Furthermore, we show how existing symmetric-key DC-nets can \"fall back\" to a verifiable DC-net to quickly identify mis- behavior improving previous detections schemes by two orders of magnitude than previous approaches.",
                        "Citation Paper Authors": "Authors:Henry Corrigan-Gibbs, David Isaac Wolinsky, Bryan Ford"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.03403v2": {
            "Paper Title": "TAN Without a Burn: Scaling Laws of DP-SGD",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.12251v2": {
            "Paper Title": "Adversarial Zoom Lens: A Novel Physical-World Attack to DNNs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.11739v2": {
            "Paper Title": "Adversarial Catoptric Light: An Effective, Stealthy and Robust\n  Physical-World Attack to DNNs",
            "Sentences": [
                {
                    "Sentence ID": 49,
                    "Sentence": "proposed\nOPAD, which amplifies and projects subtle digital pertur-\nbations onto the target object to generate physical samples.\nHuman observers, on the other hand, are suspicious of itsirregular projection patterns. Zhong et al. ",
                    "Citation Text": "Yiqi Zhong, Xianming Liu, Deming Zhai, Junjun Jiang, and\nXiangyang Ji. Shadows can be dangerous: Stealthy and\neffective physical-world adversarial attack by natural phe-\nnomenon. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 15345\u2013\n15354, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.03818",
                        "Citation Paper Title": "Title:Shadows can be Dangerous: Stealthy and Effective Physical-world Adversarial Attack by Natural Phenomenon",
                        "Citation Paper Abstract": "Abstract:Estimating the risk level of adversarial examples is essential for safely deploying machine learning models in the real world. One popular approach for physical-world attacks is to adopt the \"sticker-pasting\" strategy, which however suffers from some limitations, including difficulties in access to the target or printing by valid colors. A new type of non-invasive attacks emerged recently, which attempt to cast perturbation onto the target by optics based tools, such as laser beam and projector. However, the added optical patterns are artificial but not natural. Thus, they are still conspicuous and attention-grabbed, and can be easily noticed by humans. In this paper, we study a new type of optical adversarial examples, in which the perturbations are generated by a very common natural phenomenon, shadow, to achieve naturalistic and stealthy physical-world adversarial attack under the black-box setting. We extensively evaluate the effectiveness of this new attack on both simulated and real-world environments. Experimental results on traffic sign recognition demonstrate that our algorithm can generate adversarial examples effectively, reaching 98.23% and 90.47% success rates on LISA and GTSRB test sets respectively, while continuously misleading a moving camera over 95% of the time in real-world scenarios. We also offer discussions about the limitations and the defense mechanism of this attack.",
                        "Citation Paper Authors": "Authors:Yiqi Zhong, Xianming Liu, Deming Zhai, Junjun Jiang, Xiangyang Ji"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.00853v3": {
            "Paper Title": "Adversarial Neon Beam: A Light-based Physical Attack to DNNs",
            "Sentences": [
                {
                    "Sentence ID": 48,
                    "Sentence": "improved RP2 and employed the attack on a target detector,\nresulting in the detector disregarding the target object. However,\nthe perturbations cover an extensive area, they are too conspicu-\nous. Chen et al. ",
                    "Citation Text": "S. Chen, C. Cornelius, J. Martin, and D. H. P. Chau, \u201cShapeshifter: Robust physical\nadversarial attack on faster R-CNN object detector,\u201d in Machine Learning and\nKnowledge Discovery in Databases - European Conference, ECML PKDD 2018,\nDublin, Ireland, September 10-14, 2018, Proceedings, Part I (M. Berlingerio,\nF. Bonchi, T. G\u00e4rtner, N. Hurley, and G. Ifrim, eds.), vol. 11051 of Lecture Notes\nin Computer Science , pp. 52\u201368, Springer, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.05810",
                        "Citation Paper Title": "Title:ShapeShifter: Robust Physical Adversarial Attack on Faster R-CNN Object Detector",
                        "Citation Paper Abstract": "Abstract:Given the ability to directly manipulate image pixels in the digital input space, an adversary can easily generate imperceptible perturbations to fool a Deep Neural Network (DNN) image classifier, as demonstrated in prior work. In this work, we propose ShapeShifter, an attack that tackles the more challenging problem of crafting physical adversarial perturbations to fool image-based object detectors like Faster R-CNN. Attacking an object detector is more difficult than attacking an image classifier, as it needs to mislead the classification results in multiple bounding boxes with different scales. Extending the digital attack to the physical world adds another layer of difficulty, because it requires the perturbation to be robust enough to survive real-world distortions due to different viewing distances and angles, lighting conditions, and camera limitations. We show that the Expectation over Transformation technique, which was originally proposed to enhance the robustness of adversarial perturbations in image classification, can be successfully adapted to the object detection setting. ShapeShifter can generate adversarially perturbed stop signs that are consistently mis-detected by Faster R-CNN as other objects, posing a potential threat to autonomous vehicles and other safety-critical computer vision systems.",
                        "Citation Paper Authors": "Authors:Shang-Tse Chen, Cory Cornelius, Jason Martin, Duen Horng Chau"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "2.1 Digital attacks\nThe concept of adversarial attacks was initially introduced by Szegedy\net al. ",
                    "Citation Text": "C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Goodfellow,\nand R. Fergus, \u201cIntriguing properties of neural networks,\u201d in 2nd International\nConference on Learning Representations, ICLR 2014, Banff, AB, Canada, April\n14-16, 2014, Conference Track Proceedings (Y . Bengio and Y . LeCun, eds.), 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1312.6199",
                        "Citation Paper Title": "Title:Intriguing properties of neural networks",
                        "Citation Paper Abstract": "Abstract:Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.\nFirst, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks.\nSecond, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
                        "Citation Paper Authors": "Authors:Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.14414v3": {
            "Paper Title": "On the (Im)Possibility of Estimating Various Notions of Differential\n  Privacy",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": "achieve their possibility result using randomized algorithms.\nThe paper that is most closely related to ours is ",
                    "Citation Text": "\u00a8O. Askin, T. Kutta, and H. Dette, \u201cStatistical quantification of differential\nprivacy: A local approach,\u201d in 43rd IEEE Symposium on Security and\nPrivacy . IEEE, 2022, pp. 402\u2013421.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.09528",
                        "Citation Paper Title": "Title:Statistical Quantification of Differential Privacy: A Local Approach",
                        "Citation Paper Abstract": "Abstract:In this work, we introduce a new approach for statistical quantification of differential privacy in a black box setting. We present estimators and confidence intervals for the optimal privacy parameter of a randomized algorithm $A$, as well as other key variables (such as the \"data-centric privacy level\"). Our estimators are based on a local characterization of privacy and in contrast to the related literature avoid the process of \"event selection\" - a major obstacle to privacy validation. This makes our methods easy to implement and user-friendly. We show fast convergence rates of the estimators and asymptotic validity of the confidence intervals. An experimental study of various algorithms confirms the efficacy of our approach.",
                        "Citation Paper Authors": "Authors:\u00d6nder Askin, Tim Kutta, Holger Dette"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": ". Some practical implementations of LDP are\ngiven by Google with RAPPOR ",
                    "Citation Text": "\u00b4U. Erlingsson, V . Pihur, and A. Korolova, \u201cRAPPOR: randomized\naggregatable privacy-preserving ordinal response,\u201d in Proceedings of\nthe 2014 ACM SIGSAC Conference on Computer and Communications\nSecurity (CCS) . ACM, 2014, pp. 1054\u20131067.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1407.6981",
                        "Citation Paper Title": "Title:RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response",
                        "Citation Paper Abstract": "Abstract:Randomized Aggregatable Privacy-Preserving Ordinal Response, or RAPPOR, is a technology for crowdsourcing statistics from end-user client software, anonymously, with strong privacy guarantees. In short, RAPPORs allow the forest of client data to be studied, without permitting the possibility of looking at individual trees. By applying randomized response in a novel manner, RAPPOR provides the mechanisms for such collection as well as for efficient, high-utility analysis of the collected data. In particular, RAPPOR permits statistics to be collected on the population of client-side strings with strong privacy guarantees for each client, and without linkability of their reports. This paper describes and motivates RAPPOR, details its differential-privacy and utility guarantees, discusses its practical deployment and properties in the face of different attack models, and, finally, gives results of its application to both synthetic and real-world data.",
                        "Citation Paper Authors": "Authors:\u00dalfar Erlingsson, Vasyl Pihur, Aleksandra Korolova"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "Many works in the literature face the problem of verifying\nDP of an algorithm, e.g. through formal verification. For\ninstance, ShadowDP ",
                    "Citation Text": "Y . Wang, Z. Ding, G. Wang, D. Kifer, and D. Zhang, \u201cProving differen-\ntial privacy with shadow execution,\u201d in 40th ACM SIGPLAN Conference\non Programming Language Design and Implementation (PLDI) . ACM,\n2019, pp. 655\u2013669.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.12254",
                        "Citation Paper Title": "Title:Proving Differential Privacy with Shadow Execution",
                        "Citation Paper Abstract": "Abstract:Recent work on formal verification of differential privacy shows a trend toward usability and expressiveness -- generating a correctness proof of sophisticated algorithm while minimizing the annotation burden on programmers. Sometimes, combining those two requires substantial changes to program logics: one recent paper is able to verify Report Noisy Max automatically, but it involves a complex verification system using customized program logics and verifiers.\nIn this paper, we propose a new proof technique, called shadow execution, and embed it into a language called ShadowDP. ShadowDP uses shadow execution to generate proofs of differential privacy with very few programmer annotations and without relying on customized logics and verifiers. In addition to verifying Report Noisy Max, we show that it can verify a new variant of Sparse Vector that reports the gap between some noisy query answers and the noisy threshold. Moreover, ShadowDP reduces the complexity of verification: for all of the algorithms we have evaluated, type checking and verification in total takes at most 3 seconds, while prior work takes minutes on the same algorithms.",
                        "Citation Paper Authors": "Authors:Yuxin Wang, Zeyu Ding, Guanhong Wang, Daniel Kifer, Danfeng Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.07623v2": {
            "Paper Title": "D-LNBot: A Scalable, Cost-Free and Covert Hybrid Botnet on Bitcoin's\n  Lightning Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.10206v2": {
            "Paper Title": "DECLOAK: Enable Secure and Cheap Multi-Party Transactions on Legacy\n  Blockchains by a Minimally Trusted TEE Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.03277v2": {
            "Paper Title": "On The Empirical Effectiveness of Unrealistic Adversarial Hardening\n  Against Realistic Adversarial Attacks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.13325v2": {
            "Paper Title": "Lattice Codes for Lattice-Based PKE",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": "show high dimensional random lattice codes\ncan achieve the capacity of additive white Gaussian noise (AWGN) channels.\nRecent years have also witnessed the use of Polar lattices ",
                    "Citation Text": "Liu, L., Yan, Y., Ling, C., Wu, X.: Construction of capacity-achieving\nlattice codes: Polar lattices. IEEE Trans. Commun. 67(2), 915{928 (2019)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1411.0187",
                        "Citation Paper Title": "Title:Construction of Capacity-Achieving Lattice Codes: Polar Lattices",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose a new class of lattices constructed from polar codes, namely polar lattices, to achieve the capacity $\\frac{1}{2}\\log(1+\\SNR)$ of the additive white Gaussian-noise (AWGN) channel. Our construction follows the multilevel approach of Forney \\textit{et al.}, where we construct a capacity-achieving polar code on each level. The component polar codes are shown to be naturally nested, thereby fulfilling the requirement of the multilevel lattice construction. We prove that polar lattices are \\emph{AWGN-good}. Furthermore, using the technique of source polarization, we propose discrete Gaussian shaping over the polar lattice to satisfy the power constraint. Both the construction and shaping are explicit, and the overall complexity of encoding and decoding is $O(N\\log N)$ for any fixed target error probability.",
                        "Citation Paper Authors": "Authors:Ling Liu, Yanfei Yan, Cong Ling, Xiaofu Wu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.10665v3": {
            "Paper Title": "CryptOpt: Verified Compilation with Randomized Program Search for\n  Cryptographic Primitives (full version)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.05491v2": {
            "Paper Title": "Black-Hole Radiation Decoding is Quantum Cryptography",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.11534v3": {
            "Paper Title": "Towards Adversarially Robust Recommendation from Adaptive Fraudster\n  Detection",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": "train a poisoned RS model to\npredict the ratings for \ufb01ller items. In addition, another line\nof works [20, 21, 24] explore the utilization of generative\nmodels (e.g., GAN ",
                    "Citation Text": "I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,\nD. Warde-Farley, S. Ozair, A. Courville, and Y . Bengio,\n\u201cGenerative adversarial networks,\u201d Communications of\nthe ACM , vol. 63, no. 11, pp. 139\u2013144, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.2661",
                        "Citation Paper Title": "Title:Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "and [20, 21].\nRecently, more sophisticated methods have been proposed\nbased on techniques such as optimization, generative models,\nand so on. For instance, ",
                    "Citation Text": "M. Fang, N. Z. Gong, and J. Liu, \u201cIn\ufb02uence function\nbased data poisoning attacks to top-n recommender sys-\ntems,\u201d in Proceedings of The Web Conference 2020 ,\n2020, pp. 3019\u20133025.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.08025",
                        "Citation Paper Title": "Title:Influence Function based Data Poisoning Attacks to Top-N Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Recommender system is an essential component of web services to engage users. Popular recommender systems model user preferences and item properties using a large amount of crowdsourced user-item interaction data, e.g., rating scores; then top-$N$ items that match the best with a user's preference are recommended to the user. In this work, we show that an attacker can launch a data poisoning attack to a recommender system to make recommendations as the attacker desires via injecting fake users with carefully crafted user-item interaction data. Specifically, an attacker can trick a recommender system to recommend a target item to as many normal users as possible. We focus on matrix factorization based recommender systems because they have been widely deployed in industry. Given the number of fake users the attacker can inject, we formulate the crafting of rating scores for the fake users as an optimization problem. However, this optimization problem is challenging to solve as it is a non-convex integer programming problem. To address the challenge, we develop several techniques to approximately solve the optimization problem. For instance, we leverage influence function to select a subset of normal users who are influential to the recommendations and solve our formulated optimization problem based on these influential users. Our results show that our attacks are effective and outperform existing methods.",
                        "Citation Paper Authors": "Authors:Minghong Fang, Neil Zhenqiang Gong, Jia Liu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.15993v3": {
            "Paper Title": "An Empirical Study on Snapshot DAOs",
            "Sentences": [
                {
                    "Sentence ID": 68,
                    "Sentence": "have partially incorporated this notion into Web3, emphasiz-\ning the importance of DAO governance in Web3. Liu et al. ",
                    "Citation Text": "Yue Liu, Qinghua Lu, Liming Zhu, Hye-Young Paik, et al. A systematic literature\nreview on blockchain governance. Journal of Systems and Software , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.05460",
                        "Citation Paper Title": "Title:A Systematic Literature Review on Blockchain Governance",
                        "Citation Paper Abstract": "Abstract:Blockchain has been increasingly used as a software component to enable decentralisation in software architecture for a variety of applications. Blockchain governance has received considerable attention to ensure the safe and appropriate use and evolution of blockchain, especially after the Ethereum DAO attack in 2016. However, there are no systematic efforts to analyse existing governance solutions. To understand the state-of-the-art of blockchain governance, we conducted a systematic literature review with 37 primary studies. The extracted data from primary studies are synthesised to answer identified research questions. The study results reveal several major findings: 1) governance can improve the adaptability and upgradability of blockchain, whilst the current studies neglect broader ethical responsibilities as the objectives of blockchain governance; 2) governance is along with the development process of a blockchain platform, while ecosystem-level governance process is missing, and; 3) the responsibilities and capabilities of blockchain stakeholders are briefly discussed, whilst the decision rights, accountability, and incentives of blockchain stakeholders are still under studied. We provide actionable guidelines for academia and practitioners to use throughout the lifecycle of blockchain, and identify future trends to support researchers in this area.",
                        "Citation Paper Authors": "Authors:Yue Liu, Qinghua Lu, Liming Zhu, Hye-Young Paik, Mark Staples"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "conduct an empirical study on 21 DAOs to explore the\nhidden problems, including high centralization, monetary costs,\nand pointless activities. Sharma et al. ",
                    "Citation Text": "Tanusree Sharma, Yujin Kwon, Kornrapat Pongmala, Henry Wang, Andrew\nMiller, Dawn Song, and Yang Wang. Unpacking how decentralized autonomous\norganizations (daos) work in practice. arXiv preprint arXiv:2304.09822 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2304.09822",
                        "Citation Paper Title": "Title:Unpacking How Decentralized Autonomous Organizations (DAOs) Work in Practice",
                        "Citation Paper Abstract": "Abstract:Decentralized Autonomous Organizations (DAOs) have emerged as a novel way to coordinate a group of (pseudonymous) entities towards a shared vision (e.g., promoting sustainability), utilizing self-executing smart contracts on blockchains to support decentralized governance and decision-making. In just a few years, over 4,000 DAOs have been launched in various domains, such as investment, education, health, and research. Despite such rapid growth and diversity, it is unclear how these DAOs actually work in practice and to what extent they are effective in achieving their goals. Given this, we aim to unpack how (well) DAOs work in practice. We conducted an in-depth analysis of a diverse set of 10 DAOs of various categories and smart contracts, leveraging on-chain (e.g., voting results) and off-chain data (e.g., community discussions) as well as our interviews with DAO organizers/members. Specifically, we defined metrics to characterize key aspects of DAOs, such as the degrees of decentralization and autonomy. We observed CompoundDAO, AssangeDAO, Bankless, and Krausehouse having poor decentralization in voting, while decentralization has improved over time for one-person-one-vote DAOs (e.g., Proof of Humanity). Moreover, the degree of autonomy varies among DAOs, with some (e.g., Compound and Krausehouse) relying more on third parties than others. Lastly, we offer a set of design implications for future DAO systems based on our findings.",
                        "Citation Paper Authors": "Authors:Tanusree Sharma, Yujin Kwon, Kornrapat Pongmala, Henry Wang, Andrew Miller, Dawn Song, Yang Wang"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "provide a quick review of existing DAO literature and\ndeliver their responses by statistically reviewed papers. Feichtinger\net al. ",
                    "Citation Text": "Rainer Feichtinger, Robin Fritsch, et al. The hidden shortcomings of DAOs\u2013an\nempirical study of on-chain governance. arXiv preprint arXiv:2302.12125 , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2302.12125",
                        "Citation Paper Title": "Title:The Hidden Shortcomings of (D)AOs -- An Empirical Study of On-Chain Governance",
                        "Citation Paper Abstract": "Abstract:Decentralized autonomous organizations (DAOs) are a recent innovation in organizational structures, which are already widely used in the blockchain ecosystem. We empirically study the on-chain governance systems of 21 DAOs and open source the live dataset. The DAOs we study are of various size and activity, and govern a wide range of protocols and services, such as decentralized exchanges, lending protocols, infrastructure projects and common goods funding. Our analysis unveils a high concentration of voting rights, a significant hidden monetary costs of on-chain governance systems, as well as a remarkably high amount of pointless governance activity.",
                        "Citation Paper Authors": "Authors:Rainer Feichtinger, Robin Fritsch, Yann Vonlanthen, Roger Wattenhofer"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "propose a potential attack form called\nDark DAO , which means a group of members form a decentralized\ncartel and can opaquely manipulate (e.g., buy) on-chain votes. Yu\net al. ",
                    "Citation Text": "Guangsheng Yu, Qin Wang, Tingting Bi, Shiping Chen, and Sherry Xu. Leverag-\ning architectural approaches in Web3 applications\u2013a DAO perspective focused.\nIEEE International Conference on Blockchain and Cryptocurrency Workshop on\nCryptocurrency Exchanges (CryptoEx@ICBC) , 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.05314",
                        "Citation Paper Title": "Title:Leveraging Architectural Approaches in Web3 Applications -- A DAO Perspective Focused",
                        "Citation Paper Abstract": "Abstract:Architectural design contexts contain a set of factors that influence software application development. Among them, organizational design contexts consist of high-level company concerns and how it is structured, for example, stakeholders and development schedule, heavily impacting design considerations. Decentralized Autonomous Organization (DAO), as a vital concept in the Web3 space, is an organization constructed by automatically executed rules such as via smart contracts, holding features of the permissionless committee, transparent proposals, and fair contribution by stakeholders. In this work, we conduct a systematic literature review to summarize how DAO is structured as well as explore its benefits&challenges in Web3 applications.",
                        "Citation Paper Authors": "Authors:Guangsheng Yu, Qin Wang, Tingting Bi, Shiping Chen, Sherry Xu"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "from the technical level by diving into the source\ncode. They point out the reasons for recursive send vulnerabilities in\nEthereum that cause a monetary loss ($150M). Robin et al. ",
                    "Citation Text": "Robin Fritsch, Marino M\u00fcller, et al. Analyzing voting power in decentralized\ngovernance: Who controls DAOs? arXiv preprint arXiv:2204.01176 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.01176",
                        "Citation Paper Title": "Title:Analyzing Voting Power in Decentralized Governance: Who controls DAOs?",
                        "Citation Paper Abstract": "Abstract:We empirically study the state of three prominent DAO governance systems on the Ethereum blockchain: Compound, Uniswap and ENS. In particular, we examine how the voting power is distributed in these systems. Using a comprehensive dataset of all governance token holders, delegates, proposals and votes, we analyze who holds the voting rights and how they are used to influence governance decisions.",
                        "Citation Paper Authors": "Authors:Robin Fritsch, Marino M\u00fcller, Roger Wattenhofer"
                    }
                },
                {
                    "Sentence ID": 62,
                    "Sentence": "in 2018.\nThe project introduced an on-chain governance system to produce\na deposited stablecoin protocol (a.k.a. DAI). Then, in 2020, a surge\nof decentralized finance (DeFi) protocols ",
                    "Citation Text": "Sam M Werner, Daniel Perez, Lewis Gudgeon, Ariah Klages-Mundt, Dominik\nHarz, and William J Knottenbelt. SoK: Decentralized finance (DeFi). International\nConference on Financial Cryptography and Data Security (FC) , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.08778",
                        "Citation Paper Title": "Title:SoK: Decentralized Finance (DeFi)",
                        "Citation Paper Abstract": "Abstract:Decentralized Finance (DeFi), a blockchain powered peer-to-peer financial system, is mushrooming. Two years ago the total value locked in DeFi systems was approximately 700m USD, now, as of April 2022, it stands at around 150bn USD. The frenetic evolution of the ecosystem has created challenges in understanding the basic principles of these systems and their security risks. In this Systematization of Knowledge (SoK) we delineate the DeFi ecosystem along the following axes: its primitives, its operational protocol types and its security. We provide a distinction between technical security, which has a healthy literature, and economic security, which is largely unexplored, connecting the latter with new models and thereby synthesizing insights from computer science, economics and finance. Finally, we outline the open research challenges in the ecosystem across these security types.",
                        "Citation Paper Authors": "Authors:Sam M. Werner, Daniel Perez, Lewis Gudgeon, Ariah Klages-Mundt, Dominik Harz, William J. Knottenbelt"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.11311v3": {
            "Paper Title": "Federated Learning via Decentralized Dataset Distillation in\n  Resource-Constrained Edge Environments",
            "Sentences": [
                {
                    "Sentence ID": 58,
                    "Sentence": ", we demon-\nstrate our methods and baselines on IID and Non-IID datasets\nin distributed systems with a different number of clients. We\nvary the number of classes in each client Ckto design Non-\nIID datasets ",
                    "Citation Text": "Q. Li, Y . Diao, Q. Chen, and B. He, \u201cFederated learning on non-iid\ndata silos: An experimental study,\u201d in IEEE International Conference\non Data Engineering , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.02079",
                        "Citation Paper Title": "Title:Federated Learning on Non-IID Data Silos: An Experimental Study",
                        "Citation Paper Abstract": "Abstract:Due to the increasing privacy concerns and data regulations, training data have been increasingly fragmented, forming distributed databases of multiple \"data silos\" (e.g., within different organizations and countries). To develop effective machine learning services, there is a must to exploit data from such distributed databases without exchanging the raw data. Recently, federated learning (FL) has been a solution with growing interests, which enables multiple parties to collaboratively train a machine learning model without exchanging their local data. A key and common challenge on distributed databases is the heterogeneity of the data distribution among the parties. The data of different parties are usually non-independently and identically distributed (i.e., non-IID). There have been many FL algorithms to address the learning effectiveness under non-IID data settings. However, there lacks an experimental study on systematically understanding their advantages and disadvantages, as previous studies have very rigid data partitioning strategies among parties, which are hardly representative and thorough. In this paper, to help researchers better understand and study the non-IID data setting in federated learning, we propose comprehensive data partitioning strategies to cover the typical non-IID data cases. Moreover, we conduct extensive experiments to evaluate state-of-the-art FL algorithms. We find that non-IID does bring significant challenges in learning accuracy of FL algorithms, and none of the existing state-of-the-art FL algorithms outperforms others in all cases. Our experiments provide insights for future studies of addressing the challenges in \"data silos\".",
                        "Citation Paper Authors": "Authors:Qinbin Li, Yiqun Diao, Quan Chen, Bingsheng He"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "Federated learning Federated learning was \ufb01rst introduced\nby McMahan et al. ",
                    "Citation Text": "B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y. Arcas,\n\u201cCommunication-ef\ufb01cient learning of deep networks from decentralizedFig. 4. Number of clients effects the test accuracy (in %) of model. We consider test accuracy changing with the different number of clients. As it is naturally\nclear that the test accuracy of the \ufb01nal trained model can be improved by the increasing number of the \ufb01nal distilled images number at the server, through\nvarying the number of the distilled images at each client with respect to the number of clients, we keep the size of the global distilled dataset always the\nsame across the experiments, and hence eliminate the effects from it. Note that for Non-IID dataset, if the number of clients is less than 10 (as shown in\nthe gray area), not all classes in the entire dataset can be covered, therefore, the test accuracy reduces. Apart from this impact, the results show that ( i) the\ntest accuracy increases with smaller client numbers; ( ii) the test accuracy on Non-IID datasets and on IID datasets are similar, which indicates the FedD3 is\nrobust to data heterogeneity.\nFig. 5. Performance comparison of FedD3 and OSFL for training a CNN\nmodel on distributed Non-IID CIFAR-100 in heterogeneous networks (with\ndifferent ratios of stragglers).\ndata,\u201d in International Conference on Arti\ufb01cial Intelligence and Statistics\n(AISTATS) , vol. 54, 2017, pp. 1273\u20131282.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.05629",
                        "Citation Paper Title": "Title:Communication-Efficient Learning of Deep Networks from Decentralized Data",
                        "Citation Paper Abstract": "Abstract:Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning.\nWe present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.",
                        "Citation Paper Authors": "Authors:H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, Blaise Ag\u00fcera y Arcas"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": ", have men-\ntioned the dataset distillation might be bene\ufb01cial for feder-\nated learning, they have not provided detailed analysis and\nexperimental evaluation on it. Only little research has explored\nthe dataset distillation approaches in federated learning: Zhou\net al. ",
                    "Citation Text": "Y . Zhou, G. Pu, X. Ma, X. Li, and D. Wu, \u201cDistilled one-shot federated\nlearning,\u201d arXiv preprint arXiv:2009.07999 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.07999",
                        "Citation Paper Title": "Title:Distilled One-Shot Federated Learning",
                        "Citation Paper Abstract": "Abstract:Current federated learning algorithms take tens of communication rounds transmitting unwieldy model weights under ideal circumstances and hundreds when data is poorly distributed. Inspired by recent work on dataset distillation and distributed one-shot learning, we propose Distilled One-Shot Federated Learning (DOSFL) to significantly reduce the communication cost while achieving comparable performance. In just one round, each client distills their private dataset, sends the synthetic data (e.g. images or sentences) to the server, and collectively trains a global model. The distilled data look like noise and are only useful to the specific model weights, i.e., become useless after the model updates. With this weight-less and gradient-less design, the total communication cost of DOSFL is up to three orders of magnitude less than FedAvg while preserving between 93% to 99% performance of a centralized counterpart. Afterwards, clients could switch to traditional methods such as FedAvg to finetune the last few percent to fit personalized local models with local datasets. Through comprehensive experiments, we show the accuracy and communication performance of DOSFL on both vision and language tasks with different models including CNN, LSTM, Transformer, etc. We demonstrate that an eavesdropping attacker cannot properly train a good model using the leaked distilled data, without knowing the initial model weights. DOSFL serves as an inexpensive method to quickly converge on a performant pre-trained model with less than 0.1% communication cost of traditional methods.",
                        "Citation Paper Authors": "Authors:Yanlin Zhou, George Pu, Xiyao Ma, Xiaolin Li, Dapeng Wu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2201.08786v3": {
            "Paper Title": "FedComm: Federated Learning as a Medium for Covert Communication",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": "against ML algorithms where the adversary\nmanipulates model parameters or training data in order to\nchange the classi\ufb01cation label given by the model to speci\ufb01c\ninputs. Bagdasaryan et al. ",
                    "Citation Text": "E. Bagdasaryan, A. Veit, Y . Hua, D. Estrin, and V . Shmatikov, \u201cHow to\nbackdoor federated learning,\u201d in International Conference on Arti\ufb01cial\nIntelligence and Statistics . PMLR, 2020, pp. 2938\u20132948.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.00459",
                        "Citation Paper Title": "Title:How To Backdoor Federated Learning",
                        "Citation Paper Abstract": "Abstract:Federated learning enables thousands of participants to construct a deep learning model without sharing their private training data with each other. For example, multiple smartphones can jointly train a next-word predictor for keyboards without revealing what individual users type. We demonstrate that any participant in federated learning can introduce hidden backdoor functionality into the joint global model, e.g., to ensure that an image classifier assigns an attacker-chosen label to images with certain features, or that a word predictor completes certain sentences with an attacker-chosen word.\nWe design and evaluate a new model-poisoning methodology based on model replacement. An attacker selected in a single round of federated learning can cause the global model to immediately reach 100% accuracy on the backdoor task. We evaluate the attack under different assumptions for the standard federated-learning tasks and show that it greatly outperforms data poisoning. Our generic constrain-and-scale technique also evades anomaly detection-based defenses by incorporating the evasion into the attacker's loss function during training.",
                        "Citation Paper Authors": "Authors:Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, Vitaly Shmatikov"
                    }
                },
                {
                    "Sentence ID": 75,
                    "Sentence": "were the \ufb01rst to show that\nFL is vulnerable to this class of attacks. Simultaneously,\nWang et al. ",
                    "Citation Text": "H. Wang, K. Sreenivasan, S. Rajput, H. Vishwakarma, S. Agarwal, J.-y.\nSohn, K. Lee, and D. Papailiopoulos, \u201cAttack of the tails: Yes, you\nreally can backdoor federated learning,\u201d Advances in Neural Information\nProcessing Systems , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.05084",
                        "Citation Paper Title": "Title:Attack of the Tails: Yes, You Really Can Backdoor Federated Learning",
                        "Citation Paper Abstract": "Abstract:Due to its decentralized nature, Federated Learning (FL) lends itself to adversarial attacks in the form of backdoors during training. The goal of a backdoor is to corrupt the performance of the trained model on specific sub-tasks (e.g., by classifying green cars as frogs). A range of FL backdoor attacks have been introduced in the literature, but also methods to defend against them, and it is currently an open question whether FL systems can be tailored to be robust against backdoors. In this work, we provide evidence to the contrary. We first establish that, in the general case, robustness to backdoors implies model robustness to adversarial examples, a major open problem in itself. Furthermore, detecting the presence of a backdoor in a FL model is unlikely assuming first order oracles or polynomial time. We couple our theoretical results with a new family of backdoor attacks, which we refer to as edge-case backdoors. An edge-case backdoor forces a model to misclassify on seemingly easy inputs that are however unlikely to be part of the training, or test data, i.e., they live on the tail of the input distribution. We explain how these edge-case backdoors can lead to unsavory failures and may have serious repercussions on fairness, and exhibit that with careful tuning at the side of the adversary, one can insert them across a range of machine learning tasks (e.g., image classification, OCR, text prediction, sentiment analysis).",
                        "Citation Paper Authors": "Authors:Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh Agarwal, Jy-yong Sohn, Kangwook Lee, Dimitris Papailiopoulos"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "demonstrated\nthat a malicious participant in a collaborative deep-learning\nscenario can use generative adversarial networks (GANs) to\nreconstruct class representatives. On the other hand, Bhagoji et\nal. ",
                    "Citation Text": "A. N. Bhagoji, S. Chakraborty, P. Mittal, and S. Calo, \u201cAnalyzing feder-\nated learning through an adversarial lens,\u201d in International Conference\non Machine Learning . ICML, 2019, pp. 634\u2013643.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.12470",
                        "Citation Paper Title": "Title:Analyzing Federated Learning through an Adversarial Lens",
                        "Citation Paper Abstract": "Abstract:Federated learning distributes model training among a multitude of agents, who, guided by privacy concerns, perform training using their local data but share only model parameter updates, for iterative aggregation at the server. In this work, we explore the threat of model poisoning attacks on federated learning initiated by a single, non-colluding malicious agent where the adversarial objective is to cause the model to misclassify a set of chosen inputs with high confidence. We explore a number of strategies to carry out this attack, starting with simple boosting of the malicious agent's update to overcome the effects of other agents' updates. To increase attack stealth, we propose an alternating minimization strategy, which alternately optimizes for the training loss and the adversarial objective. We follow up by using parameter estimation for the benign agents' updates to improve on attack success. Finally, we use a suite of interpretability techniques to generate visual explanations of model decisions for both benign and malicious models and show that the explanations are nearly visually indistinguishable. Our results indicate that even a highly constrained adversary can carry out model poisoning attacks while simultaneously maintaining stealth, thus highlighting the vulnerability of the federated learning setting and the need to develop effective defense strategies.",
                        "Citation Paper Authors": "Authors:Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, Seraphin Calo"
                    }
                },
                {
                    "Sentence ID": 70,
                    "Sentence": "by proposing a model-inversion attack that exploits\nthe con\ufb01dence values revealed by ML models. Along this\nline of work, Song et al. ",
                    "Citation Text": "C. Song, T. Ristenpart, and V . Shmatikov, \u201cMachine learning models\nthat remember too much,\u201d in Proceedings of the 2017 ACM SIGSAC\nConference on computer and communications security , 2017, pp. 587\u2013\n601.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.07886",
                        "Citation Paper Title": "Title:Machine Learning Models that Remember Too Much",
                        "Citation Paper Abstract": "Abstract:Machine learning (ML) is becoming a commodity. Numerous ML frameworks and services are available to data holders who are not ML experts but want to train predictive models on their data. It is important that ML models trained on sensitive inputs (e.g., personal images or documents) not leak too much information about the training data.\nWe consider a malicious ML provider who supplies model-training code to the data holder, does not observe the training, but then obtains white- or black-box access to the resulting model. In this setting, we design and implement practical algorithms, some of them very similar to standard ML techniques such as regularization and data augmentation, that \"memorize\" information about the training dataset in the model yet the model is as accurate and predictive as a conventionally trained model. We then explain how the adversary can extract memorized information from the model.\nWe evaluate our techniques on standard ML tasks for image classification (CIFAR10), face recognition (LFW and FaceScrub), and text analysis (20 Newsgroups and IMDB). In all cases, we show how our algorithms create models that have high predictive power yet allow accurate extraction of subsets of their training data.",
                        "Citation Paper Authors": "Authors:Congzheng Song, Thomas Ristenpart, Vitaly Shmatikov"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": ".\nFor instance, while FL is designed with privacy in mind ",
                    "Citation Text": "B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,\n\u201cCommunication-ef\ufb01cient learning of deep networks from decentralized\ndata,\u201d in Arti\ufb01cial Intelligence and Statistics . PMLR, 2017, pp. 1273\u2013\n1282.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.05629",
                        "Citation Paper Title": "Title:Communication-Efficient Learning of Deep Networks from Decentralized Data",
                        "Citation Paper Abstract": "Abstract:Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning.\nWe present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.",
                        "Citation Paper Authors": "Authors:H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, Blaise Ag\u00fcera y Arcas"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.08085v4": {
            "Paper Title": "Detection and Mitigation of Byzantine Attacks in Distributed Training",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.08392v6": {
            "Paper Title": "Bitcoin-Enhanced Proof-of-Stake Security: Possibilities and\n  Impossibilities",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.16346v4": {
            "Paper Title": "Improving Hyperspectral Adversarial Robustness Under Multiple Attacks",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": "extends PGD to create an\naggregate attack and achieves lower robust networks. A union of attacks similar to AutoAttack is created in ",
                    "Citation Text": "Pratyush Maini, Eric Wong, and Zico Kolter. Adversarial robustness against the union of multiple perturbation\nmodels. In International Conference on Machine Learning , pages 6640\u20136650. PMLR, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.04068",
                        "Citation Paper Title": "Title:Adversarial Robustness Against the Union of Multiple Perturbation Models",
                        "Citation Paper Abstract": "Abstract:Owing to the susceptibility of deep learning systems to adversarial attacks, there has been a great deal of work in developing (both empirically and certifiably) robust classifiers. While most work has defended against a single type of attack, recent work has looked at defending against multiple perturbation models using simple aggregations of multiple attacks. However, these methods can be difficult to tune, and can easily result in imbalanced degrees of robustness to individual perturbation models, resulting in a sub-optimal worst-case loss over the union. In this work, we develop a natural generalization of the standard PGD-based procedure to incorporate multiple perturbation models into a single attack, by taking the worst-case over all steepest descent directions. This approach has the advantage of directly converging upon a trade-off between different perturbation models which minimizes the worst-case performance over the union. With this approach, we are able to train standard architectures which are simultaneously robust against $\\ell_\\infty$, $\\ell_2$, and $\\ell_1$ attacks, outperforming past approaches on the MNIST and CIFAR10 datasets and achieving adversarial accuracy of 47.0% against the union of ($\\ell_\\infty$, $\\ell_2$, $\\ell_1$) perturbations with radius = (0.03, 0.5, 12) on the latter, improving upon previous approaches which achieve 40.6% accuracy.",
                        "Citation Paper Authors": "Authors:Pratyush Maini, Eric Wong, J. Zico Kolter"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.09057v2": {
            "Paper Title": "Towards Adversarial-Resilient Deep Neural Networks for False Data\n  Injection Attack Detection in Power Grids",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.13375v2": {
            "Paper Title": "Automated Mapping of Vulnerability Advisories onto their Fix Commits in\n  Open Source Repositories",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.08486v2": {
            "Paper Title": "Using Anomaly Detection to Detect Poisoning Attacks in Federated\n  Learning Applications",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": "(2022) Distance based Data/Modelless than 50% or\nknown number of\nattackersHigh IID/Non-IID O(Kd) ",
                    "Citation Text": "H. Jeong, H. Son, S. Lee, J. Hyun, and T.-M. Chung,\n\u201cFedcc: Robust federated learning against model poi-\nsoning attacks,\u201d arXiv preprint arXiv:2212.01976 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.01976",
                        "Citation Paper Title": "Title:FedCC: Robust Federated Learning against Model Poisoning Attacks",
                        "Citation Paper Abstract": "Abstract:Federated Learning has emerged to cope with raising concerns about privacy breaches in using Machine or Deep Learning models. This new paradigm allows the leverage of deep learning models in a distributed manner, enhancing privacy preservation. However, the server's blindness to local datasets introduces its vulnerability to model poisoning attacks and data heterogeneity, tampering with the global model performance. Numerous works have proposed robust aggregation algorithms and defensive mechanisms, but the approaches are orthogonal to individual attacks or issues. FedCC, the proposed method, provides robust aggregation by comparing the Centered Kernel Alignment of Penultimate Layers Representations. The experiment results on FedCC demonstrate that it mitigates untargeted and targeted model poisoning or backdoor attacks while also being effective in non-Independently and Identically Distributed data environments. By applying FedCC against untargeted attacks, global model accuracy is recovered the most. Against targeted backdoor attacks, FedCC nullified attack confidence while preserving the test accuracy. Most of the experiment results outstand the baseline methods.",
                        "Citation Paper Authors": "Authors:Hyejun Jeong, Hamin Son, Seohu Lee, Jayun Hyun, Tai-Myoung Chung"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "(2017) Distance based Data/Model Less than 50% Medium IID O(K2d) ",
                    "Citation Text": "C. Fung, C. J. Yoon, and I. Beschastnikh, \u201cMitigating\nsybils in federated learning poisoning,\u201d 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.04866",
                        "Citation Paper Title": "Title:Mitigating Sybils in Federated Learning Poisoning",
                        "Citation Paper Abstract": "Abstract:Machine learning (ML) over distributed multi-party data is required for a variety of domains. Existing approaches, such as federated learning, collect the outputs computed by a group of devices at a central aggregator and run iterative algorithms to train a globally shared model. Unfortunately, such approaches are susceptible to a variety of attacks, including model poisoning, which is made substantially worse in the presence of sybils.\nIn this paper we first evaluate the vulnerability of federated learning to sybil-based poisoning attacks. We then describe \\emph{FoolsGold}, a novel defense to this problem that identifies poisoning sybils based on the diversity of client updates in the distributed learning process. Unlike prior work, our system does not bound the expected number of attackers, requires no auxiliary information outside of the learning process, and makes fewer assumptions about clients and their data.\nIn our evaluation we show that FoolsGold exceeds the capabilities of existing state of the art approaches to countering sybil-based label-flipping and backdoor poisoning attacks. Our results hold for different distributions of client data, varying poisoning targets, and various sybil strategies.\nCode can be found at: this https URL",
                        "Citation Paper Authors": "Authors:Clement Fung, Chris J.M. Yoon, Ivan Beschastnikh"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.14527v2": {
            "Paper Title": "Towards Measuring Vulnerabilities and Exposures in Open-Source Packages",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.15259v2": {
            "Paper Title": "On the Impossible Safety of Large AI Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.07376v2": {
            "Paper Title": "ScionFL: Efficient and Robust Secure Quantized Aggregation",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": "provide a comprehensive analysis of secure aggregation\nschemes w.r.t. their suitability for FL.\nTo the best of our knowledge, only Chen et al. ",
                    "Citation Text": "Wei-Ning Chen, Christopher A. Choquette-Choo, Peter Kairouz, and\nAnanda Theertha Suresh. 2022. The Fundamental Price of Secure Aggrega-\ntion in Differentially Private Federated Learning. In ICML .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.03761",
                        "Citation Paper Title": "Title:The Fundamental Price of Secure Aggregation in Differentially Private Federated Learning",
                        "Citation Paper Abstract": "Abstract:We consider the problem of training a $d$ dimensional model with distributed differential privacy (DP) where secure aggregation (SecAgg) is used to ensure that the server only sees the noisy sum of $n$ model updates in every training round. Taking into account the constraints imposed by SecAgg, we characterize the fundamental communication cost required to obtain the best accuracy achievable under $\\varepsilon$ central DP (i.e. under a fully trusted server and no communication constraints). Our results show that $\\tilde{O}\\left( \\min(n^2\\varepsilon^2, d) \\right)$ bits per client are both sufficient and necessary, and this fundamental limit can be achieved by a linear scheme based on sparse random projections. This provides a significant improvement relative to state-of-the-art SecAgg distributed DP schemes which use $\\tilde{O}(d\\log(d/\\varepsilon^2))$ bits per client.\nEmpirically, we evaluate our proposed scheme on real-world federated learning tasks. We find that our theoretical analysis is well matched in practice. In particular, we show that we can reduce the communication cost significantly to under $1.2$ bits per parameter in realistic privacy settings without decreasing test-time performance. Our work hence theoretically and empirically specifies the fundamental price of using SecAgg.",
                        "Citation Paper Authors": "Authors:Wei-Ning Chen, Christopher A. Choquette-Choo, Peter Kairouz, Ananda Theertha Suresh"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.11222v2": {
            "Paper Title": "Learning-Augmented Private Algorithms for Multiple Quantile Release",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.07114v2": {
            "Paper Title": "Recent Advances in Reliable Deep Graph Learning: Inherent Noise,\n  Distribution Shift, and Adversarial Attack",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.07321v4": {
            "Paper Title": "Machine Generated Text: A Comprehensive Survey of Threat Models and\n  Detection Methods",
            "Sentences": [
                {
                    "Sentence ID": 81,
                    "Sentence": "\u2014 but such\ndefenses represent an important first step to increase the difficulty of automation. As spam results in large\nvolumes of text, and detection of machine generated text is easier on long sequence lengths ",
                    "Citation Text": "Daphne Ippolito, Daniel Duckworth, et al .2020. Automatic Detection of Generated Text is Easiest when Humans are Fooled. In ACL.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.00650",
                        "Citation Paper Title": "Title:Automatic Detection of Generated Text is Easiest when Humans are Fooled",
                        "Citation Paper Abstract": "Abstract:Recent advancements in neural language modelling make it possible to rapidly generate vast amounts of human-sounding text. The capabilities of humans and automatic discriminators to detect machine-generated text have been a large source of research interest, but humans and machines rely on different cues to make their decisions. Here, we perform careful benchmarking and analysis of three popular sampling-based decoding strategies---top-$k$, nucleus sampling, and untruncated random sampling---and show that improvements in decoding methods have primarily optimized for fooling humans. This comes at the expense of introducing statistical abnormalities that make detection easy for automatic systems. We also show that though both human and automatic detector performance improve with longer excerpt length, even multi-sentence excerpts can fool expert human raters over 30% of the time. Our findings reveal the importance of using both human and automatic detectors to assess the humanness of text generation systems.",
                        "Citation Paper Authors": "Authors:Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck"
                    }
                },
                {
                    "Sentence ID": 74,
                    "Sentence": ". CAPTCHA is not a perfect defense \u2014 iterative versions of\nhuman-verification schemes and bypass methods are in continuous adversarial development ",
                    "Citation Text": "Meriem Guerar, Luca Verderame, et al .2021. Gotta CAPTCHA \u2019Em All: A Survey of 20 Years of the Human-or-Computer Dilemma.\nACM Comput. Surv. 54, 9, Article 192 (oct 2021), 33 pages. https://doi.org/10.1145/3477142",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.01748",
                        "Citation Paper Title": "Title:Gotta CAPTCHA 'Em All: A Survey of Twenty years of the Human-or-Computer Dilemma",
                        "Citation Paper Abstract": "Abstract:A recent study has found that malicious bots generated nearly a quarter of overall website traffic in 2019 [100]. These malicious bots perform activities such as price and content scraping, account creation and takeover, credit card fraud, denial of service, etc. Thus, they represent a serious threat to all businesses in general, but are especially troublesome for e-commerce, travel and financial services. One of the most common defense mechanisms against bots abusing online services is the introduction of Completely Automated Public Turing test to tell Computers and Humans Apart (CAPTCHA), so it is extremely important to understand which CAPTCHA schemes have been designed and their actual effectiveness against the ever-evolving bots. To this end, this work provides an overview of the current state-of-the-art in the field of CAPTCHA schemes and defines a new classification that includes all the emerging schemes. In addition, for each identified CAPTCHA category, the most successful attack methods are summarized by also describing how CAPTCHA schemes evolved to resist bot attacks, and discussing the limitations of different CAPTCHA schemes from the security, usability and compatibility point of view. Finally, an assessment of the open issues, challenges, and opportunities for further study is provided, paving the road toward the design of the next-generation secure and user-friendly CAPTCHA schemes.",
                        "Citation Paper Authors": "Authors:Meriem Guerar, Luca Verderame, Mauro Migliardi, Francesco Palmieri, Alessio Merlo"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.01392v3": {
            "Paper Title": "JShelter: Give Me My Browser Back",
            "Sentences": [
                {
                    "Sentence ID": 57,
                    "Sentence": "Peter Snyder, Lara Ansari, Cynthia Taylor, and Chris Kanich. 2016. Browser\nFeature Usage on the Modern Web. In Proceedings of the 2016 Internet Measurement\nConference (Santa Monica, California, USA) (IMC \u201916) . ACM, 97\u2013110. https:\n//doi.org/10.1145/2987443.2987466 ",
                    "Citation Text": "Peter Snyder, Cynthia Taylor, and Chris Kanich. 2017. Most Websites Don\u2019T\nNeed to Vibrate: A Cost-Benefit Approach to Improving Browser Security. In\nProceedings of the 2017 ACM SIGSAC Conference on Computer and Communications\nSecurity (Dallas, Texas, USA) (CCS \u201917) . ACM, 179\u2013194. https://doi.org/10.1145/\n3133956.3133966",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.08510",
                        "Citation Paper Title": "Title:Most Websites Don't Need to Vibrate: A Cost-Benefit Approach to Improving Browser Security",
                        "Citation Paper Abstract": "Abstract:Modern web browsers have accrued an incredibly broad set of features since being invented for hypermedia dissemination in 1990. Many of these features benefit users by enabling new types of web applications. However, some features also bring risk to users' privacy and security, whether through implementation error, unexpected composition, or unintended use. Currently there is no general methodology for weighing these costs and benefits. Restricting access to only the features which are necessary for delivering desired functionality on a given website would allow users to enforce the principle of lease privilege on use of the myriad APIs present in the modern web browser.\nHowever, security benefits gained by increasing restrictions must be balanced against the risk of breaking existing websites. This work addresses this problem with a methodology for weighing the costs and benefits of giving websites default access to each browser feature. We model the benefit as the number of websites that require the feature for some user-visible benefit, and the cost as the number of CVEs, lines of code, and academic attacks related to the functionality. We then apply this methodology to 74 Web API standards implemented in modern browsers. We find that allowing websites default access to large parts of the Web API poses significant security and privacy risks, with little corresponding benefit.\nWe also introduce a configurable browser extension that allows users to selectively restrict access to low-benefit, high-risk features on a per site basis. We evaluated our extension with two hardened browser configurations, and found that blocking 15 of the 74 standards avoids 52.0% of code paths related to previous CVEs, and 50.0% of implementation code identified by our metric, without affecting the functionality of 94.7% of measured websites.",
                        "Citation Paper Authors": "Authors:Peter Snyder, Cynthia Taylor, Chris Kanich"
                    }
                },
                {
                    "Sentence ID": 56,
                    "Sentence": "Michael Smith, Craig Disselkoen, Shravan Narayan, Fraser Brown, and Deian\nStefan. 2018. Browser history re:visited. In 12th USENIX Workshop on Offen-\nsive Technologies (WOOT 18) . USENIX Association. https://www.usenix.org/\nconference/woot18/presentation/smith ",
                    "Citation Text": "Peter Snyder, Lara Ansari, Cynthia Taylor, and Chris Kanich. 2016. Browser\nFeature Usage on the Modern Web. In Proceedings of the 2016 Internet Measurement\nConference (Santa Monica, California, USA) (IMC \u201916) . ACM, 97\u2013110. https:\n//doi.org/10.1145/2987443.2987466",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.06467",
                        "Citation Paper Title": "Title:Browser Feature Usage on the Modern Web",
                        "Citation Paper Abstract": "Abstract:Modern web browsers are incredibly complex, with millions of lines of code and over one thousand JavaScript functions and properties available to website authors. This work investigates how these browser features are used on the modern, open web. We find that JavaScript features differ wildly in popularity, with over 50% of provided features never used in the Alexa 10k.\nWe also look at how popular ad and tracking blockers change the distribution of features used by sites, and identify a set of approximately 10% of features that are disproportionately blocked (prevented from executing by these extensions at least 90% of the time they are used). We additionally find that in the presence of these blockers, over 83% of available features are executed on less than 1% of the most popular 10,000 websites.\nWe additionally measure a variety of aspects of browser feature usage on the web, including how complex sites have become in terms of feature usage, how the length of time a browser feature has been in the browser relates to its usage on the web, and how many security vulnerabilities have been associated with related browser features.",
                        "Citation Paper Authors": "Authors:Peter Snyder, Lara Ansari, Cynthia Taylor, Chris Kanich"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "Nick Nikiforakis, Alexandros Kapravelos, Wouter Joosen, Christopher Kruegel,\nFrank Piessens, and Giovanni Vigna. 2013. Cookieless Monster: Exploring the\nEcosystem of Web-Based Device Fingerprinting. In 2013 IEEE Symposium on\nSecurity and Privacy . 541\u2013555. ",
                    "Citation Text": "Yossef Oren, Vasileios P. Kemerlis, Simha Sethumadhavan, and Angelos D.\nKeromytis. 2015. The Spy in the Sandbox: Practical Cache Attacks in JavaScript\nand Their Implications. In Proceedings of the 22nd ACM SIGSAC Conference on\nComputer and Communications Security (Denver, Colorado, USA) (CCS \u201915) . ACM,\n1406\u20131418. https://doi.org/10.1145/2810103.2813708",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1502.07373",
                        "Citation Paper Title": "Title:The Spy in the Sandbox -- Practical Cache Attacks in Javascript",
                        "Citation Paper Abstract": "Abstract:We present the first micro-architectural side-channel attack which runs entirely in the browser. In contrast to other works in this genre, this attack does not require the attacker to install any software on the victim's machine -- to facilitate the attack, the victim needs only to browse to an untrusted webpage with attacker-controlled content. This makes the attack model highly scalable and extremely relevant and practical to today's web, especially since most desktop browsers currently accessing the Internet are vulnerable to this attack. Our attack, which is an extension of the last-level cache attacks of Yarom et al., allows a remote adversary recover information belonging to other processes, other users and even other virtual machines running on the same physical host as the victim web browser. We describe the fundamentals behind our attack, evaluate its performance using a high bandwidth covert channel and finally use it to construct a system-wide mouse/network activity logger. Defending against this attack is possible, but the required countermeasures can exact an impractical cost on other benign uses of the web browser and of the computer.",
                        "Citation Paper Authors": "Authors:Yossef Oren, Vasileios P. Kemerlis, Simha Sethumadhavan, Angelos D. Keromytis"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "Zach Jorgensen and Ting Yu. 2011. On Mouse Dynamics as a Behavioral Biometric\nfor Authentication. In Proceedings of the 6th ACM Symposium on Information,\nComputer and Communications Security (Hong Kong, China) (ASIACCS \u201911) . ACM,\n476\u2013482. https://doi.org/10.1145/1966913.1966983 ",
                    "Citation Text": "Paul Kocher, Daniel Genkin, Daniel Gruss, Werner Haas, Mike Hamburg, Moritz\nLipp, Stefan Mangard, Thomas Prescher, Michael Schwarz, and Yuval Yarom.\n2018. Spectre Attacks: Exploiting Speculative Execution. CoRR abs/1801.01203\n(2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.01203",
                        "Citation Paper Title": "Title:Spectre Attacks: Exploiting Speculative Execution",
                        "Citation Paper Abstract": "Abstract:Modern processors use branch prediction and speculative execution to maximize performance. For example, if the destination of a branch depends on a memory value that is in the process of being read, CPUs will try guess the destination and attempt to execute ahead. When the memory value finally arrives, the CPU either discards or commits the speculative computation. Speculative logic is unfaithful in how it executes, can access to the victim's memory and registers, and can perform operations with measurable side effects.\nSpectre attacks involve inducing a victim to speculatively perform operations that would not occur during correct program execution and which leak the victim's confidential information via a side channel to the adversary. This paper describes practical attacks that combine methodology from side channel attacks, fault attacks, and return-oriented programming that can read arbitrary memory from the victim's process. More broadly, the paper shows that speculative execution implementations violate the security assumptions underpinning numerous software security mechanisms, including operating system process separation, static analysis, containerization, just-in-time (JIT) compilation, and countermeasures to cache timing/side-channel attacks. These attacks represent a serious threat to actual systems, since vulnerable speculative execution capabilities are found in microprocessors from Intel, AMD, and ARM that are used in billions of devices.\nWhile makeshift processor-specific countermeasures are possible in some cases, sound solutions will require fixes to processor designs as well as updates to instruction set architectures (ISAs) to give hardware architects and software developers a common understanding as to what computation state CPU implementations are (and are not) permitted to leak.",
                        "Citation Paper Authors": "Authors:Paul Kocher, Daniel Genkin, Daniel Gruss, Werner Haas, Mike Hamburg, Moritz Lipp, Stefan Mangard, Thomas Prescher, Michael Schwarz, Yuval Yarom"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "D. Gruss, M. Lipp, M. Schwarz, D. Genkin, J. Juffinger, S. O\u2019Connell, W. Schoechl,\nand Y. Yarom. 2018. Another Flip in the Wall of Rowhammer Defenses. In 2018\nIEEE Symposium on Security and Privacy (SP) . 245\u2013261. https://doi.org/10.1109/\nSP.2018.00031 ",
                    "Citation Text": "Daniel Gruss, Cl\u00e9mentine Maurice, and Stefan Mangard. 2016. Rowhammer.js: A\nRemote Software-Induced Fault Attack in JavaScript. In Detection of Intrusions\nand Malware, and Vulnerability Assessment . Springer International Publishing,\n300\u2013321.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1507.06955",
                        "Citation Paper Title": "Title:Rowhammer.js: A Remote Software-Induced Fault Attack in JavaScript",
                        "Citation Paper Abstract": "Abstract:A fundamental assumption in software security is that a memory location can only be modified by processes that may write to this memory location. However, a recent study has shown that parasitic effects in DRAM can change the content of a memory cell without accessing it, but by accessing other memory locations in a high frequency. This so-called Rowhammer bug occurs in most of today's memory modules and has fatal consequences for the security of all affected systems, e.g., privilege escalation attacks.\nAll studies and attacks related to Rowhammer so far rely on the availability of a cache flush instruction in order to cause accesses to DRAM modules at a sufficiently high frequency. We overcome this limitation by defeating complex cache replacement policies. We show that caches can be forced into fast cache eviction to trigger the Rowhammer bug with only regular memory accesses. This allows to trigger the Rowhammer bug in highly restricted and even scripting environments.\nWe demonstrate a fully automated attack that requires nothing but a website with JavaScript to trigger faults on remote hardware. Thereby we can gain unrestricted access to systems of website visitors. We show that the attack works on off-the-shelf systems. Existing countermeasures fail to protect against this new Rowhammer attack.",
                        "Citation Paper Authors": "Authors:Daniel Gruss, Cl\u00e9mentine Maurice, Stefan Mangard"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "Daniel Gruss, David Bidner, and Stefan Mangard. 2015. Practical Memory Dedu-\nplication Attacks in Sandboxed Javascript. In Computer Security \u2013 ESORICS 2015 .\nSpringer International Publishing, 108\u2013122. ",
                    "Citation Text": "D. Gruss, M. Lipp, M. Schwarz, D. Genkin, J. Juffinger, S. O\u2019Connell, W. Schoechl,\nand Y. Yarom. 2018. Another Flip in the Wall of Rowhammer Defenses. In 2018\nIEEE Symposium on Security and Privacy (SP) . 245\u2013261. https://doi.org/10.1109/\nSP.2018.00031",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.00551",
                        "Citation Paper Title": "Title:Another Flip in the Wall of Rowhammer Defenses",
                        "Citation Paper Abstract": "Abstract:The Rowhammer bug allows unauthorized modification of bits in DRAM cells from unprivileged software, enabling powerful privilege-escalation attacks. Sophisticated Rowhammer countermeasures have been presented, aiming at mitigating the Rowhammer bug or its exploitation. However, the state of the art provides insufficient insight on the completeness of these defenses. In this paper, we present novel Rowhammer attack and exploitation primitives, showing that even a combination of all defenses is ineffective. Our new attack technique, one-location hammering, breaks previous assumptions on requirements for triggering the Rowhammer bug, i.e., we do not hammer multiple DRAM rows but only keep one DRAM row constantly open. Our new exploitation technique, opcode flipping, bypasses recent isolation mechanisms by flipping bits in a predictable and targeted way in userspace binaries. We replace conspicuous and memory-exhausting spraying and grooming techniques with a novel reliable technique called memory waylaying. Memory waylaying exploits system-level optimizations and a side channel to coax the operating system into placing target pages at attacker-chosen physical locations. Finally, we abuse Intel SGX to hide the attack entirely from the user and the operating system, making any inspection or detection of the attack infeasible. Our Rowhammer enclave can be used for coordinated denial-of-service attacks in the cloud and for privilege escalation on personal computers. We demonstrate that our attacks evade all previously proposed countermeasures for commodity systems.",
                        "Citation Paper Authors": "Authors:Daniel Gruss, Moritz Lipp, Michael Schwarz, Daniel Genkin, Jonas Juffinger, Sioli O'Connell, Wolfgang Schoechl, Yuval Yarom"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.09199v3": {
            "Paper Title": "A Non-invasive Technique to Detect Authentic/Counterfeit SRAM Chips",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.10024v3": {
            "Paper Title": "Diagnostics for Deep Neural Networks with Automated Copy/Paste Attacks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.07202v3": {
            "Paper Title": "Dizzy: Large-Scale Crawling and Analysis of Onion Services",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.13947v2": {
            "Paper Title": "Privacy-Preserving Federated Recurrent Neural Networks",
            "Sentences": [
                {
                    "Sentence ID": 85,
                    "Sentence": ", and various recurrent-weight initialization techniques via identity or orthogonal matrices ",
                    "Citation Text": "Q. V . Le, N. Jaitly, and G. E. Hinton. A simple way to initialize recurrent networks of rectified linear units. ArXiv , abs/1504.00941, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1504.00941",
                        "Citation Paper Title": "Title:A Simple Way to Initialize Recurrent Networks of Rectified Linear Units",
                        "Citation Paper Abstract": "Abstract:Learning long term dependencies in recurrent networks is difficult due to vanishing and exploding gradients. To overcome this difficulty, researchers have developed sophisticated optimization techniques and network architectures. In this paper, we propose a simpler solution that use recurrent neural networks composed of rectified linear units. Key to our solution is the use of the identity matrix or its scaled version to initialize the recurrent weight matrix. We find that our solution is comparable to LSTM on our four benchmarks: two toy problems involving long-range temporal structures, a large language modeling problem and a benchmark speech recognition problem.",
                        "Citation Paper Authors": "Authors:Quoc V. Le, Navdeep Jaitly, Geoffrey E. Hinton"
                    }
                },
                {
                    "Sentence ID": 159,
                    "Sentence": ". Overall, existing SMC solutions enable the training of regression models and feed-forward\nneural networks, whereas RNNs have not been studied. One recent work, which relies on additive secret-sharing, investigated the training\nof recurrent neural networks as a case study ",
                    "Citation Text": "Y . Zhang, G. Bai, X. Li, C. Curtis, C. Chen, and R. K. L. Ko. Privcoll: Practical privacy-preserving collaborative machine learning. In Computer Security \u2013 ESORICS\n2020 , pages 399\u2013418, Cham, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.06953",
                        "Citation Paper Title": "Title:PrivColl: Practical Privacy-Preserving Collaborative Machine Learning",
                        "Citation Paper Abstract": "Abstract:Collaborative learning enables two or more participants, each with their own training dataset, to collaboratively learn a joint model. It is desirable that the collaboration should not cause the disclosure of either the raw datasets of each individual owner or the local model parameters trained on them. This privacy-preservation requirement has been approached through differential privacy mechanisms, homomorphic encryption (HE) and secure multiparty computation (MPC), but existing attempts may either introduce the loss of model accuracy or imply significant computational and/or communicational overhead. In this work, we address this problem with the lightweight additive secret sharing technique. We propose PrivColl, a framework for protecting local data and local models while ensuring the correctness of training processes. PrivColl employs secret sharing technique for securely evaluating addition operations in a multiparty computation environment, and achieves practicability by employing only the homomorphic addition operations. We formally prove that it guarantees privacy preservation even though the majority (n-2 out of n) of participants are corrupted. With experiments on real-world datasets, we further demonstrate that PrivColl retains high efficiency. It achieves a speedup of more than 45X over the state-of-the-art MPC/HE based schemes for training linear/logistic regression, and 216X faster for training neural network.",
                        "Citation Paper Authors": "Authors:Yanjun Zhang, Guangdong Bai, Xue Li, Caitlin Curtis, Chen Chen, Ryan K L Ko"
                    }
                },
                {
                    "Sentence ID": 99,
                    "Sentence": "such that the aggregated global model meets the DP requirements. McMahan et al. recently applied DP to RNNs\nto protect user-level privacy ",
                    "Citation Text": "H. B. McMahan, D. Ramage, K. Talwar, and L. Zhang. Learning differentially private recurrent language models. CoRR , abs/1710.06963, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.06963",
                        "Citation Paper Title": "Title:Learning Differentially Private Recurrent Language Models",
                        "Citation Paper Abstract": "Abstract:We demonstrate that it is possible to train large recurrent language models with user-level differential privacy guarantees with only a negligible cost in predictive accuracy. Our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent. In particular, we add user-level privacy protection to the federated averaging algorithm, which makes \"large step\" updates from user-level data. Our work demonstrates that given a dataset with a sufficiently large number of users (a requirement easily met by even small internet-scale datasets), achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work. We find that our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a large dataset.",
                        "Citation Paper Authors": "Authors:H. Brendan McMahan, Daniel Ramage, Kunal Talwar, Li Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.00879v2": {
            "Paper Title": "Quantum Cryptography in Algorithmica",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.03117v3": {
            "Paper Title": "Going In Style: Audio Backdoors Through Stylistic Transformations",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": ", where the adversary adds the trigger to\nsome training samples but also alters their labels, and clean-\nlabel ",
                    "Citation Text": "A. Turner, D. Tsipras, and A. Madry, \u201cLabel-consistent\nbackdoor attacks,\u201d arXiv preprint arXiv:1912.02771 ,\n2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.02771",
                        "Citation Paper Title": "Title:Label-Consistent Backdoor Attacks",
                        "Citation Paper Abstract": "Abstract:Deep neural networks have been demonstrated to be vulnerable to backdoor attacks. Specifically, by injecting a small number of maliciously constructed inputs into the training set, an adversary is able to plant a backdoor into the trained model. This backdoor can then be activated during inference by a backdoor trigger to fully control the model's behavior. While such attacks are very effective, they crucially rely on the adversary injecting arbitrary inputs that are---often blatantly---mislabeled. Such samples would raise suspicion upon human inspection, potentially revealing the attack. Thus, for backdoor attacks to remain undetected, it is crucial that they maintain label-consistency---the condition that injected inputs are consistent with their labels. In this work, we leverage adversarial perturbations and generative models to execute efficient, yet label-consistent, backdoor attacks. Our approach is based on injecting inputs that appear plausible, yet are hard to classify, hence causing the model to rely on the (easier-to-learn) backdoor trigger.",
                        "Citation Paper Authors": "Authors:Alexander Turner, Dimitris Tsipras, Aleksander Madry"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "(CV), the trigger is a patch at a pixel level that\nreplaces a part of the original sample. In audio, a static trigger\ncould be a tone superimposed on the original sample ",
                    "Citation Text": "S. Koffas, J. Xu, M. Conti, and S. Picek, \u201cCan you hear\nit? backdoor attacks via ultrasonic triggers,\u201d in Proceed-\nings of the 2022 ACM Workshop on Wireless Security\nand Machine Learning , p. 57\u201362, Association for Com-\nputing Machinery, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.14569",
                        "Citation Paper Title": "Title:Can You Hear It? Backdoor Attacks via Ultrasonic Triggers",
                        "Citation Paper Abstract": "Abstract:This work explores backdoor attacks for automatic speech recognition systems where we inject inaudible triggers. By doing so, we make the backdoor attack challenging to detect for legitimate users, and thus, potentially more dangerous. We conduct experiments on two versions of a speech dataset and three neural networks and explore the performance of our attack concerning the duration, position, and type of the trigger. Our results indicate that less than 1% of poisoned data is sufficient to deploy a backdoor attack and reach a 100% attack success rate. We observed that short, non-continuous triggers result in highly successful attacks. However, since our trigger is inaudible, it can be as long as possible without raising any suspicions making the attack more effective. Finally, we conducted our attack in actual hardware and saw that an adversary could manipulate inference in an Android application by playing the inaudible trigger over the air.",
                        "Citation Paper Authors": "Authors:Stefanos Koffas, Jing Xu, Mauro Conti, Stjepan Picek"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": ". For example,\ncrowdsourced datasets like Mozilla\u2019s Common V oice ",
                    "Citation Text": "R. Ardila, M. Branson, K. Davis, M. Henretty,\nM. Kohler, J. Meyer, R. Morais, L. Saunders,\nF. M. Tyers, and G. Weber, \u201cCommon voice: A\nmassively-multilingual speech corpus,\u201d arXiv preprint\narXiv:1912.06670 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.06670",
                        "Citation Paper Title": "Title:Common Voice: A Massively-Multilingual Speech Corpus",
                        "Citation Paper Abstract": "Abstract:The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla's DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 +/- 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition.",
                        "Citation Paper Authors": "Authors:Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis M. Tyers, Gregor Weber"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": ". For im-\nages,\u000fis usually an additive noise at a pixel level computed\nvia a gradient-based procedure ",
                    "Citation Text": "S. Huang, N. Papernot, I. Goodfellow, Y . Duan, and\nP. Abbeel, \u201cAdversarial attacks on neural network poli-\ncies,\u201d 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.02284",
                        "Citation Paper Title": "Title:Adversarial Attacks on Neural Network Policies",
                        "Citation Paper Abstract": "Abstract:Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at this http URL.",
                        "Citation Paper Authors": "Authors:Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, Pieter Abbeel"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.16000v3": {
            "Paper Title": "StyleFool: Fooling Video Classification Systems via Style Transfer",
            "Sentences": [
                {
                    "Sentence ID": 15,
                    "Sentence": ".\nIn the image domain, RS is proved to be effective for `2-norm\nattacks ",
                    "Citation Text": "J. Cohen, E. Rosenfeld, and Z. Kolter, \u201cCerti\ufb01ed adversarial robustness\nvia randomized smoothing,\u201d in Proceedings of the 36th International\nConference on Machine Learning (ICML) , 2019, pp. 1310\u20131320.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.02918",
                        "Citation Paper Title": "Title:Certified Adversarial Robustness via Randomized Smoothing",
                        "Citation Paper Abstract": "Abstract:We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the $\\ell_2$ norm. This \"randomized smoothing\" technique has been proposed recently in the literature, but existing guarantees are loose. We prove a tight robustness guarantee in $\\ell_2$ norm for smoothing with Gaussian noise. We use randomized smoothing to obtain an ImageNet classifier with e.g. a certified top-1 accuracy of 49% under adversarial perturbations with $\\ell_2$ norm less than 0.5 (=127/255). No certified defense has been shown feasible on ImageNet except for smoothing. On smaller-scale datasets where competing approaches to certified $\\ell_2$ robustness are viable, smoothing delivers higher certified accuracies. Our strong empirical results suggest that randomized smoothing is a promising direction for future research into adversarially robust classification. Code and models are available at this http URL.",
                        "Citation Paper Authors": "Authors:Jeremy M Cohen, Elan Rosenfeld, J. Zico Kolter"
                    }
                },
                {
                    "Sentence ID": 81,
                    "Sentence": ". RS has\nbeen proved to certify `2perturbations, but is not effective\nfor`p(p>2), especially `1perturbations ",
                    "Citation Text": "A. Kumar, A. Levine, T. Goldstein, and S. Feizi, \u201cCurse of dimensional-\nity on randomized smoothing for certi\ufb01able robustness,\u201d in Proceedings\nof the International Conference on Machine Learning (ICML) , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.03239",
                        "Citation Paper Title": "Title:Curse of Dimensionality on Randomized Smoothing for Certifiable Robustness",
                        "Citation Paper Abstract": "Abstract:Randomized smoothing, using just a simple isotropic Gaussian distribution, has been shown to produce good robustness guarantees against $\\ell_2$-norm bounded adversaries. In this work, we show that extending the smoothing technique to defend against other attack models can be challenging, especially in the high-dimensional regime. In particular, for a vast class of i.i.d.~smoothing distributions, we prove that the largest $\\ell_p$-radius that can be certified decreases as $O(1/d^{\\frac{1}{2} - \\frac{1}{p}})$ with dimension $d$ for $p > 2$. Notably, for $p \\geq 2$, this dependence on $d$ is no better than that of the $\\ell_p$-radius that can be certified using isotropic Gaussian smoothing, essentially putting a matching lower bound on the robustness radius. When restricted to {\\it generalized} Gaussian smoothing, these two bounds can be shown to be within a constant factor of each other in an asymptotic sense, establishing that Gaussian smoothing provides the best possible results, up to a constant factor, when $p \\geq 2$. We present experimental results on CIFAR to validate our theory. For other smoothing distributions, such as, a uniform distribution within an $\\ell_1$ or an $\\ell_\\infty$-norm ball, we show upper bounds of the form $O(1 / d)$ and $O(1 / d^{1 - \\frac{1}{p}})$ respectively, which have an even worse dependence on $d$.",
                        "Citation Paper Authors": "Authors:Aounon Kumar, Alexander Levine, Tom Goldstein, Soheil Feizi"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "as our benchmarks in the experiments. Consid-\nering that existing black-box attacks ",
                    "Citation Text": "N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and\nA. Swami, \u201cPractical black-box attacks against machine learning,\u201d in\nProceedings of the 2017 ACM on Asia Conference on Computer and\nCommunications Security (AsiaCCS) , 2017, pp. 506\u2013519.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.02697",
                        "Citation Paper Title": "Title:Practical Black-Box Attacks against Machine Learning",
                        "Citation Paper Abstract": "Abstract:Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19% and 88.94%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.",
                        "Citation Paper Authors": "Authors:Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik, Ananthram Swami"
                    }
                },
                {
                    "Sentence ID": 72,
                    "Sentence": ". Therefore,\nthe attack begins with an instance xadvselected from the\ntarget classytand is optimized by Projected Gradient Decent\n(PGD) ",
                    "Citation Text": "A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, \u201cTowards\ndeep learning models resistant to adversarial attacks,\u201d in Proceedings of\nthe International Conference on Learning Representations (ICLR) , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06083",
                        "Citation Paper Title": "Title:Towards Deep Learning Models Resistant to Adversarial Attacks",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL.",
                        "Citation Paper Authors": "Authors:Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu"
                    }
                },
                {
                    "Sentence ID": 61,
                    "Sentence": "\ufb01rst explored image style\ntransfer using image analog. Subsequently, it was found that\ncontent loss and style loss can better preserve the similarity\nbetween an input image and the stylized image in the feature\nspace ",
                    "Citation Text": "L. A. Gatys, A. S. Ecker, and B. Matthias, \u201cA neural algorithm of artistic\nstyle,\u201d arXiv preprint arXiv:1508.06576, 2015 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1508.06576",
                        "Citation Paper Title": "Title:A Neural Algorithm of Artistic Style",
                        "Citation Paper Abstract": "Abstract:In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.",
                        "Citation Paper Authors": "Authors:Leon A. Gatys, Alexander S. Ecker, Matthias Bethge"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.00328v3": {
            "Paper Title": "Differentially Private Learning with Per-Sample Adaptive Clipping",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.06284v4": {
            "Paper Title": "Visual Prompting for Adversarial Robustness",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.11835v2": {
            "Paper Title": "Towards a Theory of Maximal Extractable Value I: Constant Function\n  Market Makers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.05872v2": {
            "Paper Title": "Byzantine-robust Federated Learning through Collaborative Malicious\n  Gradient Filtering",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": ".III. R ETHINKING OF LIE A TTACK\nIn this section, we present our theoretical analysis along\nwith empirical evidence of the Little is Enough (LIE) attack ",
                    "Citation Text": "G. Baruch, M. Baruch, and Y . Goldberg, \u201cA little is enough: Circumvent-\ning defenses for distributed learning,\u201d in Advances in Neural Information\nProcessing Systems (NIPS) , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.06156",
                        "Citation Paper Title": "Title:A Little Is Enough: Circumventing Defenses For Distributed Learning",
                        "Citation Paper Abstract": "Abstract:Distributed learning is central for large-scale training of deep-learning models. However, they are exposed to a security threat in which Byzantine participants can interrupt or control the learning process. Previous attack models and their corresponding defenses assume that the rogue participants are (a) omniscient (know the data of all other participants), and (b) introduce large change to the parameters. We show that small but well-crafted changes are sufficient, leading to a novel non-omniscient attack on distributed learning that go undetected by all existing defenses. We demonstrate our attack method works not only for preventing convergence but also for repurposing of the model behavior (backdooring). We show that 20% of corrupt workers are sufficient to degrade a CIFAR10 model accuracy by 50%, as well as to introduce backdoors into MNIST and CIFAR10 models without hurting their accuracy",
                        "Citation Paper Authors": "Authors:Moran Baruch, Gilad Baruch, Yoav Goldberg"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": ". Besides, the client-side momentum SGD\ncan also be considered as a history-aided method and can help\nto alleviate the impact of Byzantine attacks ",
                    "Citation Text": "S. P. Karimireddy, L. He, and M. Jaggi, \u201cLearning from history for\nbyzantine robust optimization,\u201d in Proceedings of International Confer-\nence on Machine Learning, ICML , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.10333",
                        "Citation Paper Title": "Title:Learning from History for Byzantine Robust Optimization",
                        "Citation Paper Abstract": "Abstract:Byzantine robustness has received significant attention recently given its importance for distributed and federated learning. In spite of this, we identify severe flaws in existing algorithms even when the data across the participants is identically distributed. First, we show realistic examples where current state of the art robust aggregation rules fail to converge even in the absence of any Byzantine attackers. Secondly, we prove that even if the aggregation rules may succeed in limiting the influence of the attackers in a single round, the attackers can couple their attacks across time eventually leading to divergence. To address these issues, we present two surprisingly simple strategies: a new robust iterative clipping procedure, and incorporating worker momentum to overcome time-coupled attacks. This is the first provably robust method for the standard stochastic optimization setting. Our code is open sourced at this https URL.",
                        "Citation Paper Authors": "Authors:Sai Praneeth Karimireddy, Lie He, Martin Jaggi"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "use error rate based and loss function based\nrejection mechanism to reject gradients that have a bad impact\non model updating. In ",
                    "Citation Text": "X. Cao, M. Fang, J. Liu, and N. Z. Gong, \u201cFLTrust: Byzantine-\nrobust federated learning via trust bootstrapping,\u201d ISOC Network and\nDistributed Systems Security (NDSS) Symposium , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.13995",
                        "Citation Paper Title": "Title:FLTrust: Byzantine-robust Federated Learning via Trust Bootstrapping",
                        "Citation Paper Abstract": "Abstract:Byzantine-robust federated learning aims to enable a service provider to learn an accurate global model when a bounded number of clients are malicious. The key idea of existing Byzantine-robust federated learning methods is that the service provider performs statistical analysis among the clients' local model updates and removes suspicious ones, before aggregating them to update the global model. However, malicious clients can still corrupt the global models in these methods via sending carefully crafted local model updates to the service provider. The fundamental reason is that there is no root of trust in existing federated learning methods.\nIn this work, we bridge the gap via proposing FLTrust, a new federated learning method in which the service provider itself bootstraps trust. In particular, the service provider itself collects a clean small training dataset (called root dataset) for the learning task and the service provider maintains a model (called server model) based on it to bootstrap trust. In each iteration, the service provider first assigns a trust score to each local model update from the clients, where a local model update has a lower trust score if its direction deviates more from the direction of the server model update. Then, the service provider normalizes the magnitudes of the local model updates such that they lie in the same hyper-sphere as the server model update in the vector space. Our normalization limits the impact of malicious local model updates with large magnitudes. Finally, the service provider computes the average of the normalized local model updates weighted by their trust scores as a global model update, which is used to update the global model. Our extensive evaluations on six datasets from different domains show that our FLTrust is secure against both existing attacks and strong adaptive attacks.",
                        "Citation Paper Authors": "Authors:Xiaoyu Cao, Minghong Fang, Jia Liu, Neil Zhenqiang Gong"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": ". In particular, the\nByzantine threats can be viewed as worst-case attacks, in\nwhich corrupted clients can produce arbitrary outputs and are\nallowed to collude ",
                    "Citation Text": "P. Blanchard, E. M. E. Mhamdi, R. Guerraoui, and J. Stainer, \u201cMa-\nchine learning with adversaries: Byzantine tolerant gradient descent,\u201d in\nAdvances in Neural Information Processing Systems (NIPS) , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.02757",
                        "Citation Paper Title": "Title:Byzantine-Tolerant Machine Learning",
                        "Citation Paper Abstract": "Abstract:The growth of data, the need for scalability and the complexity of models used in modern machine learning calls for distributed implementations. Yet, as of today, distributed machine learning frameworks have largely ignored the possibility of arbitrary (i.e., Byzantine) failures. In this paper, we study the robustness to Byzantine failures at the fundamental level of stochastic gradient descent (SGD), the heart of most machine learning algorithms. Assuming a set of $n$ workers, up to $f$ of them being Byzantine, we ask how robust can SGD be, without limiting the dimension, nor the size of the parameter space.\nWe first show that no gradient descent update rule based on a linear combination of the vectors proposed by the workers (i.e, current approaches) tolerates a single Byzantine failure. We then formulate a resilience property of the update rule capturing the basic requirements to guarantee convergence despite $f$ Byzantine workers. We finally propose Krum, an update rule that satisfies the resilience property aforementioned. For a $d$-dimensional learning problem, the time complexity of Krum is $O(n^2 \\cdot (d + \\log n))$.",
                        "Citation Paper Authors": "Authors:Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, Julien Stainer"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "A. Safety & Security in Federated Learning\nThe model safety and data security are essential principles\nof federated learning due to the concern of privacy risks and\nadversarial threats ",
                    "Citation Text": "Q. Yang, Y . Liu, T. Chen, and Y . Tong, \u201cFederated machine learning:\nConcept and applications,\u201d ACM Trans. Intell. Syst. Technol. , vol. 10,\nno. 2, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.04885",
                        "Citation Paper Title": "Title:Federated Machine Learning: Concept and Applications",
                        "Citation Paper Abstract": "Abstract:Today's AI still faces two major challenges. One is that in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security. We propose a possible solution to these challenges: secure federated learning. Beyond the federated learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated learning framework, which includes horizontal federated learning, vertical federated learning and federated transfer learning. We provide definitions, architectures and applications for the federated learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allow knowledge to be shared without compromising user privacy.",
                        "Citation Paper Authors": "Authors:Qiang Yang, Yang Liu, Tianjian Chen, Yongxin Tong"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.02701v2": {
            "Paper Title": "On the Discredibility of Membership Inference Attacks",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": "attacks\ndoes not work well in the low false positive regime. We omit other\nmembership inference attacks in this study, such as Jayaraman ",
                    "Citation Text": "Bargav Jayaraman, Lingxiao Wang, Katherine Knipmeyer, Quanquan Gu, and\nDavid Evans. 2021. Revisiting membership inference under realistic assumptions.\nIn Proceedings on Privacy Enhancing Technologies (PoPETs) (2021).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.10881",
                        "Citation Paper Title": "Title:Revisiting Membership Inference Under Realistic Assumptions",
                        "Citation Paper Abstract": "Abstract:We study membership inference in settings where some of the assumptions typically used in previous research are relaxed. First, we consider skewed priors, to cover cases such as when only a small fraction of the candidate pool targeted by the adversary are actually members and develop a PPV-based metric suitable for this setting. This setting is more realistic than the balanced prior setting typically considered by researchers. Second, we consider adversaries that select inference thresholds according to their attack goals and develop a threshold selection procedure that improves inference attacks. Since previous inference attacks fail in imbalanced prior setting, we develop a new inference attack based on the intuition that inputs corresponding to training set members will be near a local minimum in the loss function, and show that an attack that combines this with thresholds on the per-instance loss can achieve high PPV even in settings where other attacks appear to be ineffective. Code for our experiments can be found here: this https URL.",
                        "Citation Paper Authors": "Authors:Bargav Jayaraman, Lingxiao Wang, Katherine Knipmeyer, Quanquan Gu, David Evans"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": ". For Algorithm 1 to work, we need a large public\ndataset to search from. For CIFAR-10, we use the CINIC dataset ",
                    "Citation Text": "Luke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. 2018.\nCinic-10 is not imagenet or cifar-10. arXiv preprint arXiv:1810.03505 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.03505",
                        "Citation Paper Title": "Title:CINIC-10 is not ImageNet or CIFAR-10",
                        "Citation Paper Abstract": "Abstract:In this brief technical report we introduce the CINIC-10 dataset as a plug-in extended alternative for CIFAR-10. It was compiled by combining CIFAR-10 with images selected and downsampled from the ImageNet database. We present the approach to compiling the dataset, illustrate the example images for different classes, give pixel distributions for each part of the repository, and give some standard benchmarks for well known models. Details for download, usage, and compilation can be found in the associated github repository.",
                        "Citation Paper Authors": "Authors:Luke N. Darlow, Elliot J. Crowley, Antreas Antoniou, Amos J. Storkey"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.11973v4": {
            "Paper Title": "Liquidity Risks in Lending Protocols: Evidence from Aave Protocol",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.14921v3": {
            "Paper Title": "IvySyn: Automated Vulnerability Discovery in Deep Learning Frameworks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.11367v3": {
            "Paper Title": "Combining AI and AM - Improving Approximate Matching through Transformer\n  Networks",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": ". Compared to BERT,\nTinyBERT models have signi\ufb01cantly less number of param-\neters, which improves training and inference time notice-\nably. All our experiments were implemented in the deep-\nlearning framework PyTorch ",
                    "Citation Text": "Adam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas K \u00a8opf, Edward Z.\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. Pytorch: An imper-\native style, high-performance deep learning library. In\nAdvances in Neural Information Processing Systems\n(NeurIPS) , pages 8024\u20138035, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.01703",
                        "Citation Paper Title": "Title:PyTorch: An Imperative Style, High-Performance Deep Learning Library",
                        "Citation Paper Abstract": "Abstract:Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.\nIn this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.\nWe demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.",
                        "Citation Paper Authors": "Authors:Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": ", transformers are the domi-\nnating architecture for natural language processing and are\neven increasingly used for image processing ",
                    "Citation Text": "Alexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. An image is worth\n16x16 words: Transformers for image recognition\nat scale. In International Conference on Learning\nRepresentations (ICLR) , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.11929",
                        "Citation Paper Title": "Title:An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
                        "Citation Paper Abstract": "Abstract:While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",
                        "Citation Paper Authors": "Authors:Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.09187v3": {
            "Paper Title": "Quantization Backdoors to Deep Learning Commercial Frameworks",
            "Sentences": [
                {
                    "Sentence ID": 46,
                    "Sentence": "recently\nproposed differs from our PQ backdoor in twomajor aspects:\nmethodology and ef\ufb01cacy; impact and generality. We also\nnote that the other works ",
                    "Citation Text": "S. Hong, M.-A. Panaitescu-Liess, Y. Kaya, and T. Dumitras, \u201cQu-\nanti-zation: Exploiting quantization artifacts for achieving adver-\nsarial outcomes,\u201d Advances in Neural Information Processing Systems ,\nvol. 34, pp. 9303\u20139316, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.13541",
                        "Citation Paper Title": "Title:Qu-ANTI-zation: Exploiting Quantization Artifacts for Achieving Adversarial Outcomes",
                        "Citation Paper Abstract": "Abstract:Quantization is a popular technique that $transforms$ the parameter representation of a neural network from floating-point numbers into lower-precision ones ($e.g.$, 8-bit integers). It reduces the memory footprint and the computational cost at inference, facilitating the deployment of resource-hungry models. However, the parameter perturbations caused by this transformation result in $behavioral$ $disparities$ between the model before and after quantization. For example, a quantized model can misclassify some test-time samples that are otherwise classified correctly. It is not known whether such differences lead to a new security vulnerability. We hypothesize that an adversary may control this disparity to introduce specific behaviors that activate upon quantization. To study this hypothesis, we weaponize quantization-aware training and propose a new training framework to implement adversarial quantization outcomes. Following this framework, we present three attacks we carry out with quantization: (i) an indiscriminate attack for significant accuracy loss; (ii) a targeted attack against specific samples; and (iii) a backdoor attack for controlling the model with an input trigger. We further show that a single compromised model defeats multiple quantization schemes, including robust quantization techniques. Moreover, in a federated learning scenario, we demonstrate that a set of malicious participants who conspire can inject our quantization-activated backdoor. Lastly, we discuss potential counter-measures and show that only re-training consistently removes the attack artifacts. Our code is available at this https URL",
                        "Citation Paper Authors": "Authors:Sanghyun Hong, Michael-Andrei Panaitescu-Liess, Yi\u011fitcan Kaya, Tudor Dumitra\u015f"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.03692v4": {
            "Paper Title": "Incompatibility Clustering as a Defense Against Backdoor Poisoning\n  Attacks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.02814v4": {
            "Paper Title": "A Subexponential Quantum Algorithm for the Semidirect Discrete Logarithm\n  Problem",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.12044v2": {
            "Paper Title": "Fed-FSNet: Mitigating Non-I.I.D. Federated Learning via Fuzzy\n  Synthesizing Network",
            "Sentences": [
                {
                    "Sentence ID": 52,
                    "Sentence": "proposed an attention mechanism in FL considering the\ncontribution of each local model for aggregation and constructed a more generalized mobile keyboard\nsuggestion model. More recently, AFL ",
                    "Citation Text": "Jack Goetz, Kshitiz Malik, Duc Bui, Seungwhan Moon, Honglei Liu, and Anuj Kumar. Active\nfederated learning. arXiv preprint arXiv:1909.12641 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.12641",
                        "Citation Paper Title": "Title:Active Federated Learning",
                        "Citation Paper Abstract": "Abstract:Federated Learning allows for population level models to be trained without centralizing client data by transmitting the global model to clients, calculating gradients locally, then averaging the gradients. Downloading models and uploading gradients uses the client's bandwidth, so minimizing these transmission costs is important. The data on each client is highly variable, so the benefit of training on different clients may differ dramatically. To exploit this we propose Active Federated Learning, where in each round clients are selected not uniformly at random, but with a probability conditioned on the current model and the data on the client to maximize efficiency. We propose a cheap, simple and intuitive sampling scheme which reduces the number of required training iterations by 20-70% while maintaining the same model accuracy, and which mimics well known resampling techniques under certain conditions.",
                        "Citation Paper Authors": "Authors:Jack Goetz, Kshitiz Malik, Duc Bui, Seungwhan Moon, Honglei Liu, Anuj Kumar"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "proposed a multi-task learning framework to handle the statistical Non-I.I.D.\nchallenge which is robust to practical systems issues. But this solution differs signi\ufb01cantly from the\nprevious work on FL. FedAvg ",
                    "Citation Text": "Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.\nCommunication-ef\ufb01cient learning of deep networks from decentralized data. In AISTATS , pages\n1273\u20131282. PMLR, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.05629",
                        "Citation Paper Title": "Title:Communication-Efficient Learning of Deep Networks from Decentralized Data",
                        "Citation Paper Abstract": "Abstract:Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning.\nWe present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.",
                        "Citation Paper Authors": "Authors:H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, Blaise Ag\u00fcera y Arcas"
                    }
                },
                {
                    "Sentence ID": 65,
                    "Sentence": "incorporated a novel multi-\ntask discrimination process on the reality, category, and identity of the target device for inferring\nuser-level class representatives. Differently, DLG ",
                    "Citation Text": "Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. In Advances in Neural\nInformation Processing Systems , pages 14774\u201314784, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.08935",
                        "Citation Paper Title": "Title:Deep Leakage from Gradients",
                        "Citation Paper Abstract": "Abstract:Exchanging gradients is a widely used method in modern multi-node machine learning system (e.g., distributed training, collaborative learning). For a long time, people believed that gradients are safe to share: i.e., the training data will not be leaked by gradient exchange. However, we show that it is possible to obtain the private training data from the publicly shared gradients. We name this leakage as Deep Leakage from Gradient and empirically validate the effectiveness on both computer vision and natural language processing tasks. Experimental results show that our attack is much stronger than previous approaches: the recovery is pixel-wise accurate for images and token-wise matching for texts. We want to raise people's awareness to rethink the gradient's safety. Finally, we discuss several possible strategies to prevent such deep leakage. The most effective defense method is gradient pruning.",
                        "Citation Paper Authors": "Authors:Ligeng Zhu, Zhijian Liu, Song Han"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": "has demonstrated that the iterative model averaging can work\nwith certain non-I.I.D. data. FedAtt ",
                    "Citation Text": "S Ji, S Pan, G Long, X Li, J Jiang, and Z Huang. Learning private neural language modeling\nwith attentive aggregation. In The 2019 International Joint Conference on Neural Networks\n(IJCNN) , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.07108",
                        "Citation Paper Title": "Title:Learning Private Neural Language Modeling with Attentive Aggregation",
                        "Citation Paper Abstract": "Abstract:Mobile keyboard suggestion is typically regarded as a word-level language modeling problem. Centralized machine learning technique requires massive user data collected to train on, which may impose privacy concerns for sensitive personal typing data of users. Federated learning (FL) provides a promising approach to learning private language modeling for intelligent personalized keyboard suggestion by training models in distributed clients rather than training in a central server. To obtain a global model for prediction, existing FL algorithms simply average the client models and ignore the importance of each client during model aggregation. Furthermore, there is no optimization for learning a well-generalized global model on the central server. To solve these problems, we propose a novel model aggregation with the attention mechanism considering the contribution of clients models to the global model, together with an optimization technique during server aggregation. Our proposed attentive aggregation method minimizes the weighted distance between the server model and client models through iterative parameters updating while attends the distance between the server model and client models. Through experiments on two popular language modeling datasets and a social media dataset, our proposed method outperforms its counterparts in terms of perplexity and communication cost in most settings of comparison.",
                        "Citation Paper Authors": "Authors:Shaoxiong Ji, Shirui Pan, Guodong Long, Xue Li, Jing Jiang, Zi Huang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.07568v6": {
            "Paper Title": "StratDef: Strategic Defense Against Adversarial Attacks in ML-based\n  Malware Detection",
            "Sentences": [
                {
                    "Sentence ID": 65,
                    "Sentence": ". Another\napproach is to mask and obfuscate gradients, though this\nhas been found ineffective in later work ",
                    "Citation Text": "N. Papernot, P . McDaniel, I. Goodfellow, S. Jha, Z. Celik, and\nA. Swami, \u201cPractical black-box attacks against machine learn-\ning.\u201d Proceedings of the 2017 ACM on Asia conference on computer\nand communications security , pp. 506\u2013519, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.02697",
                        "Citation Paper Title": "Title:Practical Black-Box Attacks against Machine Learning",
                        "Citation Paper Abstract": "Abstract:Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19% and 88.94%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.",
                        "Citation Paper Authors": "Authors:Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik, Ananthram Swami"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": ". For\nexample, an image of a panda may be incorrectly classi\ufb01ed\nas a gibbon ",
                    "Citation Text": "I. J. Goodfellow, J. Shlens, and C. Szegedy, \u201cExplaining and\nharnessing adversarial examples,\u201d arXiv preprint arXiv:1412.6572 ,\n2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.6572",
                        "Citation Paper Title": "Title:Explaining and Harnessing Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy"
                    }
                },
                {
                    "Sentence ID": 95,
                    "Sentence": ".\n5.2 Training Models & Defenses\nOther Models & Defenses. To construct all models, we use\nthe scikit-learn ",
                    "Citation Text": "F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion,\nO. Grisel, M. Blondel, P . Prettenhofer, R. Weiss, V . Dubourg,\nJ. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot,\nand E. Duchesnay, \u201cScikit-learn: Machine learning in Python,\u201d\nJournal of Machine Learning Research , vol. 12, pp. 2825\u20132830, 2011.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1201.0490",
                        "Citation Paper Title": "Title:Scikit-learn: Machine Learning in Python",
                        "Citation Paper Abstract": "Abstract:Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from this http URL.",
                        "Citation Paper Authors": "Authors:Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Andreas M\u00fcller, Joel Nothman, Gilles Louppe, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, \u00c9douard Duchesnay"
                    }
                },
                {
                    "Sentence ID": 64,
                    "Sentence": "also3\nproposed a randomization-based defense, though this has\nbeen shown to be ineffective by Athayle et al. ",
                    "Citation Text": "A. Athalye, N. Carlini, and D. Wagner, \u201cObfuscated gradients\ngive a false sense of security: Circumventing defenses to adver-17\nsarial examples,\u201d in International conference on machine learning .\nPMLR, 2018, pp. 274\u2013283.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.00420",
                        "Citation Paper Title": "Title:Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimization-based attacks, we find defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining non-certified white-box-secure defenses at ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers.",
                        "Citation Paper Authors": "Authors:Anish Athalye, Nicholas Carlini, David Wagner"
                    }
                },
                {
                    "Sentence ID": 63,
                    "Sentence": "which involves utilizing different-sized neural\nnetworks to improve the generalization of the main model,\nthough Stokes et al. ",
                    "Citation Text": "J. W. Stokes, D. Wang, M. Marinescu, M. Marino, and B. Bus-\nsone, \u201cAttack and defense of dynamic analysis-based, ad-\nversarial neural malware classi\ufb01cation models,\u201d arXiv preprint\narXiv:1712.05919 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.05919",
                        "Citation Paper Title": "Title:Attack and Defense of Dynamic Analysis-Based, Adversarial Neural Malware Classification Models",
                        "Citation Paper Abstract": "Abstract:Recently researchers have proposed using deep learning-based systems for malware detection. Unfortunately, all deep learning classification systems are vulnerable to adversarial attacks. Previous work has studied adversarial attacks against static analysis-based malware classifiers which only classify the content of the unknown file without execution. However, since the majority of malware is either packed or encrypted, malware classification based on static analysis often fails to detect these types of files. To overcome this limitation, anti-malware companies typically perform dynamic analysis by emulating each file in the anti-malware engine or performing in-depth scanning in a virtual machine. These strategies allow the analysis of the malware after unpacking or decryption. In this work, we study different strategies of crafting adversarial samples for dynamic analysis. These strategies operate on sparse, binary inputs in contrast to continuous inputs such as pixels in images. We then study the effects of two, previously proposed defensive mechanisms against crafted adversarial samples including the distillation and ensemble defenses. We also propose and evaluate the weight decay defense. Experiments show that with these three defensive strategies, the number of successfully crafted adversarial samples is reduced compared to a standard baseline system without any defenses. In particular, the ensemble defense is the most resilient to adversarial attacks. Importantly, none of the defenses significantly reduce the classification accuracy for detecting malware. Finally, we demonstrate that while adding additional hidden layers to neural models does not significantly improve the malware classification accuracy, it does significantly increase the classifier's robustness to adversarial attacks.",
                        "Citation Paper Authors": "Authors:Jack W. Stokes, De Wang, Mady Marinescu, Marc Marino, Brian Bussone"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.05978v2": {
            "Paper Title": "From the Hardness of Detecting Superpositions to Cryptography: Quantum\n  Public Key Encryption and Commitments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.03944v2": {
            "Paper Title": "Robust and Imperceptible Black-box DNN Watermarking Based on Fourier\n  Perturbation Analysis and Frequency Sensitivity Clustering",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": "propose a novel method based on spread transform dither\nmodulation. The above methods directly modulate the existing\nweights of the host DNN, which rarely consider the ambiguity\nattack. Fan et al. ",
                    "Citation Text": "L. Fan, K. W. Ng, and C. S. Chan, \u201cRethinking deep neural network\nownership veri\ufb01cation: embedding passports to defeat ambiguity attacks,\u201d\nIn:Proc. Neural Information Processing Systems , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.07830",
                        "Citation Paper Title": "Title:[Extended version] Rethinking Deep Neural Network Ownership Verification: Embedding Passports to Defeat Ambiguity Attacks",
                        "Citation Paper Abstract": "Abstract:With substantial amount of time, resources and human (team) efforts invested to explore and develop successful deep neural networks (DNN), there emerges an urgent need to protect these inventions from being illegally copied, redistributed, or abused without respecting the intellectual properties of legitimate owners. Following recent progresses along this line, we investigate a number of watermark-based DNN ownership verification methods in the face of ambiguity attacks, which aim to cast doubts on the ownership verification by forging counterfeit watermarks. It is shown that ambiguity attacks pose serious threats to existing DNN watermarking methods. As remedies to the above-mentioned loophole, this paper proposes novel passport-based DNN ownership verification schemes which are both robust to network modifications and resilient to ambiguity attacks. The gist of embedding digital passports is to design and train DNN models in a way such that, the DNN inference performance of an original task will be significantly deteriorated due to forged passports. In other words, genuine passports are not only verified by looking for the predefined signatures, but also reasserted by the unyielding DNN model inference performances. Extensive experimental results justify the effectiveness of the proposed passport-based DNN ownership verification schemes. Code and models are available at this https URL",
                        "Citation Paper Authors": "Authors:Lixin Fan, Kam Woh Ng, Chee Seng Chan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.12265v2": {
            "Paper Title": "High-Throughput GPU Implementation of Dilithium Post-Quantum Digital\n  Signature",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.04785v2": {
            "Paper Title": "SALSA: Attacking Lattice Cryptography with Transformers",
            "Sentences": [
                {
                    "Sentence ID": 58,
                    "Sentence": "discussed the dif\ufb01culty of\nperforming arithmetic operations with language models. Be spoke network architectures have been\nproposed for arithmetic operations [39, 68], and transform ers were used for addition and similar\noperations ",
                    "Citation Text": "Power, A., Burda, Y ., Edwards, H., Babuschkin, I., and M isra, V . (2022). Grokking: General-\nization beyond over\ufb01tting on small algorithmic datasets. arXiv preprint arXiv:2022.\n16",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.02177",
                        "Citation Paper Title": "Title:Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
                        "Citation Paper Abstract": "Abstract:In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of \"grokking\" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.",
                        "Citation Paper Authors": "Authors:Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, Vedant Misra"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "discussed their limits when genera lizing out of their training distribution.\nTransformers have been applied to dynamical systems ",
                    "Citation Text": "Charton, F., Hayat, A., and Lample, G. (2020). Learning advanced mathematical computations\nfrom examples. arXiv preprint arXiv:2006.06462.\n14",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.06462",
                        "Citation Paper Title": "Title:Learning advanced mathematical computations from examples",
                        "Citation Paper Abstract": "Abstract:Using transformers over large generated datasets, we train models to learn mathematical properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect prediction of qualitative characteristics, and good approximations of numerical features of the system. This demonstrates that neural networks can learn to perform complex computations, grounded in advanced theory, from examples, without built-in mathematical knowledge.",
                        "Citation Paper Authors": "Authors:Fran\u00e7ois Charton, Amaury Hayat, Guillaume Lample"
                    }
                },
                {
                    "Sentence ID": 70,
                    "Sentence": "showed that symbolic math problems could be s olved to state-of-the-art accuracy\nwith transformers. ",
                    "Citation Text": "Welleck, S., West, P., Cao, J., and Choi, Y . (2021). Symb olic brittleness in sequence models:\non systematic generalization in symbolic mathematics. arXiv preprint arXiv:2109.13986.\n17Appendix\nA Further Details of LWE\nA.1 Ring Learning with Errors (\u00a72)\nWe now de\ufb01ne RLWE samples and explain how to get LWE instances from them. Let nbe a power\nof 2, and let Rq=Zq[x]/(xn+1) be the set of polynomials whose degrees are at most n\u22121and\ncoef\ufb01cients are from Zq. The set Rqforms a ring with additions and multiplications de\ufb01ned as th e\nusual polynomial additions and multiplications in Zp[x]moduloxn+1. One RLWE sample refers\nto the pair\n(a(x),b(x) :=a(x)\u00b7s(x)+e(x)),\nwheres(x)\u2208Rqis the secret and e(x)\u2208Rqis the error with coef\ufb01cients subject to the error\ndistribution.\nLeta,sande\u2208Zn\nqbe the coef\ufb01cient vectors of a(x),s(x)ande(x). Then the coef\ufb01cient vector b\nofb(x)can be obtained via the formula\nb=Acirc\na(x)\u00b7s+e,\nhere Acirc\na(x)represents the n\u00d7ngeneralized circulant matrix of a(x). Precisely, let a(x) =a0+\na1x+...+an\u22122xn\u22122+an\u22121xn\u22121, then a= (a0,a1,...,a n\u22122,an\u22121)and\nAcirc\na(x)=\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0a0\u2212an\u22121\u2212an\u22122...\u2212a1\na1a0\u2212an\u22121...\u2212a2\na2a1a0...\u2212a3\n...............\nan\u22121an\u22122an\u22123... a 0\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb.\nTherefore, one RLWE sample gives rise to nLWE instances by taking the rows of Acirc\na(x)and the\ncorresponding entries in b.\nA.2 Search to Decision Reduction for Binary Secrets (\u00a72)\nWe give a proof of the search binary-LWE to decisional binary -LWE reduction. This is a simple adap-\ntion of the reduction in",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.13986",
                        "Citation Paper Title": "Title:Symbolic Brittleness in Sequence Models: on Systematic Generalization in Symbolic Mathematics",
                        "Citation Paper Abstract": "Abstract:Neural sequence models trained with maximum likelihood estimation have led to breakthroughs in many tasks, where success is defined by the gap between training and test performance. However, their ability to achieve stronger forms of generalization remains unclear. We consider the problem of symbolic mathematical integration, as it requires generalizing systematically beyond the test set. We develop a methodology for evaluating generalization that takes advantage of the problem domain's structure and access to a verifier. Despite promising in-distribution performance of sequence-to-sequence models in this domain, we demonstrate challenges in achieving robustness, compositionality, and out-of-distribution generalization, through both carefully constructed manual test suites and a genetic algorithm that automatically finds large collections of failures in a controllable manner. Our investigation highlights the difficulty of generalizing well with the predominant modeling and learning approach, and the importance of evaluating beyond the test set, across different aspects of generalization.",
                        "Citation Paper Authors": "Authors:Sean Welleck, Peter West, Jize Cao, Yejin Choi"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "showed large transformers could achieve hig h accuracy on elementary/high school\nproblems. A second line explores various applications of tr ansformers on formalized symbolic\nproblems. ",
                    "Citation Text": "Lample, G. and Charton, F. (2019). Deep learning for sym bolic mathematics. arXiv preprint\narXiv:1912.01412.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.01412",
                        "Citation Paper Title": "Title:Deep Learning for Symbolic Mathematics",
                        "Citation Paper Abstract": "Abstract:Neural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica.",
                        "Citation Paper Authors": "Authors:Guillaume Lample, Fran\u00e7ois Charton"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.04792v3": {
            "Paper Title": "Robust Fingerprint of Location Trajectories Under Differential Privacy",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": "as a popular privacy definition has been\nused to protect location datasets in recent years [ 8,28,29,35].\nGeo-indistinguishability ",
                    "Citation Text": "Miguel E Andr\u00e9s, Nicol\u00e1s E Bordenabe, Konstantinos Chatzikokolakis, and Catus-\ncia Palamidessi. 2013. Geo-indistinguishability: Differential privacy for location-\nbased systems. In Proceedings of the 2013 ACM SIGSAC conference on Computer &\ncommunications security . 901\u2013914.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1212.1984",
                        "Citation Paper Title": "Title:Geo-Indistinguishability: Differential Privacy for Location-Based Systems",
                        "Citation Paper Abstract": "Abstract:The growing popularity of location-based systems, allowing unknown/untrusted servers to easily collect huge amounts of information regarding users' location, has recently started raising serious privacy concerns. In this paper we study geo-indistinguishability, a formal notion of privacy for location-based systems that protects the user's exact location, while allowing approximate information - typically needed to obtain a certain desired service - to be released. Our privacy definition formalizes the intuitive notion of protecting the user's location within a radius r with a level of privacy that depends on r, and corresponds to a generalized version of the well-known concept of differential privacy. Furthermore, we present a perturbation technique for achieving geo-indistinguishability by adding controlled random noise to the user's location. We demonstrate the applicability of our technique on a LBS application. Finally, we compare our mechanism with other ones in the literature. It turns our that our mechanism offers the best privacy guarantees, for the same utility, among all those which do not depend on the prior.",
                        "Citation Paper Authors": "Authors:Miguel E. Andr\u00e9s, Nicol\u00e1s E. Bordenabe, Konstantinos Chatzikokolakis, Catuscia Palamidessi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.03205v3": {
            "Paper Title": "Unlimited Lives: Secure In-Process Rollback with Isolated Domains",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.06140v2": {
            "Paper Title": "Differentially Private Bootstrap: New Privacy Analysis and Inference\n  Strategies",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.10075v3": {
            "Paper Title": "ICSML: Industrial Control Systems ML Framework for native inference\n  using IEC 61131-3 code",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": ", Alves et al. use an open\nsource PLC platform to embed an ML based intrusion prevention\nsystem onto the device, that thwarts network flood attacks like De-\nnial of Service. Teixera et al. ",
                    "Citation Text": "Marcio Andrey Teixeira, Tara Salman, Maede Zolanvari, Raj Jain, Nader Me-\nskin, and Mohammed Samaka. 2018. SCADA System Testbed for Cybersecu-\nrity Research Using Machine Learning Approach. Future Internet 10, 8 (2018).\nhttps://doi.org/10.3390/fi10080076",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.00753",
                        "Citation Paper Title": "Title:SCADA System Testbed for Cybersecurity Research Using Machine Learning Approach",
                        "Citation Paper Abstract": "Abstract:This paper presents the development of a Supervisory Control and Data Acquisition (SCADA) system testbed used for cybersecurity research. The testbed consists of a water storage tank's control system, which is a stage in the process of water treatment and distribution. Sophisticated cyber-attacks were conducted against the testbed. During the attacks, the network traffic was captured, and features were extracted from the traffic to build a dataset for training and testing different machine learning algorithms. Five traditional machine learning algorithms were trained to detect the attacks: Random Forest, Decision Tree, Logistic Regression, Naive Bayes and KNN. Then, the trained machine learning models were built and deployed in the network, where new tests were made using online network traffic. The performance obtained during the training and testing of the machine learning models was compared to the performance obtained during the online deployment of these models in the network. The results show the efficiency of the machine learning models in detecting the attacks in real time. The testbed provides a good understanding of the effects and consequences of attacks on real SCADA environments",
                        "Citation Paper Authors": "Authors:Marcio Andrey Teixeira, Tara Salman, Maede Zolanvari, Raj Jain, Nader Meskin, Mohammed Samaka"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.10986v2": {
            "Paper Title": "SoK: Let the Privacy Games Begin! A Unified Treatment of Data Inference\n  Privacy in Machine Learning",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": "de\ufb01ne subject-level differential\nprivacy by considering datasets as adjacent when they differ\nin the data of a user, which can be seen as a counterpart to\nuser-level membership inference. Humphries et al. ",
                    "Citation Text": "T. Humphries, S. Oya, L. Tulloch, M. Rafuse, I. Gold-\nberg, U. Hengartner, and F. Kerschbaum. Investigating\nmembership inference attacks under data dependencies.\narXiv preprint arXiv:2010.12112 [cs.CR] , 2020. Cited\non pp. 3, 5, 6, 9, 10, and 11",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.12112",
                        "Citation Paper Title": "Title:Investigating Membership Inference Attacks under Data Dependencies",
                        "Citation Paper Abstract": "Abstract:Training machine learning models on privacy-sensitive data has become a popular practice, driving innovation in ever-expanding fields. This has opened the door to new attacks that can have serious privacy implications. One such attack, the Membership Inference Attack (MIA), exposes whether or not a particular data point was used to train a model. A growing body of literature uses Differentially Private (DP) training algorithms as a defence against such attacks. However, these works evaluate the defence under the restrictive assumption that all members of the training set, as well as non-members, are independent and identically distributed. This assumption does not hold for many real-world use cases in the literature. Motivated by this, we evaluate membership inference with statistical dependencies among samples and explain why DP does not provide meaningful protection (the privacy parameter $\\epsilon$ scales with the training set size $n$) in this more general case. We conduct a series of empirical evaluations with off-the-shelf MIAs using training sets built from real-world data showing different types of dependencies among samples. Our results reveal that training set dependencies can severely increase the performance of MIAs, and therefore assuming that data samples are statistically independent can significantly underestimate the performance of MIAs.",
                        "Citation Paper Authors": "Authors:Thomas Humphries, Simon Oya, Lindsey Tulloch, Matthew Rafuse, Ian Goldberg, Urs Hengartner, Florian Kerschbaum"
                    }
                },
                {
                    "Sentence ID": 70,
                    "Sentence": "X \u2013 X \u2013 \u2013 \u2013 X \u2013 X \u2013 \u2013\nAttribute Inference and Model Inversion\nAI Game 5 ",
                    "Citation Text": "S. Yeom, I. Giacomelli, A. Menaged, M. Fredrikson, and\nS. Jha. Over\ufb01tting, robustness, and malicious algorithms:\nA study of potential causes of privacy risk in machine\nlearning. J. of Comput. Secur. , 28(1):35\u201370, 2020. Cited\non pp. 2, 3, 4, 5, 6, 9, and 10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.01604",
                        "Citation Paper Title": "Title:Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting",
                        "Citation Paper Abstract": "Abstract:Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role.\nThis paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.",
                        "Citation Paper Authors": "Authors:Samuel Yeom, Irene Giacomelli, Matt Fredrikson, Somesh Jha"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "X \u2013 X \u2013 \u2013 X X \u2013 X \u2013 \u2013\nMIUserGame 4 ",
                    "Citation Text": "S. Mahloujifar, H. A. Inan, M. Chase, E. Ghosh, and\nM. Hasegawa. Membership inference on word em-\nbedding and beyond. arXiv preprint arXiv:2106.11384\n[cs.CL] , 2021. Cited on pp. 3, 6, and 10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.11384",
                        "Citation Paper Title": "Title:Membership Inference on Word Embedding and Beyond",
                        "Citation Paper Abstract": "Abstract:In the text processing context, most ML models are built on word embeddings. These embeddings are themselves trained on some datasets, potentially containing sensitive data. In some cases this training is done independently, in other cases, it occurs as part of training a larger, task-specific model. In either case, it is of interest to consider membership inference attacks based on the embedding layer as a way of understanding sensitive information leakage. But, somewhat surprisingly, membership inference attacks on word embeddings and their effect in other natural language processing (NLP) tasks that use these embeddings, have remained relatively unexplored.\nIn this work, we show that word embeddings are vulnerable to black-box membership inference attacks under realistic assumptions. Furthermore, we show that this leakage persists through two other major NLP applications: classification and text-generation, even when the embedding layer is not exposed to the attacker. We show that our MI attack achieves high attack accuracy against a classifier model and an LSTM-based language model. Indeed, our attack is a cheaper membership inference attack on text-generative models, which does not require the knowledge of the target model or any expensive training of text-generative models as shadow models.",
                        "Citation Paper Authors": "Authors:Saeed Mahloujifar, Huseyin A. Inan, Melissa Chase, Esha Ghosh, Marcello Hasegawa"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "\u2013 X X \u2013 \u2013 X \u2013 \u2013 X \u2013 \u2013\nMIBBGame 2 ",
                    "Citation Text": "N. Carlini, S. Chien, M. Nasr, S. Song, A. Terzis, and\nF. Tram `er. Membership inference attacks from \ufb01rst\nprinciples. In 43rd IEEE Symposium on Security and\nPrivacy (S&P) , pp. 1546\u20131564. IEEE, 2022. Cited on\npp. 4, 5, 6, and 10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.03570",
                        "Citation Paper Title": "Title:Membership Inference Attacks From First Principles",
                        "Citation Paper Abstract": "Abstract:A membership inference attack allows an adversary to query a trained machine learning model to predict whether or not a particular example was contained in the model's training dataset. These attacks are currently evaluated using average-case \"accuracy\" metrics that fail to characterize whether the attack can confidently identify any members of the training set. We argue that attacks should instead be evaluated by computing their true-positive rate at low (e.g., <0.1%) false-positive rates, and find most prior attacks perform poorly when evaluated in this way. To address this we develop a Likelihood Ratio Attack (LiRA) that carefully combines multiple ideas from the literature. Our attack is 10x more powerful at low false-positive rates, and also strictly dominates prior attacks on existing metrics.",
                        "Citation Paper Authors": "Authors:Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, Florian Tramer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.03195v5": {
            "Paper Title": "More is Better (Mostly): On the Backdoor Attacks in Federated Graph\n  Neural Networks",
            "Sentences": [
                {
                    "Sentence ID": 37,
                    "Sentence": ". As shown in Exp. III and\nIV, we assume 10,20, and 100clients for a synthetic dataset, i.e.,\nTRIANGLES, which is a realistic real-world cross-silo scenario ",
                    "Citation Text": "Virat Shejwalkar, Amir Houmansadr, Peter Kairouz, and Daniel Ramage. 2022.\nBack to the drawing board: A critical evaluation of poisoning attacks on pro-\nduction federated learning. In 2022 IEEE Symposium on Security and Privacy (SP) .\nIEEE, 1354\u20131371.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.10241",
                        "Citation Paper Title": "Title:Back to the Drawing Board: A Critical Evaluation of Poisoning Attacks on Production Federated Learning",
                        "Citation Paper Abstract": "Abstract:While recent works have indicated that federated learning (FL) may be vulnerable to poisoning attacks by compromised clients, their real impact on production FL systems is not fully understood. In this work, we aim to develop a comprehensive systemization for poisoning attacks on FL by enumerating all possible threat models, variations of poisoning, and adversary capabilities. We specifically put our focus on untargeted poisoning attacks, as we argue that they are significantly relevant to production FL deployments.\nWe present a critical analysis of untargeted poisoning attacks under practical, production FL environments by carefully characterizing the set of realistic threat models and adversarial capabilities. Our findings are rather surprising: contrary to the established belief, we show that FL is highly robust in practice even when using simple, low-cost defenses. We go even further and propose novel, state-of-the-art data and model poisoning attacks, and show via an extensive set of experiments across three benchmark datasets how (in)effective poisoning attacks are in the presence of simple defense mechanisms. We aim to correct previous misconceptions and offer concrete guidelines to conduct more accurate (and more realistic) research on this topic.",
                        "Citation Paper Authors": "Authors:Virat Shejwalkar, Amir Houmansadr, Peter Kairouz, Daniel Ramage"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "put\nforward a framework to train Federated GNNs for vertical FL and\nrecommendation system, respectively. Moreover, SpreadGNN was\nproposed in ",
                    "Citation Text": "Chaoyang He, Emir Ceyani, Keshav Balasubramanian, Murali Annavaram, and\nSalman Avestimehr. 2021. SpreadGNN: Serverless Multi-task Federated Learning\nfor Graph Neural Networks. arXiv:2106.02743 [cs.LG]",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.02743",
                        "Citation Paper Title": "Title:SpreadGNN: Serverless Multi-task Federated Learning for Graph Neural Networks",
                        "Citation Paper Abstract": "Abstract:Graph Neural Networks (GNNs) are the first choice methods for graph machine learning problems thanks to their ability to learn state-of-the-art level representations from graph-structured data. However, centralizing a massive amount of real-world graph data for GNN training is prohibitive due to user-side privacy concerns, regulation restrictions, and commercial competition. Federated Learning is the de-facto standard for collaborative training of machine learning models over many distributed edge devices without the need for centralization. Nevertheless, training graph neural networks in a federated setting is vaguely defined and brings statistical and systems challenges. This work proposes SpreadGNN, a novel multi-task federated training framework capable of operating in the presence of partial labels and absence of a central server for the first time in the literature. SpreadGNN extends federated multi-task learning to realistic serverless settings for GNNs, and utilizes a novel optimization algorithm with a convergence guarantee, Decentralized Periodic Averaging SGD (DPA-SGD), to solve decentralized multi-task learning problems. We empirically demonstrate the efficacy of our framework on a variety of non-I.I.D. distributed graph-level molecular property prediction datasets with partial labels. Our results show that SpreadGNN outperforms GNN models trained over a central server-dependent federated learning system, even in constrained topologies. The source code is publicly available at this https URL",
                        "Citation Paper Authors": "Authors:Chaoyang He, Emir Ceyani, Keshav Balasubramanian, Murali Annavaram, Salman Avestimehr"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "proposed the\nframework for Federated GNNs to optimize the machine learning\nmodel. Besides, other research works [ 21,43,57] have been dedi-\ncated to enhancing the security of Federated GNNs. By using secure\naggregation, ",
                    "Citation Text": "Meng Jiang, Taeho Jung, Ryan Karl, and Tong Zhao. 2020. Federated dynamic\ngnn with secure aggregation. arXiv preprint arXiv:2009.07351 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.07351",
                        "Citation Paper Title": "Title:Federated Dynamic GNN with Secure Aggregation",
                        "Citation Paper Abstract": "Abstract:Given video data from multiple personal devices or street cameras, can we exploit the structural and dynamic information to learn dynamic representation of objects for applications such as distributed surveillance, without storing data at a central server that leads to a violation of user privacy? In this work, we introduce Federated Dynamic Graph Neural Network (Feddy), a distributed and secured framework to learn the object representations from multi-user graph sequences: i) It aggregates structural information from nearby objects in the current graph as well as dynamic information from those in the previous graph. It uses a self-supervised loss of predicting the trajectories of objects. ii) It is trained in a federated learning manner. The centrally located server sends the model to user devices. Local models on the respective user devices learn and periodically send their learning to the central server without ever exposing the user's data to server. iii) Studies showed that the aggregated parameters could be inspected though decrypted when broadcast to clients for model synchronizing, after the server performed a weighted average. We design an appropriate aggregation mechanism of secure aggregation primitives that can protect the security and privacy in federated learning with scalability. Experiments on four video camera datasets (in four different scenes) as well as simulation demonstrate that Feddy achieves great effectiveness and security.",
                        "Citation Paper Authors": "Authors:Meng Jiang, Taeho Jung, Ryan Karl, Tong Zhao"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": ".\n7In the previous version of the paper (published at the ACSAC conference ",
                    "Citation Text": "Jing Xu, Rui Wang, Stefanos Koffas, Kaitai Liang, and Stjepan Picek. 2022. More is\nBetter (Mostly): On the Backdoor Attacks in Federated Graph Neural Networks. In\nProceedings of the 38th Annual Computer Security Applications Conference (Austin,\nTX, USA) (ACSAC \u201922) . Association for Computing Machinery, New York, NY,\nUSA, 684\u2013698. https://doi.org/10.1145/3564625.3567999",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.03195",
                        "Citation Paper Title": "Title:More is Better (Mostly): On the Backdoor Attacks in Federated Graph Neural Networks",
                        "Citation Paper Abstract": "Abstract:Graph Neural Networks (GNNs) are a class of deep learning-based methods for processing graph domain information. GNNs have recently become a widely used graph analysis method due to their superior ability to learn representations for complex graph data. However, due to privacy concerns and regulation restrictions, centralized GNNs can be difficult to apply to data-sensitive scenarios. Federated learning (FL) is an emerging technology developed for privacy-preserving settings when several parties need to train a shared global model collaboratively. Although several research works have applied FL to train GNNs (Federated GNNs), there is no research on their robustness to backdoor attacks.\nThis paper bridges this gap by conducting two types of backdoor attacks in Federated GNNs: centralized backdoor attacks (CBA) and distributed backdoor attacks (DBA). Our experiments show that the DBA attack success rate is higher than CBA in almost all evaluated cases. For CBA, the attack success rate of all local triggers is similar to the global trigger even if the training set of the adversarial party is embedded with the global trigger. To further explore the properties of two backdoor attacks in Federated GNNs, we evaluate the attack performance for a different number of clients, trigger sizes, poisoning intensities, and trigger densities. Moreover, we explore the robustness of DBA and CBA against one defense. We find that both attacks are robust against the investigated defense, necessitating the need to consider backdoor attacks in Federated GNNs as a novel threat that requires custom defenses.",
                        "Citation Paper Authors": "Authors:Jing Xu, Rui Wang, Stefanos Koffas, Kaitai Liang, Stjepan Picek"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ". The choice of small datasets may\nbe a limitation of our work, but real-world cross-silo settings could\ninvolve only a few different organizations (from two to one hun-\ndred) ",
                    "Citation Text": "Peter Kairouz, H Brendan McMahan, Brendan Avent, et al .2019. Advances and\nopen problems in federated learning. arXiv preprint arXiv:1912.04977 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.04977",
                        "Citation Paper Title": "Title:Advances and Open Problems in Federated Learning",
                        "Citation Paper Abstract": "Abstract:Federated learning (FL) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this paper discusses recent advances and presents an extensive collection of open problems and challenges.",
                        "Citation Paper Authors": "Authors:Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G.L. D'Oliveira, Hubert Eichner, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adri\u00e0 Gasc\u00f3n, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Kone\u010dn\u00fd, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancr\u00e8de Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer \u00d6zg\u00fcr, Rasmus Pagh, Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tram\u00e8r, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, Sen Zhao"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": ". In this work, we use data poisoning for our attacks\nin Federated GNNs as model poisoning requires multiplying large\nfactors to model weights when conducting attacks, which can be\ndetected by traditional byzantine-robust aggregation rules such as\nMedian ",
                    "Citation Text": "Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. 2018.\nByzantine-robust distributed learning: Towards optimal statistical rates. In Inter-\nnational Conference on Machine Learning . PMLR, 5650\u20135659.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.01498",
                        "Citation Paper Title": "Title:Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates",
                        "Citation Paper Abstract": "Abstract:In large-scale distributed learning, security issues have become increasingly important. Particularly in a decentralized environment, some computing units may behave abnormally, or even exhibit Byzantine failures -- arbitrary and potentially adversarial behavior. In this paper, we develop distributed learning algorithms that are provably robust against such failures, with a focus on achieving optimal statistical performance. A main result of this work is a sharp analysis of two robust distributed gradient descent algorithms based on median and trimmed mean operations, respectively. We prove statistical error rates for three kinds of population loss functions: strongly convex, non-strongly convex, and smooth non-convex. In particular, these algorithms are shown to achieve order-optimal statistical error rates for strongly convex losses. To achieve better communication efficiency, we further propose a median-based distributed algorithm that is provably robust, and uses only one communication round. For strongly convex quadratic loss, we show that this algorithm achieves the same optimal error rate as the robust distributed gradient descent algorithms.",
                        "Citation Paper Authors": "Authors:Dong Yin, Yudong Chen, Kannan Ramchandran, Peter Bartlett"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.11234v3": {
            "Paper Title": "Development of a hardware-In-the-Loop (HIL) testbed for cyber-physical\n  security in smart buildings",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.01754v5": {
            "Paper Title": "Cryptography with Certified Deletion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.00686v2": {
            "Paper Title": "Federated Learning in Non-IID Settings Aided by Differentially Private\n  Synthetic Data",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": "to introduce control variates and correct local up-\ndates. The above two studies were verified in experiments\non MNIST and EMINIST with multinomial logistic regres-\nsion and fully connected two-layer networks. Recently, ",
                    "Citation Text": "Qinbin Li, Bingsheng He, and Dawn Song. Model-\ncontrastive federated learning. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition , 2021. 1, 2, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.16257",
                        "Citation Paper Title": "Title:Model-Contrastive Federated Learning",
                        "Citation Paper Abstract": "Abstract:Federated learning enables multiple parties to collaboratively train a machine learning model without communicating their local data. A key challenge in federated learning is to handle the heterogeneity of local data distribution across parties. Although many studies have been proposed to address this challenge, we find that they fail to achieve high performance in image datasets with deep learning models. In this paper, we propose MOON: model-contrastive federated learning. MOON is a simple and effective federated learning framework. The key idea of MOON is to utilize the similarity between model representations to correct the local training of individual parties, i.e., conducting contrastive learning in model-level. Our extensive experiments show that MOON significantly outperforms the other state-of-the-art federated learning algorithms on various image classification tasks.",
                        "Citation Paper Authors": "Authors:Qinbin Li, Bingsheng He, Dawn Song"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": ", employs zero-\nshot data augmentation and relies on the statistics of the\nbatch normalization (BN) layers to reduce the variance of\ntest accuracy. FedMix ",
                    "Citation Text": "Tehrim Yoon, Sumin Shin, Sung Ju Hwang, and Eunho Yang.\nFedmix: Approximation of mixup under mean augmented\nfederated learning. arXiv preprint arXiv:2107.00233 , 2021.\n2, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.00233",
                        "Citation Paper Title": "Title:FedMix: Approximation of Mixup under Mean Augmented Federated Learning",
                        "Citation Paper Abstract": "Abstract:Federated learning (FL) allows edge devices to collectively learn a model without directly sharing data within each device, thus preserving privacy and eliminating the need to store data globally. While there are promising results under the assumption of independent and identically distributed (iid) local data, current state-of-the-art algorithms suffer from performance degradation as the heterogeneity of local data across clients increases. To resolve this issue, we propose a simple framework, Mean Augmented Federated Learning (MAFL), where clients send and receive averaged local data, subject to the privacy requirements of target applications. Under our framework, we propose a new augmentation algorithm, named FedMix, which is inspired by a phenomenal yet simple data augmentation method, Mixup, but does not require local raw data to be directly shared among devices. Our method shows greatly improved performance in the standard benchmark datasets of FL, under highly non-iid federated settings, compared to conventional algorithms.",
                        "Citation Paper Authors": "Authors:Tehrim Yoon, Sumin Shin, Sung Ju Hwang, Eunho Yang"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "presents a framework for shar-\ning clients\u2019 averaged local data via Mixup ",
                    "Citation Text": "Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. arXiv preprint arXiv:1710.09412 , 2017. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.09412",
                        "Citation Paper Title": "Title:mixup: Beyond Empirical Risk Minimization",
                        "Citation Paper Abstract": "Abstract:Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.",
                        "Citation Paper Authors": "Authors:Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ", pro-\nposes a dynamic regularizer to promote convergence of the\nlocal loss to a stationary point of the global loss. Build-\ning on top of SCAFFOLD, FedDC ",
                    "Citation Text": "Liang Gao, Huazhu Fu, Li Li, Yingwen Chen, Ming Xu, and\nCheng-Zhong Xu. Feddc: Federated learning with non-iid\ndata via local drift decoupling and correction. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pages 10112\u201310121, 2022. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.11751",
                        "Citation Paper Title": "Title:FedDC: Federated Learning with Non-IID Data via Local Drift Decoupling and Correction",
                        "Citation Paper Abstract": "Abstract:Federated learning (FL) allows multiple clients to collectively train a high-performance global model without sharing their private data. However, the key challenge in federated learning is that the clients have significant statistical heterogeneity among their local data distributions, which would cause inconsistent optimized local models on the client-side. To address this fundamental dilemma, we propose a novel federated learning algorithm with local drift decoupling and correction (FedDC). Our FedDC only introduces lightweight modifications in the local training phase, in which each client utilizes an auxiliary local drift variable to track the gap between the local model parameter and the global model parameters. The key idea of FedDC is to utilize this learned local drift variable to bridge the gap, i.e., conducting consistency in parameter-level. The experiment results and analysis demonstrate that FedDC yields expediting convergence and better performance on various image classification tasks, robust in partial participation settings, non-iid data, and heterogeneous clients.",
                        "Citation Paper Authors": "Authors:Liang Gao, Huazhu Fu, Li Li, Yingwen Chen, Ming Xu, Cheng-Zhong Xu"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "propose the Moon algorithm motivated by the local/global\nmodel proximity idea of FedProx but instead of the l2-norm\nterm, the proximity is imposed via a contrastive term in the\nobjective of local training. Another study, FedDyn ",
                    "Citation Text": "Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew\nMattina, Paul Whatmough, and Venkatesh Saligrama. Fed-\nerated learning based on dynamic regularization. In Interna-\ntional Conference on Learning Representations , 2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.04263",
                        "Citation Paper Title": "Title:Federated Learning Based on Dynamic Regularization",
                        "Citation Paper Abstract": "Abstract:We propose a novel federated learning method for distributively training neural network models, where the server orchestrates cooperation between a subset of randomly chosen devices in each round. We view Federated Learning problem primarily from a communication perspective and allow more device level computations to save transmission costs. We point out a fundamental dilemma, in that the minima of the local-device level empirical loss are inconsistent with those of the global empirical loss. Different from recent prior works, that either attempt inexact minimization or utilize devices for parallelizing gradient computation, we propose a dynamic regularizer for each device at each round, so that in the limit the global and device solutions are aligned. We demonstrate both through empirical results on real and synthetic data as well as analytical results that our scheme leads to efficient training, in both convex and non-convex settings, while being fully agnostic to device heterogeneity and robust to large number of devices, partial participation and unbalanced data.",
                        "Citation Paper Authors": "Authors:Durmus Alp Emre Acar, Yue Zhao, Ramon Matas Navarro, Matthew Mattina, Paul N. Whatmough, Venkatesh Saligrama"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "introduces\na proximal term to the learning objective of each client with\nthe goal of making local training aligned with the global ob-\njective. SCAFFOLD ",
                    "Citation Text": "Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri,\nSashank J Reddi, Sebastian U Stich, and Ananda Theertha\nSuresh. Scaffold: Stochastic controlled averaging for on-\ndevice federated learning. 2019. 1, 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.06378",
                        "Citation Paper Title": "Title:SCAFFOLD: Stochastic Controlled Averaging for Federated Learning",
                        "Citation Paper Abstract": "Abstract:Federated Averaging (FedAvg) has emerged as the algorithm of choice for federated learning due to its simplicity and low communication cost. However, in spite of recent research efforts, its performance is not fully understood. We obtain tight convergence rates for FedAvg and prove that it suffers from `client-drift' when the data is heterogeneous (non-iid), resulting in unstable and slow convergence.\nAs a solution, we propose a new algorithm (SCAFFOLD) which uses control variates (variance reduction) to correct for the `client-drift' in its local updates. We prove that SCAFFOLD requires significantly fewer communication rounds and is not affected by data heterogeneity or client sampling. Further, we show that (for quadratics) SCAFFOLD can take advantage of similarity in the client's data yielding even faster convergence. The latter is the first result to quantify the usefulness of local-steps in distributed optimization.",
                        "Citation Paper Authors": "Authors:Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J. Reddi, Sebastian U. Stich, Ananda Theertha Suresh"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": ", a Bayesian non-parametric approach for ex-\ntracting layers from local models and using them to up-\ndate the corresponding layers in the global model. While\nPFNM targets relatively simple architectures, FedMA ",
                    "Citation Text": "Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Pa-\npailiopoulos, and Yasaman Khazaeni. Federated learning\nwith matched averaging. arXiv preprint arXiv:2002.06440 ,\n2020. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.06440",
                        "Citation Paper Title": "Title:Federated Learning with Matched Averaging",
                        "Citation Paper Abstract": "Abstract:Federated learning allows edge devices to collaboratively learn a shared model while keeping the training data on device, decoupling the ability to do model training from the need to store the data in the cloud. We propose Federated matched averaging (FedMA) algorithm designed for federated learning of modern neural network architectures e.g. convolutional neural networks (CNNs) and LSTMs. FedMA constructs the shared global model in a layer-wise manner by matching and averaging hidden elements (i.e. channels for convolution layers; hidden states for LSTM; neurons for fully connected layers) with similar feature extraction signatures. Our experiments indicate that FedMA not only outperforms popular state-of-the-art federated learning algorithms on deep CNN and LSTM architectures trained on real world datasets, but also reduces the overall communication burden.",
                        "Citation Paper Authors": "Authors:Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, Yasaman Khazaeni"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.00052v3": {
            "Paper Title": "CodeAttack: Code-Based Adversarial Attacks for Pre-trained Programming\n  Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.11595v2": {
            "Paper Title": "Differentially private partitioned variational inference",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.14995v2": {
            "Paper Title": "Privacy-preserving Automatic Speaker Diarization",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": ".\n4.2. Speaker embedding extraction\nOur experiments use SpeechBrain\u2019s pre-trained x-vector model ",
                    "Citation Text": "Mirco Ravanelli, Titouan Parcollet, Peter Plantinga, et al.,\n\u201cSpeechBrain: A general-purpose speech toolkit,\u201d 2021,\narXiv:2106.04624.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.04624",
                        "Citation Paper Title": "Title:SpeechBrain: A General-Purpose Speech Toolkit",
                        "Citation Paper Abstract": "Abstract:SpeechBrain is an open-source and all-in-one speech toolkit. It is designed to facilitate the research and development of neural speech processing technologies by being simple, flexible, user-friendly, and well-documented. This paper describes the core architecture designed to support several tasks of common interest, allowing users to naturally conceive, compare and share novel speech processing pipelines. SpeechBrain achieves competitive or state-of-the-art performance in a wide range of speech benchmarks. It also provides training recipes, pretrained models, and inference scripts for popular speech datasets, as well as tutorials which allow anyone with basic Python proficiency to familiarize themselves with speech technologies.",
                        "Citation Paper Authors": "Authors:Mirco Ravanelli, Titouan Parcollet, Peter Plantinga, Aku Rouhe, Samuele Cornell, Loren Lugosch, Cem Subakan, Nauman Dawalatabad, Abdelwahab Heba, Jianyuan Zhong, Ju-Chieh Chou, Sung-Lin Yeh, Szu-Wei Fu, Chien-Feng Liao, Elena Rastorgueva, Fran\u00e7ois Grondin, William Aris, Hwidong Na, Yan Gao, Renato De Mori, Yoshua Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.06506v2": {
            "Paper Title": "Order-Disorder: Imitation Adversarial Attacks for Black-box Neural\n  Ranking Models",
            "Sentences": [
                {
                    "Sentence ID": 46,
                    "Sentence": "adapt knowledge distillation\nto neural ranking models. However, the performance improvement\nof BERT-based text ranking models also inherited the vulnerabil-\nities of neural networks, which have been detected ",
                    "Citation Text": "Congzheng Song, Alexander Rush, and Vitaly Shmatikov. 2020. Adversarial\nSemantic Collisions. In Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP) . Association for Computational Linguistics,\nOnline, 4198\u20134210.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.04743",
                        "Citation Paper Title": "Title:Adversarial Semantic Collisions",
                        "Citation Paper Abstract": "Abstract:We study semantic collisions: texts that are semantically unrelated but judged as similar by NLP models. We develop gradient-based approaches for generating semantic collisions and demonstrate that state-of-the-art models for many tasks which rely on analyzing the meaning and similarity of texts-- including paraphrase identification, document retrieval, response suggestion, and extractive summarization-- are vulnerable to semantic collisions. For example, given a target query, inserting a crafted collision into an irrelevant document can shift its retrieval rank from 1000 to top 3. We show how to generate semantic collisions that evade perplexity-based filtering and discuss other potential mitigations. Our code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Congzheng Song, Alexander M. Rush, Vitaly Shmatikov"
                    }
                },
                {
                    "Sentence ID": 59,
                    "Sentence": "and are under\nwhite-box setting [15, 43, 47, 60].\nConsidering a majority of real-world text ranking systems do not\nallow white-box access, recently, Wu et al . ",
                    "Citation Text": "Chen Wu, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, and Xueqi\nCheng. 2022. PRADA: Practical Black-Box Adversarial Attacks against Neural\nRanking Models. ArXiv preprint abs/2204.01321 (2022).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.01321",
                        "Citation Paper Title": "Title:PRADA: Practical Black-Box Adversarial Attacks against Neural Ranking Models",
                        "Citation Paper Abstract": "Abstract:Neural ranking models (NRMs) have shown remarkable success in recent years, especially with pre-trained language models. However, deep neural models are notorious for their vulnerability to adversarial examples. Adversarial attacks may become a new type of web spamming technique given our increased reliance on neural information retrieval models. Therefore, it is important to study potential adversarial attacks to identify vulnerabilities of NRMs before they are deployed. In this paper, we introduce the Word Substitution Ranking Attack (WSRA) task against NRMs, which aims to promote a target document in rankings by adding adversarial perturbations to its text. We focus on the decision-based black-box attack setting, where the attackers cannot directly get access to the model information, but can only query the target model to obtain the rank positions of the partial retrieved list. This attack setting is realistic in real-world search engines. We propose a novel Pseudo Relevance-based ADversarial ranking Attack method (PRADA) that learns a surrogate model based on Pseudo Relevance Feedback (PRF) to generate gradients for finding the adversarial perturbations. Experiments on two web search benchmark datasets show that PRADA can outperform existing attack strategies and successfully fool the NRM with small indiscernible perturbations of text.",
                        "Citation Paper Authors": "Authors:Chen Wu, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, Xueqi Cheng"
                    }
                },
                {
                    "Sentence ID": 61,
                    "Sentence": ". Previous adversarial rank-\ning attacks primarily focus on DNN-based image ranking systems\n[28,53,65]. Likewise, the performance improvement of BERT-based\ndocument ranking models ",
                    "Citation Text": "Wei Yang, Haotian Zhang, and Jimmy Lin. 2019. Simple applications of BERT for\nad hoc document retrieval. ArXiv preprint abs/1903.10972 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.10972",
                        "Citation Paper Title": "Title:Simple Applications of BERT for Ad Hoc Document Retrieval",
                        "Citation Paper Abstract": "Abstract:Following recent successes in applying BERT to question answering, we explore simple applications to ad hoc document retrieval. This required confronting the challenge posed by documents that are typically longer than the length of input BERT was designed to handle. We address this issue by applying inference on sentences individually, and then aggregating sentence scores to produce document scores. Experiments on TREC microblog and newswire test collections show that our approach is simple yet effective, as we report the highest average precision on these datasets by neural approaches that we are aware of.",
                        "Citation Paper Authors": "Authors:Wei Yang, Haotian Zhang, Jimmy Lin"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ". To lower\nthe query latency and make the ranking model feasible for produc-\ntion deployment, Hofst\u00e4tter et al . ",
                    "Citation Text": "Sebastian Hofst\u00e4tter, Sophia Althammer, Michael Schr\u00f6der, Mete Sertkan, and\nAllan Hanbury. 2020. Improving efficient neural ranking models with cross-\narchitecture knowledge distillation. ArXiv preprint abs/2010.02666 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.02666",
                        "Citation Paper Title": "Title:Improving Efficient Neural Ranking Models with Cross-Architecture Knowledge Distillation",
                        "Citation Paper Abstract": "Abstract:Retrieval and ranking models are the backbone of many applications such as web search, open domain QA, or text-based recommender systems. The latency of neural ranking models at query time is largely dependent on the architecture and deliberate choices by their designers to trade-off effectiveness for higher efficiency. This focus on low query latency of a rising number of efficient ranking architectures make them feasible for production deployment. In machine learning an increasingly common approach to close the effectiveness gap of more efficient models is to apply knowledge distillation from a large teacher model to a smaller student model. We find that different ranking architectures tend to produce output scores in different magnitudes. Based on this finding, we propose a cross-architecture training procedure with a margin focused loss (Margin-MSE), that adapts knowledge distillation to the varying score output distributions of different BERT and non-BERT passage ranking architectures. We apply the teachable information as additional fine-grained labels to existing training triples of the MSMARCO-Passage collection. We evaluate our procedure of distilling knowledge from state-of-the-art concatenated BERT models to four different efficient architectures (TK, ColBERT, PreTT, and a BERT CLS dot product model). We show that across our evaluated architectures our Margin-MSE knowledge distillation significantly improves re-ranking effectiveness without compromising their efficiency. Additionally, we show our general distillation method to improve nearest neighbor based index retrieval with the BERT dot product model, offering competitive results with specialized and much more costly training methods. To benefit the community, we publish the teacher-score training files in a ready-to-use package.",
                        "Citation Paper Authors": "Authors:Sebastian Hofst\u00e4tter, Sophia Althammer, Michael Schr\u00f6der, Mete Sertkan, Allan Hanbury"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": ". As for the fine-tuned\nMiniLM, it is the cross-encoder architecture ",
                    "Citation Text": "Yi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. 2021. Sparse,\nDense, and Attentional Representations for Text Retrieval. Transactions of the\nAssociation for Computational Linguistics 9 (2021), 329\u2013345.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.00181",
                        "Citation Paper Title": "Title:Sparse, Dense, and Attentional Representations for Text Retrieval",
                        "Citation Paper Abstract": "Abstract:Dual encoders perform retrieval by encoding documents and queries into dense lowdimensional vectors, scoring each document by its inner product with the query. We investigate the capacity of this architecture relative to sparse bag-of-words models and attentional neural networks. Using both theoretical and empirical analysis, we establish connections between the encoding dimension, the margin between gold and lower-ranked documents, and the document length, suggesting limitations in the capacity of fixed-length encodings to support precise retrieval of long documents. Building on these insights, we propose a simple neural model that combines the efficiency of dual encoders with some of the expressiveness of more costly attentional architectures, and explore sparse-dense hybrids to capitalize on the precision of sparse retrieval. These models outperform strong alternatives in large-scale retrieval.",
                        "Citation Paper Authors": "Authors:Yi Luan, Jacob Eisenstein, Kristina Toutanova, Michael Collins"
                    }
                },
                {
                    "Sentence ID": 56,
                    "Sentence": ".\nTarget Attack Model. We select BERT-base [ 10,35] and MiniLM-\nL-12 ",
                    "Citation Text": "Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020.\nMiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-\nTrained Transformers. In Advances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,\nDecember 6-12, 2020, virtual .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.10957",
                        "Citation Paper Title": "Title:MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers",
                        "Citation Paper Abstract": "Abstract:Pre-trained language models (e.g., BERT (Devlin et al., 2018) and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer (Vaswani et al., 2017) based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant (Mirzadeh et al., 2019) also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models.",
                        "Citation Paper Authors": "Authors:Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, Ming Zhou"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": ".\nDeep transformer models pre-trained with language model objec-\ntives, represented by BERT ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers) . Association for Computational Linguistics, Minneapolis, Minnesota,\n4171\u20134186.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": ": It is a classic exact lexical match algorithm with\nhigh efficiency that is widely adopted for the first stage retrieval\n(recall). We adopt the tuned parameters and results from Anserini6.\n2)TK ",
                    "Citation Text": "Sebastian Hofst\u00e4tter, Markus Zlabinger, and Allan Hanbury. 2020. Interpretable\n& time-budget-constrained contextualization for re-ranking. ArXiv preprint\nabs/2002.01854 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.01854",
                        "Citation Paper Title": "Title:Interpretable & Time-Budget-Constrained Contextualization for Re-Ranking",
                        "Citation Paper Abstract": "Abstract:Search engines operate under a strict time constraint as a fast response is paramount to user satisfaction. Thus, neural re-ranking models have a limited time-budget to re-rank documents. Given the same amount of time, a faster re-ranking model can incorporate more documents than a less efficient one, leading to a higher effectiveness. To utilize this property, we propose TK (Transformer-Kernel): a neural re-ranking model for ad-hoc search using an efficient contextualization mechanism. TK employs a very small number of Transformer layers (up to three) to contextualize query and document word embeddings. To score individual term interactions, we use a document-length enhanced kernel-pooling, which enables users to gain insight into the model. TK offers an optimal ratio between effectiveness and efficiency: under realistic time constraints (max. 200 ms per query) TK achieves the highest effectiveness in comparison to BERT and other re-ranking models. We demonstrate this on three large-scale ranking collections: MSMARCO-Passage, MSMARCO-Document, and TREC CAR. In addition, to gain insight into TK, we perform a clustered query analysis of TK's results, highlighting its strengths and weaknesses on queries with different types of information need and we show how to interpret the cause of ranking differences of two documents by comparing their internal scores.",
                        "Citation Paper Authors": "Authors:Sebastian Hofst\u00e4tter, Markus Zlabinger, Allan Hanbury"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "for open-domain QA. The original dataset contains 300K\nquestions collected from Google search logs. We adopt the new\nversion of NQ ",
                    "Citation Text": "Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey\nEdunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-\nDomain Question Answering. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) . Association for Computational\nLinguistics, Online, 6769\u20136781.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.04906",
                        "Citation Paper Title": "Title:Dense Passage Retrieval for Open-Domain Question Answering",
                        "Citation Paper Abstract": "Abstract:Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
                        "Citation Paper Authors": "Authors:Vladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "but still\nremain under-explored by prior studies.\n2.2 Adversarial Ranking Attack\nSzegedy et al . ",
                    "Citation Text": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,\nIan J. Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural networks.\nIn2nd International Conference on Learning Representations, ICLR 2014, Banff, AB,\nCanada, April 14-16, 2014, Conference Track Proceedings .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1312.6199",
                        "Citation Paper Title": "Title:Intriguing properties of neural networks",
                        "Citation Paper Abstract": "Abstract:Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.\nFirst, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks.\nSecond, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
                        "Citation Paper Authors": "Authors:Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "coupled\nwith neural networks to produce soft matching score. These neu-\nral ranking models can be generally classified into three classes:\nrepresentation-based models [ 22,45], interaction-based models\n[16, 20], and hybrid models ",
                    "Citation Text": "Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017. Learning to Match using\nLocal and Distributed Representations of Text for Web Search. In Proceedings of\nthe 26th International Conference on World Wide Web, WWW 2017, Perth, Australia,\nApril 3-7, 2017 . ACM, 1291\u20131299.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.08136",
                        "Citation Paper Title": "Title:Learning to Match Using Local and Distributed Representations of Text for Web Search",
                        "Citation Paper Abstract": "Abstract:Models such as latent semantic analysis and those based on neural embeddings learn distributed representations of text, and match the query against the document in the latent semantic space. In traditional information retrieval models, on the other hand, terms have discrete or local representations, and the relevance of a document is determined by the exact matches of query terms in the body text. We hypothesize that matching with distributed representations complements matching with traditional local representations, and that a combination of the two is favorable. We propose a novel document ranking model composed of two separate deep neural networks, one that matches the query and the document using a local representation, and another that matches the query and the document using learned distributed representations. The two networks are jointly trained as part of a single neural network. We show that this combination or `duet' performs significantly better than either neural network individually on a Web page ranking task, and also significantly outperforms traditional baselines and other recently proposed models based on neural networks.",
                        "Citation Paper Authors": "Authors:Bhaskar Mitra, Fernando Diaz, Nick Craswell"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.12195v2": {
            "Paper Title": "Employing Deep Ensemble Learning for Improving the Security of Computer\n  Networks against Adversarial Attacks",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "-CNN\n-SVM-RAISE -Improve a security -Works against few attacks\nMultimedia Forensics ",
                    "Citation Text": "M. Barni, E. Nowroozi, and B. Tondi, \u201cImproving the security of image\nmanipulation detection through one-and-a-half-class multiple classi\ufb01ca-\ntion,\u201d Multimedia Tools and Applications , vol. 79, no. 3, pp. 2383\u20132408,\n2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.08446",
                        "Citation Paper Title": "Title:Improving the security of Image Manipulation Detection through One-and-a-half-class Multiple Classification",
                        "Citation Paper Abstract": "Abstract:Protecting image manipulation detectors against perfect knowledge attacks requires the adoption of detector architectures which are intrinsically difficult to attack. In this paper, we do so, by exploiting a recently proposed multiple-classifier architecture combining the improved security of 1-Class (1C) classification and the good performance ensured by conventional 2-Class (2C) classification in the absence of attacks. The architecture, also known as 1.5-Class (1.5C) classifier, consists of one 2C classifier and two 1C classifiers run in parallel followed by a final 1C classifier. In our system, the first three classifiers are implemented by means of Support Vector Machines (SVM) fed with SPAM features. The outputs of such classifiers are then processed by a final 1C SVM in charge of making the final decision. Particular care is taken to design a proper strategy to train the SVMs the 1.5C classifier relies on. This is a crucial task, due to the difficulty of training the two 1C classifiers at the front end of the system. We assessed the performance of the proposed solution with regard to three manipulation detection tasks, namely image resizing, contrast enhancement and median filtering. As a result, the security improvement allowed by the 1.5C architecture with respect to a conventional 2C solution is confirmed, with a performance loss in the absence of attacks that remains at a negligible level.",
                        "Citation Paper Authors": "Authors:Mauro Barni, Ehsan Nowroozi, Benedetta Tondi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.03567v2": {
            "Paper Title": "Proof-of-Learning is Currently More Broken Than You Think",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.03868v2": {
            "Paper Title": "CRC: Fully General Model of Confidential Remote Computing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.15997v2": {
            "Paper Title": "Universal Adversarial Directions",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": "solves the following min-max\noptimization problem where the perturbations are generated separately for di\ufb00erent input data:\nmin\nf2F1\nnnX\ni=1\u0014\nmax\n\u000ei:k\u000eik\u0014\u000f`\u0000\nf(xi+\u000ei);yi\u0001\u0015\n\u0011min\nf2Fmax\n\u000e1;:::;\u000en:\n8i;k\u000eik\u0014\u000f1\nnnX\ni=1\u0002\n`\u0000\nf(xi+\u000ei);yi\u0001\u0003\n(3)\nTo perform universal adversarial training through UAPs, ",
                    "Citation Text": "Ali Shafahi, Mahyar Najibi, Zheng Xu, John Dickerson, Larry S Davis, and Tom Goldstein. Universal\nadversarial training. arXiv preprint arXiv:1811.11304 , 2018. (Cited on pages 3 and 4.)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.11304",
                        "Citation Paper Title": "Title:Universal Adversarial Training",
                        "Citation Paper Abstract": "Abstract:Standard adversarial attacks change the predicted class label of a selected image by adding specially tailored small perturbations to its pixels. In contrast, a universal perturbation is an update that can be added to any image in a broad class of images, while still changing the predicted class label. We study the efficient generation of universal adversarial perturbations, and also efficient methods for hardening networks to these attacks. We propose a simple optimization-based universal attack that reduces the top-1 accuracy of various network architectures on ImageNet to less than 20%, while learning the universal perturbation 13X faster than the standard method.\nTo defend against these perturbations, we propose universal adversarial training, which models the problem of robust classifier generation as a two-player min-max game, and produces robust models with only 2X the cost of natural training. We also propose a simultaneous stochastic gradient method that is almost free of extra computation, which allows us to do universal adversarial training on ImageNet.",
                        "Citation Paper Authors": "Authors:Ali Shafahi, Mahyar Najibi, Zheng Xu, John Dickerson, Larry S. Davis, Tom Goldstein"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2207.09572v3": {
            "Paper Title": "Robust Multivariate Time-Series Forecasting: Adversarial Attacks and\n  Defense Mechanisms",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.03793v2": {
            "Paper Title": "RADAR: A TTP-based Extensible, Explainable, and Effective System for\n  Network Traffic Analysis and Malware Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.02918v2": {
            "Paper Title": "When the Curious Abandon Honesty: Federated Learning Is Not Private",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.12005v3": {
            "Paper Title": "Self-Ensemble Protection: Training Checkpoints Are Good Data Protectors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.14772v3": {
            "Paper Title": "Unfooling Perturbation-Based Post Hoc Explainers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.02775v3": {
            "Paper Title": "ADI: Adversarial Dominating Inputs in Vertical Federated Learning\n  Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.02265v3": {
            "Paper Title": "Fiat-Shamir for Proofs Lacks a Proof Even in the Presence of Shared\n  Entanglement",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": "gave a construction for entropy preserving hash functions from robust random-\nness condensers with some extra properties but without providing any con-\nstruction for them. Canetti, Goldreich, and Halevi ",
                    "Citation Text": "R. Canetti, O. Goldreich, and S. Halevi. \u201cThe Random Oracle Meth odol-\nogy, Revisited\u201d. In: J. ACM 51.4 (July 2004), pp. 557\u2013594.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:cs/0010019",
                        "Citation Paper Title": "Title:The Random Oracle Methodology, Revisited",
                        "Citation Paper Abstract": "Abstract:  We take a critical look at the relationship between the security of cryptographic schemes in the Random Oracle Model, and the security of the schemes that result from implementing the random oracle by so called \"cryptographic hash functions\". The main result of this paper is a negative one: There exist signature and encryption schemes that are secure in the Random Oracle Model, but for which any implementation of the random oracle results in insecure schemes.\nIn the process of devising the above schemes, we consider possible definitions for the notion of a \"good implementation\" of a random oracle, pointing out limitations and challenges.",
                        "Citation Paper Authors": "Authors:Ran Canetti, Oded Goldreich, Shai Halevi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.08459v2": {
            "Paper Title": "CommCSL: Proving Information Flow Security for Concurrent Programs using\n  Abstract Commutativity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.01560v3": {
            "Paper Title": "High-Dimensional Private Empirical Risk Minimization by Greedy\n  Coordinate Descent",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.06388v2": {
            "Paper Title": "TSFool: Crafting Highly-imperceptible Adversarial Time Series through\n  Multi-objective Black-box Attack to Fool RNN Classifiers",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": "and not always possible, espe-\ncially in NNs with complex non-convexity and non-linearity ",
                    "Citation Text": "N. Papernot, P. McDaniel, A. Swami, and R. Harang, \u201cCrafting ad-\nversarial input sequences for recurrent neural networks,\u201d in MILCOM\n2016-2016 IEEE Military Communications Conference . IEEE, 2016,\npp. 49\u201354.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1604.08275",
                        "Citation Paper Title": "Title:Crafting Adversarial Input Sequences for Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:Machine learning models are frequently used to solve complex security problems, as well as to make decisions in sensitive situations like guiding autonomous vehicles or predicting financial market behaviors. Previous efforts have shown that numerous machine learning models were vulnerable to adversarial manipulations of their inputs taking the form of adversarial samples. Such inputs are crafted by adding carefully selected perturbations to legitimate inputs so as to force the machine learning model to misbehave, for instance by outputting a wrong class if the machine learning task of interest is classification. In fact, to the best of our knowledge, all previous work on adversarial samples crafting for neural network considered models used to solve classification tasks, most frequently in computer vision applications. In this paper, we contribute to the field of adversarial machine learning by investigating adversarial input sequences for recurrent neural networks processing sequential data. We show that the classes of algorithms introduced previously to craft adversarial samples misclassified by feed-forward neural networks can be adapted to recurrent neural networks. In a experiment, we show that adversaries can craft adversarial sequences misleading both categorical and sequential recurrent neural networks.",
                        "Citation Paper Authors": "Authors:Nicolas Papernot, Patrick McDaniel, Ananthram Swami, Richard Harang"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": ", which\nmeans all the information of the target classi\ufb01er is available\nand the perturbation is guided by the differentiating functions\nde\ufb01ned over model structure and parameters. For instance,\nthefast gradient sign method (FGSM) ",
                    "Citation Text": "I. J. Goodfellow, J. Shlens, and C. Szegedy, \u201cExplaining and harnessing\nadversarial examples,\u201d arXiv preprint arXiv:1412.6572 , 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.6572",
                        "Citation Paper Title": "Title:Explaining and Harnessing Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": ", the common practice is to \ufb01nd the approximative solutions\nto estimate adversarial examples.\nThe most commonly used adversarial attacks to date are\ngradient-based methods under white-box setting ",
                    "Citation Text": "Y . Li, M. Cheng, C.-J. Hsieh, and T. C. Lee, \u201cA review of adversarial at-\ntack and defense for classi\ufb01cation methods,\u201d The American Statistician ,\npp. 1\u201317, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.09961",
                        "Citation Paper Title": "Title:A Review of Adversarial Attack and Defense for Classification Methods",
                        "Citation Paper Abstract": "Abstract:Despite the efficiency and scalability of machine learning systems, recent studies have demonstrated that many classification methods, especially deep neural networks (DNNs), are vulnerable to adversarial examples; i.e., examples that are carefully crafted to fool a well-trained classification model while being indistinguishable from natural data to human. This makes it potentially unsafe to apply DNNs or related methods in security-critical areas. Since this issue was first identified by Biggio et al. (2013) and Szegedy et al.(2014), much work has been done in this field, including the development of attack methods to generate adversarial examples and the construction of defense techniques to guard against such examples. This paper aims to introduce this topic and its latest developments to the statistical community, primarily focusing on the generation and guarding of adversarial examples. Computing codes (in python and R) used in the numerical experiments are publicly available for readers to explore the surveyed methods. It is the hope of the authors that this paper will encourage more statisticians to work on this important and exciting field of generating and defending against adversarial examples.",
                        "Citation Paper Authors": "Authors:Yao Li, Minhao Cheng, Cho-Jui Hsieh, Thomas C. M. Lee"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "A. Basics of Adversarial Attack\nThe concept of adversarial attack is \ufb01rstly introduced by ",
                    "Citation Text": "C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow,\nand R. Fergus, \u201cIntriguing properties of neural networks,\u201d arXiv preprint\narXiv:1312.6199 , 2013.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1312.6199",
                        "Citation Paper Title": "Title:Intriguing properties of neural networks",
                        "Citation Paper Abstract": "Abstract:Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.\nFirst, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks.\nSecond, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
                        "Citation Paper Authors": "Authors:Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.13035v3": {
            "Paper Title": "SoK: Decentralized Finance (DeFi) Attacks",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": "empowered the wide adoption\nof DeFi. DeFi currently provides a wide range of \ufb01nancial\nservices such as exchanges ",
                    "Citation Text": "J. Xu, K. Paruch, S. Cousaert, and Y . Feng, \u201cSok: Decentralized\nexchanges (dex) with automated market maker (amm) protocols,\u201d ACM\nComputing Surveys , vol. 55, no. 11, pp. 1\u201350, 2023.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.12732",
                        "Citation Paper Title": "Title:SoK: Decentralized Exchanges (DEX) with Automated Market Maker (AMM) Protocols",
                        "Citation Paper Abstract": "Abstract:As an integral part of the decentralized finance (DeFi) ecosystem, decentralized exchanges (DEXs) with automated market maker (AMM) protocols have gained massive traction with the recently revived interest in blockchain and distributed ledger technology (DLT) in general. Instead of matching the buy and sell sides, automated market makers (AMMs) employ a peer-to-pool method and determine asset price algorithmically through a so-called conservation function. To facilitate the improvement and development of automated market maker (AMM)-based decentralized exchanges (DEXs), we create the first systematization of knowledge in this area. We first establish a general automated market maker (AMM) framework describing the economics and formalizing the system's state-space representation. We then employ our framework to systematically compare the top automated market maker (AMM) protocols' mechanics, illustrating their conservation functions, as well as slippage and divergence loss functions. We further discuss security and privacy concerns, how they are enabled by automated market maker (AMM)-based decentralized exchanges (DEXs)' inherent properties, and explore mitigating solutions. Finally, we conduct a comprehensive literature review on related work covering both decentralized finance (DeFi) and conventional market microstructure.",
                        "Citation Paper Authors": "Authors:Jiahua Xu, Krzysztof Paruch, Simon Cousaert, Yebo Feng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.00747v2": {
            "Paper Title": "Quantum Pseudoentanglement",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.15446v2": {
            "Paper Title": "LP-BFGS attack: An adversarial attack based on the Hessian with limited\n  pixels",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.05047v2": {
            "Paper Title": "MPC for Tech Giants (GMPC): Enabling Gulliver and the Lilliputians to\n  Cooperate Amicably",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": "in their communication-locality MPC protocol, and was use d by Cohen et al. ",
                    "Citation Text": "R. Cohen, I. Haitner, E. Omri, and L. Rotem. From fairnes s to full security in multiparty\ncomputation. In SCN, pages 216\u2013234, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.00962",
                        "Citation Paper Title": "Title:From Fairness to Full Security in Multiparty Computation",
                        "Citation Paper Abstract": "Abstract:In the setting of secure multiparty computation (MPC), a set of mutually distrusting parties wish to jointly compute a function, while guaranteeing the privacy of their inputs and the correctness of the output. An MPC protocol is called \\emph{fully secure} if no adversary can prevent the honest parties from obtaining their outputs. A protocol is called \\emph{fair} if an adversary can prematurely abort the computation, however, only before learning any new information.\nWe present highly efficient transformations from fair computations to fully secure computations, assuming the fraction of honest parties is constant (e.g., $1\\%$ of the parties are honest). Compared to previous transformations that require linear invocations (in the number of parties) of the fair computation, our transformations require super-logarithmic, and sometimes even super-constant, such invocations. The main idea is to delegate the computation to chosen random committees that invoke the fair computation. Apart from the benefit of uplifting security, the reduction in the number of parties is also useful, since only committee members are required to work, whereas the remaining parties simply \"listen\" to the computation over a broadcast channel.",
                        "Citation Paper Authors": "Authors:Ran Cohen, Iftach Haitner, Eran Omri, Lior Rotem"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2207.13253v2": {
            "Paper Title": "Fine-grained Private Knowledge Distillation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.00582v2": {
            "Paper Title": "Security Analysis of Mobile Banking Application in Qatar",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.10717v3": {
            "Paper Title": "Quantum Differential Privacy: An Information Theory Perspective",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.01144v3": {
            "Paper Title": "UniASM: Binary Code Similarity Detection without Fine-tuning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.07984v2": {
            "Paper Title": "Private Estimation with Public Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.00283v2": {
            "Paper Title": "Recurring Contingent Service Payment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.00875v3": {
            "Paper Title": "Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset\n  Copyright Protection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.04391v3": {
            "Paper Title": "Aura: Privacy-preserving Augmentation to Improve Test Set Diversity in\n  Speech Enhancement",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": ".\nThis paper integrates diversity and uncertainty-based\ntechniques to create a test set for SE models.\nA growing body of work has developed differentially pri-\nvate learning algorithms that do not expose private customer\ndata ",
                    "Citation Text": "M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan,\nI. Mironov, K. Talwar, and L. Zhang, \u201cDeep learning\nwith differential privacy,\u201d in Proceedings of the 2016\nACM SIGSAC , 2016, pp. 308\u2013318.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.00133",
                        "Citation Paper Title": "Title:Deep Learning with Differential Privacy",
                        "Citation Paper Abstract": "Abstract:Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.",
                        "Citation Paper Authors": "Authors:Mart\u00edn Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, Li Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.12929v2": {
            "Paper Title": "Sequence-Based Target Coin Prediction for Cryptocurrency Pump-and-Dump",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": "highlight the vast presence of Twitter bots and\ntheir key part in spreading the invitation links of pump channels\nin Telegram. Similar results revealed by ",
                    "Citation Text": "D. Pacheco, P.-M. Hui, C. Torres-Lugo, B. T. Truong, A. Flammini, and F. Menczer.\nUncovering coordinated networks on social media: methods and case studies. In\nProceedings of the AAAI international conference on web and social media (ICWSM) ,\n2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2001.05658",
                        "Citation Paper Title": "Title:Uncovering Coordinated Networks on Social Media: Methods and Case Studies",
                        "Citation Paper Abstract": "Abstract:Coordinated campaigns are used to influence and manipulate social media platforms and their users, a critical challenge to the free exchange of information online. Here we introduce a general, unsupervised network-based methodology to uncover groups of accounts that are likely coordinated. The proposed method constructs coordination networks based on arbitrary behavioral traces shared among accounts. We present five case studies of influence campaigns, four of which in the diverse contexts of U.S. elections, Hong Kong protests, the Syrian civil war, and cryptocurrency manipulation. In each of these cases, we detect networks of coordinated Twitter accounts by examining their identities, images, hashtag sequences, retweets, or temporal patterns. The proposed approach proves to be broadly applicable to uncover different kinds of coordination across information warfare scenarios.",
                        "Citation Paper Authors": "Authors:Diogo Pacheco, Pik-Mai Hui, Christopher Torres-Lugo, Bao Tran Truong, Alessandro Flammini, Filippo Menczer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.04843v2": {
            "Paper Title": "CopAS: A Big Data Forensic Analytics System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.02339v4": {
            "Paper Title": "DeAR: A Deep-learning-based Audio Re-recording Resilient Watermarking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.10134v2": {
            "Paper Title": "Machine Learning with Confidential Computing: A Systematization of\n  Knowledge",
            "Sentences": [
                {
                    "Sentence ID": 224,
                    "Sentence": "A. Li, J. Sun, P. Li, Y. Pu, H. Li, and Y. Chen, \u201cHermes: an efficient federated learning framework for heterogeneous mobile clients,\u201d in Proceedings of\nthe 27th Annual International Conference on Mobile Computing and Networking , 2021, pp. 420\u2013437. ",
                    "Citation Text": "E. Diao, J. Ding, and V. Tarokh, \u201cHeterofl: Computation and communication efficient federated learning for heterogeneous clients,\u201d in International\nConference on Learning Representations , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.01264",
                        "Citation Paper Title": "Title:HeteroFL: Computation and Communication Efficient Federated Learning for Heterogeneous Clients",
                        "Citation Paper Abstract": "Abstract:Federated Learning (FL) is a method of training machine learning models on private data distributed over a large number of possibly heterogeneous clients such as mobile phones and IoT devices. In this work, we propose a new federated learning framework named HeteroFL to address heterogeneous clients equipped with very different computation and communication capabilities. Our solution can enable the training of heterogeneous local models with varying computation complexities and still produce a single global inference model. For the first time, our method challenges the underlying assumption of existing work that local models have to share the same architecture as the global model. We demonstrate several strategies to enhance FL training and conduct extensive empirical evaluations, including five computation complexity levels of three model architecture on three datasets. We show that adaptively distributing subnetworks according to clients' capabilities is both computation and communication efficient.",
                        "Citation Paper Authors": "Authors:Enmao Diao, Jie Ding, Vahid Tarokh"
                    }
                },
                {
                    "Sentence ID": 220,
                    "Sentence": "I. Fedorov, R. P. Adams, M. Mattina, and P. Whatmough, \u201cSparse: Sparse architecture search for cnns on resource-constrained microcontrollers,\u201d\nAdvances in Neural Information Processing Systems , vol. 32, 2019. ",
                    "Citation Text": "C. Banbury, C. Zhou, I. Fedorov, R. Matas, U. Thakker, D. Gope, V. Janapa Reddi, M. Mattina, and P. Whatmough, \u201cMicronets: Neural network\narchitectures for deploying tinyml applications on commodity microcontrollers,\u201d Proceedings of Machine Learning and Systems , vol. 3, pp. 517\u2013532,\n2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.11267",
                        "Citation Paper Title": "Title:MicroNets: Neural Network Architectures for Deploying TinyML Applications on Commodity Microcontrollers",
                        "Citation Paper Abstract": "Abstract:Executing machine learning workloads locally on resource constrained microcontrollers (MCUs) promises to drastically expand the application space of IoT. However, so-called TinyML presents severe technical challenges, as deep neural network inference demands a large compute and memory budget. To address this challenge, neural architecture search (NAS) promises to help design accurate ML models that meet the tight MCU memory, latency and energy constraints. A key component of NAS algorithms is their latency/energy model, i.e., the mapping from a given neural network architecture to its inference latency/energy on an MCU. In this paper, we observe an intriguing property of NAS search spaces for MCU model design: on average, model latency varies linearly with model operation (op) count under a uniform prior over models in the search space. Exploiting this insight, we employ differentiable NAS (DNAS) to search for models with low memory usage and low op count, where op count is treated as a viable proxy to latency. Experimental results validate our methodology, yielding our MicroNet models, which we deploy on MCUs using Tensorflow Lite Micro, a standard open-source NN inference runtime widely used in the TinyML community. MicroNets demonstrate state-of-the-art results for all three TinyMLperf industry-standard benchmark tasks: visual wake words, audio keyword spotting, and anomaly detection. Models and training scripts can be found at this http URL.",
                        "Citation Paper Authors": "Authors:Colby Banbury, Chuteng Zhou, Igor Fedorov, Ramon Matas Navarro, Urmish Thakker, Dibakar Gope, Vijay Janapa Reddi, Matthew Mattina, Paul N. Whatmough"
                    }
                },
                {
                    "Sentence ID": 214,
                    "Sentence": "M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang, \u201cDeep learning with differential privacy,\u201d in Proceedings of\nthe 2016 ACM SIGSAC conference on computer and communications security , 2016, pp. 308\u2013318. ",
                    "Citation Text": "Y. Xu, S. Zhao, J. Song, R. Stewart, and S. Ermon, \u201cA theory of usable information under computational constraints,\u201d in International Conference on\nLearning Representations , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.10689",
                        "Citation Paper Title": "Title:A Theory of Usable Information Under Computational Constraints",
                        "Citation Paper Abstract": "Abstract:We propose a new framework for reasoning about information in complex systems. Our foundation is based on a variational extension of Shannon's information theory that takes into account the modeling power and computational constraints of the observer. The resulting \\emph{predictive $\\mathcal{V}$-information} encompasses mutual information and other notions of informativeness such as the coefficient of determination. Unlike Shannon's mutual information and in violation of the data processing inequality, $\\mathcal{V}$-information can be created through computation. This is consistent with deep neural networks extracting hierarchies of progressively more informative features in representation learning. Additionally, we show that by incorporating computational constraints, $\\mathcal{V}$-information can be reliably estimated from data even in high dimensions with PAC-style guarantees. Empirically, we demonstrate predictive $\\mathcal{V}$-information is more effective than mutual information for structure learning and fair representation learning.",
                        "Citation Paper Authors": "Authors:Yilun Xu, Shengjia Zhao, Jiaming Song, Russell Stewart, Stefano Ermon"
                    }
                },
                {
                    "Sentence ID": 213,
                    "Sentence": "NXP. Asug-i.mx android security user\u2019s guide. [Online]. Available: https://www.nxp.com/docs/en/user-guide/IMX_ANDROID_SECURITY_USERS_\nGUIDE.pdf ",
                    "Citation Text": "M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang, \u201cDeep learning with differential privacy,\u201d in Proceedings of\nthe 2016 ACM SIGSAC conference on computer and communications security , 2016, pp. 308\u2013318.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.00133",
                        "Citation Paper Title": "Title:Deep Learning with Differential Privacy",
                        "Citation Paper Abstract": "Abstract:Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.",
                        "Citation Paper Authors": "Authors:Mart\u00edn Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, Li Zhang"
                    }
                },
                {
                    "Sentence ID": 186,
                    "Sentence": "X. Li, H. Xiong, X. Li, X. Wu, X. Zhang, J. Liu, J. Bian, and D. Dou, \u201cInterpretable deep learning: Interpretation, interpretability, trustworthiness,\nand beyond,\u201d arXiv preprint arXiv:2103.10689 , 2021. ",
                    "Citation Text": "F. Tram\u00e8r, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh, and P. McDaniel, \u201cEnsemble adversarial training: Attacks and defenses,\u201d in International\nConference on Learning Representations , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.07204",
                        "Citation Paper Title": "Title:Ensemble Adversarial Training: Attacks and Defenses",
                        "Citation Paper Abstract": "Abstract:Adversarial examples are perturbed inputs designed to fool machine learning models. Adversarial training injects such examples into training data to increase robustness. To scale this technique to large datasets, perturbations are crafted using fast single-step methods that maximize a linear approximation of the model's loss. We show that this form of adversarial training converges to a degenerate global minimum, wherein small curvature artifacts near the data points obfuscate a linear approximation of the loss. The model thus learns to generate weak perturbations, rather than defend against strong ones. As a result, we find that adversarial training remains vulnerable to black-box attacks, where we transfer perturbations computed on undefended models, as well as to a powerful novel single-step attack that escapes the non-smooth vicinity of the input data via a small random step. We further introduce Ensemble Adversarial Training, a technique that augments training data with perturbations transferred from other models. On ImageNet, Ensemble Adversarial Training yields models with strong robustness to black-box attacks. In particular, our most robust model won the first round of the NIPS 2017 competition on Defenses against Adversarial Attacks. However, subsequent work found that more elaborate black-box attacks could significantly enhance transferability and reduce the accuracy of our models.",
                        "Citation Paper Authors": "Authors:Florian Tram\u00e8r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, Patrick McDaniel"
                    }
                },
                {
                    "Sentence ID": 185,
                    "Sentence": "Z. Tarkhani, \u201cSecure programming with dispersed compartments,\u201d Ph.D. dissertation, University of Cambridge, 2022. ",
                    "Citation Text": "X. Li, H. Xiong, X. Li, X. Wu, X. Zhang, J. Liu, J. Bian, and D. Dou, \u201cInterpretable deep learning: Interpretation, interpretability, trustworthiness,\nand beyond,\u201d arXiv preprint arXiv:2103.10689 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.10689",
                        "Citation Paper Title": "Title:Interpretable Deep Learning: Interpretation, Interpretability, Trustworthiness, and Beyond",
                        "Citation Paper Abstract": "Abstract:Deep neural networks have been well-known for their superb handling of various machine learning and artificial intelligence tasks. However, due to their over-parameterized black-box nature, it is often difficult to understand the prediction results of deep models. In recent years, many interpretation tools have been proposed to explain or reveal how deep models make decisions. In this paper, we review this line of research and try to make a comprehensive survey. Specifically, we first introduce and clarify two basic concepts -- interpretations and interpretability -- that people usually get confused about. To address the research efforts in interpretations, we elaborate the designs of a number of interpretation algorithms, from different perspectives, by proposing a new taxonomy. Then, to understand the interpretation results, we also survey the performance metrics for evaluating interpretation algorithms. Further, we summarize the current works in evaluating models' interpretability using \"trustworthy\" interpretation algorithms. Finally, we review and discuss the connections between deep models' interpretations and other factors, such as adversarial robustness and learning from interpretations, and we introduce several open-source libraries for interpretation algorithms and evaluation approaches.",
                        "Citation Paper Authors": "Authors:Xuhong Li, Haoyi Xiong, Xingjian Li, Xuanyu Wu, Xiao Zhang, Ji Liu, Jiang Bian, Dejing Dou"
                    }
                },
                {
                    "Sentence ID": 165,
                    "Sentence": "T. Hunt, Z. Jia, V. Miller, A. Szekely, Y. Hu, C. J. Rossbach, and E. Witchel, \u201cTelekine: Secure computing with cloud gpus,\u201d in 17th{USENIX}\nSymposium on Networked Systems Design and Implementation ( {NSDI}20), 2020, pp. 817\u2013833. ",
                    "Citation Text": "L. Hanzlik, Y. Zhang, K. Grosse, A. Salem, M. Augustin, M. Backes, and M. Fritz, \u201cMLcapsule: Guarded offline deployment of machine learning as a\nservice,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021, pp. 3300\u20133309.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.00590",
                        "Citation Paper Title": "Title:MLCapsule: Guarded Offline Deployment of Machine Learning as a Service",
                        "Citation Paper Abstract": "Abstract:With the widespread use of machine learning (ML) techniques, ML as a service has become increasingly popular. In this setting, an ML model resides on a server and users can query it with their data via an API. However, if the user's input is sensitive, sending it to the server is undesirable and sometimes even legally not possible. Equally, the service provider does not want to share the model by sending it to the client for protecting its intellectual property and pay-per-query business model.\nIn this paper, we propose MLCapsule, a guarded offline deployment of machine learning as a service. MLCapsule executes the model locally on the user's side and therefore the data never leaves the client. Meanwhile, MLCapsule offers the service provider the same level of control and security of its model as the commonly used server-side execution. In addition, MLCapsule is applicable to offline applications that require local execution. Beyond protecting against direct model access, we couple the secure offline deployment with defenses against advanced attacks on machine learning models such as model stealing, reverse engineering, and membership inference.",
                        "Citation Paper Authors": "Authors:Lucjan Hanzlik, Yang Zhang, Kathrin Grosse, Ahmed Salem, Max Augustin, Michael Backes, Mario Fritz"
                    }
                },
                {
                    "Sentence ID": 92,
                    "Sentence": "A. Sablayrolles, M. Douze, C. Schmid, Y. Ollivier, and H. J\u00e9gou, \u201cWhite-box vs black-box: Bayes optimal strategies for membership inference,\u201d in\nInternational Conference on Machine Learning . PMLR, 2019, pp. 5558\u20135567. ",
                    "Citation Text": "S. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha, \u201cPrivacy risk in machine learning: Analyzing the connection to overfitting,\u201d in 2018 IEEE 31st\ncomputer security foundations symposium (CSF) . IEEE, 2018, pp. 268\u2013282.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.01604",
                        "Citation Paper Title": "Title:Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting",
                        "Citation Paper Abstract": "Abstract:Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role.\nThis paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.",
                        "Citation Paper Authors": "Authors:Samuel Yeom, Irene Giacomelli, Matt Fredrikson, Somesh Jha"
                    }
                },
                {
                    "Sentence ID": 91,
                    "Sentence": "N. Papernot, P. McDaniel, A. Sinha, and M. P. Wellman, \u201cSoK: Security and privacy in machine learning,\u201d in 2018 IEEE European Symposium on\nSecurity and Privacy (EuroS&P) . IEEE, 2018, pp. 399\u2013414. ",
                    "Citation Text": "A. Sablayrolles, M. Douze, C. Schmid, Y. Ollivier, and H. J\u00e9gou, \u201cWhite-box vs black-box: Bayes optimal strategies for membership inference,\u201d in\nInternational Conference on Machine Learning . PMLR, 2019, pp. 5558\u20135567.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.11229",
                        "Citation Paper Title": "Title:White-box vs Black-box: Bayes Optimal Strategies for Membership Inference",
                        "Citation Paper Abstract": "Abstract:Membership inference determines, given a sample and trained parameters of a machine learning model, whether the sample was part of the training set. In this paper, we derive the optimal strategy for membership inference with a few assumptions on the distribution of the parameters. We show that optimal attacks only depend on the loss function, and thus black-box attacks are as good as white-box attacks. As the optimal strategy is not tractable, we provide approximations of it leading to several inference methods, and show that existing membership inference methods are coarser approximations of this optimal strategy. Our membership attacks outperform the state of the art in various settings, ranging from a simple logistic regression to more complex architectures and datasets, such as ResNet-101 and Imagenet.",
                        "Citation Paper Authors": "Authors:Alexandre Sablayrolles, Matthijs Douze, Yann Ollivier, Cordelia Schmid, Herv\u00e9 J\u00e9gou"
                    }
                },
                {
                    "Sentence ID": 89,
                    "Sentence": "E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov, \u201cHow to backdoor federated learning,\u201d in International Conference on Artificial\nIntelligence and Statistics . PMLR, 2020, pp. 2938\u20132948. ",
                    "Citation Text": "J. Jia, Y. Liu, and N. Z. Gong, \u201cBadencoder: Backdoor attacks to pre-trained encoders in self-supervised learning,\u201d in 2022 IEEE Symposium on\nSecurity and Privacy (SP) . IEEE, 2022, pp. 2043\u20132059.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.00352",
                        "Citation Paper Title": "Title:BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning",
                        "Citation Paper Abstract": "Abstract:Self-supervised learning in computer vision aims to pre-train an image encoder using a large amount of unlabeled images or (image, text) pairs. The pre-trained image encoder can then be used as a feature extractor to build downstream classifiers for many downstream tasks with a small amount of or no labeled training data. In this work, we propose BadEncoder, the first backdoor attack to self-supervised learning. In particular, our BadEncoder injects backdoors into a pre-trained image encoder such that the downstream classifiers built based on the backdoored image encoder for different downstream tasks simultaneously inherit the backdoor behavior. We formulate our BadEncoder as an optimization problem and we propose a gradient descent based method to solve it, which produces a backdoored image encoder from a clean one. Our extensive empirical evaluation results on multiple datasets show that our BadEncoder achieves high attack success rates while preserving the accuracy of the downstream classifiers. We also show the effectiveness of BadEncoder using two publicly available, real-world image encoders, i.e., Google's image encoder pre-trained on ImageNet and OpenAI's Contrastive Language-Image Pre-training (CLIP) image encoder pre-trained on 400 million (image, text) pairs collected from the Internet. Moreover, we consider defenses including Neural Cleanse and MNTD (empirical defenses) as well as PatchGuard (a provable defense). Our results show that these defenses are insufficient to defend against BadEncoder, highlighting the needs for new defenses against our BadEncoder. Our code is publicly available at: this https URL.",
                        "Citation Paper Authors": "Authors:Jinyuan Jia, Yupei Liu, Neil Zhenqiang Gong"
                    }
                },
                {
                    "Sentence ID": 88,
                    "Sentence": "Y. Liu, X. Ma, J. Bailey, and F. Lu, \u201cReflection backdoor: A natural backdoor attack on deep neural networks,\u201d in European Conference on Computer\nVision . Springer, 2020, pp. 182\u2013199. ",
                    "Citation Text": "E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov, \u201cHow to backdoor federated learning,\u201d in International Conference on Artificial\nIntelligence and Statistics . PMLR, 2020, pp. 2938\u20132948.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.00459",
                        "Citation Paper Title": "Title:How To Backdoor Federated Learning",
                        "Citation Paper Abstract": "Abstract:Federated learning enables thousands of participants to construct a deep learning model without sharing their private training data with each other. For example, multiple smartphones can jointly train a next-word predictor for keyboards without revealing what individual users type. We demonstrate that any participant in federated learning can introduce hidden backdoor functionality into the joint global model, e.g., to ensure that an image classifier assigns an attacker-chosen label to images with certain features, or that a word predictor completes certain sentences with an attacker-chosen word.\nWe design and evaluate a new model-poisoning methodology based on model replacement. An attacker selected in a single round of federated learning can cause the global model to immediately reach 100% accuracy on the backdoor task. We evaluate the attack under different assumptions for the standard federated-learning tasks and show that it greatly outperforms data poisoning. Our generic constrain-and-scale technique also evades anomaly detection-based defenses by incorporating the evasion into the attacker's loss function during training.",
                        "Citation Paper Authors": "Authors:Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, Vitaly Shmatikov"
                    }
                },
                {
                    "Sentence ID": 86,
                    "Sentence": "J. R. Sanchez Vicarte, B. Schreiber, R. Paccagnella, and C. W. Fletcher, \u201cGame of threads: Enabling asynchronous poisoning attacks,\u201d in Proceedings\nof the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems , 2020, pp. 35\u201352. ",
                    "Citation Text": "V. Tolpegin, S. Truex, M. E. Gursoy, and L. Liu, \u201cData poisoning attacks against federated learning systems,\u201d in European Symposium on Research in\nComputer Security . Springer, 2020, pp. 480\u2013501.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.08432",
                        "Citation Paper Title": "Title:Data Poisoning Attacks Against Federated Learning Systems",
                        "Citation Paper Abstract": "Abstract:Federated learning (FL) is an emerging paradigm for distributed training of large-scale deep neural networks in which participants' data remains on their own devices with only model updates being shared with a central server. However, the distributed nature of FL gives rise to new threats caused by potentially malicious participants. In this paper, we study targeted data poisoning attacks against FL systems in which a malicious subset of the participants aim to poison the global model by sending model updates derived from mislabeled data. We first demonstrate that such data poisoning attacks can cause substantial drops in classification accuracy and recall, even with a small percentage of malicious participants. We additionally show that the attacks can be targeted, i.e., they have a large negative impact only on classes that are under attack. We also study attack longevity in early/late round training, the impact of malicious participant availability, and the relationships between the two. Finally, we propose a defense strategy that can help identify malicious participants in FL to circumvent poisoning attacks, and demonstrate its effectiveness.",
                        "Citation Paper Authors": "Authors:Vale Tolpegin, Stacey Truex, Mehmet Emre Gursoy, Ling Liu"
                    }
                },
                {
                    "Sentence ID": 83,
                    "Sentence": "H. Yu, K. Yang, T. Zhang, Y.-Y. Tsai, T.-Y. Ho, and Y. Jin, \u201cCloudleak: Large-scale deep learning models stealing through adversarial examples.\u201d in\nNetwork and Distributed System Security Symposium (NDSS) , 2020. ",
                    "Citation Text": "S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard, \u201cUniversal adversarial perturbations,\u201d in Proceedings of the IEEE conference on computer\nvision and pattern recognition , 2017, pp. 1765\u20131773.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.08401",
                        "Citation Paper Title": "Title:Universal adversarial perturbations",
                        "Citation Paper Abstract": "Abstract:Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images.",
                        "Citation Paper Authors": "Authors:Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal Frossard"
                    }
                },
                {
                    "Sentence ID": 80,
                    "Sentence": "M. Nasr, R. Shokri, and A. Houmansadr, \u201cComprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against\ncentralized and federated learning,\u201d in 2019 IEEE symposium on security and privacy (SP) . IEEE, 2019, pp. 739\u2013753. ",
                    "Citation Text": "B. Hitaj, G. Ateniese, and F. Perez-Cruz, \u201cDeep models under the gan: information leakage from collaborative deep learning,\u201d in Proceedings of the\n2017 ACM SIGSAC Conference on Computer and Communications Security , 2017, pp. 603\u2013618.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.07464",
                        "Citation Paper Title": "Title:Deep Models Under the GAN: Information Leakage from Collaborative Deep Learning",
                        "Citation Paper Abstract": "Abstract:Deep Learning has recently become hugely popular in machine learning, providing significant improvements in classification accuracy in the presence of highly-structured and large databases.\nResearchers have also considered privacy implications of deep learning. Models are typically trained in a centralized manner with all the data being processed by the same training algorithm. If the data is a collection of users' private data, including habits, personal pictures, geographical positions, interests, and more, the centralized server will have access to sensitive information that could potentially be mishandled. To tackle this problem, collaborative deep learning models have recently been proposed where parties locally train their deep learning structures and only share a subset of the parameters in the attempt to keep their respective training sets private. Parameters can also be obfuscated via differential privacy (DP) to make information extraction even more challenging, as proposed by Shokri and Shmatikov at CCS'15.\nUnfortunately, we show that any privacy-preserving collaborative deep learning is susceptible to a powerful attack that we devise in this paper. In particular, we show that a distributed, federated, or decentralized deep learning approach is fundamentally broken and does not protect the training sets of honest participants. The attack we developed exploits the real-time nature of the learning process that allows the adversary to train a Generative Adversarial Network (GAN) that generates prototypical samples of the targeted training set that was meant to be private (the samples generated by the GAN are intended to come from the same distribution as the training data). Interestingly, we show that record-level DP applied to the shared parameters of the model, as suggested in previous work, is ineffective (i.e., record-level DP is not designed to address our attack).",
                        "Citation Paper Authors": "Authors:Briland Hitaj, Giuseppe Ateniese, Fernando Perez-Cruz"
                    }
                },
                {
                    "Sentence ID": 75,
                    "Sentence": "S. Reddi, Z. Charles, M. Zaheer, Z. Garrett, K. Rush, J. Kone\u010dn `y, S. Kumar, and H. B. McMahan, \u201cAdaptive federated optimization,\u201d arXiv preprint\narXiv:2003.00295 , 2020. ",
                    "Citation Text": "H. Wang, M. Yurochkin, Y. Sun, D. Papailiopoulos, and Y. Khazaeni, \u201cFederated learning with matched averaging,\u201d arXiv preprint arXiv:2002.06440 ,\n2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.06440",
                        "Citation Paper Title": "Title:Federated Learning with Matched Averaging",
                        "Citation Paper Abstract": "Abstract:Federated learning allows edge devices to collaboratively learn a shared model while keeping the training data on device, decoupling the ability to do model training from the need to store the data in the cloud. We propose Federated matched averaging (FedMA) algorithm designed for federated learning of modern neural network architectures e.g. convolutional neural networks (CNNs) and LSTMs. FedMA constructs the shared global model in a layer-wise manner by matching and averaging hidden elements (i.e. channels for convolution layers; hidden states for LSTM; neurons for fully connected layers) with similar feature extraction signatures. Our experiments indicate that FedMA not only outperforms popular state-of-the-art federated learning algorithms on deep CNN and LSTM architectures trained on real world datasets, but also reduces the overall communication burden.",
                        "Citation Paper Authors": "Authors:Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, Yasaman Khazaeni"
                    }
                },
                {
                    "Sentence ID": 74,
                    "Sentence": "T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith, \u201cFederated Optimization in Heterogeneous Networks, \u201d in Proc. Conf. on Machine\nLearning and Systems (MLSys) , 2020. ",
                    "Citation Text": "S. Reddi, Z. Charles, M. Zaheer, Z. Garrett, K. Rush, J. Kone\u010dn `y, S. Kumar, and H. B. McMahan, \u201cAdaptive federated optimization,\u201d arXiv preprint\narXiv:2003.00295 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.00295",
                        "Citation Paper Title": "Title:Adaptive Federated Optimization",
                        "Citation Paper Abstract": "Abstract:Federated learning is a distributed machine learning paradigm in which a large number of clients coordinate with a central server to learn a model without sharing their own training data. Standard federated optimization methods such as Federated Averaging (FedAvg) are often difficult to tune and exhibit unfavorable convergence behavior. In non-federated settings, adaptive optimization methods have had notable success in combating such issues. In this work, we propose federated versions of adaptive optimizers, including Adagrad, Adam, and Yogi, and analyze their convergence in the presence of heterogeneous data for general non-convex settings. Our results highlight the interplay between client heterogeneity and communication efficiency. We also perform extensive experiments on these methods and show that the use of adaptive optimizers can significantly improve the performance of federated learning.",
                        "Citation Paper Authors": "Authors:Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Kone\u010dn\u00fd, Sanjiv Kumar, H. Brendan McMahan"
                    }
                },
                {
                    "Sentence ID": 73,
                    "Sentence": "J. Verbraeken, M. Wolting, J. Katzy, J. Kloppenburg, T. Verbelen, and J. S. Rellermeyer, \u201cA survey on distributed machine learning,\u201d Acm computing\nsurveys (csur) , vol. 53, no. 2, pp. 1\u201333, 2020. ",
                    "Citation Text": "T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith, \u201cFederated Optimization in Heterogeneous Networks, \u201d in Proc. Conf. on Machine\nLearning and Systems (MLSys) , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.06127",
                        "Citation Paper Title": "Title:Federated Optimization in Heterogeneous Networks",
                        "Citation Paper Abstract": "Abstract:Federated Learning is a distributed learning paradigm with two key challenges that differentiate it from traditional distributed optimization: (1) significant variability in terms of the systems characteristics on each device in the network (systems heterogeneity), and (2) non-identically distributed data across the network (statistical heterogeneity). In this work, we introduce a framework, FedProx, to tackle heterogeneity in federated networks. FedProx can be viewed as a generalization and re-parametrization of FedAvg, the current state-of-the-art method for federated learning. While this re-parameterization makes only minor modifications to the method itself, these modifications have important ramifications both in theory and in practice. Theoretically, we provide convergence guarantees for our framework when learning over data from non-identical distributions (statistical heterogeneity), and while adhering to device-level systems constraints by allowing each participating device to perform a variable amount of work (systems heterogeneity). Practically, we demonstrate that FedProx allows for more robust convergence than FedAvg across a suite of realistic federated datasets. In particular, in highly heterogeneous settings, FedProx demonstrates significantly more stable and accurate convergence behavior relative to FedAvg---improving absolute test accuracy by 22% on average.",
                        "Citation Paper Authors": "Authors:Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, Virginia Smith"
                    }
                },
                {
                    "Sentence ID": 68,
                    "Sentence": "P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji, K. Bonawitz, Z. Charles, G. Cormode, R. Cummings et al. , \u201cAdvances and\nopen problems in federated learning,\u201d arXiv preprint arXiv:1912.04977 , 2019. ",
                    "Citation Text": "M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard et al. , \u201cTensorflow: A system for large-scale\nmachine learning,\u201d in 12th{USENIX}symposium on operating systems design and implementation ( {OSDI}16), 2016, pp. 265\u2013283.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.08695",
                        "Citation Paper Title": "Title:TensorFlow: A system for large-scale machine learning",
                        "Citation Paper Abstract": "Abstract:TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous \"parameter server\" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with particularly strong support for training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model in contrast to existing systems, and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.",
                        "Citation Paper Authors": "Authors:Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, Xiaoqiang Zheng"
                    }
                },
                {
                    "Sentence ID": 67,
                    "Sentence": "T. Li, A. K. Sahu, A. Talwalkar, and V. Smith, \u201cFederated learning: Challenges, methods, and future directions,\u201d IEEE Signal Processing Magazine ,\nvol. 37, no. 3, pp. 50\u201360, 2020. ",
                    "Citation Text": "P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji, K. Bonawitz, Z. Charles, G. Cormode, R. Cummings et al. , \u201cAdvances and\nopen problems in federated learning,\u201d arXiv preprint arXiv:1912.04977 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.04977",
                        "Citation Paper Title": "Title:Advances and Open Problems in Federated Learning",
                        "Citation Paper Abstract": "Abstract:Federated learning (FL) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this paper discusses recent advances and presents an extensive collection of open problems and challenges.",
                        "Citation Paper Authors": "Authors:Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G.L. D'Oliveira, Hubert Eichner, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adri\u00e0 Gasc\u00f3n, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Kone\u010dn\u00fd, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancr\u00e8de Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer \u00d6zg\u00fcr, Rasmus Pagh, Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tram\u00e8r, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, Sen Zhao"
                    }
                },
                {
                    "Sentence ID": 66,
                    "Sentence": "B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, \u201cCommunication-efficient learning of deep networks from decentralized data,\u201d\ninArtificial intelligence and statistics . PMLR, 2017, pp. 1273\u20131282. ",
                    "Citation Text": "T. Li, A. K. Sahu, A. Talwalkar, and V. Smith, \u201cFederated learning: Challenges, methods, and future directions,\u201d IEEE Signal Processing Magazine ,\nvol. 37, no. 3, pp. 50\u201360, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.07873",
                        "Citation Paper Title": "Title:Federated Learning: Challenges, Methods, and Future Directions",
                        "Citation Paper Abstract": "Abstract:Federated learning involves training statistical models over remote devices or siloed data centers, such as mobile phones or hospitals, while keeping data localized. Training in heterogeneous and potentially massive networks introduces novel challenges that require a fundamental departure from standard approaches for large-scale machine learning, distributed optimization, and privacy-preserving data analysis. In this article, we discuss the unique characteristics and challenges of federated learning, provide a broad overview of current approaches, and outline several directions of future work that are relevant to a wide range of research communities.",
                        "Citation Paper Authors": "Authors:Tian Li, Anit Kumar Sahu, Ameet Talwalkar, Virginia Smith"
                    }
                },
                {
                    "Sentence ID": 65,
                    "Sentence": "H. Qi, M. Brown, and D. G. Lowe, \u201cLow-shot learning with imprinted weights,\u201d in Proceedings of the IEEE conference on computer vision and pattern\nrecognition , 2018, pp. 5822\u20135830. ",
                    "Citation Text": "B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, \u201cCommunication-efficient learning of deep networks from decentralized data,\u201d\ninArtificial intelligence and statistics . PMLR, 2017, pp. 1273\u20131282.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.05629",
                        "Citation Paper Title": "Title:Communication-Efficient Learning of Deep Networks from Decentralized Data",
                        "Citation Paper Abstract": "Abstract:Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning.\nWe present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.",
                        "Citation Paper Authors": "Authors:H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, Blaise Ag\u00fcera y Arcas"
                    }
                },
                {
                    "Sentence ID": 64,
                    "Sentence": "M. D. Zeiler and R. Fergus, \u201cVisualizing and understanding convolutional networks,\u201d in European conference on computer vision . Springer, 2014, pp.\n818\u2013833. ",
                    "Citation Text": "H. Qi, M. Brown, and D. G. Lowe, \u201cLow-shot learning with imprinted weights,\u201d in Proceedings of the IEEE conference on computer vision and pattern\nrecognition , 2018, pp. 5822\u20135830.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.07136",
                        "Citation Paper Title": "Title:Low-Shot Learning with Imprinted Weights",
                        "Citation Paper Abstract": "Abstract:Human vision is able to immediately recognize novel visual categories after seeing just one or a few training examples. We describe how to add a similar capability to ConvNet classifiers by directly setting the final layer weights from novel training examples during low-shot learning. We call this process weight imprinting as it directly sets weights for a new category based on an appropriately scaled copy of the embedding layer activations for that training example. The imprinting process provides a valuable complement to training with stochastic gradient descent, as it provides immediate good classification performance and an initialization for any further fine-tuning in the future. We show how this imprinting process is related to proxy-based embeddings. However, it differs in that only a single imprinted weight vector is learned for each novel category, rather than relying on a nearest-neighbor distance to training instances as typically used with embedding methods. Our experiments show that using averaging of imprinted weights provides better generalization than using nearest-neighbor instance embeddings.",
                        "Citation Paper Authors": "Authors:Hang Qi, Matthew Brown, David G. Lowe"
                    }
                },
                {
                    "Sentence ID": 62,
                    "Sentence": "P. Marcelino, \u201cTransfer learning from pre-trained models,\u201d Towards Data Science , 2018. ",
                    "Citation Text": "J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, \u201cHow transferable are features in deep neural networks?\u201d arXiv preprint arXiv:1411.1792 , 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1411.1792",
                        "Citation Paper Title": "Title:How transferable are features in deep neural networks?",
                        "Citation Paper Abstract": "Abstract:Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.",
                        "Citation Paper Authors": "Authors:Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": "C. Shorten and T. M. Khoshgoftaar, \u201cA survey on image data augmentation for deep learning,\u201d Journal of Big Data , vol. 6, no. 1, pp. 1\u201348, 2019. ",
                    "Citation Text": "Q. Xie, Z. Dai, E. Hovy, T. Luong, and Q. Le, \u201cUnsupervised data augmentation for consistency training,\u201d Advances in Neural Information Processing\nSystems , vol. 33, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.12848",
                        "Citation Paper Title": "Title:Unsupervised Data Augmentation for Consistency Training",
                        "Citation Paper Abstract": "Abstract:Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods such as RandAugment and back-translation, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 5.43 with only 250 examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, Quoc V. Le"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "J. E. Van Engelen and H. H. Hoos, \u201cA survey on semi-supervised learning,\u201d Machine learning , vol. 109, no. 2, pp. 373\u2013440, 2020. ",
                    "Citation Text": "M. Tschannen, O. Bachem, and M. Lucic, \u201cRecent advances in autoencoder-based representation learning,\u201d arXiv preprint arXiv:1812.05069 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.05069",
                        "Citation Paper Title": "Title:Recent Advances in Autoencoder-Based Representation Learning",
                        "Citation Paper Abstract": "Abstract:Learning useful representations with little or no supervision is a key challenge in artificial intelligence. We provide an in-depth review of recent advances in representation learning with a focus on autoencoder-based models. To organize these results we make use of meta-priors believed useful for downstream tasks, such as disentanglement and hierarchical organization of features. In particular, we uncover three main mechanisms to enforce such properties, namely (i) regularizing the (approximate or aggregate) posterior distribution, (ii) factorizing the encoding and decoding distribution, or (iii) introducing a structured prior distribution. While there are some promising results, implicit or explicit supervision remains a key enabler and all current methods use strong inductive biases and modeling assumptions. Finally, we provide an analysis of autoencoder-based representation learning through the lens of rate-distortion theory and identify a clear tradeoff between the amount of prior knowledge available about the downstream tasks, and how useful the representation is for this task.",
                        "Citation Paper Authors": "Authors:Michael Tschannen, Olivier Bachem, Mario Lucic"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, and C. A. Raffel, \u201cMixmatch: A holistic approach to semi-supervised learning,\u201d\nAdvances in Neural Information Processing Systems , vol. 32, 2019. ",
                    "Citation Text": "Q. V. Le, \u201cBuilding high-level features using large scale unsupervised learning,\u201d in 2013 IEEE international conference on acoustics, speech and signal\nprocessing . IEEE, 2013, pp. 8595\u20138598.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1112.6209",
                        "Citation Paper Title": "Title:Building high-level features using large scale unsupervised learning",
                        "Citation Paper Abstract": "Abstract:We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting with these learned features, we trained our network to obtain 15.8% accuracy in recognizing 20,000 object categories from ImageNet, a leap of 70% relative improvement over the previous state-of-the-art.",
                        "Citation Paper Authors": "Authors:Quoc V. Le, Marc'Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado, Jeff Dean, Andrew Y. Ng"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "X. Zhu and A. B. Goldberg, \u201cIntroduction to semi-supervised learning, \u201d Synthesis lectures on artificial intelligence and machine learning , vol. 3, no. 1,\npp. 1\u2013130, 2009. ",
                    "Citation Text": "D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, and C. A. Raffel, \u201cMixmatch: A holistic approach to semi-supervised learning,\u201d\nAdvances in Neural Information Processing Systems , vol. 32, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.02249",
                        "Citation Paper Title": "Title:MixMatch: A Holistic Approach to Semi-Supervised Learning",
                        "Citation Paper Abstract": "Abstract:Semi-supervised learning has proven to be a powerful paradigm for leveraging unlabeled data to mitigate the reliance on large labeled datasets. In this work, we unify the current dominant approaches for semi-supervised learning to produce a new algorithm, MixMatch, that works by guessing low-entropy labels for data-augmented unlabeled examples and mixing labeled and unlabeled data using MixUp. We show that MixMatch obtains state-of-the-art results by a large margin across many datasets and labeled data amounts. For example, on CIFAR-10 with 250 labels, we reduce error rate by a factor of 4 (from 38% to 11%) and by a factor of 2 on STL-10. We also demonstrate how MixMatch can help achieve a dramatically better accuracy-privacy trade-off for differential privacy. Finally, we perform an ablation study to tease apart which components of MixMatch are most important for its success.",
                        "Citation Paper Authors": "Authors:David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, Colin Raffel"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "F. Tramer and D. Boneh, \u201cSlalom: Fast, verifiable and private execution of neural networks in trusted hardware,\u201d in International Conference on\nLearning Representations , 2018. ",
                    "Citation Text": "T. Hunt, C. Song, R. Shokri, V. Shmatikov, and E. Witchel, \u201cChiron: Privacy-preserving machine learning as a service,\u201d arXiv preprint arXiv:1803.05961 ,\n2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.05961",
                        "Citation Paper Title": "Title:Chiron: Privacy-preserving Machine Learning as a Service",
                        "Citation Paper Abstract": "Abstract:Major cloud operators offer machine learning (ML) as a service, enabling customers who have the data but not ML expertise or infrastructure to train predictive models on this data. Existing ML-as-a-service platforms require users to reveal all training data to the service operator. We design, implement, and evaluate Chiron, a system for privacy-preserving machine learning as a service. First, Chiron conceals the training data from the service operator. Second, in keeping with how many existing ML-as-a-service platforms work, Chiron reveals neither the training algorithm nor the model structure to the user, providing only black-box access to the trained model. Chiron is implemented using SGX enclaves, but SGX alone does not achieve the dual goals of data privacy and model confidentiality. Chiron runs the standard ML training toolchain (including the popular Theano framework and C compiler) in an enclave, but the untrusted model-creation code from the service operator is further confined in a Ryoan sandbox to prevent it from leaking the training data outside the enclave. To support distributed training, Chiron executes multiple concurrent enclaves that exchange model parameters via a parameter server. We evaluate Chiron on popular deep learning models, focusing on benchmark image classification tasks such as CIFAR and ImageNet, and show that its training performance and accuracy of the resulting models are practical for common uses of ML-as-a-service.",
                        "Citation Paper Authors": "Authors:Tyler Hunt, Congzheng Song, Reza Shokri, Vitaly Shmatikov, Emmett Witchel"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "N. Carlini, S. Chien, M. Nasr, S. Song, A. Terzis, and F. Tramer, \u201cMembership inference attacks from first principles,\u201d in 2022 IEEE Symposium on\nSecurity and Privacy (SP) . IEEE, 2022, pp. 1897\u20131914. ",
                    "Citation Text": "I. J. Goodfellow, J. Shlens, and C. Szegedy, \u201cExplaining and harnessing adversarial examples,\u201d arXiv preprint arXiv:1412.6572 , 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.6572",
                        "Citation Paper Title": "Title:Explaining and Harnessing Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "L. Melis, C. Song, E. De Cristofaro, and V. Shmatikov, \u201cExploiting unintended feature leakage in collaborative learning, \u201d in 2019 IEEE Symposium on\nSecurity and Privacy (SP) . IEEE, 2019, pp. 691\u2013706. ",
                    "Citation Text": "N. Carlini, S. Chien, M. Nasr, S. Song, A. Terzis, and F. Tramer, \u201cMembership inference attacks from first principles,\u201d in 2022 IEEE Symposium on\nSecurity and Privacy (SP) . IEEE, 2022, pp. 1897\u20131914.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.03570",
                        "Citation Paper Title": "Title:Membership Inference Attacks From First Principles",
                        "Citation Paper Abstract": "Abstract:A membership inference attack allows an adversary to query a trained machine learning model to predict whether or not a particular example was contained in the model's training dataset. These attacks are currently evaluated using average-case \"accuracy\" metrics that fail to characterize whether the attack can confidently identify any members of the training set. We argue that attacks should instead be evaluated by computing their true-positive rate at low (e.g., <0.1%) false-positive rates, and find most prior attacks perform poorly when evaluated in this way. To address this we develop a Likelihood Ratio Attack (LiRA) that carefully combines multiple ideas from the literature. Our attack is 10x more powerful at low false-positive rates, and also strictly dominates prior attacks on existing metrics.",
                        "Citation Paper Authors": "Authors:Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, Florian Tramer"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "R. Shokri, M. Stronati, C. Song, and V. Shmatikov, \u201cMembership inference attacks against machine learning models,\u201d in 2017 IEEE Symposium on\nSecurity and Privacy (SP) . IEEE, 2017, pp. 3\u201318. ",
                    "Citation Text": "L. Melis, C. Song, E. De Cristofaro, and V. Shmatikov, \u201cExploiting unintended feature leakage in collaborative learning, \u201d in 2019 IEEE Symposium on\nSecurity and Privacy (SP) . IEEE, 2019, pp. 691\u2013706.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.04049",
                        "Citation Paper Title": "Title:Exploiting Unintended Feature Leakage in Collaborative Learning",
                        "Citation Paper Abstract": "Abstract:Collaborative machine learning and related techniques such as federated learning allow multiple participants, each with his own training dataset, to build a joint model by training locally and periodically exchanging model updates. We demonstrate that these updates leak unintended information about participants' training data and develop passive and active inference attacks to exploit this leakage. First, we show that an adversarial participant can infer the presence of exact data points -- for example, specific locations -- in others' training data (i.e., membership inference). Then, we show how this adversary can infer properties that hold only for a subset of the training data and are independent of the properties that the joint model aims to capture. For example, he can infer when a specific person first appears in the photos used to train a binary gender classifier. We evaluate our attacks on a variety of tasks, datasets, and learning configurations, analyze their limitations, and discuss possible defenses.",
                        "Citation Paper Authors": "Authors:Luca Melis, Congzheng Song, Emiliano De Cristofaro, Vitaly Shmatikov"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "X. Chen, C. Liu, B. Li, K. Lu, and D. Song, \u201cTargeted backdoor attacks on deep learning systems using data poisoning, \u201d arXiv preprint arXiv:1712.05526 ,\n2017. ",
                    "Citation Text": "V. Shejwalkar, A. Houmansadr, P. Kairouz, and D. Ramage, \u201cBack to the drawing board: A critical evaluation of poisoning attacks on production\nfederated learning,\u201d in 2022 IEEE Symposium on Security and Privacy (SP) . IEEE, 2022, pp. 1354\u20131371.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.10241",
                        "Citation Paper Title": "Title:Back to the Drawing Board: A Critical Evaluation of Poisoning Attacks on Production Federated Learning",
                        "Citation Paper Abstract": "Abstract:While recent works have indicated that federated learning (FL) may be vulnerable to poisoning attacks by compromised clients, their real impact on production FL systems is not fully understood. In this work, we aim to develop a comprehensive systemization for poisoning attacks on FL by enumerating all possible threat models, variations of poisoning, and adversary capabilities. We specifically put our focus on untargeted poisoning attacks, as we argue that they are significantly relevant to production FL deployments.\nWe present a critical analysis of untargeted poisoning attacks under practical, production FL environments by carefully characterizing the set of realistic threat models and adversarial capabilities. Our findings are rather surprising: contrary to the established belief, we show that FL is highly robust in practice even when using simple, low-cost defenses. We go even further and propose novel, state-of-the-art data and model poisoning attacks, and show via an extensive set of experiments across three benchmark datasets how (in)effective poisoning attacks are in the presence of simple defense mechanisms. We aim to correct previous misconceptions and offer concrete guidelines to conduct more accurate (and more realistic) research on this topic.",
                        "Citation Paper Authors": "Authors:Virat Shejwalkar, Amir Houmansadr, Peter Kairouz, Daniel Ramage"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "M. Fang, X. Cao, J. Jia, and N. Gong, \u201cLocal model poisoning attacks to byzantine-robust federated learning, \u201d in 29th{USENIX}Security Symposium\n({USENIX}Security 20) , 2020, pp. 1605\u20131622. ",
                    "Citation Text": "X. Chen, C. Liu, B. Li, K. Lu, and D. Song, \u201cTargeted backdoor attacks on deep learning systems using data poisoning, \u201d arXiv preprint arXiv:1712.05526 ,\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.05526",
                        "Citation Paper Title": "Title:Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning",
                        "Citation Paper Abstract": "Abstract:Deep learning models have achieved high performance on many tasks, and thus have been applied to many security-critical scenarios. For example, deep learning-based face recognition systems have been used to authenticate users to access many security-sensitive applications like payment apps. Such usages of deep learning systems provide the adversaries with sufficient incentives to perform attacks against these systems for their adversarial purposes. In this work, we consider a new type of attacks, called backdoor attacks, where the attacker's goal is to create a backdoor into a learning-based authentication system, so that he can easily circumvent the system by leveraging the backdoor. Specifically, the adversary aims at creating backdoor instances, so that the victim learning system will be misled to classify the backdoor instances as a target label specified by the adversary. In particular, we study backdoor poisoning attacks, which achieve backdoor attacks using poisoning strategies. Different from all existing work, our studied poisoning strategies can apply under a very weak threat model: (1) the adversary has no knowledge of the model and the training set used by the victim system; (2) the attacker is allowed to inject only a small amount of poisoning samples; (3) the backdoor key is hard to notice even by human beings to achieve stealthiness. We conduct evaluation to demonstrate that a backdoor adversary can inject only around 50 poisoning samples, while achieving an attack success rate of above 90%. We are also the first work to show that a data poisoning attack can create physically implementable backdoors without touching the training process. Our work demonstrates that backdoor poisoning attacks pose real threats to a learning system, and thus highlights the importance of further investigation and proposing defense strategies against them.",
                        "Citation Paper Authors": "Authors:Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, Dawn Song"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "R. Ravindran, M. J. Santora, and M. M. Jamali, \u201cMulti-object detection and tracking, based on dnn, for autonomous vehicles: A review, \u201d IEEE Sensors\nJournal , vol. 21, no. 5, pp. 5668\u20135677, 2020. ",
                    "Citation Text": "T. Orekondy, B. Schiele, and M. Fritz, \u201cKnockoff nets: Stealing functionality of black-box models,\u201d in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2019, pp. 4954\u20134963.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.02766",
                        "Citation Paper Title": "Title:Knockoff Nets: Stealing Functionality of Black-Box Models",
                        "Citation Paper Abstract": "Abstract:Machine Learning (ML) models are increasingly deployed in the wild to perform a wide range of tasks. In this work, we ask to what extent can an adversary steal functionality of such \"victim\" models based solely on blackbox interactions: image in, predictions out. In contrast to prior work, we present an adversary lacking knowledge of train/test data used by the model, its internals, and semantics over model outputs. We formulate model functionality stealing as a two-step approach: (i) querying a set of input images to the blackbox model to obtain predictions; and (ii) training a \"knockoff\" with queried image-prediction pairs. We make multiple remarkable observations: (a) querying random images from a different distribution than that of the blackbox training data results in a well-performing knockoff; (b) this is possible even when the knockoff is represented using a different architecture; and (c) our reinforcement learning approach additionally improves query sample efficiency in certain settings and provides performance gains. We validate model functionality stealing on a range of datasets and tasks, as well as on a popular image analysis API where we create a reasonable knockoff for as little as $30.",
                        "Citation Paper Authors": "Authors:Tribhuvanesh Orekondy, Bernt Schiele, Mario Fritz"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.02598v3": {
            "Paper Title": "Universal Private Estimators",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.10454v3": {
            "Paper Title": "Blockchain Mining with Multiple Selfish Miners",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.10844v2": {
            "Paper Title": "Learning to Generate Image Embeddings with User-level Differential\n  Privacy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.06015v2": {
            "Paper Title": "Black-box Dataset Ownership Verification via Backdoor Watermarking",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": "were proposed, regarding attack stealthiness and stability.\nCurrently, there are also a few backdoor attacks developed\noutside the context of image classi\ufb01cation ",
                    "Citation Text": "X. Chen, A. Salem, D. Chen, M. Backes, S. Ma, Q. Shen, Z. Wu, and\nY . Zhang, \u201cBadnl: Backdoor attacks against nlp models with semantic-\npreserving improvements,\u201d in ACSAC , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.01043",
                        "Citation Paper Title": "Title:BadNL: Backdoor Attacks against NLP Models with Semantic-preserving Improvements",
                        "Citation Paper Abstract": "Abstract:Deep neural networks (DNNs) have progressed rapidly during the past decade and have been deployed in various real-world applications. Meanwhile, DNN models have been shown to be vulnerable to security and privacy attacks. One such attack that has attracted a great deal of attention recently is the backdoor attack. Specifically, the adversary poisons the target model's training set to mislead any input with an added secret trigger to a target class.\nPrevious backdoor attacks predominantly focus on computer vision (CV) applications, such as image classification. In this paper, we perform a systematic investigation of backdoor attack on NLP models, and propose BadNL, a general NLP backdoor attack framework including novel attack methods. Specifically, we propose three methods to construct triggers, namely BadChar, BadWord, and BadSentence, including basic and semantic-preserving variants. Our attacks achieve an almost perfect attack success rate with a negligible effect on the original model's utility. For instance, using the BadChar, our backdoor attack achieves a 98.9% attack success rate with yielding a utility improvement of 1.5% on the SST-5 dataset when only poisoning 3% of the original set. Moreover, we conduct a user study to prove that our triggers can well preserve the semantics from humans perspective.",
                        "Citation Paper Authors": "Authors:Xiaoyi Chen, Ahmed Salem, Dingfan Chen, Michael Backes, Shiqing Ma, Qingni Shen, Zhonghai Wu, Yang Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.02658v2": {
            "Paper Title": "Longitudinal Analysis of Privacy Labels in the Apple App Store",
            "Sentences": [
                {
                    "Sentence ID": 41,
                    "Sentence": "developed a tool to assist developers to prompt them of pos-\nsible functionality that would require a privacy label. Xiao\net al. ",
                    "Citation Text": "Y . Xiao, Z. Li, Y . Qin, X. Bai, J. Guan, X. Liao,\nand L. Xing, \u201cLalaine: Measuring and characterizing\nnon-compliance of apple privacy labels at scale,\u201d\nCoRR , vol. abs/2206.06274, 2022. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2206.06274",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.06274",
                        "Citation Paper Title": "Title:Lalaine: Measuring and Characterizing Non-Compliance of Apple Privacy Labels at Scale",
                        "Citation Paper Abstract": "Abstract:As a key supplement to privacy policies that are known to be lengthy and difficult to read, Apple has launched the app privacy labels, which purportedly help users more easily understand an app's privacy practices. However, false and misleading privacy labels can dupe privacy-conscious consumers into downloading data-intensive apps, ultimately eroding the credibility and integrity of the labels. Although Apple releases requirements and guidelines for app developers to create privacy labels, little is known about whether and to what extent the privacy labels in the wild are correct and compliant, reflecting the actual data practices of iOS apps. This paper presents the first systematic study, based on our new methodology named Lalaine, to evaluate data-flow to privacy-label (flow-to-label) consistency. Lalaine analyzed the privacy labels and binaries of 5,102 iOS apps, shedding light on the prevalence and seriousness of privacy-label non-compliance. We provide detailed case studies and analyze root causes for privacy label non-compliance that complements prior understandings. This has led to new insights for improving privacy-label design and compliance requirements, so app developers, platform stakeholders, and policy-makers can better achieve their privacy and accountability goals. Lalaine is thoroughly evaluated for its high effectiveness and efficiency. We are responsibly reporting the results to stakeholders.",
                        "Citation Paper Authors": "Authors:Yue Xiao, Zhengyi Li, Yue Qin, Xiaolong Bai, Jiale Guan, Xiaojing Liao, Luyi Xing"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.13123v2": {
            "Paper Title": "Motif-aware temporal GCN for fraud detection in signed cryptocurrency\n  trust networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.08532v5": {
            "Paper Title": "New Lower Bounds for Private Estimation and a Generalized Fingerprinting\n  Lemma",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.08147v4": {
            "Paper Title": "Energy-Latency Attacks via Sponge Poisoning",
            "Sentences": [
                {
                    "Sentence ID": 45,
                    "Sentence": ", a low-time response is essential,\nand increasing the model\u2019s decision latency can thus make the\nsystem unusable. Moreover, increasing the energy consump-\ntion of mobile systems, such as wearable health-monitoring\nsystems ",
                    "Citation Text": "N. Y . Hammerla, S. Halloran, and T. Pl \u00a8otz, \u201cDeep, convolutional,\nand recurrent models for human activity recognition using wearables,\u201d\ninProceedings of the Twenty-Fifth International Joint Conference on\nArti\ufb01cial Intelligence, IJCAI 2016 , pp. 1533\u20131540, IJCAI/AAAI Press,\n2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1604.08880",
                        "Citation Paper Title": "Title:Deep, Convolutional, and Recurrent Models for Human Activity Recognition using Wearables",
                        "Citation Paper Abstract": "Abstract:Human activity recognition (HAR) in ubiquitous computing is beginning to adopt deep learning to substitute for well-established analysis techniques that rely on hand-crafted feature extraction and classification techniques. From these isolated applications of custom deep architectures it is, however, difficult to gain an overview of their suitability for problems ranging from the recognition of manipulative gestures to the segmentation and identification of physical activities like running or ascending stairs. In this paper we rigorously explore deep, convolutional, and recurrent approaches across three representative datasets that contain movement data captured with wearable sensors. We describe how to train recurrent approaches in this setting, introduce a novel regularisation approach, and illustrate how they outperform the state-of-the-art on a large benchmark dataset. Across thousands of recognition experiments with randomly sampled model configurations we investigate the suitability of each model for different tasks in HAR, explore the impact of hyperparameters using the fANOVA framework, and provide guidelines for the practitioner who wants to apply deep learning in their problem setting.",
                        "Citation Paper Authors": "Authors:Nils Y. Hammerla, Shane Halloran, Thomas Ploetz"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.14547v2": {
            "Paper Title": "A Secure Federated Learning Framework for Residential Short Term Load\n  Forecasting",
            "Sentences": [
                {
                    "Sentence ID": 43,
                    "Sentence": ". Nonetheless, recent litera-\nture has revealed its failure to suf\ufb01ciently guarantee privacy\npreservation due to update leakage ",
                    "Citation Text": "A. Bhowmick, J. Duchi, J. Freudiger, G. Kapoor, and R. Rogers, \u201cPro-\ntection against reconstruction and its applications in private federated\nlearning,\u201d arXiv preprint arXiv:1812.00984 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.00984",
                        "Citation Paper Title": "Title:Protection Against Reconstruction and Its Applications in Private Federated Learning",
                        "Citation Paper Abstract": "Abstract:In large-scale statistical learning, data collection and model fitting are moving increasingly toward peripheral devices---phones, watches, fitness trackers---away from centralized data collection. Concomitant with this rise in decentralized data are increasing challenges of maintaining privacy while allowing enough information to fit accurate, useful statistical models. This motivates local notions of privacy---most significantly, local differential privacy, which provides strong protections against sensitive data disclosures---where data is obfuscated before a statistician or learner can even observe it, providing strong protections to individuals' data. Yet local privacy as traditionally employed may prove too stringent for practical use, especially in modern high-dimensional statistical and machine learning problems. Consequently, we revisit the types of disclosures and adversaries against which we provide protections, considering adversaries with limited prior information and ensuring that with high probability, ensuring they cannot reconstruct an individual's data within useful tolerances. By reconceptualizing these protections, we allow more useful data release---large privacy parameters in local differential privacy---and we design new (minimax) optimal locally differentially private mechanisms for statistical learning problems for \\emph{all} privacy levels. We thus present practicable approaches to large-scale locally private model training that were previously impossible, showing theoretically and empirically that we can fit large-scale image classification and language models with little degradation in utility.",
                        "Citation Paper Authors": "Authors:Abhishek Bhowmick, John Duchi, Julien Freudiger, Gaurav Kapoor, Ryan Rogers"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": ". Note that the parameters \u000fand\u000e\nare assumed to be positive real numbers. This is necessary\nto ensure that the privacy guarantee of the mechanism is\nmeaningful and non-trivial ",
                    "Citation Text": "D. Ye, S. Shen, T. Zhu, B. Liu, and W. Zhou, \u201cOne parameter de-\nfense\u2014defending against data inference attacks via differential privacy,\u201d\nIEEE Transactions on Information Forensics and Security , vol. 17, pp.\n1466\u20131480, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.06580",
                        "Citation Paper Title": "Title:One Parameter Defense -- Defending against Data Inference Attacks via Differential Privacy",
                        "Citation Paper Abstract": "Abstract:Machine learning models are vulnerable to data inference attacks, such as membership inference and model inversion attacks. In these types of breaches, an adversary attempts to infer a data record's membership in a dataset or even reconstruct this data record using a confidence score vector predicted by the target model. However, most existing defense methods only protect against membership inference attacks. Methods that can combat both types of attacks require a new model to be trained, which may not be time-efficient. In this paper, we propose a differentially private defense method that handles both types of attacks in a time-efficient manner by tuning only one parameter, the privacy budget. The central idea is to modify and normalize the confidence score vectors with a differential privacy mechanism which preserves privacy and obscures membership and reconstructed data. Moreover, this method can guarantee the order of scores in the vector to avoid any loss in classification accuracy. The experimental results show the method to be an effective and timely defense against both membership inference and model inversion attacks with no reduction in accuracy.",
                        "Citation Paper Authors": "Authors:Dayong Ye, Sheng Shen, Tianqing Zhu, Bo Liu, Wanlei Zhou"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "introduced Bulyan as\nan extension of Krum to recursively \ufb01nd subset of nodes using\nKrum and eventually perform an element-wise pruned mean on\nthe updates to exclude the high magnitude values. The authors\nin ",
                    "Citation Text": "X. Chen, T. Chen, H. Sun, S. Z. Wu, and M. Hong, \u201cDistributed\ntraining with heterogeneous data: Bridging median-and mean-based al-\ngorithms,\u201d Advances in Neural Information Processing Systems , vol. 33,\npp. 21 616\u201321 626, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.01736",
                        "Citation Paper Title": "Title:Distributed Training with Heterogeneous Data: Bridging Median- and Mean-Based Algorithms",
                        "Citation Paper Abstract": "Abstract:Recently, there is a growing interest in the study of median-based algorithms for distributed non-convex optimization. Two prominent such algorithms include signSGD with majority vote, an effective approach for communication reduction via 1-bit compression on the local gradients, and medianSGD, an algorithm recently proposed to ensure robustness against Byzantine workers. The convergence analyses for these algorithms critically rely on the assumption that all the distributed data are drawn iid from the same distribution. However, in applications such as Federated Learning, the data across different nodes or machines can be inherently heterogeneous, which violates such an iid assumption. This work analyzes signSGD and medianSGD in distributed settings with heterogeneous data. We show that these algorithms are non-convergent whenever there is some disparity between the expected median and mean over the local gradients. To overcome this gap, we provide a novel gradient correction mechanism that perturbs the local gradients with noise, together with a series results that provable close the gap between mean and median of the gradients. The proposed methods largely preserve nice properties of these methods, such as the low per-iteration communication complexity of signSGD, and further enjoy global convergence to stationary solutions. Our perturbation technique can be of independent interest when one wishes to estimate mean through a median estimator.",
                        "Citation Paper Authors": "Authors:Xiangyi Chen, Tiancong Chen, Haoran Sun, Zhiwei Steven Wu, Mingyi Hong"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "proposed a computationally expensive\nKrum algorithm which performs gradient update selection and\nhas the least sum of distances from the nearest gradient updates\nduring each iteration. In addition, ",
                    "Citation Text": "E. M. El Mhamdi, R. Guerraoui, and S. Rouault, \u201cThe hidden vul-\nnerability of distributed learning in Byzantium,\u201d in Proceedings of the\n35th International Conference on Machine Learning , ser. Proceedings\nof Machine Learning Research, J. Dy and A. Krause, Eds., vol. 80.\nPMLR, 10\u201315 Jul 2018, pp. 3521\u20133530.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.07927",
                        "Citation Paper Title": "Title:The Hidden Vulnerability of Distributed Learning in Byzantium",
                        "Citation Paper Abstract": "Abstract:While machine learning is going through an era of celebrated success, concerns have been raised about the vulnerability of its backbone: stochastic gradient descent (SGD). Recent approaches have been proposed to ensure the robustness of distributed SGD against adversarial (Byzantine) workers sending poisoned gradients during the training phase. Some of these approaches have been proven Byzantine-resilient: they ensure the convergence of SGD despite the presence of a minority of adversarial workers.\nWe show in this paper that convergence is not enough. In high dimension $d \\gg 1$, an adver\\-sary can build on the loss function's non-convexity to make SGD converge to ineffective models. More precisely, we bring to light that existing Byzantine-resilient schemes leave a margin of poisoning of $\\Omega\\left(f(d)\\right)$, where $f(d)$ increases at least like $\\sqrt{d~}$. Based on this leeway, we build a simple attack, and experimentally show its strong to utmost effectivity on CIFAR-10 and MNIST.\nWe introduce Bulyan, and prove it significantly reduces the attackers leeway to a narrow $O( \\frac{1}{\\sqrt{d~}})$ bound. We empirically show that Bulyan does not suffer the fragility of existing aggregation rules and, at a reasonable cost in terms of required batch size, achieves convergence as if only non-Byzantine gradients had been used to update the model.",
                        "Citation Paper Authors": "Authors:El Mahdi El Mhamdi, Rachid Guerraoui, S\u00e9bastien Rouault"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.10310v2": {
            "Paper Title": "PreFair: Privately Generating Justifiably Fair Synthetic Data",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": "which is a\ncausal-based notion of fairness. Other works consider associational\nfairness measures based on some statistical measures relative to\nthe protected attributes. Additionally, conditional associational fair-\nness measures ",
                    "Citation Text": "Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. 2017.\nAlgorithmic Decision Making and the Cost of Fairness. In SIGKDD . ACM, 797\u2013\n806.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1701.08230",
                        "Citation Paper Title": "Title:Algorithmic decision making and the cost of fairness",
                        "Citation Paper Abstract": "Abstract:Algorithms are now regularly used to decide whether defendants awaiting trial are too dangerous to be released back into the community. In some cases, black defendants are substantially more likely than white defendants to be incorrectly classified as high risk. To mitigate such disparities, several techniques recently have been proposed to achieve algorithmic fairness. Here we reformulate algorithmic fairness as constrained optimization: the objective is to maximize public safety while satisfying formal fairness constraints designed to reduce racial disparities. We show that for several past definitions of fairness, the optimal algorithms that result require detaining defendants above race-specific risk thresholds. We further show that the optimal unconstrained algorithm requires applying a single, uniform threshold to all defendants. The unconstrained algorithm thus maximizes public safety while also satisfying one important understanding of equality: that all individuals are held to the same standard, irrespective of race. Because the optimal constrained and unconstrained algorithms generally differ, there is tension between improving public safety and satisfying prevailing notions of algorithmic fairness. By examining data from Broward County, Florida, we show that this trade-off can be large in practice. We focus on algorithms for pretrial release decisions, but the principles we discuss apply to other domains, and also to human decision makers carrying out structured decision rules.",
                        "Citation Paper Authors": "Authors:Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, Aziz Huq"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.17104v3": {
            "Paper Title": "Agent-Cells with DNA Programming: A Dynamic Decentralized System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.07513v2": {
            "Paper Title": "Can't Steal? Cont-Steal! Contrastive Stealing Attacks Against Image\n  Encoders",
            "Sentences": [
                {
                    "Sentence ID": 51,
                    "Sentence": "formulated a model stealing attack against BERT-\nbased API. Besides, Wu et al. ",
                    "Citation Text": "Bang Wu, Xiangwen Yang, Shirui Pan, and Xingliang\nYuan. Model Extraction Attacks on Graph Neu-\nral Networks: Taxonomy and Realization. CoRR\nabs/2010.12751 , 2020. 1, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.12751",
                        "Citation Paper Title": "Title:Model Extraction Attacks on Graph Neural Networks: Taxonomy and Realization",
                        "Citation Paper Abstract": "Abstract:Machine learning models are shown to face a severe threat from Model Extraction Attacks, where a well-trained private model owned by a service provider can be stolen by an attacker pretending as a client. Unfortunately, prior works focus on the models trained over the Euclidean space, e.g., images and texts, while how to extract a GNN model that contains a graph structure and node features is yet to be explored. In this paper, for the first time, we comprehensively investigate and develop model extraction attacks against GNN models. We first systematically formalise the threat modelling in the context of GNN model extraction and classify the adversarial threats into seven categories by considering different background knowledge of the attacker, e.g., attributes and/or neighbour connections of the nodes obtained by the attacker. Then we present detailed methods which utilise the accessible knowledge in each threat to implement the attacks. By evaluating over three real-world datasets, our attacks are shown to extract duplicated models effectively, i.e., 84% - 89% of the inputs in the target domain have the same output predictions as the victim model.",
                        "Citation Paper Authors": "Authors:Bang Wu, Xiangwen Yang, Shirui Pan, Xingliang Yuan"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": "proposed knockoff nets, which aim at\nstealing the functionality of black-box models. Krishna et\nal. ",
                    "Citation Text": "Kalpesh Krishna, Gaurav Singh Tomar, Ankur P.\nParikh, Nicolas Papernot, and Mohit Iyyer. Thieves on\nSesame Street! Model Extraction of BERT-based APIs.\nInInternational Conference on Learning Representa-\ntions (ICLR) , 2020. 1, 2, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.12366",
                        "Citation Paper Title": "Title:Thieves on Sesame Street! Model Extraction of BERT-based APIs",
                        "Citation Paper Abstract": "Abstract:We study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al. 2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks, including natural language inference and question answering. Our work thus highlights an exploit only made feasible by the shift towards transfer learning methods within the NLP community: for a query budget of a few hundred dollars, an attacker can extract a model that performs only slightly worse than the victim model. Finally, we study two defense strategies against model extraction---membership classification and API watermarking---which while successful against naive adversaries, are ineffective against more sophisticated ones.",
                        "Citation Paper Authors": "Authors:Kalpesh Krishna, Gaurav Singh Tomar, Ankur P. Parikh, Nicolas Papernot, Mohit Iyyer"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "also tried to steal ma-\nchine learning model\u2019s architectures and hyperparameters.\nOrekondy et al. ",
                    "Citation Text": "Tribhuvanesh Orekondy, Bernt Schiele, and Mario\nFritz. Knockoff Nets: Stealing Functionality of Black-\nBox Models. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) , pages 4954\u20134963.\nIEEE, 2019. 1, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.02766",
                        "Citation Paper Title": "Title:Knockoff Nets: Stealing Functionality of Black-Box Models",
                        "Citation Paper Abstract": "Abstract:Machine Learning (ML) models are increasingly deployed in the wild to perform a wide range of tasks. In this work, we ask to what extent can an adversary steal functionality of such \"victim\" models based solely on blackbox interactions: image in, predictions out. In contrast to prior work, we present an adversary lacking knowledge of train/test data used by the model, its internals, and semantics over model outputs. We formulate model functionality stealing as a two-step approach: (i) querying a set of input images to the blackbox model to obtain predictions; and (ii) training a \"knockoff\" with queried image-prediction pairs. We make multiple remarkable observations: (a) querying random images from a different distribution than that of the blackbox training data results in a well-performing knockoff; (b) this is possible even when the knockoff is represented using a different architecture; and (c) our reinforcement learning approach additionally improves query sample efficiency in certain settings and provides performance gains. We validate model functionality stealing on a range of datasets and tasks, as well as on a popular image analysis API where we create a reasonable knockoff for as little as $30.",
                        "Citation Paper Authors": "Authors:Tribhuvanesh Orekondy, Bernt Schiele, Mario Fritz"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "proposed the \ufb01rst hyperparameter stealing attacks\nagainst ML models. Oh et al. ",
                    "Citation Text": "Seong Joon Oh, Max Augustin, Bernt Schiele, and\nMario Fritz. Towards Reverse-Engineering Black-Box\nNeural Networks. In International Conference on\nLearning Representations (ICLR) , 2018. 7\n9",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.01768",
                        "Citation Paper Title": "Title:Towards Reverse-Engineering Black-Box Neural Networks",
                        "Citation Paper Abstract": "Abstract:Many deployed learned models are black boxes: given input, returns output. Internal information about the model, such as the architecture, optimisation procedure, or training data, is not disclosed explicitly as it might contain proprietary information or make the system more vulnerable. This work shows that such attributes of neural networks can be exposed from a sequence of queries. This has multiple implications. On the one hand, our work exposes the vulnerability of black-box neural networks to different types of attacks -- we show that the revealed internal information helps generate more effective adversarial examples against the black box model. On the other hand, this technique can be used for better protection of private content from automatic recognition models using adversarial examples. Our paper suggests that it is actually hard to draw a line between white box and black box models.",
                        "Citation Paper Authors": "Authors:Seong Joon Oh, Max Augustin, Bernt Schiele, Mario Fritz"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.10779v6": {
            "Paper Title": "CgAT: Center-Guided Adversarial Training for Deep Hashing-Based\n  Retrieval",
            "Sentences": [
                {
                    "Sentence ID": 47,
                    "Sentence": "learns\nthe pairwise similarity between the database and query set in an\nasymmetric way. However, CSQ ",
                    "Citation Text": "Li Yuan, Tao Wang, Xiaopeng Zhang, Francis EH Tay, Zequn Jie, Wei Liu, and\nJiashi Feng. 2020. Central similarity quantization for efficient image and video\nretrieval. In IEEE/CVF Conference on Computer Vision and Pattern Recognition .\nIEEE, Seattle, WA, USA, 3083\u20133092.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.00347",
                        "Citation Paper Title": "Title:Central Similarity Quantization for Efficient Image and Video Retrieval",
                        "Citation Paper Abstract": "Abstract:Existing data-dependent hashing methods usually learn hash functions from pairwise or triplet data relationships, which only capture the data similarity locally, and often suffer from low learning efficiency and low collision rate. In this work, we propose a new \\emph{global} similarity metric, termed as \\emph{central similarity}, with which the hash codes of similar data pairs are encouraged to approach a common center and those for dissimilar pairs to converge to different centers, to improve hash learning efficiency and retrieval accuracy. We principally formulate the computation of the proposed central similarity metric by introducing a new concept, i.e., \\emph{hash center} that refers to a set of data points scattered in the Hamming space with a sufficient mutual distance between each other. We then provide an efficient method to construct well separated hash centers by leveraging the Hadamard matrix and Bernoulli distributions. Finally, we propose the Central Similarity Quantization (CSQ) that optimizes the central similarity between data points w.r.t.\\ their hash centers instead of optimizing the local similarity. CSQ is generic and applicable to both image and video hashing scenarios. Extensive experiments on large-scale image and video retrieval tasks demonstrate that CSQ can generate cohesive hash codes for similar data pairs and dispersed hash codes for dissimilar pairs, achieving a noticeable boost in retrieval performance, i.e. 3\\%-20\\% in mAP over the previous state-of-the-arts. The code is at: \\url{this https URL}",
                        "Citation Paper Authors": "Authors:Li Yuan, Tao Wang, Xiaopeng Zhang, Francis EH Tay, Zequn Jie, Wei Liu, Jiashi Feng"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "proposed a pairwise loss-based method to preserve the\nsemantic similarity between data items in an end-to-end DNN,\ncalled Deep Pairwise-Supervised Hashing (DPSH). HashNet ",
                    "Citation Text": "Zhangjie Cao, Mingsheng Long, Jianmin Wang, and Philip S Yu. 2017. Hashnet:\nDeep Learning to Hash by Continuation. In IEEE/CVF International Conference\non Computer Vision . IEEE, Venice, Italy, 5608\u20135617.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.00758",
                        "Citation Paper Title": "Title:HashNet: Deep Learning to Hash by Continuation",
                        "Citation Paper Abstract": "Abstract:Learning to hash has been widely applied to approximate nearest neighbor search for large-scale multimedia retrieval, due to its computation efficiency and retrieval quality. Deep learning to hash, which improves retrieval quality by end-to-end representation learning and hash encoding, has received increasing attention recently. Subject to the ill-posed gradient difficulty in the optimization with sign activations, existing deep learning to hash methods need to first learn continuous representations and then generate binary hash codes in a separated binarization step, which suffer from substantial loss of retrieval quality. This work presents HashNet, a novel deep architecture for deep learning to hash by continuation method with convergence guarantees, which learns exactly binary hash codes from imbalanced similarity data. The key idea is to attack the ill-posed gradient problem in optimizing deep networks with non-smooth binary activations by continuation method, in which we begin from learning an easier network with smoothed activation function and let it evolve during the training, until it eventually goes back to being the original, difficult to optimize, deep network with the sign activation function. Comprehensive empirical evidence shows that HashNet can generate exactly binary hash codes and yield state-of-the-art multimedia retrieval performance on standard benchmarks.",
                        "Citation Paper Authors": "Authors:Zhangjie Cao, Mingsheng Long, Jianmin Wang, Philip S. Yu"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "leverage a fully-connected network to learn the prototype code\nas target code for superior targeted attack, which is called THA\nin this paper. ProS-GAN ",
                    "Citation Text": "Xunguang Wang, Zheng Zhang, Baoyuan Wu, Fumin Shen, and Guangming Lu.\n2021. Prototype-Supervised Adversarial Network for Targeted Attack of Deep\nHashing. In IEEE/CVF Conference on Computer Vision and Pattern Recognition .\nIEEE, Nashville, TN, USA, 16357\u201316366.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.07553",
                        "Citation Paper Title": "Title:Prototype-supervised Adversarial Network for Targeted Attack of Deep Hashing",
                        "Citation Paper Abstract": "Abstract:Due to its powerful capability of representation learning and high-efficiency computation, deep hashing has made significant progress in large-scale image retrieval. However, deep hashing networks are vulnerable to adversarial examples, which is a practical secure problem but seldom studied in hashing-based retrieval field. In this paper, we propose a novel prototype-supervised adversarial network (ProS-GAN), which formulates a flexible generative architecture for efficient and effective targeted hashing attack. To the best of our knowledge, this is the first generation-based method to attack deep hashing networks. Generally, our proposed framework consists of three parts, i.e., a PrototypeNet, a generator, and a discriminator. Specifically, the designed PrototypeNet embeds the target label into the semantic representation and learns the prototype code as the category-level representative of the target label. Moreover, the semantic representation and the original image are jointly fed into the generator for a flexible targeted attack. Particularly, the prototype code is adopted to supervise the generator to construct the targeted adversarial example by minimizing the Hamming distance between the hash code of the adversarial example and the prototype code. Furthermore, the generator is against the discriminator to simultaneously encourage the adversarial examples visually realistic and the semantic representation informative. Extensive experiments verify that the proposed framework can efficiently produce adversarial examples with better targeted attack performance and transferability over state-of-the-art targeted attack methods of deep hashing. The related codes could be available at this https URL .",
                        "Citation Paper Authors": "Authors:Xunguang Wang, Zheng Zhang, Baoyuan Wu, Fumin Shen, Guangming Lu"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "generates more effective adversarial queries than HAG. As\nfor the targeted attack, it aims that the retrieved images of the\nadversarial example are semantically relevant to the given target\nlabel [ 1,42]. To reach the targeted attack, P2P and DHTA ",
                    "Citation Text": "Jiawang Bai, Bin Chen, Yiming Li, Dongxian Wu, Weiwei Guo, Shu-tao Xia,\nand En-hui Yang. 2020. Targeted Attack for Deep Hashing based Retrieval.\nInEuropean Conference on Computer Vision . Springer International Publishing,\nCham, 618\u2013634.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.07955",
                        "Citation Paper Title": "Title:Targeted Attack for Deep Hashing based Retrieval",
                        "Citation Paper Abstract": "Abstract:The deep hashing based retrieval method is widely adopted in large-scale image and video retrieval. However, there is little investigation on its security. In this paper, we propose a novel method, dubbed deep hashing targeted attack (DHTA), to study the targeted attack on such retrieval. Specifically, we first formulate the targeted attack as a point-to-set optimization, which minimizes the average distance between the hash code of an adversarial example and those of a set of objects with the target label. Then we design a novel component-voting scheme to obtain an anchor code as the representative of the set of hash codes of objects with the target label, whose optimality guarantee is also theoretically derived. To balance the performance and perceptibility, we propose to minimize the Hamming distance between the hash code of the adversarial example and the anchor code under the $\\ell^\\infty$ restriction on the perturbation. Extensive experiments verify that DHTA is effective in attacking both deep hashing based image retrieval and video retrieval.",
                        "Citation Paper Authors": "Authors:Jiawang Bai, Bin Chen, Yiming Li, Dongxian Wu, Weiwei Guo, Shu-tao Xia, En-hui Yang"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "crafts adversarial samples by maximizing the\nloss along the gradient direction with a large step. As the multi-\nstep variant of FGSM, I-FGSM ",
                    "Citation Text": "Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2017. Adversarial Ma-\nchine Learning at Scale. In International Conference on Learning Representations .\nhttp://OpenReview.net, Toulon, France, 1\u201317.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.01236",
                        "Citation Paper Title": "Title:Adversarial Machine Learning at Scale",
                        "Citation Paper Abstract": "Abstract:Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a \"label leaking\" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.",
                        "Citation Paper Authors": "Authors:Alexey Kurakin, Ian Goodfellow, Samy Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2207.10242v4": {
            "Paper Title": "Malware Triage Approach using a Task Memory based on Meta-Transfer\n  Learning Framework",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.11103v3": {
            "Paper Title": "To Fix or Not to Fix: A Critical Study of Crypto-misuses in the Wild",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "information regard-\ning crypto API misuses in Java and introduce CogniCrypt SAST ",
                    "Citation Text": "Kr\u00fcger, S., Sp\u00e4th, J., Ali, K., Bodden, E., Mezini, M.: Crysl: An\nextensible approach to validating the correct usage of cryptographic apis.\nIn: IEEE Transactions on Software Engineering. TSE, IEEE (2019)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.00564",
                        "Citation Paper Title": "Title:CrySL: Validating Correct Usage of Cryptographic APIs",
                        "Citation Paper Abstract": "Abstract:Various studies have empirically shown that the majority of Java and Android apps misuse cryptographic libraries, causing devastating breaches of data security. Therefore, it is crucial to detect such misuses early in the development process. The fact that insecure usages are not the exception but the norm precludes approaches based on property inference and anomaly detection.\nIn this paper, we present CrySL, a definition language that enables cryptography experts to specify the secure usage of the cryptographic libraries that they provide. CrySL combines the generic concepts of method-call sequences and data-flow constraints with domain-specific constraints related to cryptographic algorithms and their parameters. We have implemented a compiler that translates a CrySL ruleset into a context- and flow-sensitive demand-driven static analysis. The analysis automatically checks a given Java or Android app for violations of the CrySL-encoded rules.\nWe empirically evaluated our ruleset through analyzing 10,001 Android apps. Our results show that misuse of cryptographic APIs is still widespread, with 96% of apps containing at least one misuse. However, we observed fewer of the misuses that were reported in previous work.",
                        "Citation Paper Authors": "Authors:Stefan Kr\u00fcger, Johannes Sp\u00e4th, Karim Ali, Eric Bodden, Mira Mezini"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.12960v3": {
            "Paper Title": "An Automated Analyzer for Financial Security of Ethereum Smart Contracts",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": ". Using Tamarin gives us the flex-\nibility to add or modify rules in our models to verify hyperprop-\nerties ",
                    "Citation Text": "Jan Baumeister, Norine Coenen, Borzoo Bonakdarpour, Bernd Finkbeiner, and\nC\u00e9sar S\u00e1nchez. 2021. A Temporal Logic for Asynchronous Hyperproperties. In\nComputer Aided Verification - 33rd International Conference, CAV 2021, Virtual\nEvent, July 20-23, 2021, Proceedings, Part I (Lecture Notes in Computer Science) ,\nAlexandra Silva and K. Rustan M. Leino (Eds.), Vol. 12759. Springer, 694\u2013717.\nhttps://doi.org/10.1007/978-3-030-81685-8_33\n12",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.14025",
                        "Citation Paper Title": "Title:A Temporal Logic for Asynchronous Hyperproperties",
                        "Citation Paper Abstract": "Abstract:Hyperproperties are properties of computational systems that require more than one trace to evaluate, e.g., many information-flow security and concurrency requirements. Where a trace property defines a set of traces, a hyperproperty defines a set of sets of traces. The temporal logics HyperLTL and HyperCTL* have been proposed to express hyperproperties. However, their semantics are synchronous in the sense that all traces proceed at the same speed and are evaluated at the same position. This precludes the use of these logics to analyze systems whose traces can proceed at different speeds and allow that different traces take stuttering steps independently. To solve this problem in this paper, we propose an asynchronous variant of HyperLTL. On the negative side, we show that the model-checking problem for this variant is undecidable. On the positive side, we identify a decidable fragment which covers a rich set of formulas with practical applications. We also propose two model-checking algorithms that reduce our problem to the HyperLTL model-checking problem in the synchronous semantics.",
                        "Citation Paper Authors": "Authors:Jan Baumeister, Norine Coenen, Borzoo Bonakdarpour, Bernd Finkbeiner, Cesar Sanchez"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.00091v2": {
            "Paper Title": "Query Efficient Decision Based Sparse Attacks Against Black-Box Deep\n  Learning Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.06312v4": {
            "Paper Title": "A Coupled Design of Exploiting Record Similarity for Practical Vertical\n  Federated Learning",
            "Sentences": [
                {
                    "Sentence ID": 56,
                    "Sentence": "supports neural networks but it requires all the parties to hold labels. SplitNN ",
                    "Citation Text": "Praneeth Vepakomma, Otkrist Gupta, Tristan Swedish, and Ramesh Raskar. Split learning for\nhealth: Distributed deep learning without sharing raw patient data. arXiv , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.00564",
                        "Citation Paper Title": "Title:Split learning for health: Distributed deep learning without sharing raw patient data",
                        "Citation Paper Abstract": "Abstract:Can health entities collaboratively train deep learning models without sharing sensitive raw data? This paper proposes several configurations of a distributed deep learning method called SplitNN to facilitate such collaborations. SplitNN does not share raw data or model details with collaborating institutions. The proposed configurations of splitNN cater to practical settings of i) entities holding different modalities of patient data, ii) centralized and local health entities collaborating on multiple tasks and iii) learning without sharing labels. We compare performance and resource efficiency trade-offs of splitNN and other distributed deep learning methods like federated learning, large batch synchronous stochastic gradient descent and show highly encouraging results for splitNN.",
                        "Citation Paper Authors": "Authors:Praneeth Vepakomma, Otkrist Gupta, Tristan Swedish, Ramesh Raskar"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "or private set intersection (PSI) [ 7,36,60]. Nonetheless, these approaches incur performance loss\nof VFL and are also impractical since the common features of many real-world federated datasets\ncannot be exactly linked (e.g. GPS location). ",
                    "Citation Text": "Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Richard Nock, Giorgio Patrini, and et al.\nPrivate federated learning on vertically partitioned data via entity resolution and additively\nhomomorphic encryption. arXiv , 2017.\n11",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.10677",
                        "Citation Paper Title": "Title:Private federated learning on vertically partitioned data via entity resolution and additively homomorphic encryption",
                        "Citation Paper Abstract": "Abstract:Consider two data providers, each maintaining private records of different feature sets about common entities. They aim to learn a linear model jointly in a federated setting, namely, data is local and a shared model is trained from locally computed updates. In contrast with most work on distributed learning, in this scenario (i) data is split vertically, i.e. by features, (ii) only one data provider knows the target variable and (iii) entities are not linked across the data providers. Hence, to the challenge of private learning, we add the potentially negative consequences of mistakes in entity resolution. Our contribution is twofold. First, we describe a three-party end-to-end solution in two phases ---privacy-preserving entity resolution and federated logistic regression over messages encrypted with an additively homomorphic scheme---, secure against a honest-but-curious adversary. The system allows learning without either exposing data in the clear or sharing which entities the data providers have in common. Our implementation is as accurate as a naive non-private solution that brings all data in one place, and scales to problems with millions of entities with hundreds of features. Second, we provide what is to our knowledge the first formal analysis of the impact of entity resolution's mistakes on learning, with results on how optimal classifiers, empirical losses, margins and generalisation abilities are affected. Our results bring a clear and strong support for federated learning: under reasonable assumptions on the number and magnitude of entity resolution's mistakes, it can be extremely beneficial to carry out federated learning in the setting where each peer's data provides a significant uplift to the other.",
                        "Citation Paper Authors": "Authors:Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Richard Nock, Giorgio Patrini, Guillaume Smith, Brian Thorne"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.16612v2": {
            "Paper Title": "Decentralization illusion in Decentralized Finance: Evidence from\n  tokenized voting in MakerDAO polls",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.03764v2": {
            "Paper Title": "Towards Flexible Anonymous Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2301.01217v4": {
            "Paper Title": "Unlearnable Clusters: Towards Label-agnostic Unlearnable Examples",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": ". A unsupervised\nUE generation method was then proposed to craft UEs un-\nlearnable to unsupervised contrastive learning. However, a\nvery recent work by Ren et al. ",
                    "Citation Text": "Jie Ren, Han Xu, Yuxuan Wan, Xingjun Ma, Lichao Sun,\nand Jiliang Tang. Transferable unlearnable examples. arXiv\npreprint arXiv:2210.10114 , 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.10114",
                        "Citation Paper Title": "Title:Transferable Unlearnable Examples",
                        "Citation Paper Abstract": "Abstract:With more people publishing their personal data online, unauthorized data usage has become a serious concern. The unlearnable strategies have been introduced to prevent third parties from training on the data without permission. They add perturbations to the users' data before publishing, which aims to make the models trained on the perturbed published dataset invalidated. These perturbations have been generated for a specific training setting and a target dataset. However, their unlearnable effects significantly decrease when used in other training settings and datasets. To tackle this issue, we propose a novel unlearnable strategy based on Classwise Separability Discriminant (CSD), which aims to better transfer the unlearnable effects to other training settings and datasets by enhancing the linear separability. Extensive experiments demonstrate the transferability of the proposed unlearnable examples across training settings and datasets.",
                        "Citation Paper Authors": "Authors:Jie Ren, Han Xu, Yuxuan Wan, Xingjun Ma, Lichao Sun, Jiliang Tang"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": ". This was later on solved by a recent work\npublished at ICLR 2022 ",
                    "Citation Text": "Shaopeng Fu, Fengxiang He, Yang Liu, Li Shen, and Dacheng\nTao. Robust unlearnable examples: Protecting data against\nadversarial learning. In International Conference on Learning\nRepresentations , 2022. 1, 3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.14533",
                        "Citation Paper Title": "Title:Robust Unlearnable Examples: Protecting Data Against Adversarial Learning",
                        "Citation Paper Abstract": "Abstract:The tremendous amount of accessible data in cyberspace face the risk of being unauthorized used for training deep learning models. To address this concern, methods are proposed to make data unlearnable for deep learning models by adding a type of error-minimizing noise. However, such conferred unlearnability is found fragile to adversarial training. In this paper, we design new methods to generate robust unlearnable examples that are protected from adversarial training. We first find that the vanilla error-minimizing noise, which suppresses the informative knowledge of data via minimizing the corresponding training loss, could not effectively minimize the adversarial training loss. This explains the vulnerability of error-minimizing noise in adversarial training. Based on the observation, robust error-minimizing noise is then introduced to reduce the adversarial training loss. Experiments show that the unlearnability brought by robust error-minimizing noise can effectively protect data from adversarial training in various scenarios. The code is available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Shaopeng Fu, Fengxiang He, Yang Liu, Li Shen, Dacheng Tao"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.03652v3": {
            "Paper Title": "Real-time privacy preserving disease diagnosis using ECG signal",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.06324v3": {
            "Paper Title": "Pool-Party: Exploiting Browser Resource Pools as Side-Channels for Web\n  Tracking",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.05087v5": {
            "Paper Title": "Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion\n  Attacks in Deep RL",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.11765v2": {
            "Paper Title": "Byzantine-Robust Federated Learning with Optimal Statistical Rates and\n  Privacy Guarantees",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.04803v3": {
            "Paper Title": "Detecting Anomalous Cryptocurrency Transactions: an AML/CFT Application\n  of Machine Learning-based Forensics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.11459v3": {
            "Paper Title": "CELEST: Federated Learning for Globally Coordinated Threat Detection",
            "Sentences": [
                {
                    "Sentence ID": 22,
                    "Sentence": ".\nResilience to Poisoning. Other studies have proposed to use\na trust dataset ",
                    "Citation Text": "X. Cao, M. Fang, J. Liu, and N. Z. Gong, \u201cFLTrust: Byzantine-robust\nfederated learning via trust bootstrapping,\u201d in NDSS , 2021. [On-\nline]. Available: https://www.ndss-symposium.org/ndss-paper/\ufb02trust-\nbyzantine-robust-federated-learning-via-trust-bootstrapping/",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.13995",
                        "Citation Paper Title": "Title:FLTrust: Byzantine-robust Federated Learning via Trust Bootstrapping",
                        "Citation Paper Abstract": "Abstract:Byzantine-robust federated learning aims to enable a service provider to learn an accurate global model when a bounded number of clients are malicious. The key idea of existing Byzantine-robust federated learning methods is that the service provider performs statistical analysis among the clients' local model updates and removes suspicious ones, before aggregating them to update the global model. However, malicious clients can still corrupt the global models in these methods via sending carefully crafted local model updates to the service provider. The fundamental reason is that there is no root of trust in existing federated learning methods.\nIn this work, we bridge the gap via proposing FLTrust, a new federated learning method in which the service provider itself bootstraps trust. In particular, the service provider itself collects a clean small training dataset (called root dataset) for the learning task and the service provider maintains a model (called server model) based on it to bootstrap trust. In each iteration, the service provider first assigns a trust score to each local model update from the clients, where a local model update has a lower trust score if its direction deviates more from the direction of the server model update. Then, the service provider normalizes the magnitudes of the local model updates such that they lie in the same hyper-sphere as the server model update in the vector space. Our normalization limits the impact of malicious local model updates with large magnitudes. Finally, the service provider computes the average of the normalized local model updates weighted by their trust scores as a global model update, which is used to update the global model. Our extensive evaluations on six datasets from different domains show that our FLTrust is secure against both existing attacks and strong adaptive attacks.",
                        "Citation Paper Authors": "Authors:Xiaoyu Cao, Minghong Fang, Jia Liu, Neil Zhenqiang Gong"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "proposed federated learning to enable effective cyber-\nrisk intelligence sharing for mobile devices. Several studies\nhave looked at poisoning attacks against federated learning ",
                    "Citation Text": "E. Bagdasaryan, A. Veit, Y . Hua, D. Estrin, and V . Shmatikov, \u201cHow\nto backdoor federated learning,\u201d in AISTATS , 2020, pp. 2938\u20132948.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.00459",
                        "Citation Paper Title": "Title:How To Backdoor Federated Learning",
                        "Citation Paper Abstract": "Abstract:Federated learning enables thousands of participants to construct a deep learning model without sharing their private training data with each other. For example, multiple smartphones can jointly train a next-word predictor for keyboards without revealing what individual users type. We demonstrate that any participant in federated learning can introduce hidden backdoor functionality into the joint global model, e.g., to ensure that an image classifier assigns an attacker-chosen label to images with certain features, or that a word predictor completes certain sentences with an attacker-chosen word.\nWe design and evaluate a new model-poisoning methodology based on model replacement. An attacker selected in a single round of federated learning can cause the global model to immediately reach 100% accuracy on the backdoor task. We evaluate the attack under different assumptions for the standard federated-learning tasks and show that it greatly outperforms data poisoning. Our generic constrain-and-scale technique also evades anomaly detection-based defenses by incorporating the evasion into the attacker's loss function during training.",
                        "Citation Paper Authors": "Authors:Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, Vitaly Shmatikov"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.01452v2": {
            "Paper Title": "MPCFormer: fast, performant and private Transformer inference with MPC",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.01713v3": {
            "Paper Title": "SoK: Fully Homomorphic Encryption Accelerators",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": ". However, few FHE\naccelerators consider leveraging existing FHE compilers,\nsuch as EV A ",
                    "Citation Text": "R. Dathathri, B. Kostova, O. Saarikivi, W. Dai, K. Laine, and\nM. Musuvathi, \u201cEV A: an encrypted vector arithmetic language and\ncompiler for ef\ufb01cient homomorphic computation,\u201d in Proceedings of\nthe 41st ACM SIGPLAN International Conference on Programming\nLanguage Design and Implementation, PLDI 2020, London, UK,\nJune 15-20, 2020 , A. F. Donaldson and E. Torlak, Eds. ACM,\n2020, pp. 546\u2013561. [Online]. Available: https://doi.org/10.1145/\n3385412.3386023",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.11951",
                        "Citation Paper Title": "Title:EVA: An Encrypted Vector Arithmetic Language and Compiler for Efficient Homomorphic Computation",
                        "Citation Paper Abstract": "Abstract:Fully-Homomorphic Encryption (FHE) offers powerful capabilities by enabling secure offloading of both storage and computation, and recent innovations in schemes and implementations have made it all the more attractive. At the same time, FHE is notoriously hard to use with a very constrained programming model, a very unusual performance profile, and many cryptographic constraints. Existing compilers for FHE either target simpler but less efficient FHE schemes or only support specific domains where they can rely on expert-provided high-level runtimes to hide complications.\nThis paper presents a new FHE language called Encrypted Vector Arithmetic (EVA), which includes an optimizing compiler that generates correct and secure FHE programs, while hiding all the complexities of the target FHE scheme. Bolstered by our optimizing compiler, programmers can develop efficient general-purpose FHE applications directly in EVA. For example, we have developed image processing applications using EVA, with a very few lines of code.\nEVA is designed to also work as an intermediate representation that can be a target for compiling higher-level domain-specific languages. To demonstrate this, we have re-targeted CHET, an existing domain-specific compiler for neural network inference, onto EVA. Due to the novel optimizations in EVA, its programs are on average 5.3x faster than those generated by CHET. We believe that EVA would enable a wider adoption of FHE by making it easier to develop FHE applications and domain-specific FHE compilers.",
                        "Citation Paper Authors": "Authors:Roshan Dathathri, Blagovesta Kostova, Olli Saarikivi, Wei Dai, Kim Laine, Madanlal Musuvathi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.05314v2": {
            "Paper Title": "Leveraging Architectural Approaches in Web3 Applications -- A DAO\n  Perspective Focused",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.10272v2": {
            "Paper Title": "Training set cleansing of backdoor poisoning by self-supervised\n  representation learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.03076v2": {
            "Paper Title": "Dynamic Efficient Adversarial Training Guided by Gradient Magnitude",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": ", while reducing the number of iterations would be the most straightforward strategy\nto accelerate the training procedure. In Dynamic Adversarial Training (DAT) ",
                    "Citation Text": "Y. Wang, X. Ma, J. Bailey, J. Yi, B. Zhou, and Q. Gu. On the convergence and\nrobustness of adversarial training. In ICML, Long Beach, CA, USA, June 9-15, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.08304",
                        "Citation Paper Title": "Title:On the Convergence and Robustness of Adversarial Training",
                        "Citation Paper Abstract": "Abstract:Improving the robustness of deep neural networks (DNNs) to adversarial examples is an important yet challenging problem for secure deep learning. Across existing defense techniques, adversarial training with Projected Gradient Decent (PGD) is amongst the most effective. Adversarial training solves a min-max optimization problem, with the \\textit{inner maximization} generating adversarial examples by maximizing the classification loss, and the \\textit{outer minimization} finding model parameters by minimizing the loss on adversarial examples generated from the inner maximization. A criterion that measures how well the inner maximization is solved is therefore crucial for adversarial training. In this paper, we propose such a criterion, namely First-Order Stationary Condition for constrained optimization (FOSC), to quantitatively evaluate the convergence quality of adversarial examples found in the inner maximization. With FOSC, we find that to ensure better robustness, it is essential to use adversarial examples with better convergence quality at the \\textit{later stages} of training. Yet at the early stages, high convergence quality adversarial examples are not necessary and may even lead to poor robustness. Based on these observations, we propose a \\textit{dynamic} training strategy to gradually increase the convergence quality of the generated adversarial examples, which significantly improves the robustness of adversarial training. Our theoretical and empirical results show the effectiveness of the proposed method.",
                        "Citation Paper Authors": "Authors:Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, Quanquan Gu"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "show that these advanced surrogate losses can also bene\ufb01t from\ne\ufb03cient AT methods.\nLipschitz Condition The Lipschitz constant for f(\u00b7;\u03b8)gives an upper bound on how fast\nthe loss value changes when small perturbations are added to the network\u2019s input ",
                    "Citation Text": "C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Goodfellow, and\nR. Fergus. Intriguing properties of neural networks. In ICLR, Ban\ufb00, AB, Canada, April\n14-16, 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1312.6199",
                        "Citation Paper Title": "Title:Intriguing properties of neural networks",
                        "Citation Paper Abstract": "Abstract:Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.\nFirst, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks.\nSecond, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
                        "Citation Paper Authors": "Authors:Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": ").Let/bardbl\u00b7/bardblpbe the dual norm of /bardbl\u00b7/bardblq.f(\u00b7;\u03b8)satis\ufb01es the\nLipschitzian smoothness condition\n/bardbl\u2207xf(x;\u03b8)\u2212\u2207xf(x+\u03b4;\u03b8)/bardblp6lxx/bardbl\u03b4/bardblq,\u2200\u03b4\u2208B/epsilon1, (3)\nwherelxxis a positive constant.\n3Assumption 2 (Deng et al. ",
                    "Citation Text": "Z. Deng, H. He, J. Huang, and W. J. Su. Towards understanding the dynamics of the\n\ufb01rst-order adversaries. In ICML, Virtual Event, July 13-18, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.10650",
                        "Citation Paper Title": "Title:Towards Understanding the Dynamics of the First-Order Adversaries",
                        "Citation Paper Abstract": "Abstract:An acknowledged weakness of neural networks is their vulnerability to adversarial perturbations to the inputs. To improve the robustness of these models, one of the most popular defense mechanisms is to alternatively maximize the loss over the constrained perturbations (or called adversaries) on the inputs using projected gradient ascent and minimize over weights. In this paper, we analyze the dynamics of the maximization step towards understanding the experimentally observed effectiveness of this defense mechanism. Specifically, we investigate the non-concave landscape of the adversaries for a two-layer neural network with a quadratic loss. Our main result proves that projected gradient ascent finds a local maximum of this non-concave problem in a polynomial number of iterations with high probability. To our knowledge, this is the first work that provides a convergence analysis of the first-order adversaries. Moreover, our analysis demonstrates that, in the initial phase of adversarial training, the scale of the inputs matters in the sense that a smaller input scale leads to faster convergence of adversarial training and a \"more regular\" landscape. Finally, we show that these theoretical findings are in excellent agreement with a series of experiments.",
                        "Citation Paper Authors": "Authors:Zhun Deng, Hangfeng He, Jiaoyang Huang, Weijie J. Su"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "found that one step PGD attack, also known as First Gradient\nSign Method (FGSM) ",
                    "Citation Text": "I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial\nexamples. In ICLR, San Diego, CA, USA, May 7-9, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.6572",
                        "Citation Paper Title": "Title:Explaining and Harnessing Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": ", which is extremely computationally\nexpensive in practice. Therefore improving its e\ufb03ciency has become an active research\ntopic ",
                    "Citation Text": "T. Bai, J. Luo, J. Zhao, B. Wen, and Q. Wang. Recent advances in adversarial training\nfor adversarial robustness. In IJCAI, Virtual Event / Montreal, Canada, August 19-27,\n2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.01356",
                        "Citation Paper Title": "Title:Recent Advances in Adversarial Training for Adversarial Robustness",
                        "Citation Paper Abstract": "Abstract:Adversarial training is one of the most effective approaches defending against adversarial examples for deep learning models. Unlike other defense strategies, adversarial training aims to promote the robustness of models intrinsically. During the last few years, adversarial training has been studied and discussed from various aspects. A variety of improvements and developments of adversarial training are proposed, which were, however, neglected in existing surveys. For the first time in this survey, we systematically review the recent progress on adversarial training for adversarial robustness with a novel taxonomy. Then we discuss the generalization problems in adversarial training from three perspectives. Finally, we highlight the challenges which are not fully tackled and present potential future directions.",
                        "Citation Paper Authors": "Authors:Tao Bai, Jinqi Luo, Jun Zhao, Bihan Wen, Qian Wang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.14769v3": {
            "Paper Title": "Navigation as Attackers Wish? Towards Building Byzantine-Robust Embodied\n  Agents under Federated Learning",
            "Sentences": [
                {
                    "Sentence ID": 21,
                    "Sentence": "proposes a manually designed prompt pattern\nfor NLP tasks, which is a language instruction prepended\nto the input text. ",
                    "Citation Text": "Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yu-\njie Qian, Zhilin Yang, and Jie Tang. Gpt understands, too.\narXiv:2103.10385 , 2021. 8",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.10385",
                        "Citation Paper Title": "Title:GPT Understands, Too",
                        "Citation Paper Abstract": "Abstract:Prompting a pretrained language model with natural language patterns has been proved effective for natural language understanding (NLU). However, our preliminary study reveals that manual discrete prompts often lead to unstable performance -- e.g., changing a single word in the prompt might result in substantial performance drop. We propose a novel method P-Tuning that employs trainable continuous prompt embeddings in concatenation with discrete prompts. Empirically, P-Tuning not only stabilizes training by minimizing the gap between various discrete prompts, but also improves performance by a sizeable margin on a wide range of NLU tasks including LAMA and SuperGLUE. P-Tuning is generally effective for both frozen and tuned language models, under both the fully-supervised and few-shot settings.",
                        "Citation Paper Authors": "Authors:Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, Jie Tang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2207.03933v3": {
            "Paper Title": "A law of adversarial risk, interpolation, and label noise",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.02022v5": {
            "Paper Title": "VESPo: Verified Evaluation of Secret Polynomials",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.09050v2": {
            "Paper Title": "Determining Distributions of Security Means for WSNs based on the Model\n  of a Neighbourhood Watch",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.09088v5": {
            "Paper Title": "XG-BoT: An Explainable Deep Graph Neural Network for Botnet Detection\n  and Forensics",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": "to assign an importance score\nbased on an approximation of the integral of the gradient or Clustering-based approaches ",
                    "Citation Text": "G. K. Kulatilleke, M. Portmann, S. S. Chandra, Scgc: Self-supervised contrastive graph clustering, arXiv preprint arXiv:2204.12656 (2022).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.12656",
                        "Citation Paper Title": "Title:SCGC : Self-Supervised Contrastive Graph Clustering",
                        "Citation Paper Abstract": "Abstract:Graph clustering discovers groups or communities within networks. Deep learning methods such as autoencoders (AE) extract effective clustering and downstream representations but cannot incorporate rich structural information. While Graph Neural Networks (GNN) have shown great success in encoding graph structure, typical GNNs based on convolution or attention variants suffer from over-smoothing, noise, heterophily, are computationally expensive and typically require the complete graph being present. Instead, we propose Self-Supervised Contrastive Graph Clustering (SCGC), which imposes graph-structure via contrastive loss signals to learn discriminative node representations and iteratively refined soft cluster labels. We also propose SCGC*, with a more effective, novel, Influence Augmented Contrastive (IAC) loss to fuse richer structural information, and half the original model parameters. SCGC(*) is faster with simple linear units, completely eliminate convolutions and attention of traditional GNNs, yet efficiently incorporates structure. It is impervious to layer depth and robust to over-smoothing, incorrect edges and heterophily. It is scalable by batching, a limitation in many prior GNN models, and trivially parallelizable. We obtain significant improvements over state-of-the-art on a wide range of benchmark graph datasets, including images, sensor data, text, and citation networks efficiently. Specifically, 20% on ARI and 18% on NMI for DBLP; overall 55% reduction in training time and overall, 81% reduction on inference time. Our code is available at : this https URL",
                        "Citation Paper Authors": "Authors:Gayan K. Kulatilleke, Marius Portmann, Shekhar S. Chandra"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": "P2P \u0000 \u0000 97.85% 0 :02%\n87. Explainability for Automatic Network Forensics\nWhile there is a huge interest in the explainability of deep learning model predictions ",
                    "Citation Text": "T. Kasanishi, X. Wang, T. Yamasaki, Edge-level explanations for graph neural networks by extending explainability methods for convolutional\nneural networks, in: 2021 IEEE International Symposium on Multimedia (ISM), IEEE, 2021, pp. 249\u2013252.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.00722",
                        "Citation Paper Title": "Title:Edge-Level Explanations for Graph Neural Networks by Extending Explainability Methods for Convolutional Neural Networks",
                        "Citation Paper Abstract": "Abstract:Graph Neural Networks (GNNs) are deep learning models that take graph data as inputs, and they are applied to various tasks such as traffic prediction and molecular property prediction. However, owing to the complexity of the GNNs, it has been difficult to analyze which parts of inputs affect the GNN model's outputs. In this study, we extend explainability methods for Convolutional Neural Networks (CNNs), such as Local Interpretable Model-Agnostic Explanations (LIME), Gradient-Based Saliency Maps, and Gradient-Weighted Class Activation Mapping (Grad-CAM) to GNNs, and predict which edges in the input graphs are important for GNN decisions. The experimental results indicate that the LIME-based approach is the most efficient explainability method for multiple tasks in the real-world situation, outperforming even the state-of-the-art method in GNN explainability.",
                        "Citation Paper Authors": "Authors:Tetsu Kasanishi, Xueting Wang, Toshihiko Yamasaki"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": ": These are the derivatives of the class probability Pito the input image X ",
                    "Citation Text": "K. Simonyan, A. Vedaldi, A. Zisserman, Deep inside convolutional networks: Visualising image classi\ufb01cation models and saliency maps, in:\nProceedings of the International Conference on Learning Representations, 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1312.6034",
                        "Citation Paper Title": "Title:Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps",
                        "Citation Paper Abstract": "Abstract:This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].",
                        "Citation Paper Authors": "Authors:Karen Simonyan, Andrea Vedaldi, Andrew Zisserman"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": ". In this section, we discuss the XG-BoT explainability methods, with a\nspeci\ufb01c focus on automatic network forensics via subgraph visualization.\nGNNExplainer ",
                    "Citation Text": "R. Ying, D. Bourgeois, J. You, M. Zitnik, J. Leskovec, Gnnexplainer: Generating explanations for graph neural networks, Advances in neural\ninformation processing systems 32 (2019) 9240.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.03894",
                        "Citation Paper Title": "Title:GNNExplainer: Generating Explanations for Graph Neural Networks",
                        "Citation Paper Abstract": "Abstract:Graph Neural Networks (GNNs) are a powerful tool for machine learning on graphs.GNNs combine node feature information with the graph structure by recursively passing neural messages along edges of the input graph. However, incorporating both graph structure and feature information leads to complex models, and explaining predictions made by GNNs remains unsolved. Here we propose GNNExplainer, the first general, model-agnostic approach for providing interpretable explanations for predictions of any GNN-based model on any graph-based machine learning task. Given an instance, GNNExplainer identifies a compact subgraph structure and a small subset of node features that have a crucial role in GNN's prediction. Further, GNNExplainer can generate consistent and concise explanations for an entire class of instances. We formulate GNNExplainer as an optimization task that maximizes the mutual information between a GNN's prediction and distribution of possible subgraph structures. Experiments on synthetic and real-world graphs show that our approach can identify important graph structures as well as node features, and outperforms baselines by 17.1% on average. GNNExplainer provides a variety of benefits, from the ability to visualize semantically relevant structures to interpretability, to giving insights into errors of faulty GNNs.",
                        "Citation Paper Authors": "Authors:Rex Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, Jure Leskovec"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ", such as [15, 16, 17], consider network \ufb02ows independently, without taking into account their\ninterconnected relationship, which is important in botnet detection. On the other hand, other GNN approaches, such\nas those presented in ",
                    "Citation Text": "J. Zhou, Z. Xu, A. M. Rush, M. Yu, Automating botnet detection with graph neural networks, AutoML for Networking and Systems\nWorkshop of MLSys 2020 Conference (2020).\n10",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.06344",
                        "Citation Paper Title": "Title:Automating Botnet Detection with Graph Neural Networks",
                        "Citation Paper Abstract": "Abstract:Botnets are now a major source for many network attacks, such as DDoS attacks and spam. However, most traditional detection methods heavily rely on heuristically designed multi-stage detection criteria. In this paper, we consider the neural network design challenges of using modern deep learning techniques to learn policies for botnet detection automatically. To generate training data, we synthesize botnet connections with different underlying communication patterns overlaid on large-scale real networks as datasets. To capture the important hierarchical structure of centralized botnets and the fast-mixing structure for decentralized botnets, we tailor graph neural networks (GNN) to detect the properties of these structures. Experimental results show that GNNs are better able to capture botnet structure than previous non-learning methods when trained with appropriate data, and that deeper GNNs are crucial for learning difficult botnet topologies. We believe our data and studies can be useful for both the network security and graph learning communities.",
                        "Citation Paper Authors": "Authors:Jiawei Zhou, Zhiying Xu, Alexander M. Rush, Minlan Yu"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "Ensemble KNN Network \ufb02ow features Custom dataset No\nAbou-Rjeili et al. ",
                    "Citation Text": "A. Abou Daya, M. A. Salahuddin, N. Limam, R. Boutaba, A graph-based machine learning approach for bot detection, in: 2019 IFIP /IEEE\nSymposium on Integrated Network and Service Management (IM), IEEE, 2019, pp. 144\u2013152.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.08538",
                        "Citation Paper Title": "Title:A Graph-Based Machine Learning Approach for Bot Detection",
                        "Citation Paper Abstract": "Abstract:Bot detection using machine learning (ML), with network flow-level features, has been extensively studied in the literature. However, existing flow-based approaches typically incur a high computational overhead and do not completely capture the network communication patterns, which can expose additional aspects of malicious hosts. Recently, bot detection systems which leverage communication graph analysis using ML have gained attention to overcome these limitations. A graph-based approach is rather intuitive, as graphs are true representations of network communications. In this paper, we propose a two-phased, graph-based bot detection system which leverages both unsupervised and supervised ML. The first phase prunes presumable benign hosts, while the second phase achieves bot detection with high precision. Our system detects multiple types of bots and is robust to zero-day attacks. It also accommodates different network topologies and is suitable for large-scale data.",
                        "Citation Paper Authors": "Authors:Abbas Abou Daya, Mohammad A. Salahuddin, Noura Limam, Raouf Boutaba"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2207.02303v2": {
            "Paper Title": "A Dataset on Malicious Paper Bidding in Peer Review",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.11782v4": {
            "Paper Title": "Fine-grained Poisoning Attack to Local Differential Privacy Protocols\n  for Mean and Variance Estimation",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": ": The IPAand\nOPA could be adapted to attack the pure LDP protocols for\nfrequency, such as kRR ",
                    "Citation Text": "John C Duchi, Michael I Jordan, and Martin J Wainwright. Local\nprivacy and statistical minimax rates. In IEEE FOCS , 2013.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1302.3203",
                        "Citation Paper Title": "Title:Local Privacy, Data Processing Inequalities, and Statistical Minimax Rates",
                        "Citation Paper Abstract": "Abstract:Working under a model of privacy in which data remains private even from the statistician, we study the tradeoff between privacy guarantees and the utility of the resulting statistical estimators. We prove bounds on information-theoretic quantities, including mutual information and Kullback-Leibler divergence, that depend on the privacy guarantees. When combined with standard minimax techniques, including the Le Cam, Fano, and Assouad methods, these inequalities allow for a precise characterization of statistical rates under local privacy constraints. We provide a treatment of several canonical families of problems: mean estimation, parameter estimation in fixed-design regression, multinomial probability estimation, and nonparametric density estimation. For all of these families, we provide lower and upper bounds that match up to constant factors, and exhibit new (optimal) privacy-preserving mechanisms and computationally efficient estimators that achieve the bounds.",
                        "Citation Paper Authors": "Authors:John C. Duchi, Michael I. Jordan, Martin J. Wainwright"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": ", differentially private data augmentation was studied\nfor attack mitigation. ",
                    "Citation Text": "Yuzhe Ma, Xiaojin Zhu, and Justin Hsu. Data poisoning against\ndifferentially-private learners: Attacks and defenses. arXiv preprint\narXiv:1903.09860 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1903.09860",
                        "Citation Paper Title": "Title:Data Poisoning against Differentially-Private Learners: Attacks and Defenses",
                        "Citation Paper Abstract": "Abstract:Data poisoning attacks aim to manipulate the model produced by a learning algorithm by adversarially modifying the training set. We consider differential privacy as a defensive measure against this type of attack. We show that such learners are resistant to data poisoning attacks when the adversary is only able to poison a small number of items. However, this protection degrades as the adversary poisons more data. To illustrate, we design attack algorithms targeting objective and output perturbation learners, two standard approaches to differentially-private machine learning. Experiments show that our methods are effective when the attacker is allowed to poison sufficiently many training items.",
                        "Citation Paper Authors": "Authors:Yuzhe Ma, Xiaojin Zhu, Justin Hsu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.07723v2": {
            "Paper Title": "Privacy-Preserving and Lossless Distributed Estimation of\n  High-Dimensional Generalized Additive Mixed Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.04871v2": {
            "Paper Title": "Certified Training: Small Boxes are All You Need",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.11202v3": {
            "Paper Title": "Indiscriminate Poisoning Attacks on Unsupervised Contrastive Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.09117v2": {
            "Paper Title": "Part-Based Models Improve Adversarial Robustness",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.00169v2": {
            "Paper Title": "Trojan Source: Invisible Vulnerabilities",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": ", which sets rules for handling Bidi\nand invisible characters to prevent some look-alike domains.\n5.2 Adversarial NLP\nBidi control characters and homoglyphs have both been used\nto create adversarial examples in the machine learning NLP\nsetting ",
                    "Citation Text": "N. Boucher, I. Shumailov, R. Anderson, and N. Papernot, \u201cBad\nCharacters: Imperceptible NLP Attacks,\u201d in 43rd IEEE Sympo-\nsium on Security and Privacy . IEEE, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.09898",
                        "Citation Paper Title": "Title:Bad Characters: Imperceptible NLP Attacks",
                        "Citation Paper Abstract": "Abstract:Several years of research have shown that machine-learning systems are vulnerable to adversarial examples, both in theory and in practice. Until now, such attacks have primarily targeted visual models, exploiting the gap between human and machine perception. Although text-based models have also been attacked with adversarial examples, such attacks struggled to preserve semantic meaning and indistinguishability. In this paper, we explore a large class of adversarial examples that can be used to attack text-based models in a black-box setting without making any human-perceptible visual modification to inputs. We use encoding-specific perturbations that are imperceptible to the human eye to manipulate the outputs of a wide range of Natural Language Processing (NLP) systems from neural machine-translation pipelines to web search engines. We find that with a single imperceptible encoding injection -- representing one invisible character, homoglyph, reordering, or deletion -- an attacker can significantly reduce the performance of vulnerable models, and with three injections most models can be functionally broken. Our attacks work against currently-deployed commercial systems, including those produced by Microsoft and Google, in addition to open source models published by Facebook, IBM, and HuggingFace. This novel series of attacks presents a significant threat to many language processing systems: an attacker can affect systems in a targeted manner without any assumptions about the underlying model. We conclude that text-based NLP systems require careful input sanitization, just like conventional applications, and that given such systems are now being deployed rapidly at scale, the urgent attention of architects and operators is required.",
                        "Citation Paper Authors": "Authors:Nicholas Boucher, Ilia Shumailov, Ross Anderson, Nicolas Papernot"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.00081v2": {
            "Paper Title": "Sampling Attacks on Meta Reinforcement Learning: A Minimax Formulation\n  and Complexity Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.01888v4": {
            "Paper Title": "Reward Poisoning Attacks on Offline Multi-Agent Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.07138v2": {
            "Paper Title": "Self-Healing Secure Blockchain Framework in Microgrids",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.09330v3": {
            "Paper Title": "ACon$^2$: Adaptive Conformal Consensus for Provable Blockchain Oracles",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": "; similarly, PAC\nprediction sets, also called training-conditional inductive con-\nformal prediction, achieves a desired PAC guarantee under a\nsmoothness assumption on distributions ",
                    "Citation Text": "Sangdon Park, Edgar Dobriban, Insup Lee, and Osbert Bastani. PAC\nprediction sets under covariate shift. In International Conference on\nLearning Representations , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.09848",
                        "Citation Paper Title": "Title:PAC Prediction Sets Under Covariate Shift",
                        "Citation Paper Abstract": "Abstract:An important challenge facing modern machine learning is how to rigorously quantify the uncertainty of model predictions. Conveying uncertainty is especially important when there are changes to the underlying data distribution that might invalidate the predictive model. Yet, most existing uncertainty quantification algorithms break down in the presence of such shifts. We propose a novel approach that addresses this challenge by constructing \\emph{probably approximately correct (PAC)} prediction sets in the presence of covariate shift. Our approach focuses on the setting where there is a covariate shift from the source distribution (where we have labeled training examples) to the target distribution (for which we want to quantify uncertainty). Our algorithm assumes given importance weights that encode how the probabilities of the training examples change under the covariate shift. In practice, importance weights typically need to be estimated; thus, we extend our algorithm to the setting where we are given confidence intervals for the importance weights. We demonstrate the effectiveness of our approach on covariate shifts based on DomainNet and ImageNet. Our algorithm satisfies the PAC constraint, and gives prediction sets with the smallest average normalized size among approaches that always satisfy the PAC constraint.",
                        "Citation Paper Authors": "Authors:Sangdon Park, Edgar Dobriban, Insup Lee, Osbert Bastani"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": ", where the label distribution\np(y)can be shifted in prediction, while the conditional covari-\nate distribution p(x|y)is not changing. In particular, weighted\nconformal prediction can achieve a desired marginal cover-\nage rate given true importance weights ",
                    "Citation Text": "Ryan J Tibshirani, Rina Foygel Barber, Emmanuel Candes, and Aaditya\nRamdas. Conformal prediction under covariate shift. Advances in\nNeural Information Processing Systems , 32:2530\u20132540, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.06019",
                        "Citation Paper Title": "Title:Conformal Prediction Under Covariate Shift",
                        "Citation Paper Abstract": "Abstract:We extend conformal prediction methodology beyond the case of exchangeable data. In particular, we show that a weighted version of conformal prediction can be used to compute distribution-free prediction intervals for problems in which the test and training covariate distributions differ, but the likelihood ratio between these two distributions is known---or, in practice, can be estimated accurately with access to a large set of unlabeled data (test covariate points). Our weighted extension of conformal prediction also applies more generally, to settings in which the data satisfies a certain weighted notion of exchangeability. We discuss other potential applications of our new conformal methodology, including latent variable and missing data problems.",
                        "Citation Paper Authors": "Authors:Ryan J. Tibshirani, Rina Foygel Barber, Emmanuel J. Candes, Aaditya Ramdas"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": ", where the covariate distribution p(x)can\nbe shifted in prediction, while the labeling distribution p(y|x)\nis unchanged, or label shift ",
                    "Citation Text": "Zachary Lipton, Yu-Xiang Wang, and Alexander Smola. Detecting and\ncorrecting for label shift with black box predictors. In International\nconference on machine learning , pages 3122\u20133130. PMLR, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.03916",
                        "Citation Paper Title": "Title:Detecting and Correcting for Label Shift with Black Box Predictors",
                        "Citation Paper Abstract": "Abstract:Faced with distribution shift between training and test set, we wish to detect and quantify the shift, and to correct our classifiers without test set labels. Motivated by medical diagnosis, where diseases (targets) cause symptoms (observations), we focus on label shift, where the label marginal $p(y)$ changes but the conditional $p(x| y)$ does not. We propose Black Box Shift Estimation (BBSE) to estimate the test distribution $p(y)$. BBSE exploits arbitrary black box predictors to reduce dimensionality prior to shift correction. While better predictors give tighter estimates, BBSE works even when predictors are biased, inaccurate, or uncalibrated, so long as their confusion matrices are invertible. We prove BBSE's consistency, bound its error, and introduce a statistical test that uses BBSE to detect shift. We also leverage BBSE to correct classifiers. Experiments demonstrate accurate estimates and improved prediction, even on high-dimensional datasets of natural images.",
                        "Citation Paper Authors": "Authors:Zachary C. Lipton, Yu-Xiang Wang, Alex Smola"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "construct a weighted voting matrix on data\nchoices, from which they extract common vote patterns via\nsingular value decomposition and incentivizes or penalizes in-\nlier or outlier voters, respectively. Augur ",
                    "Citation Text": "Jack Peterson, Joseph Krug, Micah Zoltu, Austin K Williams, and\nStephanie Alexander. Augur: a decentralized oracle and prediction\nmarket platform. arXiv preprint arXiv:1501.01042 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1501.01042",
                        "Citation Paper Title": "Title:Augur: a decentralized oracle and prediction market platform",
                        "Citation Paper Abstract": "Abstract:Augur is a trustless, decentralized oracle and platform for prediction markets. The outcomes of Augur's prediction markets are chosen by users that hold Augur's native Reputation token, who stake their tokens on the actual observed outcome and, in return, receive settlement fees from the markets. Augur's incentive structure is designed to ensure that honest, accurate reporting of outcomes is always the most profitable option for Reputation token holders. Token holders can post progressively-larger Reputation bonds to dispute proposed market outcomes. If the size of these bonds reaches a certain threshold, Reputation splits into multiple versions, one for each possible outcome of the disputed market; token holders must then exchange their Reputation tokens for one of these versions. Versions of Reputation which do not correspond to the real-world outcome will become worthless, as no one will participate in prediction markets unless they are confident that the markets will resolve correctly. Therefore, token holders will select the only version of Reputation which they know will continue to have value: the version that corresponds to reality.",
                        "Citation Paper Authors": "Authors:Jack Peterson, Joseph Krug, Micah Zoltu, Austin K. Williams, Stephanie Alexander"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.09657v4": {
            "Paper Title": "Survey of Machine Learning Based Intrusion Detection Methods for\n  Internet of Medical Things",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.06623v3": {
            "Paper Title": "ROMEO: Exploring Juliet through the Lens of Assembly Language",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": ". Conversely, our evaluation uses\na much larger subset, missing only Windows-speci\ufb01c weaknesses. Th e Juliet\ntest suite is further used as an alternative training and evaluation d ataset in\ntheReVeal study ",
                    "Citation Text": "Chakraborty, S., Krishna, R., Ding, Y., Ray, B., 2021. Deep learnin g based\nvulnerability detection: Are we there yet. IEEE Transactions on So ftware\nEngineering doi: 10.1109/tse.2021.3087402 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.07235",
                        "Citation Paper Title": "Title:Deep Learning based Vulnerability Detection: Are We There Yet?",
                        "Citation Paper Abstract": "Abstract:Automated detection of software vulnerabilities is a fundamental problem in software security. Existing program analysis techniques either suffer from high false positives or false negatives. Recent progress in Deep Learning (DL) has resulted in a surge of interest in applying DL for automated vulnerability detection. Several recent studies have demonstrated promising results achieving an accuracy of up to 95% at detecting vulnerabilities. In this paper, we ask, \"how well do the state-of-the-art DL-based techniques perform in a real-world vulnerability prediction scenario?\". To our surprise, we find that their performance drops by more than 50%. A systematic investigation of what causes such precipitous performance drop reveals that existing DL-based vulnerability prediction approaches suffer from challenges with the training data (e.g., data duplication, unrealistic distribution of vulnerable classes, etc.) and with the model choices (e.g., simple token-based models). As a result, these approaches often do not learn features related to the actual cause of the vulnerabilities. Instead, they learn unrelated artifacts from the dataset (e.g., specific variable/function names, etc.). Leveraging these empirical findings, we demonstrate how a more principled approach to data collection and model design, based on realistic settings of vulnerability prediction, can lead to better solutions. The resulting tools perform significantly better than the studied baseline: up to 33.57% boost in precision and 128.38% boost in recall compared to the best performing model in the literature. Overall, this paper elucidates existing DL-based vulnerability prediction systems' potential issues and draws a roadmap for future DL-based vulnerability prediction research. In that spirit, we make available all the artifacts supporting our results: this https URL.",
                        "Citation Paper Authors": "Authors:Saikat Chakraborty, Rahul Krishna, Yangruibo Ding, Baishakhi Ray"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ".\nBecauseassemblylanguageislesscomplexandvariablethanthelangu agesin\nthe initial training set ofCodeBERT(which does not include assemblyd irectly),\nthe included tokenization and encoding is not optimal. Hence, we repla ce them\nwith a byte-pair encoding ",
                    "Citation Text": "Devlin, J., Chang, M.W., Lee, K., Toutanova, K., . BERT: Pre-trainin g\nof deep bidirectional transformers for language understanding, in: arXiv\npreprint arXiv:1810.04805.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "and\nrely on a \ufb01xed encoding of opcodes and a histogram-like encoding of o perands\nto represent assembly language instructions. The autoencoder is evaluated on\nthe dataset introduced alongside VulDeePecker ",
                    "Citation Text": "Li, Z., Zou, D., Xu, S., Ou, X., Jin, H., Wang, S., Deng, Z., Zhong,Y., 2 018.\nVulDeePecker: Adeeplearning-basedsystemforvulnerabilitydete ction, in:\nNetwork and Distributed System Security Symposium (NDSS), Inte rnet\nSociety. doi: 10.14722/ndss.2018.23158 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.01681",
                        "Citation Paper Title": "Title:VulDeePecker: A Deep Learning-Based System for Vulnerability Detection",
                        "Citation Paper Abstract": "Abstract:The automatic detection of software vulnerabilities is an important research problem. However, existing solutions to this problem rely on human experts to define features and often miss many vulnerabilities (i.e., incurring high false negative rate). In this paper, we initiate the study of using deep learning-based vulnerability detection to relieve human experts from the tedious and subjective task of manually defining features. Since deep learning is motivated to deal with problems that are very different from the problem of vulnerability detection, we need some guiding principles for applying deep learning to vulnerability detection. In particular, we need to find representations of software programs that are suitable for deep learning. For this purpose, we propose using code gadgets to represent programs and then transform them into vectors, where a code gadget is a number of (not necessarily consecutive) lines of code that are semantically related to each other. This leads to the design and implementation of a deep learning-based vulnerability detection system, called Vulnerability Deep Pecker (VulDeePecker). In order to evaluate VulDeePecker, we present the first vulnerability dataset for deep learning approaches. Experimental results show that VulDeePecker can achieve much fewer false negatives (with reasonable false positives) than other approaches. We further apply VulDeePecker to 3 software products (namely Xen, Seamonkey, and Libav) and detect 4 vulnerabilities, which are not reported in the National Vulnerability Database but were \"silently\" patched by the vendors when releasing later versions of these products; in contrast, these vulnerabilities are almost entirely missed by the other vulnerability detection systems we experimented with.",
                        "Citation Paper Authors": "Authors:Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan Wang, Zhijun Deng, Yuyi Zhong"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "with a deep learning model \u201cText-CNN\u201d to detect\nvulnerabilities in binary code. The encoding represents each instruc tion as a\nvector of a \ufb01xed length. Individual components of the instruction ,i.e. the\nopcodeandoperandfragments,areencodedusingacustomword 2vec ",
                    "Citation Text": "Mikolov, T., Chen, K., Corrado, G., Dean, J., 2013. E\ufb03cient estima tion\nof word representations in vector space, in: International Conf erence on\nLearning Representations (ICLR).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1301.3781",
                        "Citation Paper Title": "Title:Efficient Estimation of Word Representations in Vector Space",
                        "Citation Paper Abstract": "Abstract:We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.",
                        "Citation Paper Authors": "Authors:Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.01649v3": {
            "Paper Title": "HECO: Fully Homomorphic Encryption Compiler",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.09035v2": {
            "Paper Title": "On How Zero-Knowledge Proof Blockchain Mixers Improve, and Worsen User\n  Privacy",
            "Sentences": [
                {
                    "Sentence ID": 40,
                    "Sentence": "achieve better anonymity in a decentralized mixer. Wu et\nal. ",
                    "Citation Text": "Lei Wu, Yufeng Hu, Yajin Zhou, Haoyu Wang, Xiapu Luo, Zhi Wang, Fan Zhang,\nand Kui Ren. 2021. Towards Understanding and Demystifying Bitcoin Mixing\nServices. In Proceedings of the Web Conference 2021 . ACM / IW3C2, Virtual Event\n/ Ljubljana, Slovenia, 33\u201344.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.16274",
                        "Citation Paper Title": "Title:Towards Understanding and Demystifying Bitcoin Mixing Services",
                        "Citation Paper Abstract": "Abstract:One reason for the popularity of Bitcoin is due to its anonymity. Although several heuristics have been used to break the anonymity, new approaches are proposed to enhance its anonymity at the same time. One of them is the mixing service. Unfortunately, mixing services have been abused to facilitate criminal activities, e.g., money laundering. As such, there is an urgent need to systematically understand Bitcoin mixing services.\nIn this paper, we take the first step to understand state-of-the-art Bitcoin mixing services. Specifically, we propose a generic abstraction model for mixing services and observe that there are two mixing mechanisms in the wild, i.e. {swapping} and {obfuscating}. Based on this model, we conduct a transaction-based analysis and successfully reveal the mixing mechanisms of four representative services. Besides, we propose a method to identify mixing transactions that leverage the obfuscating mechanism. The proposed approach is able to identify over $92$\\% of the mixing transactions. Based on identified transactions, we then estimate the profit of mixing services and provide a case study of tracing the money flow of stolen Bitcoins.",
                        "Citation Paper Authors": "Authors:Lei Wu, Yufeng Hu, Yajin Zhou, Haoyu Wang, Xiaopu Luo, Zhi Wang, Fan Zhang, Kui Ren"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": ". Generalizing MEV ,\nnon-mining traders can also manipulate the transaction order and\nfront-run their victims by paying higher transaction fees to extract\nblockchain extractable value (BEV) ",
                    "Citation Text": "Kaihua Qin, Liyi Zhou, and Arthur Gervais. 2022. Quantifying Blockchain Ex-\ntractable Value: How dark is the forest?. In 2022 IEEE Symposium on Security and\nPrivacy (SP) . IEEE, San Francisco, CA, USA, 198\u2013214.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.05511",
                        "Citation Paper Title": "Title:Quantifying Blockchain Extractable Value: How dark is the forest?",
                        "Citation Paper Abstract": "Abstract:Permissionless blockchains such as Bitcoin have excelled at financial services. Yet, opportunistic traders extract monetary value from the mesh of decentralized finance (DeFi) smart contracts through so-called blockchain extractable value (BEV). The recent emergence of centralized BEV relayer portrays BEV as a positive additional revenue source. Because BEV was quantitatively shown to deteriorate the blockchain's consensus security, BEV relayers endanger the ledger security by incentivizing rational miners to fork the chain. For example, a rational miner with a 10% hashrate will fork Ethereum if a BEV opportunity exceeds 4x the block reward.\nHowever, related work is currently missing quantitative insights on past BEV extraction to assess the practical risks of BEV objectively. In this work, we allow to quantify the BEV danger by deriving the USD extracted from sandwich attacks, liquidations, and decentralized exchange arbitrage. We estimate that over 32 months, BEV yielded 540.54M USD in profit, divided among 11,289 addresses when capturing 49,691 cryptocurrencies and 60,830 on-chain markets. The highest BEV instance we find amounts to 4.1M USD, 616.6x the Ethereum block reward.\nMoreover, while the practitioner's community has discussed the existence of generalized trading bots, we are, to our knowledge, the first to provide a concrete algorithm. Our algorithm can replace unconfirmed transactions without the need to understand the victim transactions' underlying logic, which we estimate to have yielded a profit of 57,037.32 ETH (35.37M USD) over 32 months of past blockchain data.\nFinally, we formalize and analyze emerging BEV relay systems, where miners accept BEV transactions from a centralized relay server instead of the peer-to-peer (P2P) network. We find that such relay systems aggravate the consensus layer attacks and therefore further endanger blockchain security.",
                        "Citation Paper Authors": "Authors:Kaihua Qin, Liyi Zhou, Arthur Gervais"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.10550v2": {
            "Paper Title": "(Certified!!) Adversarial Robustness for Free!",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.06093v2": {
            "Paper Title": "Adversarial Machine Learning Threat Analysis and Remediation in Open\n  Radio Access Network (O-RAN)",
            "Sentences": [
                {
                    "Sentence ID": 77,
                    "Sentence": "I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, \u201cGenerative adversarial\nnetworks,\u201d Communications of the ACM , vol. 63, no. 11, pp. 139\u2013\n144, 2020. ",
                    "Citation Text": "P . Samangouei, M. Kabkab, and R. Chellappa, \u201cDefense-gan:\nProtecting classi\ufb01ers against adversarial attacks using generative\nmodels,\u201d arXiv preprint arXiv:1805.06605 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.06605",
                        "Citation Paper Title": "Title:Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models",
                        "Citation Paper Abstract": "Abstract:In recent years, deep neural network approaches have been widely adopted for machine learning tasks, including classification. However, they were shown to be vulnerable to adversarial perturbations: carefully crafted small perturbations can cause misclassification of legitimate images. We propose Defense-GAN, a new framework leveraging the expressive capability of generative models to defend deep neural networks against such attacks. Defense-GAN is trained to model the distribution of unperturbed images. At inference time, it finds a close output to a given image which does not contain the adversarial changes. This output is then fed to the classifier. Our proposed method can be used with any classification model and does not modify the classifier structure or training procedure. It can also be used as a defense against any attack as it does not assume knowledge of the process for generating the adversarial examples. We empirically show that Defense-GAN is consistently effective against different attack methods and improves on existing defense strategies. Our code has been made publicly available at this https URL",
                        "Citation Paper Authors": "Authors:Pouya Samangouei, Maya Kabkab, Rama Chellappa"
                    }
                },
                {
                    "Sentence ID": 76,
                    "Sentence": "W. Xu, D. Evans, and Y. Qi, \u201cFeature squeezing: Detecting\nadversarial examples in deep neural networks,\u201d arXiv preprint\narXiv:1704.01155 , 2017. ",
                    "Citation Text": "I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, \u201cGenerative adversarial\nnetworks,\u201d Communications of the ACM , vol. 63, no. 11, pp. 139\u2013\n144, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.2661",
                        "Citation Paper Title": "Title:Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 75,
                    "Sentence": "I. Rosenberg, A. Shabtai, Y. Elovici, and L. Rokach, \u201cSequence\nsqueezing: A defense method against adversarial examples for\napi call-based rnn variants,\u201d in 2021 International Joint Conference\non Neural Networks (IJCNN) . IEEE, 2021, pp. 1\u201310. ",
                    "Citation Text": "W. Xu, D. Evans, and Y. Qi, \u201cFeature squeezing: Detecting\nadversarial examples in deep neural networks,\u201d arXiv preprint\narXiv:1704.01155 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.01155",
                        "Citation Paper Title": "Title:Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:Although deep neural networks (DNNs) have achieved great success in many tasks, they can often be fooled by \\emph{adversarial examples} that are generated by adding small but purposeful distortions to natural examples. Previous studies to defend against adversarial examples mostly focused on refining the DNN models, but have either shown limited success or required expensive computation. We propose a new strategy, \\emph{feature squeezing}, that can be used to harden DNN models by detecting adversarial examples. Feature squeezing reduces the search space available to an adversary by coalescing samples that correspond to many different feature vectors in the original space into a single sample. By comparing a DNN model's prediction on the original input with that on squeezed inputs, feature squeezing detects adversarial examples with high accuracy and few false positives. This paper explores two feature squeezing methods: reducing the color bit depth of each pixel and spatial smoothing. These simple strategies are inexpensive and complementary to other defenses, and can be combined in a joint detection framework to achieve high detection rates against state-of-the-art attacks.",
                        "Citation Paper Authors": "Authors:Weilin Xu, David Evans, Yanjun Qi"
                    }
                },
                {
                    "Sentence ID": 73,
                    "Sentence": "N. Carlini, P . Mishra, T. Vaidya, Y. Zhang, M. Sherr, C. Shields,\nD. Wagner, and W. Zhou, \u201cHidden voice commands,\u201d in 25th\nUSENIX security symposium (USENIX security 16) , 2016, pp. 513\u2013\n530. ",
                    "Citation Text": "H. Hosseini, S. Kannan, B. Zhang, and R. Poovendran, \u201cDeceiving\ngoogle\u2019s perspective api built for detecting toxic comments,\u201d arXiv\npreprint arXiv:1702.08138 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.08138",
                        "Citation Paper Title": "Title:Deceiving Google's Perspective API Built for Detecting Toxic Comments",
                        "Citation Paper Abstract": "Abstract:Social media platforms provide an environment where people can freely engage in discussions. Unfortunately, they also enable several problems, such as online harassment. Recently, Google and Jigsaw started a project called Perspective, which uses machine learning to automatically detect toxic language. A demonstration website has been also launched, which allows anyone to type a phrase in the interface and instantaneously see the toxicity score [1]. In this paper, we propose an attack on the Perspective toxic detection system based on the adversarial examples. We show that an adversary can subtly modify a highly toxic phrase in a way that the system assigns significantly lower toxicity score to it. We apply the attack on the sample phrases provided in the Perspective website and show that we can consistently reduce the toxicity scores to the level of the non-toxic phrases. The existence of such adversarial examples is very harmful for toxic detection systems and seriously undermines their usability.",
                        "Citation Paper Authors": "Authors:Hossein Hosseini, Sreeram Kannan, Baosen Zhang, Radha Poovendran"
                    }
                },
                {
                    "Sentence ID": 65,
                    "Sentence": "C. Gentry, \u201cFully homomorphic encryption using ideal lattices,\u201d\ninProceedings of the forty-\ufb01rst annual ACM symposium on Theory of\ncomputing , 2009, pp. 169\u2013178. ",
                    "Citation Text": "T. Ryffel, A. Trask, M. Dahl, B. Wagner, J. Mancuso, D. Rueckert,\nand J. Passerat-Palmbach, \u201cA generic framework for privacy\npreserving deep learning,\u201d arXiv preprint arXiv:1811.04017 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.04017",
                        "Citation Paper Title": "Title:A generic framework for privacy preserving deep learning",
                        "Citation Paper Abstract": "Abstract:We detail a new framework for privacy preserving deep learning and discuss its assets. The framework puts a premium on ownership and secure processing of data and introduces a valuable representation based on chains of commands and tensors. This abstraction allows one to implement complex privacy preserving constructs such as Federated Learning, Secure Multiparty Computation, and Differential Privacy while still exposing a familiar deep learning API to the end-user. We report early results on the Boston Housing and Pima Indian Diabetes datasets. While the privacy features apart from Differential Privacy do not impact the prediction accuracy, the current implementation of the framework introduces a significant overhead in performance, which will be addressed at a later stage of the development. We believe this work is an important milestone introducing the first reliable, general framework for privacy preserving deep learning.",
                        "Citation Paper Authors": "Authors:Theo Ryffel, Andrew Trask, Morten Dahl, Bobby Wagner, Jason Mancuso, Daniel Rueckert, Jonathan Passerat-Palmbach"
                    }
                },
                {
                    "Sentence ID": 62,
                    "Sentence": "Y. Adi, C. Baum, M. Cisse, B. Pinkas, and J. Keshet, \u201cTurning your\nweakness into a strength: Watermarking deep neural networks\nby backdooring,\u201d in 27th USENIX Security Symposium (USENIX\nSecurity 18) , 2018, pp. 1615\u20131631. ",
                    "Citation Text": "C. Song, T. Ristenpart, and V . Shmatikov, \u201cMachine learning\nmodels that remember too much,\u201d in Proceedings of the 2017 ACM\nSIGSAC Conference on computer and communications security , 2017,\npp. 587\u2013601.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.07886",
                        "Citation Paper Title": "Title:Machine Learning Models that Remember Too Much",
                        "Citation Paper Abstract": "Abstract:Machine learning (ML) is becoming a commodity. Numerous ML frameworks and services are available to data holders who are not ML experts but want to train predictive models on their data. It is important that ML models trained on sensitive inputs (e.g., personal images or documents) not leak too much information about the training data.\nWe consider a malicious ML provider who supplies model-training code to the data holder, does not observe the training, but then obtains white- or black-box access to the resulting model. In this setting, we design and implement practical algorithms, some of them very similar to standard ML techniques such as regularization and data augmentation, that \"memorize\" information about the training dataset in the model yet the model is as accurate and predictive as a conventionally trained model. We then explain how the adversary can extract memorized information from the model.\nWe evaluate our techniques on standard ML tasks for image classification (CIFAR10), face recognition (LFW and FaceScrub), and text analysis (20 Newsgroups and IMDB). In all cases, we show how our algorithms create models that have high predictive power yet allow accurate extraction of subsets of their training data.",
                        "Citation Paper Authors": "Authors:Congzheng Song, Thomas Ristenpart, Vitaly Shmatikov"
                    }
                },
                {
                    "Sentence ID": 60,
                    "Sentence": "T. Gu, B. Dolan-Gavitt, and S. Garg, \u201cBadnets: Identifying vul-\nnerabilities in the machine learning model supply chain,\u201d arXiv\npreprint arXiv:1708.06733 , 2017. ",
                    "Citation Text": "E. Le Merrer, P . Perez, and G. Tr \u00b4edan, \u201cAdversarial frontier stitch-\ning for remote neural network watermarking,\u201d Neural Computing\nand Applications , vol. 32, no. 13, pp. 9233\u20139244, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.01894",
                        "Citation Paper Title": "Title:Adversarial Frontier Stitching for Remote Neural Network Watermarking",
                        "Citation Paper Abstract": "Abstract:The state of the art performance of deep learning models comes at a high cost for companies and institutions, due to the tedious data collection and the heavy processing requirements. Recently, [35, 22] proposed to watermark convolutional neural networks for image classification, by embedding information into their weights. While this is a clear progress towards model protection, this technique solely allows for extracting the watermark from a network that one accesses locally and entirely.\nInstead, we aim at allowing the extraction of the watermark from a neural network (or any other machine learning model) that is operated remotely, and available through a service API. To this end, we propose to mark the model's action itself, tweaking slightly its decision frontiers so that a set of specific queries convey the desired information. In the present paper, we formally introduce the problem and propose a novel zero-bit watermarking algorithm that makes use of adversarial model examples. While limiting the loss of performance of the protected model, this algorithm allows subsequent extraction of the watermark using only few queries. We experimented the approach on three neural networks designed for image classification, in the context of MNIST digit recognition task.",
                        "Citation Paper Authors": "Authors:Erwan Le Merrer, Patrick Perez, Gilles Tr\u00e9dan"
                    }
                },
                {
                    "Sentence ID": 58,
                    "Sentence": "N. Papernot, P . McDaniel, X. Wu, S. Jha, and A. Swami, \u201cDistilla-tion as a defense to adversarial perturbations against deep neural\nnetworks,\u201d in 2016 IEEE Symposium on Security and Privacy (SP) .\nIEEE, 2016, pp. 582\u2013597. ",
                    "Citation Text": "H. Chen, B. D. Rohani, and F. Koushanfar, \u201cDeepmarks: A dig-\nital \ufb01ngerprinting framework for deep neural networks,\u201d arXiv\npreprint arXiv:1804.03648 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.03648",
                        "Citation Paper Title": "Title:DeepMarks: A Digital Fingerprinting Framework for Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:This paper proposes DeepMarks, a novel end-to-end framework for systematic fingerprinting in the context of Deep Learning (DL). Remarkable progress has been made in the area of deep learning. Sharing the trained DL models has become a trend that is ubiquitous in various fields ranging from biomedical diagnosis to stock prediction. As the availability and popularity of pre-trained models are increasing, it is critical to protect the Intellectual Property (IP) of the model owner. DeepMarks introduces the first fingerprinting methodology that enables the model owner to embed unique fingerprints within the parameters (weights) of her model and later identify undesired usages of her distributed models. The proposed framework embeds the fingerprints in the Probability Density Function (pdf) of trainable weights by leveraging the extra capacity available in contemporary DL models. DeepMarks is robust against fingerprints collusion as well as network transformation attacks, including model compression and model fine-tuning. Extensive proof-of-concept evaluations on MNIST and CIFAR10 datasets, as well as a wide variety of deep neural networks architectures such as Wide Residual Networks (WRNs) and Convolutional Neural Networks (CNNs), corroborate the effectiveness and robustness of DeepMarks framework.",
                        "Citation Paper Authors": "Authors:Huili Chen, Bita Darvish Rohani, Farinaz Koushanfar"
                    }
                },
                {
                    "Sentence ID": 56,
                    "Sentence": "A. Athalye, N. Carlini, and D. Wagner, \u201cObfuscated gradients give\na false sense of security: Circumventing defenses to adversarial\nexamples,\u201d in International conference on machine learning . PMLR,\n2018, pp. 274\u2013283. ",
                    "Citation Text": "G. Hinton, O. Vinyals, J. Dean, et al. , \u201cDistilling the knowledge in\na neural network,\u201d arXiv preprint arXiv:1503.02531 , vol. 2, no. 7,\n2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1503.02531",
                        "Citation Paper Title": "Title:Distilling the Knowledge in a Neural Network",
                        "Citation Paper Abstract": "Abstract:A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.",
                        "Citation Paper Authors": "Authors:Geoffrey Hinton, Oriol Vinyals, Jeff Dean"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": "P . P . Chan, Z.-M. He, H. Li, and C.-C. Hsu, \u201cData sanitization\nagainst adversarial label contamination based on data complex-\nity,\u201d International Journal of Machine Learning and Cybernetics , vol. 9,\nno. 6, pp. 1039\u20131052, 2018. ",
                    "Citation Text": "C. Xie, Y. Wu, L. v. d. Maaten, A. L. Yuille, and K. He, \u201cFeature\ndenoising for improving adversarial robustness,\u201d in Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition ,\n2019, pp. 501\u2013509.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.03411",
                        "Citation Paper Title": "Title:Feature Denoising for Improving Adversarial Robustness",
                        "Citation Paper Abstract": "Abstract:Adversarial attacks to image classification systems present challenges to convolutional networks and opportunities for understanding them. This study suggests that adversarial perturbations on images lead to noise in the features constructed by these networks. Motivated by this observation, we develop new network architectures that increase adversarial robustness by performing feature denoising. Specifically, our networks contain blocks that denoise the features using non-local means or other filters; the entire networks are trained end-to-end. When combined with adversarial training, our feature denoising networks substantially improve the state-of-the-art in adversarial robustness in both white-box and black-box attack settings. On ImageNet, under 10-iteration PGD white-box attacks where prior art has 27.9% accuracy, our method achieves 55.7%; even under extreme 2000-iteration PGD white-box attacks, our method secures 42.6% accuracy. Our method was ranked first in Competition on Adversarial Attacks and Defenses (CAAD) 2018 --- it achieved 50.6% classification accuracy on a secret, ImageNet-like test dataset against 48 unknown attackers, surpassing the runner-up approach by ~10%. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan Yuille, Kaiming He"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": "S. Gowal, S.-A. Rebuf\ufb01, O. Wiles, F. Stimberg, D. A. Calian,\nand T. A. Mann, \u201cImproving robustness using generated data,\u201d\nAdvances in Neural Information Processing Systems , vol. 34, pp.\n4218\u20134233, 2021. ",
                    "Citation Text": "T. Bai, J. Luo, J. Zhao, B. Wen, and Q. Wang, \u201cRecent advances\nin adversarial training for adversarial robustness,\u201d arXiv preprint\narXiv:2102.01356 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.01356",
                        "Citation Paper Title": "Title:Recent Advances in Adversarial Training for Adversarial Robustness",
                        "Citation Paper Abstract": "Abstract:Adversarial training is one of the most effective approaches defending against adversarial examples for deep learning models. Unlike other defense strategies, adversarial training aims to promote the robustness of models intrinsically. During the last few years, adversarial training has been studied and discussed from various aspects. A variety of improvements and developments of adversarial training are proposed, which were, however, neglected in existing surveys. For the first time in this survey, we systematically review the recent progress on adversarial training for adversarial robustness with a novel taxonomy. Then we discuss the generalization problems in adversarial training from three perspectives. Finally, we highlight the challenges which are not fully tackled and present potential future directions.",
                        "Citation Paper Authors": "Authors:Tao Bai, Jinqi Luo, Jun Zhao, Bihan Wen, Qian Wang"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "E. D. Cubuk, B. Zoph, J. Shlens, and Q. V . Le, \u201cRandaugment:\nPractical automated data augmentation with a reduced search\nspace,\u201d in Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition workshops , 2020, pp. 702\u2013703. ",
                    "Citation Text": "D. Hendrycks, S. Basart, N. Mu, S. Kadavath, F. Wang,\nE. Dorundo, R. Desai, T. Zhu, S. Parajuli, M. Guo, et al. , \u201cThe\nmany faces of robustness: A critical analysis of out-of-distribution\ngeneralization,\u201d in Proceedings of the IEEE/CVF International Con-\nference on Computer Vision , 2021, pp. 8340\u20138349.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.16241",
                        "Citation Paper Title": "Title:The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
                        "Citation Paper Abstract": "Abstract:We introduce four new real-world distribution shift datasets consisting of changes in image style, image blurriness, geographic location, camera operation, and more. With our new datasets, we take stock of previously proposed methods for improving out-of-distribution robustness and put them to the test. We find that using larger models and artificial data augmentations can improve robustness on real-world distribution shifts, contrary to claims in prior work. We find improvements in artificial robustness benchmarks can transfer to real-world distribution shifts, contrary to claims in prior work. Motivated by our observation that data augmentations can help with real-world distribution shifts, we also introduce a new data augmentation method which advances the state-of-the-art and outperforms models pretrained with 1000 times more labeled data. Overall we find that some methods consistently help with distribution shifts in texture and local image statistics, but these methods do not help with some other distribution shifts like geographic changes. Our results show that future research must study multiple distribution shifts simultaneously, as we demonstrate that no evaluated method consistently improves robustness.",
                        "Citation Paper Authors": "Authors:Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, Justin Gilmer"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "knowledge align-\nment,\u201d in Proceedings of the 2019 ACM SIGSAC Conference on\nComputer and Communications Security , 2019, pp. 225\u2013240. ",
                    "Citation Text": "J. Chen, M. I. Jordan, and M. J. Wainwright, \u201cHopskipjumpattack:\nA query-ef\ufb01cient decision-based attack,\u201d in 2020 ieee symposium on\nsecurity and privacy (sp) . IEEE, 2020, pp. 1277\u20131294.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.02144",
                        "Citation Paper Title": "Title:HopSkipJumpAttack: A Query-Efficient Decision-Based Attack",
                        "Citation Paper Abstract": "Abstract:The goal of a decision-based adversarial attack on a trained model is to generate adversarial examples based solely on observing output labels returned by the targeted model. We develop HopSkipJumpAttack, a family of algorithms based on a novel estimate of the gradient direction using binary information at the decision boundary. The proposed family includes both untargeted and targeted attacks optimized for $\\ell_2$ and $\\ell_\\infty$ similarity metrics respectively. Theoretical analysis is provided for the proposed algorithms and the gradient direction estimate. Experiments show HopSkipJumpAttack requires significantly fewer model queries than Boundary Attack. It also achieves competitive performance in attacking several widely-used defense mechanisms. (HopSkipJumpAttack was named Boundary Attack++ in a previous version of the preprint.)",
                        "Citation Paper Authors": "Authors:Jianbo Chen, Michael I. Jordan, Martin J. Wainwright"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.13345v3": {
            "Paper Title": "Principled Data-Driven Decision Support for Cyber-Forensic\n  Investigations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.10605v2": {
            "Paper Title": "SoK: Explainable Machine Learning for Computer Security Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.04662v2": {
            "Paper Title": "SoK: Rethinking Sensor Spoofing Attacks against Robotic Vehicles from a\n  Systematic View",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.02912v2": {
            "Paper Title": "CANIFE: Crafting Canaries for Empirical Privacy Measurement in Federated\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.13124v2": {
            "Paper Title": "Cipherfix: Mitigating Ciphertext Side-Channel Attacks in Software",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.06630v2": {
            "Paper Title": "Differentially Private Tree-Based Redescription Mining",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.13710v2": {
            "Paper Title": "Privacy of Noisy Stochastic Gradient Descent: More Iterations without\n  More Privacy Loss",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.08689v2": {
            "Paper Title": "Sniper Backdoor: Single Client Targeted Backdoor Attack in Federated\n  Learning",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "generates\ndifferent variants of the input sample, masking the dominant\ncolor, and checks the attack success rate for every pixel in each\nvariant. The model is \ufb02agged as malicious if the attack success\nrate exceeds some threshold. STRIP ",
                    "Citation Text": "Gao, Y ., Xu, C., Wang, D., Chen, S., Ranasinghe, D.C., Nepal, S.: Strip:\nA defence against trojan attacks on deep neural networks. In: Proceed-\nings of the 35th Annual Computer Security Applications Conference.\npp. 113\u2013125 (2019)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.06531",
                        "Citation Paper Title": "Title:STRIP: A Defence Against Trojan Attacks on Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:A recent trojan attack on deep neural network (DNN) models is one insidious variant of data poisoning attacks. Trojan attacks exploit an effective backdoor created in a DNN model by leveraging the difficulty in interpretability of the learned model to misclassify any inputs signed with the attacker's chosen trojan trigger. Since the trojan trigger is a secret guarded and exploited by the attacker, detecting such trojan inputs is a challenge, especially at run-time when models are in active operation. This work builds STRong Intentional Perturbation (STRIP) based run-time trojan attack detection system and focuses on vision system. We intentionally perturb the incoming input, for instance by superimposing various image patterns, and observe the randomness of predicted classes for perturbed inputs from a given deployed model---malicious or benign. A low entropy in predicted classes violates the input-dependence property of a benign model and implies the presence of a malicious input---a characteristic of a trojaned input. The high efficacy of our method is validated through case studies on three popular and contrasting datasets: MNIST, CIFAR10 and GTSRB. We achieve an overall false acceptance rate (FAR) of less than 1%, given a preset false rejection rate (FRR) of 1%, for different types of triggers. Using CIFAR10 and GTSRB, we have empirically achieved result of 0% for both FRR and FAR. We have also evaluated STRIP robustness against a number of trojan attack variants and adaptive attacks.",
                        "Citation Paper Authors": "Authors:Yansong Gao, Chang Xu, Derui Wang, Shiping Chen, Damith C.Ranasinghe, Surya Nepal"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": ". For the CIFAR-\n100 dataset, we use VGG11 with batch normalization ",
                    "Citation Text": "Simonyan, K., Zisserman, A.: Very deep convolutional networks for\nlarge-scale image recognition. arXiv preprint arXiv:1409.1556 (2014)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.1556",
                        "Citation Paper Title": "Title:Very Deep Convolutional Networks for Large-Scale Image Recognition",
                        "Citation Paper Abstract": "Abstract:In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.",
                        "Citation Paper Authors": "Authors:Karen Simonyan, Andrew Zisserman"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": ". However, it has been shown that FL is also\nvulnerable to attacks that make the DL models misbehave at\ninference time, i.e., poisoning or backdoor attacks ",
                    "Citation Text": "Bagdasaryan, E., Veit, A., Hua, Y ., Estrin, D., Shmatikov, V .: How to\nbackdoor federated learning. In: International Conference on Arti\ufb01cial\nIntelligence and Statistics. pp. 2938\u20132948. PMLR (2020)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.00459",
                        "Citation Paper Title": "Title:How To Backdoor Federated Learning",
                        "Citation Paper Abstract": "Abstract:Federated learning enables thousands of participants to construct a deep learning model without sharing their private training data with each other. For example, multiple smartphones can jointly train a next-word predictor for keyboards without revealing what individual users type. We demonstrate that any participant in federated learning can introduce hidden backdoor functionality into the joint global model, e.g., to ensure that an image classifier assigns an attacker-chosen label to images with certain features, or that a word predictor completes certain sentences with an attacker-chosen word.\nWe design and evaluate a new model-poisoning methodology based on model replacement. An attacker selected in a single round of federated learning can cause the global model to immediately reach 100% accuracy on the backdoor task. We evaluate the attack under different assumptions for the standard federated-learning tasks and show that it greatly outperforms data poisoning. Our generic constrain-and-scale technique also evades anomaly detection-based defenses by incorporating the evasion into the attacker's loss function during training.",
                        "Citation Paper Authors": "Authors:Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, Vitaly Shmatikov"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": ".\nComprehensive studies about the attacks on FL are given\nin ",
                    "Citation Text": "Abad, G., Picek, S., Ram\u00edrez-Dur\u00e1n, V .J., Urbieta, A.:\nOn the security & privacy in federated learning (2021).\nhttps://doi.org/10.48550/ARXIV .2112.05423, https://arxiv.org/abs/\n2112.05423",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.05423",
                        "Citation Paper Title": "Title:On the Security & Privacy in Federated Learning",
                        "Citation Paper Abstract": "Abstract:Recent privacy awareness initiatives such as the EU General Data Protection Regulation subdued Machine Learning (ML) to privacy and security assessments. Federated Learning (FL) grants a privacy-driven, decentralized training scheme that improves ML models' security. The industry's fast-growing adaptation and security evaluations of FL technology exposed various vulnerabilities that threaten FL's confidentiality, integrity, or availability (CIA). This work assesses the CIA of FL by reviewing the state-of-the-art (SoTA) and creating a threat model that embraces the attack's surface, adversarial actors, capabilities, and goals. We propose the first unifying taxonomy for attacks and defenses and provide promising future research directions.",
                        "Citation Paper Authors": "Authors:Gorka Abad, Stjepan Picek, V\u00edctor Julio Ram\u00edrez-Dur\u00e1n, Aitor Urbieta"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "FL has gained attention as a privacy-driven alternative to\ncentralized learning\u2014granting the ability to train a DL algo-\nrithm without sharing the data and splitting the computational\npower ",
                    "Citation Text": "Singh, A., Vepakomma, P., Gupta, O., Raskar, R.: Detailed comparison\nof communication ef\ufb01ciency of split learning and federated learning.\narXiv preprint arXiv:1909.09145 (2019)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.09145",
                        "Citation Paper Title": "Title:Detailed comparison of communication efficiency of split learning and federated learning",
                        "Citation Paper Abstract": "Abstract:We compare communication efficiencies of two compelling distributed machine learning approaches of split learning and federated learning. We show useful settings under which each method outperforms the other in terms of communication efficiency. We consider various practical scenarios of distributed learning setup and juxtapose the two methods under various real-life scenarios. We consider settings of small and large number of clients as well as small models (1M - 6M parameters), large models (10M - 200M parameters) and very large models (1 Billion-100 Billion parameters). We show that increasing number of clients or increasing model size favors split learning setup over the federated while increasing the number of data samples while keeping the number of clients or model size low makes federated learning more communication efficient.",
                        "Citation Paper Authors": "Authors:Abhishek Singh, Praneeth Vepakomma, Otkrist Gupta, Ramesh Raskar"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.14317v3": {
            "Paper Title": "Quantum Merkle Trees",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.12623v2": {
            "Paper Title": "Good Artists Copy, Great Artists Steal: Model Extraction Attacks Against\n  Image Translation Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.08270v2": {
            "Paper Title": "On the Privacy Effect of Data Enhancement via the Lens of Memorization",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": ". We use the widely\nadopted maximum predication con\ufb01dence-based attack ",
                    "Citation Text": "Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal\nBerrang, Mario Fritz, and Michael Backes. Ml-leaks: Model\nand data independent membership inference attacks and de-\nfenses on machine learning models. Network and Distributed\nSystems Security Symposium , 2019. 1, 2, 3, 4, 6, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.01246",
                        "Citation Paper Title": "Title:ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models",
                        "Citation Paper Abstract": "Abstract:Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications.\nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains.\nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model.",
                        "Citation Paper Authors": "Authors:Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, Michael Backes"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": ".\nIn this paper we assume a black-box setting where the\nadversary only has query access to the outputs of the target\nmodel on given samples. Most MIAs follow ",
                    "Citation Text": "Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly\nShmatikov. Membership inference attacks against machine\nlearning models. In IEEE symposium on security and privacy\n(SP), pages 3\u201318, 2017. 1, 2, 3, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.05820",
                        "Citation Paper Title": "Title:Membership Inference Attacks against Machine Learning Models",
                        "Citation Paper Abstract": "Abstract:We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on.\nWe empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                        "Citation Paper Authors": "Authors:Reza Shokri, Marco Stronati, Congzheng Song, Vitaly Shmatikov"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": ": randomly change the brightness, contrast,\nsaturation and hue of each image.\n8) Distillation ",
                    "Citation Text": "Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Dis-\ntilling the knowledge in a neural network. arXiv preprint\narXiv:1503.02531 , 2015. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1503.02531",
                        "Citation Paper Title": "Title:Distilling the Knowledge in a Neural Network",
                        "Citation Paper Abstract": "Abstract:A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.",
                        "Citation Paper Authors": "Authors:Geoffrey Hinton, Oriol Vinyals, Jeff Dean"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": ": mask out a random square area of size M\u0002\nMfrom each feature.\n6) Mixup ",
                    "Citation Text": "Hongyi Zhang, Moustapha Ciss \u00b4e, Yann N. Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. In Int. Conf. Learn. Represent. (ICLR) , 2018. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.09412",
                        "Citation Paper Title": "Title:mixup: Beyond Empirical Risk Minimization",
                        "Citation Paper Abstract": "Abstract:Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.",
                        "Citation Paper Authors": "Authors:Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": ": change a portion of the ground-\ntruth (GT) labels to incorrect labels, namely, ~y=\u000fy+\n(1\u0000\u000f)yf, where\u000fis randomly sampled from f0;1gand\nyf2f1;2;:::;ngnfygdenotes the incorrect label.\n4) Gaussian Augmentation ",
                    "Citation Text": "Jeremy M. Cohen, Elan Rosenfeld, and J. Zico Kolter. Cer-\nti\ufb01ed adversarial robustness via randomized smoothing. In\nInt. Conf. Mach. Learn. (ICML) , pages 1310\u20131320, 2019. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.02918",
                        "Citation Paper Title": "Title:Certified Adversarial Robustness via Randomized Smoothing",
                        "Citation Paper Abstract": "Abstract:We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the $\\ell_2$ norm. This \"randomized smoothing\" technique has been proposed recently in the literature, but existing guarantees are loose. We prove a tight robustness guarantee in $\\ell_2$ norm for smoothing with Gaussian noise. We use randomized smoothing to obtain an ImageNet classifier with e.g. a certified top-1 accuracy of 49% under adversarial perturbations with $\\ell_2$ norm less than 0.5 (=127/255). No certified defense has been shown feasible on ImageNet except for smoothing. On smaller-scale datasets where competing approaches to certified $\\ell_2$ robustness are viable, smoothing delivers higher certified accuracies. Our strong empirical results suggest that randomized smoothing is a promising direction for future research into adversarially robust classification. Code and models are available at this http URL.",
                        "Citation Paper Authors": "Authors:Jeremy M Cohen, Elan Rosenfeld, J. Zico Kolter"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": ". The latter as-\nsumes a stronger scenario where the particular augmented\ndata used in training is known to the adversary. ",
                    "Citation Text": "Christopher A. Choquette-Choo, Florian Tram `er, Nicholas\nCarlini, and Nicolas Papernot. Label-only membership in-\nference attacks. In Int. Conf. Mach. Learn. (ICML) , pages\n1964\u20131974. PMLR, 2021. 2",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.14321",
                        "Citation Paper Title": "Title:Label-Only Membership Inference Attacks",
                        "Citation Paper Abstract": "Abstract:Membership inference attacks are one of the simplest forms of privacy leakage for machine learning models: given a data point and model, determine whether the point was used to train the model. Existing membership inference attacks exploit models' abnormal confidence when queried on their training data. These attacks do not apply if the adversary only gets access to models' predicted labels, without a confidence measure. In this paper, we introduce label-only membership inference attacks. Instead of relying on confidence scores, our attacks evaluate the robustness of a model's predicted labels under perturbations to obtain a fine-grained membership signal. These perturbations include common data augmentations or adversarial examples. We empirically show that our label-only membership inference attacks perform on par with prior attacks that required access to model confidences. We further demonstrate that label-only attacks break multiple defenses against membership inference attacks that (implicitly or explicitly) rely on a phenomenon we call confidence masking. These defenses modify a model's confidence scores in order to thwart attacks, but leave the model's predicted labels unchanged. Our label-only attacks demonstrate that confidence-masking is not a viable defense strategy against membership inference. Finally, we investigate worst-case label-only attacks, that infer membership for a small number of outlier data points. We show that label-only attacks also match confidence-based attacks in this setting. We find that training models with differential privacy and (strong) L2 regularization are the only known defense strategies that successfully prevents all attacks. This remains true even when the differential privacy budget is too high to offer meaningful provable guarantees.",
                        "Citation Paper Authors": "Authors:Christopher A. Choquette-Choo, Florian Tramer, Nicholas Carlini, Nicolas Papernot"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.12873v2": {
            "Paper Title": "FLIP: A Provable Defense Framework for Backdoor Mitigation in Federated\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.09727v4": {
            "Paper Title": "Lost at C: A User Study on the Security Implications of Large Language\n  Model Code Assistants",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": ".\nThat said, LLMs may also generate secure code as a re-\nplacement for insecure code ",
                    "Citation Text": "H. Pearce, B. Tan, B. Ahmad, R. Karri, and B. Dolan-\nGavitt, \u201cExamining Zero-Shot Vulnerability Repair withLarge Language Models,\u201d in 2023 IEEE Symposium on\nSecurity and Privacy (SP) . Los Alamitos, CA, USA:\nIEEE Computer Society, May 2023, pp. 1\u201318. [Online].\nAvailable: https://doi :ieeecomputersociety :org/10 :1109/\nSP46215 :2023 :00001",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.02125",
                        "Citation Paper Title": "Title:Examining Zero-Shot Vulnerability Repair with Large Language Models",
                        "Citation Paper Abstract": "Abstract:Human developers can produce code with cybersecurity bugs. Can emerging 'smart' code completion tools help repair those bugs? In this work, we examine the use of large language models (LLMs) for code (such as OpenAI's Codex and AI21's Jurassic J-1) for zero-shot vulnerability repair. We investigate challenges in the design of prompts that coax LLMs into generating repaired versions of insecure code. This is difficult due to the numerous ways to phrase key information - both semantically and syntactically - with natural languages. We perform a large scale study of five commercially available, black-box, \"off-the-shelf\" LLMs, as well as an open-source model and our own locally-trained model, on a mix of synthetic, hand-crafted, and real-world security bug scenarios. Our experiments demonstrate that while the approach has promise (the LLMs could collectively repair 100% of our synthetically generated and hand-crafted scenarios), a qualitative evaluation of the model's performance over a corpus of historical real-world examples highlights challenges in generating functionally correct code.",
                        "Citation Paper Authors": "Authors:Hammond Pearce, Benjamin Tan, Baleegh Ahmad, Ramesh Karri, Brendan Dolan-Gavitt"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.13893v2": {
            "Paper Title": "Data Isotopes for Data Provenance in DNNs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.10603v2": {
            "Paper Title": "Investigating the Security of EV Charging Mobile Applications As an\n  Attack Surface",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.13216v3": {
            "Paper Title": "Encoded Gradients Aggregation against Gradient Leakage in Federated\n  Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.14485v3": {
            "Paper Title": "Noise-Aware Statistical Inference with Differentially Private Synthetic\n  Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.13058v2": {
            "Paper Title": "Adversarial Robustness for Tabular Data through Cost and Utility\n  Awareness",
            "Sentences": [
                {
                    "Sentence ID": 56,
                    "Sentence": "Distance based \u2014 Any 7 Gradient-based \u2014\nKantchelian et al. ",
                    "Citation Text": "Alex Kantchelian, J. D. Tygar, and Anthony D. Joseph. Evasion and hardening of tree ensemble classi/f_iers. In\nICML , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1509.07892",
                        "Citation Paper Title": "Title:Evasion and Hardening of Tree Ensemble Classifiers",
                        "Citation Paper Abstract": "Abstract:Classifier evasion consists in finding for a given instance $x$ the nearest instance $x'$ such that the classifier predictions of $x$ and $x'$ are different. We present two novel algorithms for systematically computing evasions for tree ensembles such as boosted trees and random forests. Our first algorithm uses a Mixed Integer Linear Program solver and finds the optimal evading instance under an expressive set of constraints. Our second algorithm trades off optimality for speed by using symbolic prediction, a novel algorithm for fast finite differences on tree ensembles. On a digit recognition task, we demonstrate that both gradient boosted trees and random forests are extremely susceptible to evasions. Finally, we harden a boosted tree model without loss of predictive accuracy by augmenting the training set of each boosting round with evading instances, a technique we call adversarial boosting.",
                        "Citation Paper Authors": "Authors:Alex Kantchelian, J. D. Tygar, Anthony D. Joseph"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": "Feature-importance based \u2014 Di\ufb00erentiable 7 Gradient-based \u2014\nCartella et al. ",
                    "Citation Text": "Francesco Cartella, Orlando Anunciacao, Yuki Funabiki, Daisuke Yamaguchi, Toru Akishita, and Olivier Elshocht.\nAdversarial attacks for tabular data: Application to fraud detection and imbalanced data. SafeAI Workshop at\nAAAI , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.08030",
                        "Citation Paper Title": "Title:Adversarial Attacks for Tabular Data: Application to Fraud Detection and Imbalanced Data",
                        "Citation Paper Abstract": "Abstract:Guaranteeing the security of transactional systems is a crucial priority of all institutions that process transactions, in order to protect their businesses against cyberattacks and fraudulent attempts. Adversarial attacks are novel techniques that, other than being proven to be effective to fool image classification models, can also be applied to tabular data. Adversarial attacks aim at producing adversarial examples, in other words, slightly modified inputs that induce the Artificial Intelligence (AI) system to return incorrect outputs that are advantageous for the attacker. In this paper we illustrate a novel approach to modify and adapt state-of-the-art algorithms to imbalanced tabular data, in the context of fraud detection. Experimental results show that the proposed modifications lead to a perfect attack success rate, obtaining adversarial examples that are also less perceptible when analyzed by humans. Moreover, when applied to a real-world production system, the proposed techniques shows the possibility of posing a serious threat to the robustness of advanced AI-based fraud detection procedures.",
                        "Citation Paper Authors": "Authors:Francesco Cartella, Orlando Anunciacao, Yuki Funabiki, Daisuke Yamaguchi, Toru Akishita, Olivier Elshocht"
                    }
                },
                {
                    "Sentence ID": 59,
                    "Sentence": "Per-feature constraints \u2014 \u2014 \u2014 \u2014 Tree-based\nCalzavara et al. ",
                    "Citation Text": "Stefano Calzavara, Claudio Lucchese, Gabriele Tolomei, Seyum Assefa Abebe, and Salvatore Orlando. Treant:\ntraining evasion-aware decision trees. Data Min. Knowl. Discov. , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.01197",
                        "Citation Paper Title": "Title:Treant: Training Evasion-Aware Decision Trees",
                        "Citation Paper Abstract": "Abstract:Despite its success and popularity, machine learning is now recognized as vulnerable to evasion attacks, i.e., carefully crafted perturbations of test inputs designed to force prediction errors. In this paper we focus on evasion attacks against decision tree ensembles, which are among the most successful predictive models for dealing with non-perceptual problems. Even though they are powerful and interpretable, decision tree ensembles have received only limited attention by the security and machine learning communities so far, leading to a sub-optimal state of the art for adversarial learning techniques. We thus propose Treant, a novel decision tree learning algorithm that, on the basis of a formal threat model, minimizes an evasion-aware loss function at each step of the tree construction. Treant is based on two key technical ingredients: robust splitting and attack invariance, which jointly guarantee the soundness of the learning process. Experimental results on three publicly available datasets show that Treant is able to generate decision tree ensembles that are at the same time accurate and nearly insensitive to evasion attacks, outperforming state-of-the-art adversarial learning techniques.",
                        "Citation Paper Authors": "Authors:Stefano Calzavara, Claudio Lucchese, Gabriele Tolomei, Seyum Assefa Abebe, Salvatore Orlando"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.03193v2": {
            "Paper Title": "Oblivious Median Slope Selection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.02927v3": {
            "Paper Title": "Unsupervised Machine Learning for Explainable Health Care Fraud\n  Detection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.12492v3": {
            "Paper Title": "Disrupting Adversarial Transferability in Deep Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.04680v2": {
            "Paper Title": "Near-Optimal Differentially Private Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.01984v3": {
            "Paper Title": "Dirichlet Mechanism for Differentially Private KL Divergence\n  Minimization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.04991v2": {
            "Paper Title": "Understanding User Awareness and Behaviors Concerning Encrypted DNS\n  Settings",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.00262v2": {
            "Paper Title": "Frequency Estimation of Evolving Data Under Local Differential Privacy",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": "for domain re-\nduction, which allows many values to collide (universal hashing\nproperty) and thus creates uncertainty about the user\u2019s actual\nvalue. Indeed, LH protocols are the least attackable LDP protocols\nin the recent studies of Arcolezi et al. ",
                    "Citation Text": "H\u00e9ber H. Arcolezi, S\u00e9bastien Gambs, Jean-Fran\u00e7ois Couchot, and Catuscia\nPalamidessi. 2022. On the Risks of Collecting Multidimensional Data Under\nLocal Differential Privacy. arXiv preprint arXiv:2209.01684 (2022).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2209.01684",
                        "Citation Paper Title": "Title:On the Risks of Collecting Multidimensional Data Under Local Differential Privacy",
                        "Citation Paper Abstract": "Abstract:The private collection of multiple statistics from a population is a fundamental statistical problem. One possible approach to realize this is to rely on the local model of differential privacy (LDP). Numerous LDP protocols have been developed for the task of frequency estimation of single and multiple attributes. These studies mainly focused on improving the utility of the algorithms to ensure the server performs the estimations accurately. In this paper, we investigate privacy threats (re-identification and attribute inference attacks) against LDP protocols for multidimensional data following two state-of-the-art solutions for frequency estimation of multiple attributes. To broaden the scope of our study, we have also experimentally assessed five widely used LDP protocols, namely, generalized randomized response, optimal local hashing, subset selection, RAPPOR and optimal unary encoding. Finally, we also proposed a countermeasure that improves both utility and robustness against the identified threats. Our contributions can help practitioners aiming to collect users' statistics privately to decide which LDP mechanism best fits their needs.",
                        "Citation Paper Authors": "Authors:H\u00e9ber H. Arcolezi, S\u00e9bastien Gambs, Jean-Fran\u00e7ois Couchot, Catuscia Palamidessi"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "generalizes this framework for optimally chaining two LDP pro-\ntocols, proposing the L-GRR protocol that is optimized for small\ndomain size \ud835\udc58and the L-OSUE protocol for higher \ud835\udc58(see Figs. 2\nand 3). Moreover, Erlingsson et al. ",
                    "Citation Text": "\u00dalfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth Raghunathan, Shuang\nSong, Kunal Talwar, and Abhradeep Thakurta. 2020. Encode, shuffle, analyze\nprivacy revisited: Formalizations and empirical evaluation. arXiv preprint\narXiv:2001.03618 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2001.03618",
                        "Citation Paper Title": "Title:Encode, Shuffle, Analyze Privacy Revisited: Formalizations and Empirical Evaluation",
                        "Citation Paper Abstract": "Abstract:Recently, a number of approaches and techniques have been introduced for reporting software statistics with strong privacy guarantees. These range from abstract algorithms to comprehensive systems with varying assumptions and built upon local differential privacy mechanisms and anonymity. Based on the Encode-Shuffle-Analyze (ESA) framework, notable results formally clarified large improvements in privacy guarantees without loss of utility by making reports anonymous. However, these results either comprise of systems with seemingly disparate mechanisms and attack models, or formal statements with little guidance to practitioners. Addressing this, we provide a formal treatment and offer prescriptive guidelines for privacy-preserving reporting with anonymity. We revisit the ESA framework with a simple, abstract model of attackers as well as assumptions covering it and other proposed systems of anonymity. In light of new formal privacy bounds, we examine the limitations of sketch-based encodings and ESA mechanisms such as data-dependent crowds. We also demonstrate how the ESA notion of fragmentation (reporting data aspects in separate, unlinkable messages) improves privacy/utility tradeoffs both in terms of local and central differential-privacy guarantees. Finally, to help practitioners understand the applicability and limitations of privacy-preserving reporting, we report on a large number of empirical experiments. We use real-world datasets with heavy-tailed or near-flat distributions, which pose the greatest difficulty for our techniques; in particular, we focus on data drawn from images that can be easily visualized in a way that highlights reconstruction errors. Showing the promise of the approach, and of independent interest, we also report on experiments using anonymous, privacy-preserving reporting to train high-accuracy deep neural networks on standard tasks---MNIST and CIFAR-10.",
                        "Citation Paper Authors": "Authors:\u00dalfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth Raghunathan, Shuang Song, Kunal Talwar, Abhradeep Thakurta"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "proposed the RAPPOR algorithm for\nfrequency monitoring that is based on the memoization solution\ndescribed in Section 2.4. The recent study of Arcolezi et al. ",
                    "Citation Text": "H\u00e9ber H. Arcolezi, Jean-Fran\u00e7ois Couchot, Bechara Al Bouna, and Xiaokui\nXiao. 2022. Improving the utility of locally differentially private protocols for\nlongitudinal and multidimensional frequency estimates. Digital Communica-\ntions and Networks (2022). https://doi.org/10.1016/j.dcan.2022.07.003",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.04636",
                        "Citation Paper Title": "Title:Improving the utility of locally differentially private protocols for longitudinal and multidimensional frequency estimates",
                        "Citation Paper Abstract": "Abstract:This paper investigates the problem of collecting multidimensional data throughout time (i.e., longitudinal studies) for the fundamental task of frequency estimation under Local Differential Privacy (LDP) guarantees. Contrary to frequency estimation of a single attribute, the multidimensional aspect demands particular attention to the privacy budget. Besides, when collecting user statistics longitudinally, privacy progressively degrades. Indeed, the \"multiple\" settings in combination (i.e., many attributes and several collections throughout time) impose several challenges, for which this paper proposes the first solution for frequency estimates under LDP. To tackle these issues, we extend the analysis of three state-of-the-art LDP protocols (Generalized Randomized Response -- GRR, Optimized Unary Encoding -- OUE, and Symmetric Unary Encoding -- SUE) for both longitudinal and multidimensional data collections. While the known literature uses OUE and SUE for two rounds of sanitization (a.k.a. memoization), i.e., L-OUE and L-SUE, respectively, we analytically and experimentally show that starting with OUE and then with SUE provides higher data utility (i.e., L-OSUE). Also, for attributes with small domain sizes, we propose Longitudinal GRR (L-GRR), which provides higher utility than the other protocols based on unary encoding. Last, we also propose a new solution named Adaptive LDP for LOngitudinal and Multidimensional FREquency Estimates (ALLOMFREE), which randomly samples a single attribute to be sent with the whole privacy budget and adaptively selects the optimal protocol, i.e., either L-GRR or L-OSUE. As shown in the results, ALLOMFREE consistently and considerably outperforms the state-of-the-art L-SUE and L-OUE protocols in the quality of the frequency estimates.",
                        "Citation Paper Authors": "Authors:H\u00e9ber H. Arcolezi, Jean-Fran\u00e7ois Couchot, Bechara Al Bouna, Xiaokui Xiao"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.00230v3": {
            "Paper Title": "Trading Off Privacy, Utility and Efficiency in Federated Learning",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": ". DLI is tailored to the VFL scenario where the\ntop model owned by client 2 is an activation function (e.g., softmax) ",
                    "Citation Text": "Yang Liu, Yan Kang, Tianyuan Zou, Yanhong Pu, Yuanqin He, Xiaozhou Ye, Ye Ouyang,\nYa-Qin Zhang, and Qiang Yang. Vertical federated learning. arXiv preprint arXiv:2211.12814,\n2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2211.12814",
                        "Citation Paper Title": "Title:Vertical Federated Learning: Concepts, Advances and Challenges",
                        "Citation Paper Abstract": "Abstract:Vertical Federated Learning (VFL) is a federated learning setting where multiple parties with different features about the same set of users jointly train machine learning models without exposing their raw data or model parameters. Motivated by the rapid growth in VFL research and real-world applications, we provide a comprehensive review of the concept and algorithms of VFL, as well as current advances and challenges in various aspects, including effectiveness, efficiency, and privacy. We provide an exhaustive categorization for VFL settings and privacy-preserving protocols and comprehensively analyze the privacy attacks and defense strategies for each protocol. In the end, we propose a unified framework, termed VFLow, which considers the VFL problem under communication, computation, privacy, as well as effectiveness and fairness constraints. Finally, we review the most recent advances in industrial applications, highlighting open challenges and future directions for VFL.",
                        "Citation Paper Authors": "Authors:Yang Liu, Yan Kang, Tianyuan Zou, Yanhong Pu, Yuanqin He, Xiaozhou Ye, Ye Ouyang, Ya-Qin Zhang, Qiang Yang"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "proposed a quantitative trade-o\u000b between utility and privacy in horizontal feder-\nated learning by exploiting some key properties of the privacy leakage and the triangle inequality of\nthe divergence. ",
                    "Citation Text": "Hanlin Lu, Changchang Liu, Ting He, Shiqiang Wang, and Kevin S Chan. Sharing models\nor coresets: A study based on membership inference attack. arXiv preprint arXiv:2007.02977,\n2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.02977",
                        "Citation Paper Title": "Title:Sharing Models or Coresets: A Study based on Membership Inference Attack",
                        "Citation Paper Abstract": "Abstract:Distributed machine learning generally aims at training a global model based on distributed data without collecting all the data to a centralized location, where two different approaches have been proposed: collecting and aggregating local models (federated learning) and collecting and training over representative data summaries (coreset). While each approach preserves data privacy to some extent thanks to not sharing the raw data, the exact extent of protection is unclear under sophisticated attacks that try to infer the raw data from the shared information. We present the first comparison between the two approaches in terms of target model accuracy, communication cost, and data privacy, where the last is measured by the accuracy of a state-of-the-art attack strategy called the membership inference attack. Our experiments quantify the accuracy-privacy-cost tradeoff of each approach, and reveal a nontrivial comparison that can be used to guide the design of model training processes.",
                        "Citation Paper Authors": "Authors:Hanlin Lu, Changchang Liu, Ting He, Shiqiang Wang, Kevin S. Chan"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "analyzed\nthe trade-o\u000b between the speed of error convergence and the wall-clock time for distributed SGD. ",
                    "Citation Text": "Wei-Ning Chen, Peter Kairouz, and Ayfer Ozgur. Breaking the communication-privacy-\naccuracy trilemma. Advances inNeural Information Processing Systems, 33:3312{3324, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.11707",
                        "Citation Paper Title": "Title:Breaking the Communication-Privacy-Accuracy Trilemma",
                        "Citation Paper Abstract": "Abstract:Two major challenges in distributed learning and estimation are 1) preserving the privacy of the local samples; and 2) communicating them efficiently to a central server, while achieving high accuracy for the end-to-end task. While there has been significant interest in addressing each of these challenges separately in the recent literature, treatments that simultaneously address both challenges are still largely missing. In this paper, we develop novel encoding and decoding mechanisms that simultaneously achieve optimal privacy and communication efficiency in various canonical settings.\nIn particular, we consider the problems of mean estimation and frequency estimation under $\\varepsilon$-local differential privacy and $b$-bit communication constraints. For mean estimation, we propose a scheme based on Kashin's representation and random sampling, with order-optimal estimation error under both constraints. For frequency estimation, we present a mechanism that leverages the recursive structure of Walsh-Hadamard matrices and achieves order-optimal estimation error for all privacy levels and communication budgets. As a by-product, we also construct a distribution estimation mechanism that is rate-optimal for all privacy regimes and communication constraints, extending recent work that is limited to $b=1$ and $\\varepsilon=O(1)$. Our results demonstrate that intelligent encoding under joint privacy and communication constraints can yield a performance that matches the optimal accuracy achievable under either constraint alone.",
                        "Citation Paper Authors": "Authors:Wei-Ning Chen, Peter Kairouz, Ayfer \u00d6zg\u00fcr"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "illustrated that the optimal privacy-utility trade-o\u000b\ncould be solved using a standard linear program and provided a closed-form solution for the special\ncase when the data to be released is a binary variable. ",
                    "Citation Text": "Weina Wang, Lei Ying, and Junshan Zhang. On the relation between identi\fability, di\u000ber-\nential privacy, and mutual-information privacy. IEEE Transactions onInformation Theory,\n62(9):5018{5029, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1402.3757",
                        "Citation Paper Title": "Title:On the Relation Between Identifiability, Differential Privacy and Mutual-Information Privacy",
                        "Citation Paper Abstract": "Abstract:This paper investigates the relation between three different notions of privacy: identifiability, differential privacy and mutual-information privacy. Under a unified privacy-distortion framework, where the distortion is defined to be the Hamming distance of the input and output databases, we establish some fundamental connections between these three privacy notions. Given a distortion level $D$, define $\\epsilon_{\\mathrm{i}}^*(D)$ to be the smallest (best) identifiability level, and $\\epsilon_{\\mathrm{d}}^*(D)$ to be the smallest differential privacy level. We characterize $\\epsilon_{\\mathrm{i}}^*(D)$ and $\\epsilon_{\\mathrm{d}}^*(D)$, and prove that $\\epsilon_{\\mathrm{i}}^*(D)-\\epsilon_X\\le\\epsilon_{\\mathrm{d}}^*(D)\\le\\epsilon_{\\mathrm{i}}^*(D)$ for $D$ in some range, where $\\epsilon_X$ is a constant depending on the distribution of the original database $X$, and diminishes to zero when the distribution of $X$ is uniform. Furthermore, we show that identifiability and mutual-information privacy are consistent in the sense that given distortion level $D$, the mechanism that optimizes the mutual-information privacy also minimizes the identifiability level.",
                        "Citation Paper Authors": "Authors:Weina Wang, Lei Ying, Junshan Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.05980v2": {
            "Paper Title": "Certified Defences Against Adversarial Patch Attacks on Semantic\n  Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.07598v4": {
            "Paper Title": "Internal Wasserstein Distance for Adversarial Attack and Defense",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": "present a projected gradient descent (PGD) training method to improve the resistance of the\nmodel to a wide range of attacks. Unfortunately, training with PGD is time-consuming and computationally intensive ",
                    "Citation Text": "Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S Davis, Gavin\nTaylor, and Tom Goldstein. Adversarial training for free! In Advances in Neural Information Processing Systems ,\n2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.12843",
                        "Citation Paper Title": "Title:Adversarial Training for Free!",
                        "Citation Paper Abstract": "Abstract:Adversarial training, in which a network is trained on adversarial examples, is one of the few defenses against adversarial attacks that withstands strong attacks. Unfortunately, the high cost of generating strong adversarial examples makes standard adversarial training impractical on large-scale problems like ImageNet. We present an algorithm that eliminates the overhead cost of generating adversarial examples by recycling the gradient information computed when updating model parameters. Our \"free\" adversarial training algorithm achieves comparable robustness to PGD adversarial training on the CIFAR-10 and CIFAR-100 datasets at negligible additional cost compared to natural training, and can be 7 to 30 times faster than other strong adversarial training methods. Using a single workstation with 4 P100 GPUs and 2 days of runtime, we can train a robust model for the large-scale ImageNet classification task that maintains 40% accuracy against PGD attacks. The code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S. Davis, Gavin Taylor, Tom Goldstein"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "has been proposed to measure the differences between two given distributions\nand widely used in generated adversarial networks (GANs) ",
                    "Citation Text": "Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein generative adversarial networks. In Interna-\ntional Conference on Machine Learning , pages 214\u2013223, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1701.07875",
                        "Citation Paper Title": "Title:Wasserstein GAN",
                        "Citation Paper Abstract": "Abstract:We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.",
                        "Citation Paper Authors": "Authors:Martin Arjovsky, Soumith Chintala, L\u00e9on Bottou"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "propose an ADef algorithm\nto craft adversarial examples by deforming images iteratively. In addition, ",
                    "Citation Text": "Eric Wong, Frank Schmidt, and Zico Kolter. Wasserstein adversarial examples via projected sinkhorn iterations.\nInInternational Conference on Machine Learning , pages 6808\u20136817. PMLR, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.07906",
                        "Citation Paper Title": "Title:Wasserstein Adversarial Examples via Projected Sinkhorn Iterations",
                        "Citation Paper Abstract": "Abstract:A rapidly growing area of work has studied the existence of adversarial examples, datapoints which have been perturbed to fool a classifier, but the vast majority of these works have focused primarily on threat models defined by $\\ell_p$ norm-bounded perturbations. In this paper, we propose a new threat model for adversarial attacks based on the Wasserstein distance. In the image classification setting, such distances measure the cost of moving pixel mass, which naturally cover \"standard\" image manipulations such as scaling, rotation, translation, and distortion (and can potentially be applied to other settings as well). To generate Wasserstein adversarial examples, we develop a procedure for projecting onto the Wasserstein ball, based upon a modified version of the Sinkhorn iteration. The resulting algorithm can successfully attack image classification models, bringing traditional CIFAR10 models down to 3% accuracy within a Wasserstein ball with radius 0.1 (i.e., moving 10% of the image mass 1 pixel), and we demonstrate that PGD-based adversarial training can improve this adversarial accuracy to 76%. In total, this work opens up a new direction of study in adversarial robustness, more formally considering convex metrics that accurately capture the invariances that we typically believe should exist in classifiers. Code for all experiments in the paper is available at this https URL.",
                        "Citation Paper Authors": "Authors:Eric Wong, Frank R. Schmidt, J. Zico Kolter"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "introduce a\nNuclear-Norm regularizer to enforce the function smoothing in the vicinity of data samples to achieve an ef\ufb01cient and\neffective adversarial training method. Moreover, ",
                    "Citation Text": "Alexander Robey, Luiz F. O. Chamon, George J. Pappas, Hamed Hassani, and Alejandro Ribeiro. Adversarial\nrobustness with semi-in\ufb01nite constrained learning. In Advances in Neural Information Processing Systems , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.15767",
                        "Citation Paper Title": "Title:Adversarial Robustness with Semi-Infinite Constrained Learning",
                        "Citation Paper Abstract": "Abstract:Despite strong performance in numerous applications, the fragility of deep learning to input perturbations has raised serious questions about its use in safety-critical domains. While adversarial training can mitigate this issue in practice, state-of-the-art methods are increasingly application-dependent, heuristic in nature, and suffer from fundamental trade-offs between nominal performance and robustness. Moreover, the problem of finding worst-case perturbations is non-convex and underparameterized, both of which engender a non-favorable optimization landscape. Thus, there is a gap between the theory and practice of adversarial training, particularly with respect to when and why adversarial training works. In this paper, we take a constrained learning approach to address these questions and to provide a theoretical foundation for robust learning. In particular, we leverage semi-infinite optimization and non-convex duality theory to show that adversarial training is equivalent to a statistical problem over perturbation distributions, which we characterize completely. Notably, we show that a myriad of previous robust training techniques can be recovered for particular, sub-optimal choices of these distributions. Using these insights, we then propose a hybrid Langevin Monte Carlo approach of which several common algorithms (e.g., PGD) are special cases. Finally, we show that our approach can mitigate the trade-off between nominal and robust performance, yielding state-of-the-art results on MNIST and CIFAR-10. Our code is available at: this https URL.",
                        "Citation Paper Authors": "Authors:Alexander Robey, Luiz F. O. Chamon, George J. Pappas, Hamed Hassani, Alejandro Ribeiro"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "2.1 Adversarial Attack\nAttack with the `pperturbation. DNNs are vulnerable to adversarial examples with a perceptible perturbation ",
                    "Citation Text": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob\nFergus. Intriguing properties of neural networks. In International Conference on Learning Representations , 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1312.6199",
                        "Citation Paper Title": "Title:Intriguing properties of neural networks",
                        "Citation Paper Abstract": "Abstract:Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.\nFirst, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks.\nSecond, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
                        "Citation Paper Authors": "Authors:Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.14462v4": {
            "Paper Title": "What Can Cryptography Do For Decentralized Mechanism Design",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.08461v2": {
            "Paper Title": "Differentially Private Bayesian Neural Networks on Accuracy, Privacy and\n  Reliability",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.08723v3": {
            "Paper Title": "Private Data Valuation and Fair Payment in Data Marketplaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.05586v3": {
            "Paper Title": "Multi-Factor Key Derivation Function (MFKDF) for Fast, Flexible, Secure,\n  & Practical Key Management",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.04558v2": {
            "Paper Title": "SoK: Anti-Facial Recognition Technology",
            "Sentences": [
                {
                    "Sentence ID": 22,
                    "Sentence": "2021 4 Both Digital UT AZ Collectively corrupts features of faces\nLowKey ",
                    "Citation Text": "V . Cherepanova, M. Goldblum, H. Foley, S. Duan, J. P.\nDickerson, G. Taylor, and T. Goldstein, \u201cLowKey: Lever-\naging Adversarial Attacks to Protect Social Media Users\nfrom Facial Recognition,\u201d in Proc. of ICLR , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.07922",
                        "Citation Paper Title": "Title:LowKey: Leveraging Adversarial Attacks to Protect Social Media Users from Facial Recognition",
                        "Citation Paper Abstract": "Abstract:Facial recognition systems are increasingly deployed by private corporations, government agencies, and contractors for consumer services and mass surveillance programs alike. These systems are typically built by scraping social media profiles for user images. Adversarial perturbations have been proposed for bypassing facial recognition systems. However, existing methods fail on full-scale systems and commercial APIs. We develop our own adversarial filter that accounts for the entire image processing pipeline and is demonstrably effective against industrial-grade pipelines that include face detection and large scale databases. Additionally, we release an easy-to-use webtool that significantly degrades the accuracy of Amazon Rekognition and the Microsoft Azure Face Recognition API, reducing the accuracy of each to below 1%.",
                        "Citation Paper Authors": "Authors:Valeriia Cherepanova, Micah Goldblum, Harrison Foley, Shiyuan Duan, John Dickerson, Gavin Taylor, Tom Goldstein"
                    }
                },
                {
                    "Sentence ID": 86,
                    "Sentence": "2021 2b BB Digital UT - GAN-based face blurring (imperceptible)\nEvtimov et al. ",
                    "Citation Text": "I. Evtimov, I. Covert, A. Kusupati, and T. Kohno, \u201cDis-\nrupting model training with adversarial shortcuts,\u201d arXiv\npreprint arXiv:2106.06654 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.06654",
                        "Citation Paper Title": "Title:Disrupting Model Training with Adversarial Shortcuts",
                        "Citation Paper Abstract": "Abstract:When data is publicly released for human consumption, it is unclear how to prevent its unauthorized usage for machine learning purposes. Successful model training may be preventable with carefully designed dataset modifications, and we present a proof-of-concept approach for the image classification setting. We propose methods based on the notion of adversarial shortcuts, which encourage models to rely on non-robust signals rather than semantic features, and our experiments demonstrate that these measures successfully prevent deep learning models from achieving high accuracy on real, unmodified data examples.",
                        "Citation Paper Authors": "Authors:Ivan Evtimov, Ian Covert, Aditya Kusupati, Tadayoshi Kohno"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "2019 2b BB Digital UT - GAN-based face blurring (perceptible)\nIdentityDP ",
                    "Citation Text": "Y . Wen, L. Song, B. Liu, M. Ding, and R. Xie, \u201cIdenti-\ntyDP: Differential Private Identi\ufb01cation Protection for F ace\nImages,\u201d arXiv preprint arXiv:2103.01745 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.01745",
                        "Citation Paper Title": "Title:IdentityDP: Differential Private Identification Protection for Face Images",
                        "Citation Paper Abstract": "Abstract:Because of the explosive growth of face photos as well as their widespread dissemination and easy accessibility in social media, the security and privacy of personal identity information becomes an unprecedented challenge. Meanwhile, the convenience brought by advanced identity-agnostic computer vision technologies is attractive. Therefore, it is important to use face images while taking careful consideration in protecting people's identities. Given a face image, face de-identification, also known as face anonymization, refers to generating another image with similar appearance and the same background, while the real identity is hidden. Although extensive efforts have been made, existing face de-identification techniques are either insufficient in photo-reality or incapable of well-balancing privacy and utility. In this paper, we focus on tackling these challenges to improve face de-identification. We propose IdentityDP, a face anonymization framework that combines a data-driven deep neural network with a differential privacy (DP) mechanism. This framework encompasses three stages: facial representations disentanglement, $\\epsilon$-IdentityDP perturbation and image reconstruction. Our model can effectively obfuscate the identity-related information of faces, preserve significant visual similarity, and generate high-quality images that can be used for identity-agnostic computer vision tasks, such as detection, tracking, etc. Different from the previous methods, we can adjust the balance of privacy and utility through the privacy budget according to pratical demands and provide a diversity of results without pre-annotations. Extensive experiments demonstrate the effectiveness and generalization ability of our proposed anonymization framework.",
                        "Citation Paper Authors": "Authors:Yunqian Wen, Li Song, Bo Liu, Ming Ding, Rong Xie"
                    }
                },
                {
                    "Sentence ID": 97,
                    "Sentence": "2021 5 WB Digital UT - Brightness-agnostic adversarial perturbations\nYang et al ",
                    "Citation Text": "X. Yang, Y . Dong, T. Pang, H. Su, J. Zhu, Y . Chen, and\nH. Xue, \u201cTowards Face Encryption by Generating Adver-\nsarial Identity Masks,\u201d in Proc. of ICCV , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.06814",
                        "Citation Paper Title": "Title:Towards Face Encryption by Generating Adversarial Identity Masks",
                        "Citation Paper Abstract": "Abstract:As billions of personal data being shared through social media and network, the data privacy and security have drawn an increasing attention. Several attempts have been made to alleviate the leakage of identity information from face photos, with the aid of, e.g., image obfuscation techniques. However, most of the present results are either perceptually unsatisfactory or ineffective against face recognition systems. Our goal in this paper is to develop a technique that can encrypt the personal photos such that they can protect users from unauthorized face recognition systems but remain visually identical to the original version for human beings. To achieve this, we propose a targeted identity-protection iterative method (TIP-IM) to generate adversarial identity masks which can be overlaid on facial images, such that the original identities can be concealed without sacrificing the visual quality. Extensive experiments demonstrate that TIP-IM provides 95\\%+ protection success rate against various state-of-the-art face recognition models under practical test scenarios. Besides, we also show the practical and effective applicability of our method on a commercial API service.",
                        "Citation Paper Authors": "Authors:Xiao Yang, Yinpeng Dong, Tianyu Pang, Hang Su, Jun Zhu, Yuefeng Chen, Hui Xue"
                    }
                },
                {
                    "Sentence ID": 96,
                    "Sentence": "2020 5 BB Digital Both AR, AZ, F++ Study on user perception on perturbation levels.\nSingh et al. ",
                    "Citation Text": "I. Singh, S. Momiyama, K. Kakizaki, and T. Araki, \u201cOn\nBrightness Agnostic Adversarial Examples Against FaceRecognition Systems,\u201d in BIOSIG . IEEE, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.14205",
                        "Citation Paper Title": "Title:On Brightness Agnostic Adversarial Examples Against Face Recognition Systems",
                        "Citation Paper Abstract": "Abstract:This paper introduces a novel adversarial example generation method against face recognition systems (FRSs). An adversarial example (AX) is an image with deliberately crafted noise to cause incorrect predictions by a target system. The AXs generated from our method remain robust under real-world brightness changes. Our method performs non-linear brightness transformations while leveraging the concept of curriculum learning during the attack generation procedure. We demonstrate that our method outperforms conventional techniques from comprehensive experimental investigations in the digital and physical world. Furthermore, this method enables practical risk assessment of FRSs against brightness agnostic AXs.",
                        "Citation Paper Authors": "Authors:Inderjeet Singh, Satoru Momiyama, Kazuya Kakizaki, Toshinori Araki"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "2019 5 BB Physical Both - Projected light patterns\nNguyen et al. ",
                    "Citation Text": "D.-L. Nguyen, S. S. Arora, Y . Wu, and H. Yang, \u201cAdver-\nsarial light projection attacks on face recognition system s:\nA feasibility study,\u201d in Proc. of CVPR , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.11145",
                        "Citation Paper Title": "Title:Adversarial Light Projection Attacks on Face Recognition Systems: A Feasibility Study",
                        "Citation Paper Abstract": "Abstract:Deep learning-based systems have been shown to be vulnerable to adversarial attacks in both digital and physical domains. While feasible, digital attacks have limited applicability in attacking deployed systems, including face recognition systems, where an adversary typically has access to the input and not the transmission channel. In such setting, physical attacks that directly provide a malicious input through the input channel pose a bigger threat. We investigate the feasibility of conducting real-time physical attacks on face recognition systems using adversarial light projections. A setup comprising a commercially available web camera and a projector is used to conduct the attack. The adversary uses a transformation-invariant adversarial pattern generation method to generate a digital adversarial pattern using one or more images of the target available to the adversary. The digital adversarial pattern is then projected onto the adversary's face in the physical domain to either impersonate a target (impersonation) or evade recognition (obfuscation). We conduct preliminary experiments using two open-source and one commercial face recognition system on a pool of 50 subjects. Our experimental results demonstrate the vulnerability of face recognition systems to light projection attacks in both white-box and black-box attack settings.",
                        "Citation Paper Authors": "Authors:Dinh-Luan Nguyen, Sunpreet S. Arora, Yuhang Wu, Hao Yang"
                    }
                },
                {
                    "Sentence ID": 92,
                    "Sentence": "2018 5 WB Physical Both - Projected adversarial IR patterns\nDong et al. ",
                    "Citation Text": "Y . Dong, H. Su, B. Wu, Z. Li, W. Liu, T. Zhang, and J. Zhu,\n\u201cEf\ufb01cient decision-based black-box adversarial attacks o n\nface recognition,\u201d in Proc. of CVPR , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.04433",
                        "Citation Paper Title": "Title:Efficient Decision-based Black-box Adversarial Attacks on Face Recognition",
                        "Citation Paper Abstract": "Abstract:Face recognition has obtained remarkable progress in recent years due to the great improvement of deep convolutional neural networks (CNNs). However, deep CNNs are vulnerable to adversarial examples, which can cause fateful consequences in real-world face recognition applications with security-sensitive purposes. Adversarial attacks are widely studied as they can identify the vulnerability of the models before they are deployed. In this paper, we evaluate the robustness of state-of-the-art face recognition models in the decision-based black-box attack setting, where the attackers have no access to the model parameters and gradients, but can only acquire hard-label predictions by sending queries to the target model. This attack setting is more practical in real-world face recognition systems. To improve the efficiency of previous methods, we propose an evolutionary attack algorithm, which can model the local geometries of the search directions and reduce the dimension of the search space. Extensive experiments demonstrate the effectiveness of the proposed method that induces a minimum perturbation to an input face image with fewer queries. We also apply the proposed method to attack a real-world face recognition system successfully.",
                        "Citation Paper Authors": "Authors:Yinpeng Dong, Hang Su, Baoyuan Wu, Zhifeng Li, Wei Liu, Tong Zhang, Jun Zhu"
                    }
                },
                {
                    "Sentence ID": 91,
                    "Sentence": "2018 5 WB Digital UT - Adversarial attack distorts face landmarks.\nZhou et al. ",
                    "Citation Text": "Z. Zhou, D. Tang, X. Wang, W. Han, X. Liu, and K. Zhang,\n\u201cInvisible mask: Practical attacks on face recognition wit h\ninfrared,\u201d arXiv preprint arXiv:1803.04683 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.04683",
                        "Citation Paper Title": "Title:Invisible Mask: Practical Attacks on Face Recognition with Infrared",
                        "Citation Paper Abstract": "Abstract:Accurate face recognition techniques make a series of critical applications possible: policemen could employ it to retrieve criminals' faces from surveillance video streams; cross boarder travelers could pass a face authentication inspection line without the involvement of officers. Nonetheless, when public security heavily relies on such intelligent systems, the designers should deliberately consider the emerging attacks aiming at misleading those systems employing face recognition.\nWe propose a kind of brand new attack against face recognition systems, which is realized by illuminating the subject using infrared according to the adversarial examples worked out by our algorithm, thus face recognition systems can be bypassed or misled while simultaneously the infrared perturbations cannot be observed by raw eyes. Through launching this kind of attack, an attacker not only can dodge surveillance cameras. More importantly, he can impersonate his target victim and pass the face authentication system, if only the victim's photo is acquired by the attacker. Again, the attack is totally unobservable by nearby people, because not only the light is invisible, but also the device we made to launch the attack is small enough. According to our study on a large dataset, attackers have a very high success rate with a over 70\\% success rate for finding such an adversarial example that can be implemented by infrared. To the best of our knowledge, our work is the first one to shed light on the severity of threat resulted from infrared adversarial examples against face recognition.",
                        "Citation Paper Authors": "Authors:Zhe Zhou, Di Tang, Xiaofeng Wang, Weili Han, Xiangyu Liu, Kehuan Zhang"
                    }
                },
                {
                    "Sentence ID": 87,
                    "Sentence": "2021 3 BB Digital UT - Data poison by user coordination\nFu et al. ",
                    "Citation Text": "S. Fu, F. He, Y . Liu, L. Shen, and D. Tao, \u201cRobust unlearn-\nable examples: Protecting data privacy against adversaria l\nlearning,\u201d in Proc. of ICLR , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.14533",
                        "Citation Paper Title": "Title:Robust Unlearnable Examples: Protecting Data Against Adversarial Learning",
                        "Citation Paper Abstract": "Abstract:The tremendous amount of accessible data in cyberspace face the risk of being unauthorized used for training deep learning models. To address this concern, methods are proposed to make data unlearnable for deep learning models by adding a type of error-minimizing noise. However, such conferred unlearnability is found fragile to adversarial training. In this paper, we design new methods to generate robust unlearnable examples that are protected from adversarial training. We first find that the vanilla error-minimizing noise, which suppresses the informative knowledge of data via minimizing the corresponding training loss, could not effectively minimize the adversarial training loss. This explains the vulnerability of error-minimizing noise in adversarial training. Based on the observation, robust error-minimizing noise is then introduced to reduce the adversarial training loss. Experiments show that the unlearnability brought by robust error-minimizing noise can effectively protect data from adversarial training in various scenarios. The code is available at \\url{this https URL}.",
                        "Citation Paper Authors": "Authors:Shaopeng Fu, Fengxiang He, Yang Liu, Li Shen, Dacheng Tao"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "2021 2a WB Digital UT - Adversarial patch on object detectors\nTreu et al. ",
                    "Citation Text": "M. Treu, T.-N. Le, H. H. Nguyen, J. Yamagishi, andI. Echizen, \u201cFashion-Guided Adversarial Attack on Person\nSegmentation,\u201d in Proc. of CVPR , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.08422",
                        "Citation Paper Title": "Title:Fashion-Guided Adversarial Attack on Person Segmentation",
                        "Citation Paper Abstract": "Abstract:This paper presents the first adversarial example based method for attacking human instance segmentation networks, namely person segmentation networks in short, which are harder to fool than classification networks. We propose a novel Fashion-Guided Adversarial Attack (FashionAdv) framework to automatically identify attackable regions in the target image to minimize the effect on image quality. It generates adversarial textures learned from fashion style images and then overlays them on the clothing regions in the original image to make all persons in the image invisible to person segmentation networks. The synthesized adversarial textures are inconspicuous and appear natural to the human eye. The effectiveness of the proposed method is enhanced by robustness training and by jointly attacking multiple components of the target network. Extensive experiments demonstrated the effectiveness of FashionAdv in terms of robustness to image manipulations and storage in cyberspace as well as appearing natural to the human eye. The code and data are publicly released on our project page this https URL",
                        "Citation Paper Authors": "Authors:Marc Treu, Trung-Nghia Le, Huy H. Nguyen, Junichi Yamagishi, Isao Echizen"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "2020 2a BB Physical UT YOLOv2 Adversarial patch on T-shirts\nWu et al. ",
                    "Citation Text": "Z. Wu, S.-N. Lim, L. S. Davis, and T. Goldstein, \u201cMaking a n\ninvisibility cloak: Real world adversarial attacks on obje ct\ndetectors,\u201d in Proc. of ECCV , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.14667",
                        "Citation Paper Title": "Title:Making an Invisibility Cloak: Real World Adversarial Attacks on Object Detectors",
                        "Citation Paper Abstract": "Abstract:We present a systematic study of adversarial attacks on state-of-the-art object detection frameworks. Using standard detection datasets, we train patterns that suppress the objectness scores produced by a range of commonly used detectors, and ensembles of detectors. Through extensive experiments, we benchmark the effectiveness of adversarially trained patches under both white-box and black-box settings, and quantify transferability of attacks between datasets, object classes, and detector models. Finally, we present a detailed study of physical world attacks using printed posters and wearable clothes, and rigorously quantify the performance of such attacks with different metrics.",
                        "Citation Paper Authors": "Authors:Zuxuan Wu, Ser-Nam Lim, Larry Davis, Tom Goldstein"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.13386v2": {
            "Paper Title": "Contraction of Locally Differentially Private Mechanisms",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.08676v3": {
            "Paper Title": "Influence of a Set of Variables on a Boolean Function",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.12154v5": {
            "Paper Title": "Towards Effective and Robust Neural Trojan Defenses via Input Filtering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.13350v3": {
            "Paper Title": "Intractable Group-theoretic Problems Around Zero-knowledge Proofs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.03205v5": {
            "Paper Title": "Synthetic Dataset Generation for Privacy-Preserving Machine Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.13926v5": {
            "Paper Title": "Cloak: Transitioning States on Legacy Blockchains Using Secure and\n  Publicly Verifiable Off-Chain Multi-Party Computation",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": "does not consider confidentiality. For negotiation, both\nFastkitten and Speedster ",
                    "Citation Text": "Jinghui Liao, Fengwei Zhang, Wenhai Sun, and Weisong Shi. 2022. Speedster:\nAn Efficient Multi-party State Channel via Enclaves. In Proceedings of the 2022\nACM on Asia Conference on Computer and Communications Security . 637\u2013651.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.01289",
                        "Citation Paper Title": "Title:Speedster: A TEE-assisted State Channel System",
                        "Citation Paper Abstract": "Abstract:State channel network is the most popular layer-2 solution to theissues of scalability, high transaction fees, and low transaction throughput of public Blockchain networks. However, the existing works have limitations that curb the wide adoption of the technology, such as the expensive creation and closure of channels, strict synchronization between the main chain and off-chain channels, frozen deposits, and inability to execute multi-party smart contracts. In this work, we present Speedster, an account-based state-channel system that aims to address the above issues. To this end, Speedster leverages the latest development of secure hardware to create dispute-free certified channels that can be operated efficiently off the Blockchain. Speedster is fully decentralized and provides better privacy protection. It supports fast native multi-party contract execution, which is missing in prior TEE-enabled channel networks. Compared to the Lightning Network, Speedster improves the throughput by about 10,000X and generates 97% less on-chain data with a comparable network scale.",
                        "Citation Paper Authors": "Authors:Jinghui Liao, Fengwei Zhang, Wenhai Sun, Weisong Shi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.11156v2": {
            "Paper Title": "Squeeze Training for Adversarial Robustness",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.12063v2": {
            "Paper Title": "Generalized Private Selection and Testing with High Confidence",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.07478v2": {
            "Paper Title": "Estimating Patch Propagation Times across (Blockchain) Forks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.14072v2": {
            "Paper Title": "Leveraging the Verifier's Dilemma to Double Spend in Bitcoin",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.04677v2": {
            "Paper Title": "On the Permanence of Backdoors in Evolving Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.03481v2": {
            "Paper Title": "Topos: A Secure, Trustless, and Decentralized Interoperability Protocol",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.12822v2": {
            "Paper Title": "A Survey on XAI for Beyond 5G Security: Technical Aspects, Use Cases,\n  Challenges and Research Directions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.13567v2": {
            "Paper Title": "Range-Based Set Reconciliation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.06836v2": {
            "Paper Title": "Security, Privacy, and Decentralization in Web3",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.05057v5": {
            "Paper Title": "Can Stochastic Gradient Langevin Dynamics Provide Differential Privacy\n  for Deep Learning?",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": "studies the non-asymptotic bounds on\nthe error of approximating a target density p\u0003where logp\u0003is\nsmooth and strongly convex.\nFor the non-convex setting, ",
                    "Citation Text": "M. Raginsky, A. Rakhlin, and M. Telgarsky, \u201cNon-convex learning\nvia stochastic gradient langevin dynamics: a nonasymptotic analysis,\u201d\ninProceedings of the 2017 Conference on Learning Theory , ser.\nProceedings of Machine Learning Research, S. Kale and O. Shamir,\nEds., vol. 65. PMLR, 07\u201310 Jul 2017, pp. 1674\u20131703. [Online].\nAvailable: https://proceedings.mlr.press/v65/raginsky17a.html",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.03849",
                        "Citation Paper Title": "Title:Non-convex learning via Stochastic Gradient Langevin Dynamics: a nonasymptotic analysis",
                        "Citation Paper Abstract": "Abstract:Stochastic Gradient Langevin Dynamics (SGLD) is a popular variant of Stochastic Gradient Descent, where properly scaled isotropic Gaussian noise is added to an unbiased estimate of the gradient at each iteration. This modest change allows SGLD to escape local minima and suffices to guarantee asymptotic convergence to global minimizers for sufficiently regular non-convex objectives (Gelfand and Mitter, 1991). The present work provides a nonasymptotic analysis in the context of non-convex learning problems, giving finite-time guarantees for SGLD to find approximate minimizers of both empirical and population risks. As in the asymptotic setting, our analysis relates the discrete-time SGLD Markov chain to a continuous-time diffusion process. A new tool that drives the results is the use of weighted transportation cost inequalities to quantify the rate of convergence of SGLD to a stationary distribution in the Euclidean $2$-Wasserstein distance.",
                        "Citation Paper Authors": "Authors:Maxim Raginsky, Alexander Rakhlin, Matus Telgarsky"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.08025v3": {
            "Paper Title": "AutoCAT: Reinforcement Learning for Automated Exploration of\n  Cache-Timing Attacks",
            "Sentences": [
                {
                    "Sentence ID": 50,
                    "Sentence": ", with\nactors generating data and the learner learning asynchronously.\nOur DNN model is implemented in PyTorch ",
                    "Citation Text": "A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf,\nE. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,\nL. Fang, J. Bai, and S. Chintala, \u201cPyTorch: An imperative style, high-\nperformance deep learning library,\u201d in Advances in Neural Information\nProcessing Systems 32 , H. Wallach, H. Larochelle, A. Beygelzimer,\nF. d'Alch \u00b4e-Buc, E. Fox, and R. Garnett, Eds. Curran Associates, Inc.,\n2019, pp. 8024\u20138035.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.01703",
                        "Citation Paper Title": "Title:PyTorch: An Imperative Style, High-Performance Deep Learning Library",
                        "Citation Paper Abstract": "Abstract:Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.\nIn this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.\nWe demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.",
                        "Citation Paper Authors": "Authors:Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "model (input feature dimensions\n128, 1 layer encoder, 8-head, feed-forward network dimension\n2048) as the backbone. Like BERT ",
                    "Citation Text": "J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBERT: Pre-training\nof deep bidirectional transformers for language understanding,\u201d in\nProceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) . Minneapolis,\nMinnesota: Association for Computational Linguistics, Jun. 2019,\npp. 4171\u20134186. [Online]. Available: https://aclanthology :org/N19-1423",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.06242v2": {
            "Paper Title": "Beyond NaN: Resiliency of Optimization Layers in The Face of\n  Infeasibility",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.11169v2": {
            "Paper Title": "Bilingual Problems: Studying the Security Risks Incurred by Native\n  Extensions in Scripting Languages",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.06735v2": {
            "Paper Title": "CompactChain:An Efficient Stateless Chain for UTXO-model Blockchain",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.10147v5": {
            "Paper Title": "Trojan Awakener: Detecting Dormant Malicious Hardware Using Laser Logic\n  State Imaging (Extended Version)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.05428v2": {
            "Paper Title": "ezDPS: An Efficient and Zero-Knowledge Machine Learning Inference\n  Pipeline",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.06300v2": {
            "Paper Title": "PINCH: An Adversarial Extraction Attack Framework for Deep Learning\n  Models",
            "Sentences": [
                {
                    "Sentence ID": 46,
                    "Sentence": "developed a generic method for extracting DNN\nmodels by optimizing training hyperparameters and generat-\ning synthetic queries. Orekondy et al. ",
                    "Citation Text": "Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz.\nKnockoff nets: Stealing functionality of black-box mod-\nels. In IEEE/CVF conference on computer vision and\npattern recognition (CVPR) , pages 4954\u20134963, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.02766",
                        "Citation Paper Title": "Title:Knockoff Nets: Stealing Functionality of Black-Box Models",
                        "Citation Paper Abstract": "Abstract:Machine Learning (ML) models are increasingly deployed in the wild to perform a wide range of tasks. In this work, we ask to what extent can an adversary steal functionality of such \"victim\" models based solely on blackbox interactions: image in, predictions out. In contrast to prior work, we present an adversary lacking knowledge of train/test data used by the model, its internals, and semantics over model outputs. We formulate model functionality stealing as a two-step approach: (i) querying a set of input images to the blackbox model to obtain predictions; and (ii) training a \"knockoff\" with queried image-prediction pairs. We make multiple remarkable observations: (a) querying random images from a different distribution than that of the blackbox training data results in a well-performing knockoff; (b) this is possible even when the knockoff is represented using a different architecture; and (c) our reinforcement learning approach additionally improves query sample efficiency in certain settings and provides performance gains. We validate model functionality stealing on a range of datasets and tasks, as well as on a popular image analysis API where we create a reasonable knockoff for as little as $30.",
                        "Citation Paper Authors": "Authors:Tribhuvanesh Orekondy, Bernt Schiele, Mario Fritz"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": "designed an avatar based approach\nto train a meta-model to predict model hyperparameters. Junti\net al. ",
                    "Citation Text": "Mika Juuti, Sebastian Szyller, Samuel Marchal, and\nN Asokan. Prada: protecting against dnn model stealing\nattacks. In IEEE European Symposium on Security and\nPrivacy (EuroS&P) , pages 512\u2013527. IEEE, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.02628",
                        "Citation Paper Title": "Title:PRADA: Protecting against DNN Model Stealing Attacks",
                        "Citation Paper Abstract": "Abstract:Machine learning (ML) applications are increasingly prevalent. Protecting the confidentiality of ML models becomes paramount for two reasons: (a) a model can be a business advantage to its owner, and (b) an adversary may use a stolen model to find transferable adversarial examples that can evade classification by the original model. Access to the model can be restricted to be only via well-defined prediction APIs. Nevertheless, prediction APIs still provide enough information to allow an adversary to mount model extraction attacks by sending repeated queries via the prediction API. In this paper, we describe new model extraction attacks using novel approaches for generating synthetic queries, and optimizing training hyperparameters. Our attacks outperform state-of-the-art model extraction in terms of transferability of both targeted and non-targeted adversarial examples (up to +29-44 percentage points, pp), and prediction accuracy (up to +46 pp) on two datasets. We provide take-aways on how to perform effective model extraction attacks. We then propose PRADA, the first step towards generic and effective detection of DNN model extraction attacks. It analyzes the distribution of consecutive API queries and raises an alarm when this distribution deviates from benign behavior. We show that PRADA can detect all prior model extraction attacks with no false positives.",
                        "Citation Paper Authors": "Authors:Mika Juuti, Sebastian Szyller, Samuel Marchal, N. Asokan"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "introduced\nthe \ufb01rst extraction attack to extract target ML models ex-\nposed in online prediction APIs. Papernot et al. ",
                    "Citation Text": "Nicolas Papernot, Patrick McDaniel, Ian Goodfellow,\nSomesh Jha, Z Berkay Celik, and Ananthram Swami.\nPractical black-box attacks against machine learning. In\nACM Asia conference on computer and communications\nsecurity (AsiaCCS) , pages 506\u2013519, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.02697",
                        "Citation Paper Title": "Title:Practical Black-Box Attacks against Machine Learning",
                        "Citation Paper Abstract": "Abstract:Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19% and 88.94%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.",
                        "Citation Paper Authors": "Authors:Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik, Ananthram Swami"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.01852v2": {
            "Paper Title": "Revisiting Hyperparameter Tuning with Differential Privacy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.07835v2": {
            "Paper Title": "SecGNN: Privacy-Preserving Graph Neural Network Training and Inference\n  as a Cloud Service",
            "Sentences": [
                {
                    "Sentence ID": 48,
                    "Sentence": ":\nReLU(x) =(\nxifx\u00150;\n0ifx<0;(3)\nand the activation function Softmax( x)is de\ufb01ned as ",
                    "Citation Text": "W. Liu, Y. Wen, Z. Yu, and M. Yang, \u201cLarge-margin softmax loss\nfor convolutional neural networks.\u201d in Proc. of ICML , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.02295",
                        "Citation Paper Title": "Title:Large-Margin Softmax Loss for Convolutional Neural Networks",
                        "Citation Paper Abstract": "Abstract:Cross-entropy loss together with softmax is arguably one of the most common used supervision components in convolutional neural networks (CNNs). Despite its simplicity, popularity and excellent performance, the component does not explicitly encourage discriminative learning of features. In this paper, we propose a generalized large-margin softmax (L-Softmax) loss which explicitly encourages intra-class compactness and inter-class separability between learned features. Moreover, L-Softmax not only can adjust the desired margin but also can avoid overfitting. We also show that the L-Softmax loss can be optimized by typical stochastic gradient descent. Extensive experiments on four benchmark datasets demonstrate that the deeply-learned features with L-softmax loss become more discriminative, hence significantly boosting the performance on a variety of visual classification and verification tasks.",
                        "Citation Paper Authors": "Authors:Weiyang Liu, Yandong Wen, Zhiding Yu, Meng Yang"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "protect graph data privacy at the cost of notable\naccuracy degradation and rely on delicate parameter tuning\nfor balancing accuracy and privacy. In independent work,\nWang et al. ",
                    "Citation Text": "B. Wang, J. Guo, A. Li, Y. Chen, and H. Li, \u201cPrivacy-preserving\nrepresentation learning on graphs: A mutual information perspec-\ntive,\u201d in Proc. of ACM KDD , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2107.01475",
                        "Citation Paper Title": "Title:Privacy-Preserving Representation Learning on Graphs: A Mutual Information Perspective",
                        "Citation Paper Abstract": "Abstract:Learning with graphs has attracted significant attention recently. Existing representation learning methods on graphs have achieved state-of-the-art performance on various graph-related tasks such as node classification, link prediction, etc. However, we observe that these methods could leak serious private information. For instance, one can accurately infer the links (or node identity) in a graph from a node classifier (or link predictor) trained on the learnt node representations by existing methods. To address the issue, we propose a privacy-preserving representation learning framework on graphs from the \\emph{mutual information} perspective. Specifically, our framework includes a primary learning task and a privacy protection task, and we consider node classification and link prediction as the two tasks of interest. Our goal is to learn node representations such that they can be used to achieve high performance for the primary learning task, while obtaining performance for the privacy protection task close to random guessing. We formally formulate our goal via mutual information objectives. However, it is intractable to compute mutual information in practice. Then, we derive tractable variational bounds for the mutual information terms, where each bound can be parameterized via a neural network. Next, we train these parameterized neural networks to approximate the true mutual information and learn privacy-preserving node representations. We finally evaluate our framework on various graph datasets.",
                        "Citation Paper Authors": "Authors:Binghui Wang, Jiayi Guo, Ang Li, Yiran Chen, Hai Li"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": ". GCN models are variants of CNNs which operate\ndirectly on graphs, and it is typically used for graphs with\nrelatively stable nodes such as recommendation systems ",
                    "Citation Text": "R. Ying, R. He, K. Chen, P . Eksombatchai, W. L. Hamilton, and\nJ. Leskovec, \u201cGraph convolutional neural networks for web-scale\nrecommender systems,\u201d in Proc. of ACM KDD , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.01973",
                        "Citation Paper Title": "Title:Graph Convolutional Neural Networks for Web-Scale Recommender Systems",
                        "Citation Paper Abstract": "Abstract:Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains a challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We also develop an efficient MapReduce model inference algorithm to generate embeddings using a trained model. We deploy PinSage at Pinterest and train it on 7.5 billion examples on a graph with 3 billion nodes representing pins and boards, and 18 billion edges. According to offline metrics, user studies and A/B tests, PinSage generates higher-quality recommendations than comparable deep learning and graph-based alternatives. To our knowledge, this is the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.",
                        "Citation Paper Authors": "Authors:Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, Jure Leskovec"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ". Since then, many advanced GNN models targeting3\ndifferent applications and with varying capabilities have\nbeen put forward. In general, GNN models can be di-\nvided into three categories: Gated Graph Neural Networks\n(GGNN) ",
                    "Citation Text": "Y. Li, D. Tarlow, M. Brockschmidt, and R. S. Zemel, \u201cGated graph\nsequence neural networks,\u201d in Proc. of ICLR , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.05493",
                        "Citation Paper Title": "Title:Gated Graph Sequence Neural Networks",
                        "Citation Paper Abstract": "Abstract:Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.",
                        "Citation Paper Authors": "Authors:Yujia Li, Daniel Tarlow, Marc Brockschmidt, Richard Zemel"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.07912v5": {
            "Paper Title": "Double Sampling Randomized Smoothing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.05577v2": {
            "Paper Title": "What Can the Neural Tangent Kernel Tell Us About Adversarial Robustness?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.11736v4": {
            "Paper Title": "Towards a Defense Against Federated Backdoor Attacks Under Continuous\n  Training",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.12888v2": {
            "Paper Title": "On Cache-Aided Multi-User Private Information Retrieval with Small\n  Caches",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.13997v4": {
            "Paper Title": "Classical Verification of Quantum Computations in Linear Time",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": ", since one existing work that\nour work relies on ",
                    "Citation Text": "Samuele Ferracin, Theodoros Kapourniotis, and Animes h Datta. Reducing resources for veri\ufb01cation of\nquantum computations. Phys. Rev. A , 98:022323, Aug 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.10050",
                        "Citation Paper Title": "Title:Reducing resources for verification of quantum computations",
                        "Citation Paper Abstract": "Abstract:We present two verification protocols where the correctness of a \"target\" computation is checked by means of \"trap\" computations that can be efficiently simulated on a classical computer. Our protocols rely on a minimal set of noise-free operations (preparation of eight single-qubit states or measurement of four observables, both on a single plane of the Bloch sphere) and achieve linear overhead. To the best of our knowledge, our protocols are the least demanding techniques able to achieve linear overhead. They represent a step towards further reducing the quantum requirements for verification.",
                        "Citation Paper Authors": "Authors:Samuele Ferracin, Theodoros Kapourniotis, Animesh Datta"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "Naturally, we would like to know\nwhether quantum computations are also veri\ufb01able, and how it could be executed in practice. Formally\nspeaking, a quantum computation veri\ufb01cation protocol is de \ufb01ned as:\nDe\ufb01nition 1.1 (Quantum computation veri\ufb01cation, review of ",
                    "Citation Text": "Dorit Aharonov, Michael Ben-or, and Elad Eban. Interact ive proofs for quantum computations, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.04487",
                        "Citation Paper Title": "Title:Interactive Proofs for Quantum Computations",
                        "Citation Paper Abstract": "Abstract:The widely held belief that BQP strictly contains BPP raises fundamental questions: if we cannot efficiently compute predictions for the behavior of quantum systems, how can we test their behavior? In other words, is quantum mechanics falsifiable? In cryptographic settings, how can a customer of a future untrusted quantum computing company be convinced of the correctness of its quantum computations? To provide answers to these questions, we define Quantum Prover Interactive Proofs (QPIP). Whereas in standard interactive proofs the prover is computationally unbounded, here our prover is in BQP, representing a quantum computer. The verifier models our current computational capabilities: it is a BPP machine, with access to only a few qubits. Our main theorem states, roughly: 'Any language in BQP has a QPIP, which also hides the computation from the prover'. We provide two proofs, one based on a quantum authentication scheme (QAS) relying on random Clifford rotations and the other based on a QAS which uses polynomial codes (BOCG+ 06), combined with secure multiparty computation methods.\nThis is the journal version of work reported in 2008 (ABOE08) and presented in ICS 2010; here we have completed the details and made the proofs rigorous. Some of the proofs required major modifications and corrections. Notably, the claim that the polynomial QPIP is fault tolerant was removed. Similar results (with different protocols) were reported independently around the same time of the original version in BFK08. The initial independent works (ABOE08, BFK08) ignited a long line of research of blind verifiable quantum computation, which we survey here, along with connections to various cryptographic problems. Importantly, the problems of making the results fault tolerant as well as removing the need for quantum communication altogether remain open.",
                        "Citation Paper Authors": "Authors:Dorit Aharonov, Michael Ben-Or, Elad Eban, Urmila Mahadev"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "designed a test based on quan tum random\naccess code ",
                    "Citation Text": "Andris Ambainis, Debbie Leung, Laura Mancinska, and Mar is Ozols. Quantum random access codes\nwith shared randomness. 10 2008.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:0810.2937",
                        "Citation Paper Title": "Title:Quantum Random Access Codes with Shared Randomness",
                        "Citation Paper Abstract": "Abstract:  We consider a communication method, where the sender encodes n classical bits into 1 qubit and sends it to the receiver who performs a certain measurement depending on which of the initial bits must be recovered. This procedure is called (n,1,p) quantum random access code (QRAC) where p > 1/2 is its success probability. It is known that (2,1,0.85) and (3,1,0.79) QRACs (with no classical counterparts) exist and that (4,1,p) QRAC with p > 1/2 is not possible.\nWe extend this model with shared randomness (SR) that is accessible to both parties. Then (n,1,p) QRAC with SR and p > 1/2 exists for any n > 0. We give an upper bound on its success probability (the known (2,1,0.85) and (3,1,0.79) QRACs match this upper bound). We discuss some particular constructions for several small values of n.\nWe also study the classical counterpart of this model where n bits are encoded into 1 bit instead of 1 qubit and SR is used. We give an optimal construction for such codes and find their success probability exactly--it is less than in the quantum case.\nInteractive 3D quantum random access codes are available on-line at this http URL .",
                        "Citation Paper Authors": "Authors:Andris Ambainis, Debbie Leung, Laura Mancinska, Maris Ozols"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "construct a new RSPV protocol that allows for preparati on of a large number of BB84 states; in ",
                    "Citation Text": "Qi Zhao Honghao Fu, Daochen Wang. Computational self-t esting of multi-qubit states and measure-\nments. 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.13430",
                        "Citation Paper Title": "Title:Parallel self-testing of EPR pairs under computational assumptions",
                        "Citation Paper Abstract": "Abstract:Self-testing is a fundamental feature of quantum mechanics that allows a classical verifier to force untrusted quantum devices to prepare certain states and perform certain measurements on them. The standard approach assumes at least two spatially separated devices. Recently, Metger and Vidick [Quantum, 2021] showed that a single EPR pair of a single quantum device can be self-tested under computational assumptions. In this work, we generalize their results to give the first parallel self-test of $N$ EPR pairs and measurements on them in the single-device setting under the same computational assumptions. We show that our protocol can be passed with probability negligibly close to $1$ by an honest quantum device using poly$(N)$ resources. Moreover, we show that any quantum device that fails our protocol with probability at most $\\epsilon$ must be poly$(N,\\epsilon)$-close to being honest in the appropriate sense. In particular, our protocol can test any distribution over tensor products of computational or Hadamard basis measurements, making it suitable for applications such as device-independent quantum key distribution under computational assumptions. Moreover, a simplified version of our protocol is the first that can efficiently certify an arbitrary number of qubits of a single cloud quantum computer using only classical communication.",
                        "Citation Paper Authors": "Authors:Honghao Fu, Daochen Wang, Qi Zhao"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "constructs a single-server self-testing protocol; t his leads to a new protocol for device-independent\nquantum key distribution ",
                    "Citation Text": "Tony Metger, Yfke Dulek, Andrea Coladangelo, and Rotem Arnon-Friedman. Device-independent quan-\ntum key distribution from computational assumptions, 10 20 20.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.04175",
                        "Citation Paper Title": "Title:Device-independent quantum key distribution from computational assumptions",
                        "Citation Paper Abstract": "Abstract:In device-independent quantum key distribution (DIQKD), an adversary prepares a device consisting of two components, distributed to Alice and Bob, who use the device to generate a secure key. The security of existing DIQKD schemes holds under the assumption that the two components of the device cannot communicate with one another during the protocol execution. This is called the no-communication assumption in DIQKD. Here, we show how to replace this assumption, which can be hard to enforce in practice, by a standard computational assumption from post-quantum cryptography: we give a protocol that produces secure keys even when the components of an adversarial device can exchange arbitrary quantum communication, assuming the device is computationally bounded. Importantly, the computational assumption only needs to hold during the protocol execution -- the keys generated at the end of the protocol are information-theoretically secure as in standard DIQKD protocols.",
                        "Citation Paper Authors": "Authors:Tony Metger, Yfke Dulek, Andrea Coladangelo, Rotem Arnon-Friedman"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "further construct a protocol w here the client-side classical computation\nis in only poly(\u03ba)time. The authors of ",
                    "Citation Text": "Kai-Min Chung, Yi Lee, Han-Hsuan Lin, and Xiaodi Wu. Con stant-round blind classical veri\ufb01cation of\nquantum sampling. ArXiv , abs/2012.04848, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.04848",
                        "Citation Paper Title": "Title:Constant-round Blind Classical Verification of Quantum Sampling",
                        "Citation Paper Abstract": "Abstract:In a recent breakthrough, Mahadev constructed a classical verification of quantum computation (CVQC) protocol for a classical client to delegate decision problems in BQP to an untrusted quantum prover under computational assumptions. In this work, we explore further the feasibility of CVQC with the more general sampling problems in BQP and with the desirable blindness property. We contribute affirmative solutions to both as follows.\n(1) Motivated by the sampling nature of many quantum applications (e.g., quantum algorithms for machine learning and quantum supremacy tasks), we initiate the study of CVQC for quantum sampling problems (denoted by SampBQP). More precisely, in a CVQC protocol for a SampBQP problem, the prover and the verifier are given an input $x\\in \\{0,1\\}^n$ and a quantum circuit $C$, and the goal of the classical client is to learn a sample from the output $z \\leftarrow C(x)$ up to a small error, from its interaction with an untrusted prover. We demonstrate its feasibility by constructing a four-message CVQC protocol for SampBQP based on the quantum Learning With Error assumption.\n(2) The blindness of CVQC protocols refers to a property of the protocol where the prover learns nothing, and hence is blind, about the client's input. It is a highly desirable property that has been intensively studied for the delegation of quantum computation. We provide a simple yet powerful generic compiler that transforms any CVQC protocol to a blind one while preserving its completeness and soundness errors as well as the number of rounds.\nApplying our compiler to (a parallel repetition of) Mahadev's CVQC protocol for BQP and our CVQC protocol for SampBQP yields the first constant-round blind CVQC protocol for BQP and SampBQP respectively, with negligible and inverse polynomial soundness errors respectively, and negligible completeness errors.",
                        "Citation Paper Authors": "Authors:Kai-Min Chung, Yi Lee, Han-Hsuan Lin, Xiaodi Wu"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "provide a positive answer to this problem using only classical channel; independently ",
                    "Citation Text": "Alexandru Cojocaru, L\u00e9o Colisson, Elham Kashe\ufb01, and Pe tros Wallden. Qfactory: Classically-instructed\nremote secret qubits preparation. In Steven D. Galbraith an d Shiho Moriai, editors, Advances in\nCryptology - ASIACRYPT 2019 - 25th International Conference on t he Theory and Application of\nCryptology and Information Security, Kobe, Japan, December 8-1 2, 2019, Proceedings, Part I , volume\n11921 of Lecture Notes in Computer Science , pages 615\u2013645. Springer, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.06303",
                        "Citation Paper Title": "Title:QFactory: classically-instructed remote secret qubits preparation",
                        "Citation Paper Abstract": "Abstract:The functionality of classically-instructed remotely prepared random secret qubits was introduced in (Cojocaru et al 2018) as a way to enable classical parties to participate in secure quantum computation and communications protocols. The idea is that a classical party (client) instructs a quantum party (server) to generate a qubit to the server's side that is random, unknown to the server but known to the client. Such task is only possible under computational assumptions. In this contribution we define a simpler (basic) primitive consisting of only BB84 states, and give a protocol that realizes this primitive and that is secure against the strongest possible adversary (an arbitrarily deviating malicious server). The specific functions used, were constructed based on known trapdoor one-way functions, resulting to the security of our basic primitive being reduced to the hardness of the Learning With Errors problem. We then give a number of extensions, building on this basic module: extension to larger set of states (that includes non-Clifford states); proper consideration of the abort case; and verifiablity on the module level. The latter is based on \"blind self-testing\", a notion we introduced, proved in a limited setting and conjectured its validity for the most general case.",
                        "Citation Paper Authors": "Authors:Alexandru Cojocaru, L\u00e9o Colisson, Elham Kashefi, Petros Wallden"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "consider whether it\u2019s possible to design a classical-channel RSPV protocol for the\ngadgets used in [15, 30] etc. In ",
                    "Citation Text": "Anne Broadbent, Joseph Fitzsimons, and Elham Kashe\ufb01. U niversal blind quantum computation. In\nProceedings of the 2009 50th Annual IEEE Symposium on Foundat ions of Computer Science , FOCS \u201909,\npages 517\u2013526, Washington, DC, USA, 2009. IEEE Computer Soc iety.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:0807.4154",
                        "Citation Paper Title": "Title:Universal blind quantum computation",
                        "Citation Paper Abstract": "Abstract:  We present a protocol which allows a client to have a server carry out a quantum computation for her such that the client's inputs, outputs and computation remain perfectly private, and where she does not require any quantum computational power or memory. The client only needs to be able to prepare single qubits randomly chosen from a finite set and send them to the server, who has the balance of the required quantum computational resources. Our protocol is interactive: after the initial preparation of quantum states, the client and server use two-way classical communication which enables the client to drive the computation, giving single-qubit measurement instructions to the server, depending on previous measurement outcomes. Our protocol works for inputs and outputs that are either classical or quantum. We give an authentication protocol that allows the client to detect an interfering server; our scheme can also be made fault-tolerant.\nWe also generalize our result to the setting of a purely classical client who communicates classically with two non-communicating entangled servers, in order to perform a blind quantum computation. By incorporating the authentication protocol, we show that any problem in BQP has an entangled two-prover interactive proof with a purely classical verifier.\nOur protocol is the first universal scheme which detects a cheating server, as well as the first protocol which does not require any quantum computation whatsoever on the client's side. The novelty of our approach is in using the unique features of measurement-based quantum computing which allows us to clearly distinguish between the quantum and classical aspects of a quantum computation.",
                        "Citation Paper Authors": "Authors:Anne Broadbent, Joseph Fitzsimons, Elham Kashefi"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "for con\ufb01rming it.\n61.2.2 Related problem: remote state preparation\nA very basic notion in quantum cryptography that our work wil l be closely related to is remote state prepara-\ntion, raised in ",
                    "Citation Text": "Charles H. Bennett, David P. DiVincenzo, Peter W. Shor, John A. Smolin, Barbara M. Terhal, and\nWilliam K. Wootters. Remote state preparation. Phys. Rev. Lett. , 87:077902, Jul 2001.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:quant-ph/0006044",
                        "Citation Paper Title": "Title:Remote State Preparation",
                        "Citation Paper Abstract": "Abstract:  Quantum teleportation uses prior entanglement and forward classical communication to transmit one instance of an unknown quantum state. Remote state preparation (RSP) has the same goal, but the sender knows classically what state is to be transmitted. We show that the asymptotic classical communication cost of RSP is one bit per qubit - half that of teleportation - and becomes even less when transmitting part of a known entangled state. We explore the tradeoff between entanglement and classical communication required for RSP, and discuss RSP capacities of general quantum channels.",
                        "Citation Paper Authors": "Authors:Charles H. Bennett, David P. DiVincenzo, Peter W. Shor, John A. Smolin, Barbara M. Terhal, William K. Wootters"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ").\nToday quantum computations are gradually coming into reali ty. ",
                    "Citation Text": "Frank Arute, Kunal Arya, Ryan Babbush, Dave Bacon, Josep h C. Bardin, Rami Barends, Rupak\nBiswas, Sergio Boixo, Fernando G. S. L. Brandao, David A. Bue ll, Brian Burkett, Yu Chen, Zijun Chen,\nBen Chiaro, Roberto Collins, William Courtney, Andrew Duns worth, Edward Farhi, Brooks Foxen,\nAustin Fowler, Craig Gidney, Marissa Giustina, Rob Gra\ufb00, Ke ith Guerin, Steve Habegger, Matthew P.\nHarrigan, Michael J. Hartmann, Alan Ho, Markus Ho\ufb00mann, Tre nt Huang, Travis S. Humble, Sergei V.\nIsakov, Evan Je\ufb00rey, Zhang Jiang, Dvir Kafri, Kostyantyn Ke chedzhi, Julian Kelly, Paul V. Klimov,\nSergey Knysh, Alexander Korotkov, Fedor Kostritsa, David L andhuis, Mike Lindmark, Erik Lucero,\nDmitry Lyakh, Salvatore Mandr\u00e0, Jarrod R. McClean, Matthew McEwen, Anthony Megrant, Xiao\nMi, Kristel Michielsen, Masoud Mohseni, Josh Mutus, Ofer Na aman, Matthew Neeley, Charles Neill,\nMurphy Yuezhen Niu, Eric Ostby, Andre Petukhov, John C. Plat t, Chris Quintana, Eleanor G. Rie\ufb00el,\nPedram Roushan, Nicholas C. Rubin, Daniel Sank, Kevin J. Sat zinger, Vadim Smelyanskiy, Kevin J.\nSung, Matthew D. Trevithick, Amit Vainsencher, Benjamin Vi llalonga, Theodore White, Z. Jamie\nYao, Ping Yeh, Adam Zalcman, Hartmut Neven, and John M. Marti nis. Quantum supremacy using a\nprogrammable superconducting processor. Nature , 574(7779):505\u2013510, 2019.\n119",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.11333",
                        "Citation Paper Title": "Title:Supplementary information for \"Quantum supremacy using a programmable superconducting processor\"",
                        "Citation Paper Abstract": "Abstract:This is an updated version of supplementary information to accompany \"Quantum supremacy using a programmable superconducting processor\", an article published in the October 24, 2019 issue of Nature. The main article is freely available at this https URL. Summary of changes since arXiv:1910.11333v1 (submitted 23 Oct 2019): added URL for qFlex source code; added Erratum section; added Figure S41 comparing statistical and total uncertainty for log and linear XEB; new References [1,65]; miscellaneous updates for clarity and style consistency; miscellaneous typographical and formatting corrections.",
                        "Citation Paper Authors": "Authors:Frank Arute, Kunal Arya, Ryan Babbush, Dave Bacon, Joseph C. Bardin, Rami Barends, Rupak Biswas, Sergio Boixo, Fernando G.S.L. Brandao, David A. Buell, Brian Burkett, Yu Chen, Zijun Chen, Ben Chiaro, Roberto Collins, William Courtney, Andrew Dunsworth, Edward Farhi, Brooks Foxen, Austin Fowler, Craig Gidney, Marissa Giustina, Rob Graff, Keith Guerin, Steve Habegger, Matthew P. Harrigan, Michael J. Hartmann, Alan Ho, Markus R. Hoffmann, Trent Huang, Travis S. Humble, Sergei V. Isakov, Evan Jeffrey, Zhang Jiang, Dvir Kafri, Kostyantyn Kechedzhi, Julian Kelly, Paul V. Klimov, Sergey Knysh, Alexander N. Korotkov, Fedor Kostritsa, David Landhuis, Mike Lindmark, Erik Lucero, Dmitry Lyakh, Salvatore Mandra, Jarrod R. McClean, Matt McEwen, Anthony Megrant, Xiao Mi, Kristel Michielsen, Masoud Mohseni, Josh Mutus, Ofer Naaman, Matthew Neeley, Charles Neill, Murphy Yuezhen Niu, Eric Ostby, Andre Petukhov, John C. Platt, Chris Quintana, Eleanor G. Rieffel, Pedram Roushan, Nicholas C. Rubin, Daniel Sank, Kevin J. Satzinger, Vadim Smelyanskiy, Kevin J. Sung, Matthew D. Trevithick, Amit Vainsencher, Benjamin Villalonga, Theodore White, Z. Jamie Yao, Ping Yeh, Adam Zalcman, Hartmut Neven, John M. Martinis"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.06669v3": {
            "Paper Title": "Walking Under the Ladder Logic: PLC-VBS, a PLC Control Logic\n  Vulnerability Discovery Tool",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.11519v2": {
            "Paper Title": "Homomorphic Sortition -- Secret Leader Election for PoS Blockchains",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.03218v2": {
            "Paper Title": "The Future of Integrated Digital Governance in the EU: EBSI and GLASS",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.13416v2": {
            "Paper Title": "Data Origin Inference in Machine Learning",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": ". The underline insight behind this\ntechnique is that ML models tend to have unique behaviours\nwhen the input data is from the training set, e.g. have a higher\ncon\ufb01dence score ",
                    "Citation Text": "Salem, A., Zhang, Y ., Humbert, M., Fritz, M. , Backes,\nM. ML-Leaks: Model and Data Independent Membership\nInference Attacks and Defenses on Machine Learning\nModels. ArXiv .abs/1806.01246 (2019)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.01246",
                        "Citation Paper Title": "Title:ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models",
                        "Citation Paper Abstract": "Abstract:Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications.\nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains.\nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model.",
                        "Citation Paper Authors": "Authors:Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, Michael Backes"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": ", is\nto reconstruct training data samples on pixel-level. Under\nsome restrictions like relatively small batch size, it is possi-\nble to reconstruct newly fed samples according to the shared\ngradient in federated learning ",
                    "Citation Text": "Zhu, L., Liu, Z. , Han, S. Deep leakage from gradients.\nAdvances In Neural Information Processing Systems .32\n(2019)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.08935",
                        "Citation Paper Title": "Title:Deep Leakage from Gradients",
                        "Citation Paper Abstract": "Abstract:Exchanging gradients is a widely used method in modern multi-node machine learning system (e.g., distributed training, collaborative learning). For a long time, people believed that gradients are safe to share: i.e., the training data will not be leaked by gradient exchange. However, we show that it is possible to obtain the private training data from the publicly shared gradients. We name this leakage as Deep Leakage from Gradient and empirically validate the effectiveness on both computer vision and natural language processing tasks. Experimental results show that our attack is much stronger than previous approaches: the recovery is pixel-wise accurate for images and token-wise matching for texts. We want to raise people's awareness to rethink the gradient's safety. Finally, we discuss several possible strategies to prevent such deep leakage. The most effective defense method is gradient pruning.",
                        "Citation Paper Authors": "Authors:Ligeng Zhu, Zhijian Liu, Song Han"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.10583v2": {
            "Paper Title": "Should Users Trust Their Android Devices? A Scoring System for Assessing\n  Security and Privacy Risks of Pre-Installed Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.13578v2": {
            "Paper Title": "Dynamic Network Reconfiguration for Entropy Maximization using Deep\n  Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.13780v2": {
            "Paper Title": "CryptoLight: An Electro-Optical Accelerator for Fully Homomorphic\n  Encryption",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.03311v4": {
            "Paper Title": "RoFL: Robustness of Secure Federated Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.08742v2": {
            "Paper Title": "Silicon-proven ASIC design for the Polynomial Operations of Fully\n  Homomorphic Encryption",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.04518v4": {
            "Paper Title": "Insecurity problem for assertions remains in NP",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.11518v2": {
            "Paper Title": "A Practical Influence Approximation for Privacy-Preserving Data\n  Filtering in Federated Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.10908v2": {
            "Paper Title": "ESTAS: Effective and Stable Trojan Attacks in Self-supervised Encoders\n  with One Target Unlabelled Sample",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.09076v3": {
            "Paper Title": "To Trust or Not To Trust Prediction Scores for Membership Inference\n  Attacks",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": "proposed temperature scaling using\na single temperature parameter Tfor scaling down the pre-\nsoftmax logits for all classes. The larger Tis, the more\nthe resulting scores approach a uniform distribution. Kris-\ntiadi et al. ",
                    "Citation Text": "Agustinus Kristiadi, Matthias Hein, and Philipp Hen-\nnig. Being bayesian, even just a bit, \ufb01xes overcon\ufb01dence\nin relu networks. In ICML , volume 119, pages 5436\u2013\n5446, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.10118",
                        "Citation Paper Title": "Title:Being Bayesian, Even Just a Bit, Fixes Overconfidence in ReLU Networks",
                        "Citation Paper Abstract": "Abstract:The point estimates of ReLU classification networks---arguably the most widely used neural network architecture---have been shown to yield arbitrarily high confidence far away from the training data. This architecture, in conjunction with a maximum a posteriori estimation scheme, is thus not calibrated nor robust. Approximate Bayesian inference has been empirically demonstrated to improve predictive uncertainty in neural networks, although the theoretical analysis of such Bayesian approximations is limited. We theoretically analyze approximate Gaussian distributions on the weights of ReLU networks and show that they fix the overconfidence problem. Furthermore, we show that even a simplistic, thus cheap, Bayesian approximation, also fixes these issues. This indicates that a sufficient condition for a calibrated uncertainty on a ReLU network is \"to be a bit Bayesian\". These theoretical results validate the usage of last-layer Bayesian approximation and motivate a range of a fidelity-cost trade-off. We further validate these findings empirically via various standard experiments using common deep ReLU networks and Laplace approximations.",
                        "Citation Paper Authors": "Authors:Agustinus Kristiadi, Matthias Hein, Philipp Hennig"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": ", their approach relies on a single shadow model. The\ninput ofhconsists of the khighest prediction scores in de-\nscending order.\nInstead of focusing solely on the scores, Yeom et al. ",
                    "Citation Text": "Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and\nSomesh Jha. Privacy risk in machine learning: Analyz-\ning the connection to over\ufb01tting. In IEEE Computer Se-\ncurity Foundations Symposium , pages 268\u2013282, 2018.\n9A Extended Proof of Theorem 1\nWe provide an extended proof of our Theorem 1,\nparticularly for the following part:\nFor almost any input x2Rmand a suf\ufb01ciently small\n\u000f > 0ifmaxi=1;:::;df(x)i\u00151\u0000\u000f, it follows that\nh(f(x)) = 1 .\nMost score-based membership inference attacks (MIAs)\nh:Rd!f0;1gare solely based on thresholds on predic-\ntion score statistics. In our work, we investigated attacks\nusing thresholds on the maximum value and the entropy of\nthe prediction score vectors. For the maximum prediction\nscore attack, the statement is trivial since for a given attack\nthreshold\u001con the maximum score, the attack\u2019s decision is\nh(f(x)) =(\n1;ifmaxi=1;:::;df(x)i\u0015\u001c\n0;otherwise(1)\nGiven any input x2Rmandmaxi=1;:::;df(x)i\u00151\u0000\u000f\nwith\u000f >0, it follows that h(f(x)) = 1()\u000f\u00141\u0000\u001c.\nThe same can be shown analogously for other threshold-\nbased attacks, such as the entropy-based attack. However,\nfor attacks using a separate neural network as an attack\nmodel to predict the membership status based on the con-\n\ufb01dence vector, a closed-form analysis is non-trivial, and\nthere might exist model inputs and corresponding predic-\ntion score vectors for which the attack model outputs a non-\nmember prediction even if maxi=1;:::;df(x)i\u00151\u0000\u000fis\nful\ufb01lled. For example, there might exist an adversarial per-\nturbation of the con\ufb01dence vector that misleads the attack\nmodel\u2019s prediction. Still, our results in Table 3 empirically\nsupport that the theorem also holds for attacks using a sep-\narate attack model.\nThe second part of our proof uses that\nlim\u000e!1maxi=1;:::;df(\u000ex)i= 1. We refer interested\nreaders to the proofs of Lemma 3.1 and Theorem 3.1 in\nHein et al.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.01604",
                        "Citation Paper Title": "Title:Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting",
                        "Citation Paper Abstract": "Abstract:Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role.\nThis paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.",
                        "Citation Paper Authors": "Authors:Samuel Yeom, Irene Giacomelli, Matt Fredrikson, Somesh Jha"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.12268v3": {
            "Paper Title": "FedPrompt: Communication-Efficient and Privacy Preserving Prompt Tuning\n  in Federated Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.11616v2": {
            "Paper Title": "An integer factorization algorithm which uses diffusion as a\n  computational engine",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.01548v2": {
            "Paper Title": "FedRolex: Model-Heterogeneous Federated Learning with Rolling Sub-Model\n  Extraction",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": ", Federated\nDropout becomes less effective when the data heterogeneity is high and the client cohort is small\ndue to its randomness in selecting sub-models. In contrast, HeteroFL ",
                    "Citation Text": "Enmao Diao, Jie Ding, and Vahid Tarokh. Hetero\ufb02: Computation and communication ef\ufb01cient\nfederated learning for heterogeneous clients. arXiv preprint arXiv:2010.01264 , 2020.\n11",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.01264",
                        "Citation Paper Title": "Title:HeteroFL: Computation and Communication Efficient Federated Learning for Heterogeneous Clients",
                        "Citation Paper Abstract": "Abstract:Federated Learning (FL) is a method of training machine learning models on private data distributed over a large number of possibly heterogeneous clients such as mobile phones and IoT devices. In this work, we propose a new federated learning framework named HeteroFL to address heterogeneous clients equipped with very different computation and communication capabilities. Our solution can enable the training of heterogeneous local models with varying computation complexities and still produce a single global inference model. For the first time, our method challenges the underlying assumption of existing work that local models have to share the same architecture as the global model. We demonstrate several strategies to enhance FL training and conduct extensive empirical evaluations, including five computation complexity levels of three model architecture on three datasets. We show that adaptively distributing subnetworks according to clients' capabilities is both computation and communication efficient.",
                        "Citation Paper Authors": "Authors:Enmao Diao, Jie Ding, Vahid Tarokh"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "Knowledge Distillation (KD)-based Model-Heterogeneous FL. One primary approach for model-\nheterogeneous FL in cross-device settings is based on knowledge distillation (KD) ",
                    "Citation Text": "Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network.\narXiv preprint arXiv:1503.02531 , 2(7), 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1503.02531",
                        "Citation Paper Title": "Title:Distilling the Knowledge in a Neural Network",
                        "Citation Paper Abstract": "Abstract:A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.",
                        "Citation Paper Authors": "Authors:Geoffrey Hinton, Oriol Vinyals, Jeff Dean"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.09011v2": {
            "Paper Title": "Verifiable Differential Privacy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.02732v2": {
            "Paper Title": "BO-DBA: Query-Efficient Decision-Based Adversarial Attacks via Bayesian\n  Optimization",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": "improves the ef\ufb01ciency of DBA through exploitation of discarded information with the\nZeroth-order gradient estimation. HSJA ",
                    "Citation Text": "Jianbo Chen, Michael I Jordan, and Martin J Wainwright. Hopskipjumpattack: A query-ef\ufb01cient decision-based\nattack. In 2020 ieee symposium on security and privacy (sp) , pages 1277\u20131294. IEEE, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.02144",
                        "Citation Paper Title": "Title:HopSkipJumpAttack: A Query-Efficient Decision-Based Attack",
                        "Citation Paper Abstract": "Abstract:The goal of a decision-based adversarial attack on a trained model is to generate adversarial examples based solely on observing output labels returned by the targeted model. We develop HopSkipJumpAttack, a family of algorithms based on a novel estimate of the gradient direction using binary information at the decision boundary. The proposed family includes both untargeted and targeted attacks optimized for $\\ell_2$ and $\\ell_\\infty$ similarity metrics respectively. Theoretical analysis is provided for the proposed algorithms and the gradient direction estimate. Experiments show HopSkipJumpAttack requires significantly fewer model queries than Boundary Attack. It also achieves competitive performance in attacking several widely-used defense mechanisms. (HopSkipJumpAttack was named Boundary Attack++ in a previous version of the preprint.)",
                        "Citation Paper Authors": "Authors:Jianbo Chen, Michael I. Jordan, Martin J. Wainwright"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": ".\nAcquisition Function in Bayesian optimization is a function that evaluates the utility of model querying at each point,\ngiven the surrogate model, to \ufb01nd the optimal candidate query point for the next iteration ",
                    "Citation Text": "Eric Brochu, Vlad M Cora, and Nando De Freitas. A tutorial on bayesian optimization of expensive cost functions,\nwith application to active user modeling and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599 ,\n2010.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1012.2599",
                        "Citation Paper Title": "Title:A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.",
                        "Citation Paper Authors": "Authors:Eric Brochu, Vlad M. Cora, Nando de Freitas"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "as the covariance function, which is de\ufb01ned\nas:\n\u0006(x;x0) = (1 +p\n5r\nl+5r2\n3l2)exp(\u0000p\n5r\nl)\nwherer=x\u0000x0andlis the length-scale parameter ",
                    "Citation Text": "Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine learning\nalgorithms. In Advances in neural information processing systems , pages 2951\u20132959, 2012.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1206.2944",
                        "Citation Paper Title": "Title:Practical Bayesian Optimization of Machine Learning Algorithms",
                        "Citation Paper Abstract": "Abstract:Machine learning algorithms frequently require careful tuning of model hyperparameters, regularization terms, and optimization parameters. Unfortunately, this tuning is often a \"black art\" that requires expert experience, unwritten rules of thumb, or sometimes brute-force search. Much more appealing is the idea of developing automatic approaches which can optimize the performance of a given learning algorithm to the task at hand. In this work, we consider the automatic tuning problem within the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). The tractable posterior distribution induced by the GP leads to efficient use of the information gathered by previous experiments, enabling optimal choices about what parameters to try next. Here we show how the effects of the Gaussian process prior and the associated inference procedure can have a large impact on the success or failure of Bayesian optimization. We show that thoughtful choices can lead to results that exceed expert-level performance in tuning machine learning algorithms. We also describe new algorithms that take into account the variable cost (duration) of learning experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization on a diverse set of contemporary algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.",
                        "Citation Paper Authors": "Authors:Jasper Snoek, Hugo Larochelle, Ryan P. Adams"
                    }
                },
                {
                    "Sentence ID": 27,
                    "Sentence": "also adopts the zeroth-order gradient estimation to optimize the con\ufb01dence score of perturbed inputs.\nIn addition to these two methods, there have been several other techniques using various ways to improve the query\nef\ufb01ciency. For example, ",
                    "Citation Text": "Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, and Shin-Ming\nCheng. Autozoom: Autoencoder-based zeroth order optimization method for attacking black-box neural networks.\nInProceedings of the AAAI Conference on Arti\ufb01cial Intelligence , volume 33, pages 742\u2013749, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.11770",
                        "Citation Paper Title": "Title:AutoZOOM: Autoencoder-based Zeroth Order Optimization Method for Attacking Black-box Neural Networks",
                        "Citation Paper Abstract": "Abstract:Recent studies have shown that adversarial examples in state-of-the-art image classifiers trained by deep neural networks (DNN) can be easily generated when the target model is transparent to an attacker, known as the white-box setting. However, when attacking a deployed machine learning service, one can only acquire the input-output correspondences of the target model; this is the so-called black-box attack setting. The major drawback of existing black-box attacks is the need for excessive model queries, which may give a false sense of model robustness due to inefficient query designs. To bridge this gap, we propose a generic framework for query-efficient black-box attacks. Our framework, AutoZOOM, which is short for Autoencoder-based Zeroth Order Optimization Method, has two novel building blocks towards efficient black-box attacks: (i) an adaptive random gradient estimation strategy to balance query counts and distortion, and (ii) an autoencoder that is either trained offline with unlabeled data or a bilinear resizing operation for attack acceleration. Experimental results suggest that, by applying AutoZOOM to a state-of-the-art black-box attack (ZOO), a significant reduction in model queries can be achieved without sacrificing the attack success rate and the visual quality of the resulting adversarial examples. In particular, when compared to the standard ZOO method, AutoZOOM can consistently reduce the mean query counts in finding successful adversarial examples (or reaching the same distortion level) by at least 93% on MNIST, CIFAR-10 and ImageNet datasets, leading to novel insights on adversarial robustness.",
                        "Citation Paper Authors": "Authors:Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, Shin-Ming Cheng"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "uses genetic algorithms to iteratively evolve a population of candidate adversarial\nexamples. The number of queries needed in this technique is only 1:3%of that in the Boundary Attack. Another\ndesign ZOO ",
                    "Citation Text": "Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order optimization based\nblack-box attacks to deep neural networks without training substitute models. In Proceedings of the 10th ACM\nWorkshop on Arti\ufb01cial Intelligence and Security , pages 15\u201326, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.03999",
                        "Citation Paper Title": "Title:ZOO: Zeroth Order Optimization based Black-box Attacks to Deep Neural Networks without Training Substitute Models",
                        "Citation Paper Abstract": "Abstract:Deep neural networks (DNNs) are one of the most prominent technologies of our time, as they achieve state-of-the-art performance in many machine learning tasks, including but not limited to image classification, text mining, and speech processing. However, recent research on DNNs has indicated ever-increasing concern on the robustness to adversarial examples, especially for security-critical tasks such as traffic sign identification for autonomous driving. Studies have unveiled the vulnerability of a well-trained DNN by demonstrating the ability of generating barely noticeable (to both human and machines) adversarial images that lead to misclassification. Furthermore, researchers have shown that these adversarial images are highly transferable by simply training and attacking a substitute model built upon the target model, known as a black-box attack to DNNs.\nSimilar to the setting of training substitute models, in this paper we propose an effective black-box attack that also only has access to the input (images) and the output (confidence scores) of a targeted DNN. However, different from leveraging attack transferability from substitute models, we propose zeroth order optimization (ZOO) based attacks to directly estimate the gradients of the targeted DNN for generating adversarial examples. We use zeroth order stochastic coordinate descent along with dimension reduction, hierarchical attack and importance sampling techniques to efficiently attack black-box models. By exploiting zeroth order optimization, improved attacks to the targeted DNN can be accomplished, sparing the need for training substitute models and avoiding the loss in attack transferability. Experimental results on MNIST, CIFAR10 and ImageNet show that the proposed ZOO attack is as effective as the state-of-the-art white-box attack and significantly outperforms existing black-box attacks via substitute models.",
                        "Citation Paper Authors": "Authors:Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, Cho-Jui Hsieh"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.10147v2": {
            "Paper Title": "Evaluating the Robustness of Trigger Set-Based Watermarks Embedded in\n  Deep Neural Networks",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": "trained a counterfeit model as a stepping stone\nfor creating adversarial examples of remote target models.\nOrekondy et al. ",
                    "Citation Text": "Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Knockoff\nnets: Stealing functionality of black-box models. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition ,\npages 4954\u20134963, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.02766",
                        "Citation Paper Title": "Title:Knockoff Nets: Stealing Functionality of Black-Box Models",
                        "Citation Paper Abstract": "Abstract:Machine Learning (ML) models are increasingly deployed in the wild to perform a wide range of tasks. In this work, we ask to what extent can an adversary steal functionality of such \"victim\" models based solely on blackbox interactions: image in, predictions out. In contrast to prior work, we present an adversary lacking knowledge of train/test data used by the model, its internals, and semantics over model outputs. We formulate model functionality stealing as a two-step approach: (i) querying a set of input images to the blackbox model to obtain predictions; and (ii) training a \"knockoff\" with queried image-prediction pairs. We make multiple remarkable observations: (a) querying random images from a different distribution than that of the blackbox training data results in a well-performing knockoff; (b) this is possible even when the knockoff is represented using a different architecture; and (c) our reinforcement learning approach additionally improves query sample efficiency in certain settings and provides performance gains. We validate model functionality stealing on a range of datasets and tasks, as well as on a popular image analysis API where we create a reasonable knockoff for as little as $30.",
                        "Citation Paper Authors": "Authors:Tribhuvanesh Orekondy, Bernt Schiele, Mario Fritz"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": ".\nSince this \ufb01nding, there has been a vast volume of research\non adversarial examples. To mitigate this threat, Paper-\nnot et al. ",
                    "Citation Text": "Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and\nAnanthram Swami. Distillation as a defense to adversarial pertur-15\nbations against deep neural networks. In Proceedings of the IEEE\nSymposium on Security and Privacy , pages 582\u2013597, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.04508",
                        "Citation Paper Title": "Title:Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content filters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95% to less than 0.5% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 10^30. We also find that distillation increases the average minimum number of features that need to be modified to create adversarial samples by about 800% on one of the DNNs we tested.",
                        "Citation Paper Authors": "Authors:Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, Ananthram Swami"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": ".\nWe thus suggest considering more datasets than the MNIST\nand CIFAR datasets when evaluating watermarking algo-\nrithms.\n9 R ELATED WORK\nBackdoor attacks. There have been several studies on back-\ndoor attacks against DNN models ",
                    "Citation Text": "Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song.\nTargeted backdoor attacks on deep learning systems using data\npoisoning. CoRR , abs/1712.05526, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.05526",
                        "Citation Paper Title": "Title:Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning",
                        "Citation Paper Abstract": "Abstract:Deep learning models have achieved high performance on many tasks, and thus have been applied to many security-critical scenarios. For example, deep learning-based face recognition systems have been used to authenticate users to access many security-sensitive applications like payment apps. Such usages of deep learning systems provide the adversaries with sufficient incentives to perform attacks against these systems for their adversarial purposes. In this work, we consider a new type of attacks, called backdoor attacks, where the attacker's goal is to create a backdoor into a learning-based authentication system, so that he can easily circumvent the system by leveraging the backdoor. Specifically, the adversary aims at creating backdoor instances, so that the victim learning system will be misled to classify the backdoor instances as a target label specified by the adversary. In particular, we study backdoor poisoning attacks, which achieve backdoor attacks using poisoning strategies. Different from all existing work, our studied poisoning strategies can apply under a very weak threat model: (1) the adversary has no knowledge of the model and the training set used by the victim system; (2) the attacker is allowed to inject only a small amount of poisoning samples; (3) the backdoor key is hard to notice even by human beings to achieve stealthiness. We conduct evaluation to demonstrate that a backdoor adversary can inject only around 50 poisoning samples, while achieving an attack success rate of above 90%. We are also the first work to show that a data poisoning attack can create physically implementable backdoors without touching the training process. Our work demonstrates that backdoor poisoning attacks pose real threats to a learning system, and thus highlights the importance of further investigation and proposing defense strategies against them.",
                        "Citation Paper Authors": "Authors:Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, Dawn Song"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "demonstrated its robustness against\nthis attack scenario. That is, the previous study took a naive\napproach to conduct evasion attacks so that it failed to\ndemonstrate a meaningful upper bound on its robustness\nagainst evasion attacks (see Supplemental Material 3 ",
                    "Citation Text": "Suyoung Lee, Wonho Song, Suman Jana, Meeyoung Cha, and\nSooel Son. Evaluating the robustness of trigger set-based water-\nmarks embedded in deep neural networks (supplemental materi-\nals). https://leeswimming.com/papers/lee-tdsc22-sup.pdf.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.10147",
                        "Citation Paper Title": "Title:Evaluating the Robustness of Trigger Set-Based Watermarks Embedded in Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:Trigger set-based watermarking schemes have gained emerging attention as they provide a means to prove ownership for deep neural network model owners. In this paper, we argue that state-of-the-art trigger set-based watermarking algorithms do not achieve their designed goal of proving ownership. We posit that this impaired capability stems from two common experimental flaws that the existing research practice has committed when evaluating the robustness of watermarking algorithms: (1) incomplete adversarial evaluation and (2) overlooked adaptive attacks. We conduct a comprehensive adversarial evaluation of 11 representative watermarking schemes against six of the existing attacks and demonstrate that each of these watermarking schemes lacks robustness against at least two non-adaptive attacks. We also propose novel adaptive attacks that harness the adversary's knowledge of the underlying watermarking algorithm of a target model. We demonstrate that the proposed attacks effectively break all of the 11 watermarking schemes, consequently allowing adversaries to obscure the ownership of any watermarked model. We encourage follow-up studies to consider our guidelines when evaluating the robustness of their watermarking schemes via conducting comprehensive adversarial evaluation that includes our adaptive attacks to demonstrate a meaningful upper bound of watermark robustness.",
                        "Citation Paper Authors": "Authors:Suyoung Lee, Wonho Song, Suman Jana, Meeyoung Cha, Sooel Son"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2102.05571v6": {
            "Paper Title": "TINKER: A framework for Open source Cyberthreat Intelligence",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.02269v4": {
            "Paper Title": "Introducing a Framework to Enable Anonymous Secure Multi-Party\n  Computation in Practice (Extended Version)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.06457v2": {
            "Paper Title": "A Privacy Glossary for Cloud Computing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.07188v4": {
            "Paper Title": "SoK: Blockchain Governance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.11724v2": {
            "Paper Title": "ECM And The Elliott-Halberstam Conjecture For Quadratic Fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.04742v3": {
            "Paper Title": "Non-Destructive Zero-Knowledge Proofs on Quantum States, and Multi-Party\n  Generation of Authorized Hidden GHZ States",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.13304v4": {
            "Paper Title": "Data Fusion Challenges Privacy: What Can Privacy Regulation Do?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.15252v2": {
            "Paper Title": "Federated Learning with Heterogeneous Differential Privacy",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": ", which can be remedied via model personalization [ 41,\n42,43,44,45,46,47,48,49]. Another type of heterogeneity include systems heterogeneity where\ndi\ufb00erent devices have di\ufb00erent capabilities, in terms of various characteristics such as connection,\ncomputational, and power capabilities ",
                    "Citation Text": "T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith, \u201cFederated optimization\nin heterogeneous networks,\u201d arXiv preprint arXiv:1812.06127 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.06127",
                        "Citation Paper Title": "Title:Federated Optimization in Heterogeneous Networks",
                        "Citation Paper Abstract": "Abstract:Federated Learning is a distributed learning paradigm with two key challenges that differentiate it from traditional distributed optimization: (1) significant variability in terms of the systems characteristics on each device in the network (systems heterogeneity), and (2) non-identically distributed data across the network (statistical heterogeneity). In this work, we introduce a framework, FedProx, to tackle heterogeneity in federated networks. FedProx can be viewed as a generalization and re-parametrization of FedAvg, the current state-of-the-art method for federated learning. While this re-parameterization makes only minor modifications to the method itself, these modifications have important ramifications both in theory and in practice. Theoretically, we provide convergence guarantees for our framework when learning over data from non-identical distributions (statistical heterogeneity), and while adhering to device-level systems constraints by allowing each participating device to perform a variable amount of work (systems heterogeneity). Practically, we demonstrate that FedProx allows for more robust convergence than FedAvg across a suite of realistic federated datasets. In particular, in highly heterogeneous settings, FedProx demonstrates significantly more stable and accurate convergence behavior relative to FedAvg---improving absolute test accuracy by 22% on average.",
                        "Citation Paper Authors": "Authors:Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, Virginia Smith"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.09024v3": {
            "Paper Title": "Dataset Inference for Self-Supervised Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.10497v3": {
            "Paper Title": "Are disentangled representations all you need to build speaker\n  anonymization systems?",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": "requiring initial alignments from a GMM\nmodel. The dataset used for training is LibriSpeech train-other-\n500 and train-clean-100.\nWork done in ",
                    "Citation Text": "A. S. Shamsabadi, B. M. L. Srivastava, A. Bellet, N. Vauquier,\nE. Vincent, M. Maouche, M. Tommasi, and N. Papernot, \u201cDiffer-\nentially private speaker anonymization,\u201d arXiv , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.11823",
                        "Citation Paper Title": "Title:Differentially Private Speaker Anonymization",
                        "Citation Paper Abstract": "Abstract:Sharing real-world speech utterances is key to the training and deployment of voice-based services. However, it also raises privacy risks as speech contains a wealth of personal data. Speaker anonymization aims to remove speaker information from a speech utterance while leaving its linguistic and prosodic attributes intact. State-of-the-art techniques operate by disentangling the speaker information (represented via a speaker embedding) from these attributes and re-synthesizing speech based on the speaker embedding of another speaker. Prior research in the privacy community has shown that anonymization often provides brittle privacy protection, even less so any provable guarantee. In this work, we show that disentanglement is indeed not perfect: linguistic and prosodic attributes still contain speaker information. We remove speaker information from these attributes by introducing differentially private feature extractors based on an autoencoder and an automatic speech recognizer, respectively, trained using noise layers. We plug these extractors in the state-of-the-art anonymization pipeline and generate, for the first time, private speech utterances with a provable upper bound on the speaker information they contain. We evaluate empirically the privacy and utility resulting from our differentially private speaker anonymization approach on the LibriSpeech data set. Experimental results show that the generated utterances retain very high utility for automatic speech recognition training and inference, while being much better protected against strong adversaries who leverage the full knowledge of the anonymization process to try to infer the speaker identity.",
                        "Citation Paper Authors": "Authors:Ali Shahin Shamsabadi, Brij Mohan Lal Srivastava, Aur\u00e9lien Bellet, Nathalie Vauquier, Emmanuel Vincent, Mohamed Maouche, Marc Tommasi, Nicolas Papernot"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": ". The V oicePrivacy challenge (VPC) focuses on\nremoving the speaker identity facet of a speech signal; there-\nfore, removing personal information from the spoken content is\nnot part of the challenge. The baseline of the VPC ",
                    "Citation Text": "F. Fang, X. Wang, J. Yamagishi, I. Echizen, M. Todisco, N. Evans,\nand J.-F. Bonastre, \u201cSpeaker Anonymization Using X-vector and\nNeural Waveform Models,\u201d in 10th ISCA Speech Synthesis Work-\nshop , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.13561",
                        "Citation Paper Title": "Title:Speaker Anonymization Using X-vector and Neural Waveform Models",
                        "Citation Paper Abstract": "Abstract:The social media revolution has produced a plethora of web services to which users can easily upload and share multimedia documents. Despite the popularity and convenience of such services, the sharing of such inherently personal data, including speech data, raises obvious security and privacy concerns. In particular, a user's speech data may be acquired and used with speech synthesis systems to produce high-quality speech utterances which reflect the same user's speaker identity. These utterances may then be used to attack speaker verification systems. One solution to mitigate these concerns involves the concealing of speaker identities before the sharing of speech data. For this purpose, we present a new approach to speaker anonymization. The idea is to extract linguistic and speaker identity features from an utterance and then to use these with neural acoustic and waveform models to synthesize anonymized speech. The original speaker identity, in the form of timbre, is suppressed and replaced with that of an anonymous pseudo identity. The approach exploits state-of-the-art x-vector speaker representations. These are used to derive anonymized pseudo speaker identities through the combination of multiple, random speaker x-vectors. Experimental results show that the proposed approach is effective in concealing speaker identities. It increases the equal error rate of a speaker verification system while maintaining high quality, anonymized speech.",
                        "Citation Paper Authors": "Authors:Fuming Fang, Xin Wang, Junichi Yamagishi, Isao Echizen, Massimiliano Todisco, Nicholas Evans, Jean-Francois Bonastre"
                    }
                },
                {
                    "Sentence ID": 3,
                    "Sentence": "in the European Union, emphasize the\nneed for service providers to ensure privacy preservation and\nprotection of personal data. As speech data can re\ufb02ect the\nspeaker\u2019s biological and behavioral characteristics, it is quali-\n\ufb01ed as personal data ",
                    "Citation Text": "A. Nautsch, C. Jasserand, E. Kindt, M. Todisco, I. Trancoso, and\nN. Evans, \u201cThe GDPR & Speech Data: Re\ufb02ections of Legal and\nTechnology Communities, First Steps Towards a Common Under-\nstanding,\u201d in In Interspeech , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.03458",
                        "Citation Paper Title": "Title:The GDPR & Speech Data: Reflections of Legal and Technology Communities, First Steps towards a Common Understanding",
                        "Citation Paper Abstract": "Abstract:Privacy preservation and the protection of speech data is in high demand, not least as a result of recent regulation, e.g. the General Data Protection Regulation (GDPR) in the EU. While there has been a period with which to prepare for its implementation, its implications for speech data is poorly understood. This assertion applies to both the legal and technology communities, and is hardly surprising since there is no universal definition of 'privacy', let alone a clear understanding of when or how the GDPR applies to the capture, storage and processing of speech data. In aiming to initiate the discussion that is needed to establish a level of harmonisation that is thus far lacking, this contribution presents some reflections of both legal and technology communities on the implications of the GDPR as regards speech data. The article outlines the need for taxonomies at the intersection of speech technology and data privacy - a discussion that is still very much in its infancy - and describes the ways to safeguards and priorities for future research. In being agnostic to any specific application, the treatment should be of interest to the speech communication community at large.",
                        "Citation Paper Authors": "Authors:Andreas Nautsch, Catherine Jasserand, Els Kindt, Massimiliano Todisco, Isabel Trancoso, Nicholas Evans"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.04303v4": {
            "Paper Title": "Canonical Noise Distributions and Private Hypothesis Tests",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.14815v3": {
            "Paper Title": "Feature Importance Guided Attack: A Model Agnostic Adversarial Attack",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": ". We collected our legitimate data using URLs\ntaken from Tranco ",
                    "Citation Text": "V . Le Pochat, T. Van Goethem, S. Tajalizadehkhoob, M. Korczy \u00b4nski,\nand W. Joosen, \u201cTranco: A research-oriented top sites ranking hardened\nagainst manipulation,\u201d in Proceedings of the 26th Annual Network and\nDistributed System Security Symposium , ser. NDSS 2019, Feb. 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.01156",
                        "Citation Paper Title": "Title:Tranco: A Research-Oriented Top Sites Ranking Hardened Against Manipulation",
                        "Citation Paper Abstract": "Abstract:In order to evaluate the prevalence of security and privacy practices on a representative sample of the Web, researchers rely on website popularity rankings such as the Alexa list. While the validity and representativeness of these rankings are rarely questioned, our findings show the contrary: we show for four main rankings how their inherent properties (similarity, stability, representativeness, responsiveness and benignness) affect their composition and therefore potentially skew the conclusions made in studies. Moreover, we find that it is trivial for an adversary to manipulate the composition of these lists. We are the first to empirically validate that the ranks of domains in each of the lists are easily altered, in the case of Alexa through as little as a single HTTP request. This allows adversaries to manipulate rankings on a large scale and insert malicious domains into whitelists or bend the outcome of research studies to their will. To overcome the limitations of such rankings, we propose improvements to reduce the fluctuations in list composition and guarantee better defenses against manipulation. To allow the research community to work with reliable and reproducible rankings, we provide Tranco, an improved ranking that we offer through an online service available at this https URL.",
                        "Citation Paper Authors": "Authors:Victor Le Pochat, Tom Van Goethem, Samaneh Tajalizadehkhoob, Maciej Korczy\u0144ski, Wouter Joosen"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.13116v2": {
            "Paper Title": "Fed-TDA: Federated Tabular Data Augmentation on Non-IID Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.05083v3": {
            "Paper Title": "Reducing Exploitability with Population Based Training",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.12183v2": {
            "Paper Title": "\"You Can't Fix What You Can't Measure\": Privately Measuring Demographic\n  Performance Disparities in Federated Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.09744v2": {
            "Paper Title": "MLMSA: Multi-Label Multi-Side-Channel-Information enabled Deep Learning\n  Attacks on APUF Variants",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.10318v3": {
            "Paper Title": "Learned Systems Security",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": ": BAO. In a traditional query optimizer, different\nexecution plans are considered to minimize resource load.\nBAO introduces ML to perform this selection ",
                    "Citation Text": "Sanjay Krishnan, Zongheng Yang, Ken Goldberg, Joseph Hellerstein,\nand Ion Stoica. Learning to optimize join queries with deep reinforce-\nment learning. arXiv preprint arXiv:1808.03196 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1808.03196",
                        "Citation Paper Title": "Title:Learning to Optimize Join Queries With Deep Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:Exhaustive enumeration of all possible join orders is often avoided, and most optimizers leverage heuristics to prune the search space. The design and implementation of heuristics are well-understood when the cost model is roughly linear, and we find that these heuristics can be significantly suboptimal when there are non-linearities in cost. Ideally, instead of a fixed heuristic, we would want a strategy to guide the search space in a more data-driven way---tailoring the search to a specific dataset and query workload. Recognizing the link between classical Dynamic Programming enumeration methods and recent results in Reinforcement Learning (RL), we propose a new method for learning optimized join search strategies. We present our RL-based DQ optimizer, which currently optimizes select-project-join blocks. We implement three versions of DQ to illustrate the ease of integration into existing DBMSes: (1) A version built on top of Apache Calcite, (2) a version integrated into PostgreSQL, and (3) a version integrated into SparkSQL. Our extensive evaluation shows that DQ achieves plans with optimization costs and query execution times competitive with the native query optimizer in each system, but can execute significantly faster after learning (often by orders of magnitude).",
                        "Citation Paper Authors": "Authors:Sanjay Krishnan, Zongheng Yang, Ken Goldberg, Joseph Hellerstein, Ion Stoica"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.13993v2": {
            "Paper Title": "Metaverse Communications, Networking, Security, and Applications:\n  Research Issues, State-of-the-Art, and Future Directions",
            "Sentences": [
                {
                    "Sentence ID": 30,
                    "Sentence": "highlighted the potential of blockchain technology\nand AI for future Metaverse construction. Huynh-The et al. ",
                    "Citation Text": "T. Huynh-The, Q.-V . Pham, X.-Q. Pham, T. T. Nguyen, Z. Han, and\nD.-S. Kim, \u201cArti\ufb01cial intelligence for the metaverse: A survey,\u201d arXiv\npreprint arXiv:2202.10336 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.10336",
                        "Citation Paper Title": "Title:Artificial Intelligence for the Metaverse: A Survey",
                        "Citation Paper Abstract": "Abstract:Along with the massive growth of the Internet from the 1990s until now, various innovative technologies have been created to bring users breathtaking experiences with more virtual interactions in cyberspace. Many virtual environments with thousands of services and applications, from social networks to virtual gaming worlds, have been developed with immersive experience and digital transformation, but most are incoherent instead of being integrated into a platform. In this context, metaverse, a term formed by combining meta and universe, has been introduced as a shared virtual world that is fueled by many emerging technologies, such as fifth-generation networks and beyond, virtual reality, and artificial intelligence (AI). Among such technologies, AI has shown the great importance of processing big data to enhance immersive experience and enable human-like intelligence of virtual agents. In this survey, we make a beneficial effort to explore the role of AI in the foundation and development of the metaverse. We first deliver a preliminary of AI, including machine learning algorithms and deep learning architectures, and its role in the metaverse. We then convey a comprehensive investigation of AI-based methods concerning six technical aspects that have potentials for the metaverse: natural language processing, machine vision, blockchain, networking, digital twin, and neural interface, and being potential for the metaverse. Subsequently, several AI-aided applications, such as healthcare, manufacturing, smart cities, and gaming, are studied to be deployed in the virtual worlds. Finally, we conclude the key contribution of this survey and open some future research directions in AI for the metaverse.",
                        "Citation Paper Authors": "Authors:Thien Huynh-The, Quoc-Viet Pham, Xuan-Qui Pham, Thanh Thi Nguyen, Zhu Han, Dong-Seong Kim"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "explored the possible privacy threats in the online game\n\u2019Second Life\u201d from the legal and social angles. Very recently,\nWang et al. ",
                    "Citation Text": "Y . Wang, Z. Su, N. Zhang, D. Liu, R. Xing, T. H. Luan, and X. Shen,\n\u201cA survey on metaverse: Fundamentals, security, and privacy,\u201d arXiv\npreprint arXiv:2203.02662 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.02662",
                        "Citation Paper Title": "Title:A Survey on Metaverse: Fundamentals, Security, and Privacy",
                        "Citation Paper Abstract": "Abstract:Metaverse, as an evolving paradigm of the next-generation Internet, aims to build a fully immersive, hyper spatiotemporal, and self-sustaining virtual shared space for humans to play, work, and socialize. Driven by recent advances in emerging technologies such as extended reality, artificial intelligence, and blockchain, metaverse is stepping from science fiction to an upcoming reality. However, severe privacy invasions and security breaches (inherited from underlying technologies or emerged in the new digital ecology) of metaverse can impede its wide deployment. At the same time, a series of fundamental challenges (e.g., scalability and interoperability) can arise in metaverse security provisioning owing to the intrinsic characteristics of metaverse, such as immersive realism, hyper spatiotemporality, sustainability, and heterogeneity. In this paper, we present a comprehensive survey of the fundamentals, security, and privacy of metaverse. Specifically, we first investigate a novel distributed metaverse architecture and its key characteristics with ternary-world interactions. Then, we discuss the security and privacy threats, present the critical challenges of metaverse systems, and review the state-of-the-art countermeasures. Finally, we draw open research directions for building future metaverse systems.",
                        "Citation Paper Authors": "Authors:Yuntao Wang, Zhou Su, Ning Zhang, Rui Xing, Dongxiao Liu, Tom H. Luan, Xuemin Shen"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "previously identi\ufb01ed four attributes of workable virtual spaces\n(or Metaverse), i.e., interoperability, scalability, realism, and\nubiquity, and talked about the current developments in funda-\nmental virtual world technologies. Lee et al. ",
                    "Citation Text": "L.-H. Lee, T. Braud, P. Zhou, L. Wang, D. Xu, Z. Lin, A. Kumar,\nC. Bermejo, and P. Hui, \u201cAll one needs to know about metaverse: A\ncomplete survey on technological singularity, virtual ecosystem, and\nresearch agenda,\u201d arXiv preprint arXiv:2110.05352 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.05352",
                        "Citation Paper Title": "Title:All One Needs to Know about Metaverse: A Complete Survey on Technological Singularity, Virtual Ecosystem, and Research Agenda",
                        "Citation Paper Abstract": "Abstract:Since the popularisation of the Internet in the 1990s, the cyberspace has kept evolving. We have created various computer-mediated virtual environments including social networks, video conferencing, virtual 3D worlds (e.g., VR Chat), augmented reality applications (e.g., Pokemon Go), and Non-Fungible Token Games (e.g., Upland). Such virtual environments, albeit non-perpetual and unconnected, have bought us various degrees of digital transformation. The term `metaverse' has been coined to further facilitate the digital transformation in every aspect of our physical lives. At the core of the metaverse stands the vision of an immersive Internet as a gigantic, unified, persistent, and shared realm. While the metaverse may seem futuristic, catalysed by emerging technologies such as Extended Reality, 5G, and Artificial Intelligence, the digital `big bang' of our cyberspace is not far away. This survey paper presents the first effort to offer a comprehensive framework that examines the latest metaverse development under the dimensions of state-of-the-art technologies and metaverse ecosystems, and illustrates the possibility of the digital `big bang'. First, technologies are the enablers that drive the transition from the current Internet to the metaverse. We thus examine eight enabling technologies rigorously - Extended Reality, User Interactivity (Human-Computer Interaction), Artificial Intelligence, Blockchain, Computer Vision, IoT and Robotics, Edge and Cloud computing, and Future Mobile Networks. In terms of applications, the metaverse ecosystem allows human users to live and play within a self-sustaining, persistent, and shared realm. Therefore, we discuss six user-centric factors -- Avatar, Content Creation, Virtual Economy, Social Acceptability, Security and Privacy, and Trust and Accountability. Finally, we propose a concrete research agenda for the development of the metaverse.",
                        "Citation Paper Authors": "Authors:Lik-Hang Lee, Tristan Braud, Pengyuan Zhou, Lin Wang, Dianlei Xu, Zijun Lin, Abhishek Kumar, Carlos Bermejo, Pan Hui"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "talked about the three components of the Metaverse\nincluding content, software, and hardware. They also reviewed\nrepresentative applications, implementation, and user interac-\ntions within the Metaverse. Xu et al. ",
                    "Citation Text": "M. Xu, W. C. Ng, W. Y . B. Lim, J. Kang, Z. Xiong, D. Niyato,\nQ. Yang, X. S. Shen, and C. Miao, \u201cA full dive into realizing the edge-\nenabled metaverse: Visions, enabling technologies, and challenges,\u201d\narXiv preprint arXiv:2203.05471 , 2022.34",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.05471",
                        "Citation Paper Title": "Title:A Full Dive into Realizing the Edge-enabled Metaverse: Visions, Enabling Technologies,and Challenges",
                        "Citation Paper Abstract": "Abstract:Dubbed \"the successor to the mobile Internet\", the concept of the Metaverse has grown in popularity. While there exist lite versions of the Metaverse today, they are still far from realizing the full vision of an immersive, embodied, and interoperable Metaverse. Without addressing the issues of implementation from the communication and networking, as well as computation perspectives, the Metaverse is difficult to succeed the Internet, especially in terms of its accessibility to billions of users today. In this survey, we focus on the edge-enabled Metaverse to realize its ultimate vision. We first provide readers with a succinct tutorial of the Metaverse, an introduction to the architecture, as well as current developments. To enable ubiquitous, seamless, and embodied access to the Metaverse, we discuss the communication and networking challenges and survey cutting-edge solutions and concepts that leverage next-generation communication systems for users to immerse as and interact with embodied avatars in the Metaverse. Moreover, given the high computation costs required, e.g., to render 3D virtual worlds and run data-hungry artificial intelligence-driven avatars, we discuss the computation challenges and cloud-edge-end computation framework-driven solutions to realize the Metaverse on resource-constrained edge devices. Next, we explore how blockchain technologies can aid in the interoperable development of the Metaverse, not just in terms of empowering the economic circulation of virtual user-generated content but also to manage physical edge resources in a decentralized, transparent, and immutable manner. Finally, we discuss the future research directions towards realizing the true vision of the edge-enabled Metaverse.",
                        "Citation Paper Authors": "Authors:Minrui Xu, Wei Chong Ng, Wei Yang Bryan Lim, Jiawen Kang, Zehui Xiong, Dusit Niyato, Qiang Yang, Xuemin Sherman Shen, Chunyan Miao"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "discussed the Metaverse development status with reference to\nthe social Metaverse, virtual reality, supporting technologies,\ninfrastructures, industrial projects, and national policies. Yang\net al. ",
                    "Citation Text": "Q. Yang, Y . Zhao, H. Huang, and Z. Zheng, \u201cFusing blockchain and\nai with metaverse: A survey,\u201d arXiv preprint arXiv:2201.03201 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.03201",
                        "Citation Paper Title": "Title:Fusing Blockchain and AI with Metaverse: A Survey",
                        "Citation Paper Abstract": "Abstract:Metaverse as the latest buzzword has attracted great attention from both industry and academia. Metaverse seamlessly integrates the real world with the virtual world and allows avatars to carry out rich activities including creation, display, entertainment, social networking, and trading. Thus, it is promising to build an exciting digital world and to transform a better physical world through the exploration of the metaverse. In this survey, we dive into the metaverse by discussing how Blockchain and Artificial Intelligence (AI) fuse with it through investigating the state-of-the-art studies across the metaverse components, digital currencies, AI applications in the virtual world, and blockchain-empowered technologies. Further exploitation and interdisciplinary research on the fusion of AI and Blockchain towards metaverse will definitely require collaboration from both academia and industries. We wish that our survey can help researchers, engineers, and educators build an open, fair, and rational future metaverse.",
                        "Citation Paper Authors": "Authors:Qinglin Yang, Yetong Zhao, Huawei Huang, Zehui Xiong, Jiawen Kang, Zibin Zheng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2201.10606v2": {
            "Paper Title": "FETA: Fair Evaluation of Touch-based Authentication",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.06998v2": {
            "Paper Title": "DE-FAKE: Detection and Attribution of Fake Images Generated by\n  Text-to-Image Generation Models",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "6.1 Text-to-Image Generation\nTypically, text-to-image generation takes a text description\n(i.e., a prompt) as input and outputs an image that matches\nthe text description. Some pioneer works of text-to-image\ngeneration [25, 38] are based on GANs ",
                    "Citation Text": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative Adversarial Nets. In Annual Con-\nference on Neural Information Processing Systems (NIPS) ,\npages 2672\u20132680. NIPS, 2014. 1, 12",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.2661",
                        "Citation Paper Title": "Title:Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "demon-\nstrate that fake images generated by various traditional gen-\neration models can be attributed to their sources and reveal\nthe fact that these traditional generation models leave \ufb01nger-\nprints in the generated images. Girish et al. ",
                    "Citation Text": "Sharath Girish, Saksham Suri, Sai Saketh Rambhatla, and\nAbhinav Shrivastava. Towards Discovery and Attribution of\nOpen-World GAN Generated Images. In IEEE International\nConference on Computer Vision (ICCV) , pages 14094\u201314103.\nIEEE, 2021. 2, 12",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.04580",
                        "Citation Paper Title": "Title:Towards Discovery and Attribution of Open-world GAN Generated Images",
                        "Citation Paper Abstract": "Abstract:With the recent progress in Generative Adversarial Networks (GANs), it is imperative for media and visual forensics to develop detectors which can identify and attribute images to the model generating them. Existing works have shown to attribute images to their corresponding GAN sources with high accuracy. However, these works are limited to a closed set scenario, failing to generalize to GANs unseen during train time and are therefore, not scalable with a steady influx of new GANs. We present an iterative algorithm for discovering images generated from previously unseen GANs by exploiting the fact that all GANs leave distinct fingerprints on their generated images. Our algorithm consists of multiple components including network training, out-of-distribution detection, clustering, merge and refine steps. Through extensive experiments, we show that our algorithm discovers unseen GANs with high accuracy and also generalizes to GANs trained on unseen real datasets. We additionally apply our algorithm to attribution and discovery of GANs in an online fashion as well as to the more standard task of real/fake detection. Our experiments demonstrate the effectiveness of our approach to discover new GANs and can be used in an open-world setup.",
                        "Citation Paper Authors": "Authors:Sharath Girish, Saksham Suri, Saketh Rambhatla, Abhinav Shrivastava"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "and low-level vi-\nsion models [4, 7]) from real images. The authors argue that\nthese fake images have some common defects that allow us\nto distinguish them from real images. Yu et al. ",
                    "Citation Text": "Ning Yu, Larry Davis, and Mario Fritz. Attributing Fake Im-\nages to GANs: Learning and Analyzing GAN Fingerprints. In\nIEEE International Conference on Computer Vision (ICCV) ,\npages 7555\u20137565. IEEE, 2019. 2, 8, 12",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.08180",
                        "Citation Paper Title": "Title:Attributing Fake Images to GANs: Learning and Analyzing GAN Fingerprints",
                        "Citation Paper Abstract": "Abstract:Recent advances in Generative Adversarial Networks (GANs) have shown increasing success in generating photorealistic images. But they also raise challenges to visual forensics and model attribution. We present the first study of learning GAN fingerprints towards image attribution and using them to classify an image as real or GAN-generated. For GAN-generated images, we further identify their sources. Our experiments show that (1) GANs carry distinct model fingerprints and leave stable fingerprints in their generated images, which support image attribution; (2) even minor differences in GAN training can result in different fingerprints, which enables fine-grained model authentication; (3) fingerprints persist across different image frequencies and patches and are not biased by GAN artifacts; (4) fingerprint finetuning is effective in immunizing against five types of adversarial image perturbations; and (5) comparisons also show our learned fingerprints consistently outperform several baselines in a variety of setups.",
                        "Citation Paper Authors": "Authors:Ning Yu, Larry Davis, Mario Fritz"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": ", has achieved state-of-the-\nart performance compared to previous works. This is also the\nreason why we focus on such models in this work.\n6.2 Fake Image Detection and Attribution\nWang et al. ",
                    "Citation Text": "Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew\nOwens, and Alexei A. Efros. CNN-Generated Images Are\nSurprisingly Easy to Spot... for Now. In IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) , pages\n8692\u20138701. IEEE, 2020. 2, 5, 12",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.11035",
                        "Citation Paper Title": "Title:CNN-generated images are surprisingly easy to spot... for now",
                        "Citation Paper Abstract": "Abstract:In this work we ask whether it is possible to create a \"universal\" detector for telling apart real images from these generated by a CNN, regardless of architecture or dataset used. To test this, we collect a dataset consisting of fake images generated by 11 different CNN-based image generator models, chosen to span the space of commonly used architectures today (ProGAN, StyleGAN, BigGAN, CycleGAN, StarGAN, GauGAN, DeepFakes, cascaded refinement networks, implicit maximum likelihood estimation, second-order attention super-resolution, seeing-in-the-dark). We demonstrate that, with careful pre- and post-processing and data augmentation, a standard image classifier trained on only one specific CNN generator (ProGAN) is able to generalize surprisingly well to unseen architectures, datasets, and training methods (including the just released StyleGAN2). Our findings suggest the intriguing possibility that today's CNN-generated images share some common systematic flaws, preventing them from achieving realistic image synthesis. Code and pre-trained networks are available at this https URL .",
                        "Citation Paper Authors": "Authors:Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, Alexei A. Efros"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.05507v3": {
            "Paper Title": "Check Your Other Door! Creating Backdoor Attacks in the Frequency Domain",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "analyzed the characteristics of spatial backdoor attacks in the frequency domain and pro-\nposed a technique to create smooth but visible spatial backdoor triggers. Recently, [11, 48]\nintroduced a simple way to apply trojan attacks in the frequency domain. Speci\ufb01cally, ",
                    "Citation Text": "Yu Feng, Benteng Ma, Jing Zhang, Shanshan Zhao, Yong Xia, and Dacheng Tao.\nFiba: Frequency-injection based backdoor attack in medical image analysis. ArXiv ,\nabs/2112.01148, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.01148",
                        "Citation Paper Title": "Title:FIBA: Frequency-Injection based Backdoor Attack in Medical Image Analysis",
                        "Citation Paper Abstract": "Abstract:In recent years, the security of AI systems has drawn increasing research attention, especially in the medical imaging realm. To develop a secure medical image analysis (MIA) system, it is a must to study possible backdoor attacks (BAs), which can embed hidden malicious behaviors into the system. However, designing a unified BA method that can be applied to various MIA systems is challenging due to the diversity of imaging modalities (e.g., X-Ray, CT, and MRI) and analysis tasks (e.g., classification, detection, and segmentation). Most existing BA methods are designed to attack natural image classification models, which apply spatial triggers to training images and inevitably corrupt the semantics of poisoned pixels, leading to the failures of attacking dense prediction models. To address this issue, we propose a novel Frequency-Injection based Backdoor Attack method (FIBA) that is capable of delivering attacks in various MIA tasks. Specifically, FIBA leverages a trigger function in the frequency domain that can inject the low-frequency information of a trigger image into the poisoned image by linearly combining the spectral amplitude of both images. Since it preserves the semantics of the poisoned image pixels, FIBA can perform attacks on both classification and dense prediction models. Experiments on three benchmarks in MIA (i.e., ISIC-2019 for skin lesion classification, KiTS-19 for kidney tumor segmentation, and EAD-2019 for endoscopic artifact detection), validate the effectiveness of FIBA and its superiority over state-of-the-art methods in attacking MIA models as well as bypassing backdoor defense. Source code will be available at this https URL.",
                        "Citation Paper Authors": "Authors:Yu Feng, Benteng Ma, Jing Zhang, Shanshan Zhao, Yong Xia, Dacheng Tao"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": "blends the low-frequency content of a trigger image with that of the target images, and ",
                    "Citation Text": "Tong Wang, Yuan Yao, Feng Xu, Shengwei An, Hanghang Tong, and Ting Wang.\nBackdoor attack through frequency domain. ArXiv , abs/2111.10991, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.10991",
                        "Citation Paper Title": "Title:Backdoor Attack through Frequency Domain",
                        "Citation Paper Abstract": "Abstract:Backdoor attacks have been shown to be a serious threat against deep learning systems such as biometric authentication and autonomous driving. An effective backdoor attack could enforce the model misbehave under certain predefined conditions, i.e., triggers, but behave normally otherwise. However, the triggers of existing attacks are directly injected in the pixel space, which tend to be detectable by existing defenses and visually identifiable at both training and inference stages. In this paper, we propose a new backdoor attack FTROJAN through trojaning the frequency domain. The key intuition is that triggering perturbations in the frequency domain correspond to small pixel-wise perturbations dispersed across the entire image, breaking the underlying assumptions of existing defenses and making the poisoning images visually indistinguishable from clean ones. We evaluate FTROJAN in several datasets and tasks showing that it achieves a high attack success rate without significantly degrading the prediction accuracy on benign inputs. Moreover, the poisoning images are nearly invisible and retain high perceptual quality. We also evaluate FTROJAN against state-of-the-art defenses as well as several adaptive defenses that are designed on the frequency domain. The results show that FTROJAN can robustly elude or significantly degenerate the performance of these defenses.",
                        "Citation Paper Authors": "Authors:Tong Wang, Yuan Yao, Feng Xu, Shengwei An, Hanghang Tong, Ting Wang"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "observes that when a poisoned\nimage is blended with a clean one, the backdoor is still activated, which allows for detecting\nbackdoor attacks by analyzing the entropy of the prediction vectors. SPECTRE ",
                    "Citation Text": "Jonathan Hayase, Weihao Kong, Raghav Somani, and Sewoong Oh. Spectre: Defend-\ning against backdoor attacks using robust statistics. ArXiv , abs/2104.11315, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.11315",
                        "Citation Paper Title": "Title:SPECTRE: Defending Against Backdoor Attacks Using Robust Statistics",
                        "Citation Paper Abstract": "Abstract:Modern machine learning increasingly requires training on a large collection of data from multiple sources, not all of which can be trusted. A particularly concerning scenario is when a small fraction of poisoned data changes the behavior of the trained model when triggered by an attacker-specified watermark. Such a compromised model will be deployed unnoticed as the model is accurate otherwise. There have been promising attempts to use the intermediate representations of such a model to separate corrupted examples from clean ones. However, these defenses work only when a certain spectral signature of the poisoned examples is large enough for detection. There is a wide range of attacks that cannot be protected against by the existing defenses. We propose a novel defense algorithm using robust covariance estimation to amplify the spectral signature of corrupted data. This defense provides a clean model, completely removing the backdoor, even in regimes where previous methods have no hope of detecting the poisoned examples. Code and pre-trained models are available at this https URL .",
                        "Citation Paper Authors": "Authors:Jonathan Hayase, Weihao Kong, Raghav Somani, Sewoong Oh"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": ", also inspired by steganography, generated sample-speci\ufb01c triggers by encod-\ning an attacker-speci\ufb01ed \u201cstring\" into clean samples using an autoencoder network. ",
                    "Citation Text": "Yi Zeng, Won Park, Zhuoqing Morley Mao, and R. Jia. Rethinking the backdoor at-\ntacks\u2019 triggers: A frequency perspective. ArXiv , abs/2104.03413, 2021.HAMMOUD, GHANEM: CHECK YOUR OTHER DOOR! 15",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.03413",
                        "Citation Paper Title": "Title:Rethinking the Backdoor Attacks' Triggers: A Frequency Perspective",
                        "Citation Paper Abstract": "Abstract:Backdoor attacks have been considered a severe security threat to deep learning. Such attacks can make models perform abnormally on inputs with predefined triggers and still retain state-of-the-art performance on clean data. While backdoor attacks have been thoroughly investigated in the image domain from both attackers' and defenders' sides, an analysis in the frequency domain has been missing thus far.\nThis paper first revisits existing backdoor triggers from a frequency perspective and performs a comprehensive analysis. Our results show that many current backdoor attacks exhibit severe high-frequency artifacts, which persist across different datasets and resolutions. We further demonstrate these high-frequency artifacts enable a simple way to detect existing backdoor triggers at a detection rate of 98.50% without prior knowledge of the attack details and the target model. Acknowledging previous attacks' weaknesses, we propose a practical way to create smooth backdoor triggers without high-frequency artifacts and study their detectability. We show that existing defense works can benefit by incorporating these smooth triggers into their design consideration. Moreover, we show that the detector tuned over stronger smooth triggers can generalize well to unseen weak smooth triggers. In short, our work emphasizes the importance of considering frequency analysis when designing both backdoor attacks and defenses in deep learning.",
                        "Citation Paper Authors": "Authors:Yi Zeng, Won Park, Z. Morley Mao, Ruoxi Jia"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.01610v4": {
            "Paper Title": "Quantum Proofs of Deletion for Learning with Errors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.08643v2": {
            "Paper Title": "A General Framework for Auditing Differentially Private Machine Learning",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": ". Additionally, we test\nDP-SGD on FMNIST and CIFAR10 (here with N= 500 ) using ",
                    "Citation Text": "Ashkan Yousefpour, Igor Shilov, Alexandre Sablayrolles, Davide Testuggine, Karthik Prasad,\nMani Malek, John Nguyen, Sayan Ghosh, Akash Bharadwaj, Jessica Zhao, Graham Cormode,\nand Ilya Mironov. Opacus: User-friendly differential privacy library in PyTorch. arXiv preprint\narXiv:2109.12298 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.12298",
                        "Citation Paper Title": "Title:Opacus: User-Friendly Differential Privacy Library in PyTorch",
                        "Citation Paper Abstract": "Abstract:We introduce Opacus, a free, open-source PyTorch library for training deep learning models with differential privacy (hosted at this http URL). Opacus is designed for simplicity, flexibility, and speed. It provides a simple and user-friendly API, and enables machine learning practitioners to make a training pipeline private by adding as little as two lines to their code. It supports a wide variety of layers, including multi-head attention, convolution, LSTM, GRU (and generic RNN), and embedding, right out of the box and provides the means for supporting other user-defined layers. Opacus computes batched per-sample gradients, providing higher efficiency compared to the traditional \"micro batch\" approach. In this paper we present Opacus, detail the principles that drove its implementation and unique features, and benchmark it against other frameworks for training models with differential privacy as well as standard PyTorch.",
                        "Citation Paper Authors": "Authors:Ashkan Yousefpour, Igor Shilov, Alexandre Sablayrolles, Davide Testuggine, Karthik Prasad, Mani Malek, John Nguyen, Sayan Ghosh, Akash Bharadwaj, Jessica Zhao, Graham Cormode, Ilya Mironov"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "Although differential privacy of a mechanism is established with mathematical proof, previous\nwork has shown that holes can exist, either in theory, such as the sparse vector mechanism (of\nwhich erroneous versions have been proposed ",
                    "Citation Text": "Zeyu Ding, Yuxin Wang, Guanhong Wang, Danfeng Zhang, and Daniel Kifer. Detecting\nviolations of differential privacy. In Proceedings of the 2018 ACM SIGSAC Conference on\nComputer and Communications Security , pages 475\u2013489, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.10277",
                        "Citation Paper Title": "Title:Detecting Violations of Differential Privacy",
                        "Citation Paper Abstract": "Abstract:The widespread acceptance of differential privacy has led to the publication of many sophisticated algorithms for protecting privacy. However, due to the subtle nature of this privacy definition, many such algorithms have bugs that make them violate their claimed privacy. In this paper, we consider the problem of producing counterexamples for such incorrect algorithms. The counterexamples are designed to be short and human-understandable so that the counterexample generator can be used in the development process -- a developer could quickly explore variations of an algorithm and investigate where they break down. Our approach is statistical in nature. It runs a candidate algorithm many times and uses statistical tests to try to detect violations of differential privacy. An evaluation on a variety of incorrect published algorithms validates the usefulness of our approach: it correctly rejects incorrect algorithms and provides counterexamples for them within a few seconds.",
                        "Citation Paper Authors": "Authors:Zeyu Ding, Yuxin Wang, Guanhong Wang, Danfeng Zhang, Daniel Kifer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.13983v3": {
            "Paper Title": "Quantum Augmented Dual Attack",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.09278v3": {
            "Paper Title": "Differentially Private Federated Learning on Heterogeneous Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.04120v2": {
            "Paper Title": "Privacy-Protecting Techniques for Behavioral Biometric Data: A Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.00810v2": {
            "Paper Title": "Offline Reinforcement Learning with Differential Privacy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.06401v3": {
            "Paper Title": "PoisonedEncoder: Poisoning the Unlabeled Pre-training Data in\n  Contrastive Learning",
            "Sentences": [
                {
                    "Sentence ID": 52,
                    "Sentence": "measures the impact of each training\nexample on the error rate of the initial model and removes\nthe training examples that have large negative impact. Tran et\nal. ",
                    "Citation Text": "Brandon Tran, Jerry Li, and Aleksander Madry. Spec-\ntral signatures in backdoor attacks. arXiv preprint\narXiv:1811.00636 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.00636",
                        "Citation Paper Title": "Title:Spectral Signatures in Backdoor Attacks",
                        "Citation Paper Abstract": "Abstract:A recent line of work has uncovered a new form of data poisoning: so-called \\emph{backdoor} attacks. These attacks are particularly dangerous because they do not affect a network's behavior on typical, benign data. Rather, the network only deviates from its expected output when triggered by a perturbation planted by an adversary.\nIn this paper, we identify a new property of all known backdoor attacks, which we call \\emph{spectral signatures}. This property allows us to utilize tools from robust statistics to thwart the attacks. We demonstrate the efficacy of these signatures in detecting and removing poisoned examples on real image sets and state of the art neural network architectures. We believe that understanding spectral signatures is a crucial first step towards designing ML systems secure against such backdoor attacks",
                        "Citation Paper Authors": "Authors:Brandon Tran, Jerry Li, Aleksander Madry"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "proposed to relabel\na training input as the most frequent label among its k-nearest\nneighbors, and the relabeled training data are used to train\na classifier. Peri et al. ",
                    "Citation Text": "Neehar Peri, Neal Gupta, W Ronny Huang, Liam Fowl,\nChen Zhu, Soheil Feizi, Tom Goldstein, and John P Dick-\nerson. Deep k-nn defense against clean-label data poi-\nsoning attacks. In European Conference on Computer\nVision , pages 55\u201370. Springer, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.13374",
                        "Citation Paper Title": "Title:Deep k-NN Defense against Clean-label Data Poisoning Attacks",
                        "Citation Paper Abstract": "Abstract:Targeted clean-label data poisoning is a type of adversarial attack on machine learning systems in which an adversary injects a few correctly-labeled, minimally-perturbed samples into the training data, causing a model to misclassify a particular test sample during inference. Although defenses have been proposed for general poisoning attacks, no reliable defense for clean-label attacks has been demonstrated, despite the attacks' effectiveness and realistic applications. In this work, we propose a simple, yet highly-effective Deep k-NN defense against both feature collision and convex polytope clean-label attacks on the CIFAR-10 dataset. We demonstrate that our proposed strategy is able to detect over 99% of poisoned examples in both attacks and remove them without compromising model performance. Additionally, through ablation studies, we discover simple guidelines for selecting the value of k as well as for implementing the Deep k-NN defense on real-world datasets with class imbalance. Our proposed defense shows that current clean-label poisoning attack strategies can be annulled, and serves as a strong yet simple-to-implement baseline defense to test future clean-label poisoning attacks. Our code is available at this https URL",
                        "Citation Paper Authors": "Authors:Neehar Peri, Neal Gupta, W. Ronny Huang, Liam Fowl, Chen Zhu, Soheil Feizi, Tom Goldstein, John P. Dickerson"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.13187v2": {
            "Paper Title": "Advancements in Biometric Technology with Artificial Intelligence",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.14049v2": {
            "Paper Title": "Differentiable Search of Accurate and Robust Architectures",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": "is proposed to generate adversarial\nexamples in an iterative manner. On this basis, the projected gradient descent (PGD) ",
                    "Citation Text": "A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, \u201cTowards deep learning models\nresistant to adversarial attacks,\u201d The 6th International Conference on Learning Representations ,\n2018. [Online]. Available: https://openreview.net/pdf?id=rJzIBfZAb",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06083",
                        "Citation Paper Title": "Title:Towards Deep Learning Models Resistant to Adversarial Attacks",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL.",
                        "Citation Paper Authors": "Authors:Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu"
                    }
                },
                {
                    "Sentence ID": 59,
                    "Sentence": "and the lack of robustness of the differentiable search algorithm itself ",
                    "Citation Text": "A. Zela, T. Elsken, T. Saikia, Y . Marrakchi, T. Brox, and F. Hutter, \u201cUnderstanding and robustify-\ning differentiable architecture search,\u201d in International Conference on Learning Representations\n(ICLR) , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.09656",
                        "Citation Paper Title": "Title:Understanding and Robustifying Differentiable Architecture Search",
                        "Citation Paper Abstract": "Abstract:Differentiable Architecture Search (DARTS) has attracted a lot of attention due to its simplicity and small search costs achieved by a continuous relaxation and an approximation of the resulting bi-level optimization problem. However, DARTS does not work robustly for new problems: we identify a wide range of search spaces for which DARTS yields degenerate architectures with very poor test performance. We study this failure mode and show that, while DARTS successfully minimizes validation loss, the found solutions generalize poorly when they coincide with high validation loss curvature in the architecture space. We show that by adding one of various types of regularization we can robustify DARTS to find solutions with less curvature and better generalization properties. Based on these observations, we propose several simple variations of DARTS that perform substantially more robustly in practice. Our observations are robust across five search spaces on three image classification tasks and also hold for the very different domains of disparity estimation (a dense regression task) and language modelling.",
                        "Citation Paper Authors": "Authors:Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, Frank Hutter"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": ". Besides, some other methods such as data compression [ 10;11], feature denoising ",
                    "Citation Text": "C. Xie, Y . Wu, L. v. d. Maaten, A. L. Yuille, and K. He, \u201cFeature denoising for improving\nadversarial robustness,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , 2019, pp. 501\u2013509.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.03411",
                        "Citation Paper Title": "Title:Feature Denoising for Improving Adversarial Robustness",
                        "Citation Paper Abstract": "Abstract:Adversarial attacks to image classification systems present challenges to convolutional networks and opportunities for understanding them. This study suggests that adversarial perturbations on images lead to noise in the features constructed by these networks. Motivated by this observation, we develop new network architectures that increase adversarial robustness by performing feature denoising. Specifically, our networks contain blocks that denoise the features using non-local means or other filters; the entire networks are trained end-to-end. When combined with adversarial training, our feature denoising networks substantially improve the state-of-the-art in adversarial robustness in both white-box and black-box attack settings. On ImageNet, under 10-iteration PGD white-box attacks where prior art has 27.9% accuracy, our method achieves 55.7%; even under extreme 2000-iteration PGD white-box attacks, our method secures 42.6% accuracy. Our method was ranked first in Competition on Adversarial Attacks and Defenses (CAAD) 2018 --- it achieved 50.6% classification accuracy on a secret, ImageNet-like test dataset against 48 unknown attackers, surpassing the runner-up approach by ~10%. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan Yuille, Kaiming He"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": ", and so on.\nIn black-box settings, because the attackers have no access to the target model, they generate\nadversarial examples in completely different ways from the white-box attacks. One commonly\nused black-box attack method is the transfer-based attacks ",
                    "Citation Text": "N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and A. Swami, \u201cPractical black-\nbox attacks against machine learning,\u201d in Proceedings of the 2017 ACM on Asia Conference on\nComputer and Communications security , 2017, pp. 506\u2013519.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.02697",
                        "Citation Paper Title": "Title:Practical Black-Box Attacks against Machine Learning",
                        "Citation Paper Abstract": "Abstract:Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19% and 88.94%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.",
                        "Citation Paper Authors": "Authors:Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik, Ananthram Swami"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "is proposed\nto generate adversarial examples by a single step attack that increases the adversarial loss. FGSM\naccelerates the generation of adversarial examples, but the generated adversarial examples are weaker.\nTo enhance the attack, the basic iterative method (BIM) ",
                    "Citation Text": "A. Kurakin, I. J. Goodfellow, and S. Bengio, \u201cAdversarial examples in the physical world,\u201d in\nArti\ufb01cial Intelligence Safety and Security . Chapman and Hall/CRC, 2018, pp. 99\u2013112.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.02533",
                        "Citation Paper Title": "Title:Adversarial examples in the physical world",
                        "Citation Paper Abstract": "Abstract:Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.",
                        "Citation Paper Authors": "Authors:Alexey Kurakin, Ian Goodfellow, Samy Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2301.00252v1": {
            "Paper Title": "A Comparative Study of Image Disguising Methods for Confidential\n  Outsourced Learning",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": "Train shallow neural network lo-\ncally and outsources intermediate\nrepresentation to the cloud for\ndeeper training.The intermediate representation of images re-\nveals the visual characteristics of the related\nimages, vulnerable to visual re-identi\ufb01cation\nattacks.NA\nLi et al ",
                    "Citation Text": "M. Li, L. Lai, N. Suda, V . Chandra, and D. Z. Pan. Privynet: A \ufb02exible\nframework for privacy-preserving deep neural network training with A\n\ufb01ne-grained privacy control. CoRR , abs/1709.06161, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.06161",
                        "Citation Paper Title": "Title:PrivyNet: A Flexible Framework for Privacy-Preserving Deep Neural Network Training",
                        "Citation Paper Abstract": "Abstract:Massive data exist among user local platforms that usually cannot support deep neural network (DNN) training due to computation and storage resource constraints. Cloud-based training schemes provide beneficial services but suffer from potential privacy risks due to excessive user data collection. To enable cloud-based DNN training while protecting the data privacy simultaneously, we propose to leverage the intermediate representations of the data, which is achieved by splitting the DNNs and deploying them separately onto local platforms and the cloud. The local neural network (NN) is used to generate the feature representations. To avoid local training and protect data privacy, the local NN is derived from pre-trained NNs. The cloud NN is then trained based on the extracted intermediate representations for the target learning task. We validate the idea of DNN splitting by characterizing the dependency of privacy loss and classification accuracy on the local NN topology for a convolutional NN (CNN) based image classification task. Based on the characterization, we further propose PrivyNet to determine the local NN topology, which optimizes the accuracy of the target learning task under the constraints on privacy loss, local computation, and storage. The efficiency and effectiveness of PrivyNet are demonstrated with the CIFAR-10 dataset.",
                        "Citation Paper Authors": "Authors:Meng Li, Liangzhen Lai, Naveen Suda, Vikas Chandra, David Z. Pan"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "Use TEEs for con\ufb01dential CPU op-\nerations and masked data for con-\n\ufb01dential GPU operationsside channel attacks are an unaddressed con-\ncernMore ef\ufb01cient than cryptographic\nprotocols, but GPU operations on\nmasked data are still expensive\nAbadi et al. ",
                    "Citation Text": "M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov,\nK. Talwar, and L. Zhang. Deep learning with differential privacy. In\nProceedings of the 2016 ACM SIGSAC Conference on Computer and\nCommunications Security , CCS \u201916, pages 308\u2013318, New York, NY ,\nUSA, 2016. ACM.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.00133",
                        "Citation Paper Title": "Title:Deep Learning with Differential Privacy",
                        "Citation Paper Abstract": "Abstract:Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.",
                        "Citation Paper Authors": "Authors:Mart\u00edn Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, Li Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2301.00188v1": {
            "Paper Title": "New Challenges in Reinforcement Learning: A Survey of Security and\n  Privacy",
            "Sentences": [
                {
                    "Sentence ID": 70,
                    "Sentence": "States, actionsStates, actions\nrewardDifferential privacyJoint differential privacy\nOptimistic strategy\nChowdhury and\nZhou ",
                    "Citation Text": "Chowdhury, S.R., Zhou, X.: Di\u000berentially private regret minimization\nin episodic markov decision processes. arXiv preprint arXiv:2112.10599\n(2021)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.10599",
                        "Citation Paper Title": "Title:Differentially Private Regret Minimization in Episodic Markov Decision Processes",
                        "Citation Paper Abstract": "Abstract:We study regret minimization in finite horizon tabular Markov decision processes (MDPs) under the constraints of differential privacy (DP). This is motivated by the widespread applications of reinforcement learning (RL) in real-world sequential decision making problems, where protecting users' sensitive and private information is becoming paramount. We consider two variants of DP -- joint DP (JDP), where a centralized agent is responsible for protecting users' sensitive data and local DP (LDP), where information needs to be protected directly on the user side. We first propose two general frameworks -- one for policy optimization and another for value iteration -- for designing private, optimistic RL algorithms. We then instantiate these frameworks with suitable privacy mechanisms to satisfy JDP and LDP requirements, and simultaneously obtain sublinear regret guarantees. The regret bounds show that under JDP, the cost of privacy is only a lower order additive term, while for a stronger privacy protection under LDP, the cost suffered is multiplicative. Finally, the regret bounds are obtained by a unified analysis, which, we believe, can be extended beyond tabular MDPs.",
                        "Citation Paper Authors": "Authors:Sayak Ray Chowdhury, Xingyu Zhou"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "State-action Policy Safe explorationConvolutional neural\nnetwork\nTransfer learning\nSecurity of\nenvironment\nin MDPRakhsha et al. ",
                    "Citation Text": "Rakhsha, A., Radanovic, G., Devidze, R., Zhu, X., Singla, A.: Policy\nteaching via environment poisoning: Training-time adversarial attacks\nagainst reinforcement learning. In: International Conference on Machine\nLearning, pp. 7974{7984 (2020). PMLR",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.12909",
                        "Citation Paper Title": "Title:Policy Teaching via Environment Poisoning: Training-time Adversarial Attacks against Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:We study a security threat to reinforcement learning where an attacker poisons the learning environment to force the agent into executing a target policy chosen by the attacker. As a victim, we consider RL agents whose objective is to find a policy that maximizes average reward in undiscounted infinite-horizon problem settings. The attacker can manipulate the rewards or the transition dynamics in the learning environment at training-time and is interested in doing so in a stealthy manner. We propose an optimization framework for finding an \\emph{optimal stealthy attack} for different measures of attack cost. We provide sufficient technical conditions under which the attack is feasible and provide lower/upper bounds on the attack cost. We instantiate our attacks in two settings: (i) an \\emph{offline} setting where the agent is doing planning in the poisoned environment, and (ii) an \\emph{online} setting where the agent is learning a policy using a regret-minimization framework with poisoned feedback. Our results show that the attacker can easily succeed in teaching any target policy to the victim under mild conditions and highlight a significant security threat to reinforcement learning agents in practice.",
                        "Citation Paper Authors": "Authors:Amin Rakhsha, Goran Radanovic, Rati Devidze, Xiaojin Zhu, Adish Singla"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": ".\nInverse reinforcement learning (IRL) is a kind of inferring algorithm that\ncan be used to infer the reward function of reinforcement learning based on the\npolicy or observed behaviour ",
                    "Citation Text": "Arora, S., Doshi, P.: A survey of inverse reinforcement learning: Chal-\nlenges, methods and progress. Arti\fcial Intelligence 297, 103500 (2021)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.06877",
                        "Citation Paper Title": "Title:A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress",
                        "Citation Paper Abstract": "Abstract:Inverse reinforcement learning (IRL) is the problem of inferring the reward function of an agent, given its policy or observed behavior. Analogous to RL, IRL is perceived both as a problem and as a class of methods. By categorically surveying the current literature in IRL, this article serves as a reference for researchers and practitioners of machine learning and beyond to understand the challenges of IRL and select the approaches best suited for the problem on hand. The survey formally introduces the IRL problem along with its central challenges such as the difficulty in performing accurate inference and its generalizability, its sensitivity to prior knowledge, and the disproportionate growth in solution complexity with problem size. The article elaborates how the current methods mitigate these challenges. We further discuss the extensions to traditional IRL methods for handling: inaccurate and incomplete perception, an incomplete model, multiple reward functions, and nonlinear reward functions. This survey concludes the discussion with some broad advances in the research area and currently open research questions.",
                        "Citation Paper Authors": "Authors:Saurabh Arora, Prashant Doshi"
                    }
                },
                {
                    "Sentence ID": 72,
                    "Sentence": "Reward function Policy Differential privacy Laplace noise\nFu et al. ",
                    "Citation Text": "Fu, J., Luo, K., Levine, S.: Learning robust rewards with adversarial\ninverse reinforcement learning. arXiv preprint arXiv:1710.11248 (2017)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.11248",
                        "Citation Paper Title": "Title:Learning Robust Rewards with Adversarial Inverse Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering. Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features, but still require a manually specified reward function. Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally difficult to apply to large, high-dimensional problems with unknown dynamics. In this work, we propose adverserial inverse reinforcement learning (AIRL), a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation. We demonstrate that AIRL is able to recover reward functions that are robust to changes in dynamics, enabling us to learn policies even under significant variation in the environment seen during training. Our experiments show that AIRL greatly outperforms prior methods in these transfer settings.",
                        "Citation Paper Authors": "Authors:Justin Fu, Katie Luo, Sergey Levine"
                    }
                },
                {
                    "Sentence ID": 63,
                    "Sentence": "State, Action, Reward,\nEnvironment, Policy\nPrivacy of reward function\nin MDPReward e.g. ",
                    "Citation Text": "Liu, Z., Yang, Y., Miller, T., Masters, P.: Deceptive reinforcement learning\nfor privacy-preserving planning. arXiv preprint arXiv:2102.03022 (2021)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.03022",
                        "Citation Paper Title": "Title:Deceptive Reinforcement Learning for Privacy-Preserving Planning",
                        "Citation Paper Abstract": "Abstract:In this paper, we study the problem of deceptive reinforcement learning to preserve the privacy of a reward function. Reinforcement learning is the problem of finding a behaviour policy based on rewards received from exploratory behaviour. A key ingredient in reinforcement learning is a reward function, which determines how much reward (negative or positive) is given and when. However, in some situations, we may want to keep a reward function private; that is, to make it difficult for an observer to determine the reward function used. We define the problem of privacy-preserving reinforcement learning, and present two models for solving it. These models are based on dissimulation -- a form of deception that `hides the truth'. We evaluate our models both computationally and via human behavioural experiments. Results show that the resulting policies are indeed deceptive, and that participants can determine the true reward function less reliably than that of an honest agent.",
                        "Citation Paper Authors": "Authors:Zhengshang Liu, Yue Yang, Tim Miller, Peta Masters"
                    }
                },
                {
                    "Sentence ID": 60,
                    "Sentence": "State, Action, Reward,\nEnvironment, Policy\nPrivacy in\nreinforcement\nlearningPrivacy of state and action\nin MDPAction e.g. ",
                    "Citation Text": "Vietri, G., Balle, B., Krishnamurthy, A., Wu, S.: Private reinforcement\nlearning with pac and regret guarantees. In: International Conference on\nMachine Learning, pp. 9754{9764 (2020). PMLR",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.09052",
                        "Citation Paper Title": "Title:Private Reinforcement Learning with PAC and Regret Guarantees",
                        "Citation Paper Abstract": "Abstract:Motivated by high-stakes decision-making domains like personalized medicine where user information is inherently sensitive, we design privacy preserving exploration policies for episodic reinforcement learning (RL). We first provide a meaningful privacy formulation using the notion of joint differential privacy (JDP)--a strong variant of differential privacy for settings where each user receives their own sets of output (e.g., policy recommendations). We then develop a private optimism-based learning algorithm that simultaneously achieves strong PAC and regret bounds, and enjoys a JDP guarantee. Our algorithm only pays for a moderate privacy cost on exploration: in comparison to the non-private bounds, the privacy parameter only appears in lower-order terms. Finally, we present lower bounds on sample complexity and regret for reinforcement learning subject to JDP.",
                        "Citation Paper Authors": "Authors:Giuseppe Vietri, Borja Balle, Akshay Krishnamurthy, Zhiwei Steven Wu"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "Non-stationary\nenvironmentRobust policiesRobust adversarial\nlearningMinimax optimization\nEnd-to-end\nlearning approach\nLin et al. ",
                    "Citation Text": "Lin, J., Dzeparoska, K., Zhang, S.Q., Leon-Garcia, A., Papernot, N.: On\nthe robustness of cooperative multi-agent reinforcement learning. In: 2020\nIEEE Security and Privacy Workshops (SPW), pp. 62{68 (2020). IEEE",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.03722",
                        "Citation Paper Title": "Title:On the Robustness of Cooperative Multi-Agent Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:In cooperative multi-agent reinforcement learning (c-MARL), agents learn to cooperatively take actions as a team to maximize a total team reward. We analyze the robustness of c-MARL to adversaries capable of attacking one of the agents on a team. Through the ability to manipulate this agent's observations, the adversary seeks to decrease the total team reward.\nAttacking c-MARL is challenging for three reasons: first, it is difficult to estimate team rewards or how they are impacted by an agent mispredicting; second, models are non-differentiable; and third, the feature space is low-dimensional. Thus, we introduce a novel attack. The attacker first trains a policy network with reinforcement learning to find a wrong action it should encourage the victim agent to take. Then, the adversary uses targeted adversarial examples to force the victim to take this action.\nOur results on the StartCraft II multi-agent benchmark demonstrate that c-MARL teams are highly vulnerable to perturbations applied to one of their agent's observations. By attacking a single agent, our attack method has highly negative impact on the overall team reward, reducing it from 20 to 9.4. This results in the team's winning rate to go down from 98.9% to 0%.",
                        "Citation Paper Authors": "Authors:Jieyu Lin, Kristina Dzeparoska, Sai Qian Zhang, Alberto Leon-Garcia, Nicolas Papernot"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2301.01218v1": {
            "Paper Title": "Tracing the Origin of Adversarial Attack for Forensic Investigation and\n  Deterrence",
            "Sentences": []
        },
        "http://arxiv.org/abs/2301.00045v1": {
            "Paper Title": "An Analysis of Honeypots and their Impact as a Cyber Deception Tactic",
            "Sentences": [
                {
                    "Sentence ID": 53,
                    "Sentence": "Abuzamak, M., & Kholidy, H. (2022). UAV Based 5G Network: A Practical Survey Study. arXiv. https://doi.org/10.48550/arXiv.2212.13329 ",
                    "Citation Text": "Abuzamak, M., & Kholidy, H. (2022). UAV Based 5G Network: A Practical Survey Study. arXiv. https://doi.org/10.48550/arXiv.2212.13329",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.13329",
                        "Citation Paper Title": "Title:UAV Based 5G Network: A Practical Survey Study",
                        "Citation Paper Abstract": "Abstract:Unmanned aerial vehicles (UAVs) are anticipated to significantly contribute to the development of new wireless networks that could handle high-speed transmissions and enable wireless broadcasts. When compared to communications that rely on permanent infrastructure, UAVs offer a number of advantages, including flexible deployment, dependable line-of-sight (LoS) connection links, and more design degrees of freedom because of controlled mobility. Unmanned aerial vehicles (UAVs) combined with 5G networks and Internet of Things (IoT) components have the potential to completely transform a variety of industries. UAVs may transfer massive volumes of data in real-time by utilizing the low latency and high-speed abilities of 5G networks, opening up a variety of applications like remote sensing, precision farming, and disaster response. This study of UAV communication with regard to 5G/B5G WLANs is presented in this research. The three UAV-assisted MEC network scenarios also include the specifics for the allocation of resources and optimization. We also concentrate on the case where a UAV does task computation in addition to serving as a MEC server to examine wind farm turbines. This paper covers the key implementation difficulties of UAV-assisted MEC, such as optimum UAV deployment, wind models, and coupled trajectory-computation performance optimization, in order to promote widespread implementations of UAV-assisted MEC in practice. The primary problem for 5G and beyond 5G (B5G) is delivering broadband access to various device kinds. Prior to discussing associated research issues faced by the developing integrated network design, we first provide a brief overview of the background information as well as the networks that integrate space, aviation, and land.",
                        "Citation Paper Authors": "Authors:Mohammed Abuzamak, Hisham Kholidy"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": "Mustafa, F.M., Kholidy, H.A., Sayed, A.F. et al. Enhanced dispersion reduction using apodized uniform fiber Bragg grating for optical MTDM transmission systems. Opt Quant Electron 55, 55 (2023). https://doi.org/10.1007/s11082-022-04339-7 ",
                    "Citation Text": "Abuzamak, M., & Kholidy, H. (2022). UAV Based 5G Network: A Practical Survey Study. arXiv. https://doi.org/10.48550/arXiv.2212.13329",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2212.13329",
                        "Citation Paper Title": "Title:UAV Based 5G Network: A Practical Survey Study",
                        "Citation Paper Abstract": "Abstract:Unmanned aerial vehicles (UAVs) are anticipated to significantly contribute to the development of new wireless networks that could handle high-speed transmissions and enable wireless broadcasts. When compared to communications that rely on permanent infrastructure, UAVs offer a number of advantages, including flexible deployment, dependable line-of-sight (LoS) connection links, and more design degrees of freedom because of controlled mobility. Unmanned aerial vehicles (UAVs) combined with 5G networks and Internet of Things (IoT) components have the potential to completely transform a variety of industries. UAVs may transfer massive volumes of data in real-time by utilizing the low latency and high-speed abilities of 5G networks, opening up a variety of applications like remote sensing, precision farming, and disaster response. This study of UAV communication with regard to 5G/B5G WLANs is presented in this research. The three UAV-assisted MEC network scenarios also include the specifics for the allocation of resources and optimization. We also concentrate on the case where a UAV does task computation in addition to serving as a MEC server to examine wind farm turbines. This paper covers the key implementation difficulties of UAV-assisted MEC, such as optimum UAV deployment, wind models, and coupled trajectory-computation performance optimization, in order to promote widespread implementations of UAV-assisted MEC in practice. The primary problem for 5G and beyond 5G (B5G) is delivering broadband access to various device kinds. Prior to discussing associated research issues faced by the developing integrated network design, we first provide a brief overview of the background information as well as the networks that integrate space, aviation, and land.",
                        "Citation Paper Authors": "Authors:Mohammed Abuzamak, Hisham Kholidy"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2301.00044v1": {
            "Paper Title": "Detecting Forged Kerberos Tickets in an Active Directory Environment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.11994v2": {
            "Paper Title": "Another Round of Breaking and Making Quantum Money: How to Not Build It\n  from Lattices, and More",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.14647v1": {
            "Paper Title": "RL and Fingerprinting to Select Moving Target Defense Mechanisms for\n  Zero-day Attacks in IoT",
            "Sentences": []
        },
        "http://arxiv.org/abs/2301.00665v1": {
            "Paper Title": "Targeted Phishing Campaigns using Large Scale Language Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.00313v2": {
            "Paper Title": "Secure Determinant Codes for Distributed Storage Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.14315v1": {
            "Paper Title": "\"Real Attackers Don't Compute Gradients\": Bridging the Gap Between\n  Adversarial ML Research and Practice",
            "Sentences": [
                {
                    "Sentence ID": 121,
                    "Sentence": ".\n8An intriguing question is: are defenses with low computational overhead\nin research (e.g., ",
                    "Citation Text": "A. Shafahi, M. Najibi, M. A. Ghiasi, Z. Xu, J. Dickerson, C. Studer,\nL. S. Davis, G. Taylor, and T. Goldstein, \u201cAdversarial training for free!\u201d\nAdvances in Neural Information Processing Systems , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.12843",
                        "Citation Paper Title": "Title:Adversarial Training for Free!",
                        "Citation Paper Abstract": "Abstract:Adversarial training, in which a network is trained on adversarial examples, is one of the few defenses against adversarial attacks that withstands strong attacks. Unfortunately, the high cost of generating strong adversarial examples makes standard adversarial training impractical on large-scale problems like ImageNet. We present an algorithm that eliminates the overhead cost of generating adversarial examples by recycling the gradient information computed when updating model parameters. Our \"free\" adversarial training algorithm achieves comparable robustness to PGD adversarial training on the CIFAR-10 and CIFAR-100 datasets at negligible additional cost compared to natural training, and can be 7 to 30 times faster than other strong adversarial training methods. Using a single workstation with 4 P100 GPUs and 2 days of runtime, we can train a robust model for the large-scale ImageNet classification task that maintains 40% accuracy against PGD attacks. The code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S. Davis, Gavin Taylor, Tom Goldstein"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": "; they can control inputs to a (trained)\nML model and observe its output ",
                    "Citation Text": "N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and\nA. Swami, \u201cPractical black-box attacks against machine learning,\u201d in\nProc. ACM Conf. Comput. Commun. Secur. , 2017, pp. 506\u2013519.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.02697",
                        "Citation Paper Title": "Title:Practical Black-Box Attacks against Machine Learning",
                        "Citation Paper Abstract": "Abstract:Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19% and 88.94%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.",
                        "Citation Paper Authors": "Authors:Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik, Ananthram Swami"
                    }
                },
                {
                    "Sentence ID": 66,
                    "Sentence": ") focus just on the\nresearch perspective. We consider both perspectives, because\nwe aim to bridge the existing gap between these two sides.\nPerhaps the closest effort to our paper is the (unpublished)\nwork by Gilmer et al. ",
                    "Citation Text": "J. Gilmer, R. P. Adams, I. Goodfellow, D. Andersen, and G. E. Dahl,\n\u201cMotivating the rules of the game for adversarial example research,\u201d\narXiv:1807.06732 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.06732",
                        "Citation Paper Title": "Title:Motivating the Rules of the Game for Adversarial Example Research",
                        "Citation Paper Abstract": "Abstract:Advances in machine learning have led to broad deployment of systems with impressive performance on important problems. Nonetheless, these systems can be induced to make errors on data that are surprisingly similar to examples the learned system handles correctly. The existence of these errors raises a variety of questions about out-of-sample generalization and whether bad actors might use such examples to abuse deployed systems. As a result of these security concerns, there has been a flurry of recent papers proposing algorithms to defend against such malicious perturbations of correctly handled examples. It is unclear how such misclassifications represent a different kind of security problem than other errors, or even other attacker-produced examples that have no specific relationship to an uncorrupted input. In this paper, we argue that adversarial example defense papers have, to date, mostly considered abstract, toy games that do not relate to any specific security concern. Furthermore, defense papers have not yet precisely described all the abilities and limitations of attackers that would be relevant in practical security. Towards this end, we establish a taxonomy of motivations, constraints, and abilities for more plausible adversaries. Finally, we provide a series of recommendations outlining a path forward for future work to more clearly articulate the threat model and perform more meaningful evaluation.",
                        "Citation Paper Authors": "Authors:Justin Gilmer, Ryan P. Adams, Ian Goodfellow, David Andersen, George E. Dahl"
                    }
                },
                {
                    "Sentence ID": 51,
                    "Sentence": ".\nSTRATEGY . To reach their GOAL , attackers exploit their\nKNOWLEDGE and CAPABILITIES and enact a STRATEGY , e.g.:\nattackers that fully know a deep learning model can use its\ngradient to craft adversarial examples ",
                    "Citation Text": "A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, \u201cTowards\ndeep learning models resistant to adversarial attacks,\u201d in ICLR , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06083",
                        "Citation Paper Title": "Title:Towards Deep Learning Models Resistant to Adversarial Attacks",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL.",
                        "Citation Paper Authors": "Authors:Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": ", or extract\nprivate data from it via membership inference ",
                    "Citation Text": "R. Shokri, M. Stronati, C. Song, and V . Shmatikov, \u201cMembership in-\nference attacks against machine learning models,\u201d in IEEE Symposium\non Security and Privacy , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.05820",
                        "Citation Paper Title": "Title:Membership Inference Attacks against Machine Learning Models",
                        "Citation Paper Abstract": "Abstract:We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on.\nWe empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                        "Citation Paper Authors": "Authors:Reza Shokri, Marco Stronati, Congzheng Song, Vitaly Shmatikov"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": ".\nAdversarial examples can certainly be used in malicious\nways: for instance, an attacker can craft an adversarial example\nthatevades an ML model powering a security system, e.g., a\nmalware classi\ufb01er ",
                    "Citation Text": "B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. \u02c7Srndi \u00b4c, P. Laskov,\nG. Giacinto, and F. Roli, \u201cEvasion attacks against machine learning\nat test time,\u201d in Proc. Europ. Conf. Mach. Learn. and Knowl. Discov.\nDatabases , 2013, pp. 387\u2013402.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1708.06131",
                        "Citation Paper Title": "Title:Evasion Attacks against Machine Learning at Test Time",
                        "Citation Paper Abstract": "Abstract:In security-sensitive applications, the success of machine learning depends on a thorough vetting of their resistance to adversarial data. In one pertinent, well-motivated attack scenario, an adversary may attempt to evade a deployed system at test time by carefully manipulating attack samples. In this work, we present a simple but effective gradient-based approach that can be exploited to systematically assess the security of several, widely-used classification algorithms against evasion attacks. Following a recently proposed framework for security evaluation, we simulate attack scenarios that exhibit different risk levels for the classifier by increasing the attacker's knowledge of the system and her ability to manipulate attack samples. This gives the classifier designer a better picture of the classifier performance under evasion attacks, and allows him to perform a more informed model selection (or parameter setting). We evaluate our approach on the relevant security task of malware detection in PDF files, and show that such systems can be easily evaded. We also sketch some countermeasures suggested by our analysis.",
                        "Citation Paper Authors": "Authors:Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic, Pavel Laskov, Giorgio Giacinto, Fabio Roli"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.14296v1": {
            "Paper Title": "Towards Comprehensively Understanding the Run-time Security of\n  Programmable Logic Controllers: A 3-year Empirical Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.14241v1": {
            "Paper Title": "Doppler Spoofing in OFDM Wireless Communication Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.13904v2": {
            "Paper Title": "Reducing Certified Regression to Certified Classification for General\n  Poisoning Attacks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.14141v1": {
            "Paper Title": "$\u03c0$QLB: A Privacy-preserving with Integrity-assuring Query Language\n  for Blockchain",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.13791v1": {
            "Paper Title": "StyleID: Identity Disentanglement for Anonymizing Faces",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": "to calculate the identity embeddings of\n\ud835\udc3c\ud835\udc51\ud835\udc41\ud835\udc52\ud835\udc61 (used to calculate identity distances). Finally, the attribute\npredictor\ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc5f\ud835\udc41\ud835\udc52\ud835\udc61 used is a custom MobileNet model ",
                    "Citation Text": "Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun\nWang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. Mobilenets:\nEfficient convolutional neural networks for mobile vision applications. arXiv\npreprint arXiv:1704.04861 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.04861",
                        "Citation Paper Title": "Title:MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications",
                        "Citation Paper Abstract": "Abstract:We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.",
                        "Citation Paper Authors": "Authors:Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.13721v1": {
            "Paper Title": "Emerging Mobile Phone-based Social Engineering Cyberattacks in the\n  Zambian ICT Sector",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.13700v1": {
            "Paper Title": "Publishing Efficient On-device Models Increases Adversarial\n  Vulnerability",
            "Sentences": [
                {
                    "Sentence ID": 41,
                    "Sentence": ": it\nturns out that if an adversary generates an adversarial example\non one model (a \u201csurrogate model\u201d), this exact adversarial ex-\nample often fool another remote target model. Prior work ",
                    "Citation Text": "Y . Liu, X. Chen, C. Liu, and D. Song, \u201cDelving into\ntransferable adversarial examples and black-box attacks,\u201d\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.02770",
                        "Citation Paper Title": "Title:Delving into Transferable Adversarial Examples and Black-box Attacks",
                        "Citation Paper Abstract": "Abstract:An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack this http URL, which is a black-box image classification system.",
                        "Citation Paper Authors": "Authors:Yanpei Liu, Xinyun Chen, Chang Liu, Dawn Song"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "presented two different\nstrategies: transfer-based andoptimization-based attacks.\nTransfer-based attacks. Transfer-based attacks exploit the\ntransferability phenomenon of adversarial examples ",
                    "Citation Text": "N. Papernot, P. McDaniel, and I. Goodfellow, \u201cTransfer-\nability in machine learning: from phenomena to black-\nbox attacks using adversarial samples,\u201d 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.07277",
                        "Citation Paper Title": "Title:Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples",
                        "Citation Paper Abstract": "Abstract:Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that affect one model often affect another model, even if the two models have different architectures or were trained on different training sets, so long as both models were trained to perform the same task. An attacker may therefore train their own substitute model, craft adversarial examples against the substitute, and transfer them to a victim model, with very little information about the victim. Recent work has further developed a technique that uses the victim model as an oracle to label a synthetic training set for the substitute, so the attacker need not even collect a training set to mount the attack. We extend these recent techniques using reservoir sampling to greatly enhance the efficiency of the training procedure for the substitute model. We introduce new transferability attacks between previously unexplored (substitute, victim) pairs of machine learning model classes, most notably SVMs and decision trees. We demonstrate our attacks on two commercial machine learning classification systems from Amazon (96.19% misclassification rate) and Google (88.94%) using only 800 queries of the victim model, thereby showing that existing machine learning approaches are in general vulnerable to systematic black-box attacks regardless of their structure.",
                        "Citation Paper Authors": "Authors:Nicolas Papernot, Patrick McDaniel, Ian Goodfellow"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.13675v1": {
            "Paper Title": "XMAM:X-raying Models with A Matrix to Reveal Backdoor Attacks for\n  Federated Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.02473v3": {
            "Paper Title": "A Robust Cybersecurity Topic Classification Tool",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.12656v2": {
            "Paper Title": "Efficiently Hardening SGX Enclaves against Memory Access Pattern Attacks\n  via Dynamic Program Partitioning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.12953v2": {
            "Paper Title": "Simulation of Networked Quantum Computing on Encrypted Data",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "and\nthe paper by V. Danos, E. Kashe\f and P. Panangaden ",
                    "Citation Text": "V. Danos, E. Kashe\f, and P. Panangaden, \\Parsimonious and robust realizations\nof unitary maps in the one-way model\", Phys. Rev. a 72, 064301, 064301 (2005).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:quant-ph/0411071",
                        "Citation Paper Title": "Title:Robust and parsimonious realisations of unitaries in the one-way model",
                        "Citation Paper Abstract": "Abstract:  We present a new set of generators for unitary maps over \\otimes^n(C^2) which differs from the traditional rotation-based generating set in that it uses a single-parameter family of 1-qubit unitaries J(a), together with a single 2-qubit unitary controlled-Z.\nEach generator is implementable in the one-way model using only two qubits, and this leads to both parsimonious and robust implementations of general unitaries. As an illustration, we give an implementation of the controlled-U family which uses only 14 qubits, and has a 2-colourable underlying entanglement graph (known to yield robust entangled states).",
                        "Citation Paper Authors": "Authors:Vincent Danos, Elham Kashefi, Prakash Panangaden"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.13452v1": {
            "Paper Title": "Financial Crimes in Web3-empowered Metaverse: Taxonomy, Countermeasures,\n  and Opportunities",
            "Sentences": [
                {
                    "Sentence ID": 123,
                    "Sentence": ", which summarizes\nthe crimes committed in 2018, provides a more in-\ndepth look at the economic losses caused by phishing\nscams and provides case examples to inspire a series of\nsubsequent papers on phishing scam detection ",
                    "Citation Text": "J. Wu, Q. Yuan, D. Lin, W. You, W. Chen, C. Chen, and Z. Zheng,\n\u201cWho are the phishers? Phishing scam detection on Ethereum via\nnetwork embedding,\u201d IEEE Transactions on Systems, Man, and\nCybernetics: Systems , vol. 52, no. 2, pp. 1156\u20131166, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.09259",
                        "Citation Paper Title": "Title:Who Are the Phishers? Phishing Scam Detection on Ethereum via Network Embedding",
                        "Citation Paper Abstract": "Abstract:Recently, blockchain technology has become a topic in the spotlight but also a hotbed of various cybercrimes. Among them, phishing scams on blockchain have been found making a notable amount of money, thus emerging as a serious threat to the trading security of the blockchain ecosystem. In order to create a favorable environment for investment, an effective method for detecting phishing scams is urgently needed in the blockchain ecosystem. To this end, this paper proposes an approach to detect phishing scams on Ethereum by mining its transaction records. Specifically, we first crawl the labeled phishing addresses from two authorized websites and reconstruct the transaction network according to the collected transaction records. Then, by taking the transaction amount and timestamp into consideration, we propose a novel network embedding algorithm called trans2vec to extract the features of the addresses for subsequent phishing identification. Finally, we adopt the oneclass support vector machine (SVM) to classify the nodes into normal and phishing ones. Experimental results demonstrate that the phishing detection method works effectively on Ethereum, and indicate the efficacy of trans2vec over existing state-of-the-art algorithms on feature extraction for transaction networks. This work is the first investigation on phishing detection on Ethereum via network embedding and provides insights into how features of large-scale transaction networks can be embedded.",
                        "Citation Paper Authors": "Authors:Jiajing Wu, Qi Yuan, Dan Lin, Wei You, Weili Chen, Chuan Chen, Zibin Zheng"
                    }
                },
                {
                    "Sentence ID": 86,
                    "Sentence": ", Wu et al. proposed tem-\nporal attribute heterogeneous modalities, and implemented\ntransactional pattern recognition using modal detection. In\nan environment where the anonymity of cryptocurrencies\nhas led to their widespread use in \ufb01nancial crimes, Akcora\net al ",
                    "Citation Text": "C. G. Akcora, Y . Li, Y . R. Gel, and M. Kantarcioglu, \u201cBitcoinheist:\nTopological data analysis for ransomware detection on the bitcoin\nblockchain,\u201d arXiv preprint arXiv:1906.07852 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.07852",
                        "Citation Paper Title": "Title:BitcoinHeist: Topological Data Analysis for Ransomware Detection on the Bitcoin Blockchain",
                        "Citation Paper Abstract": "Abstract:Proliferation of cryptocurrencies (e.g., Bitcoin) that allow pseudo-anonymous transactions, has made it easier for ransomware developers to demand ransom by encrypting sensitive user data. The recently revealed strikes of ransomware attacks have already resulted in significant economic losses and societal harm across different sectors, ranging from local governments to health care.\nMost modern ransomware use Bitcoin for payments. However, although Bitcoin transactions are permanently recorded and publicly available, current approaches for detecting ransomware depend only on a couple of heuristics and/or tedious information gathering steps (e.g., running ransomware to collect ransomware related Bitcoin addresses). To our knowledge, none of the previous approaches have employed advanced data analytics techniques to automatically detect ransomware related transactions and malicious Bitcoin addresses.\nBy capitalizing on the recent advances in topological data analysis, we propose an efficient and tractable data analytics framework to automatically detect new malicious addresses in a ransomware family, given only a limited records of previous transactions. Furthermore, our proposed techniques exhibit high utility to detect the emergence of new ransomware families, that is, ransomware with no previous records of transactions. Using the existing known ransomware data sets, we show that our proposed methodology provides significant improvements in precision and recall for ransomware transaction detection, compared to existing heuristic based approaches, and can be utilized to automate ransomware detection.",
                        "Citation Paper Authors": "Authors:Cuneyt Gurcan Akcora, Yitao Li, Yulia R. Gel, Murat Kantarcioglu"
                    }
                },
                {
                    "Sentence ID": 79,
                    "Sentence": "conducted a systematic review\nof research on smart contract security issues up to 2022. In\naddition, security tools ",
                    "Citation Text": "P. Tsankov, A. Dan, D. Drachsler-Cohen, A. Gervais, F. Buenzli, and\nM. Vechev, \u201cSecurify: Practical security analysis of smart contracts,\u201d\ninProceedings of the ACM SIGSAC Conference on Computer and\nCommunications Security , 2018, pp. 67\u201382.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.01143",
                        "Citation Paper Title": "Title:Securify: Practical Security Analysis of Smart Contracts",
                        "Citation Paper Abstract": "Abstract:Permissionless blockchains allow the execution of arbitrary programs (called smart contracts), enabling mutually untrusted entities to interact without relying on trusted third parties. Despite their potential, repeated security concerns have shaken the trust in handling billions of USD by smart contracts.\nTo address this problem, we present Securify, a security analyzer for Ethereum smart contracts that is scalable, fully automated, and able to prove contract behaviors as safe/unsafe with respect to a given property. Securify's analysis consists of two steps. First, it symbolically analyzes the contract's dependency graph to extract precise semantic information from the code. Then, it checks compliance and violation patterns that capture sufficient conditions for proving if a property holds or not. To enable extensibility, all patterns are specified in a designated domain-specific language.\nSecurify is publicly released, it has analyzed >18K contracts submitted by its users, and is regularly used to conduct security audits by experts. We present an extensive evaluation of Securify over real-world Ethereum smart contracts and demonstrate that it can effectively prove the correctness of smart contracts and discover critical violations.",
                        "Citation Paper Authors": "Authors:Petar Tsankov, Andrei Dan, Dana Drachsler Cohen, Arthur Gervais, Florian Buenzli, Martin Vechev"
                    }
                },
                {
                    "Sentence ID": 69,
                    "Sentence": ".\nWith the booming growth of the metaverse and the ex-\npansion of crypto assets therein, it is urgent to conduct the\ninvestigation and prevention of money laundering crimes on\nthe metaverse economic ecosystem. Recently, Qin et al. ",
                    "Citation Text": "H. X. Qin, Y . Wang, and P. Hui, \u201cIdentity, crimes, and law enforce-\nment in the metaverse,\u201d arXiv preprint arXiv:2210.06134 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2210.06134",
                        "Citation Paper Title": "Title:Identity, Crimes, and Law Enforcement in the Metaverse",
                        "Citation Paper Abstract": "Abstract:With the boom in metaverse-related projects in major areas of the public's life, the safety of users becomes a pressing concern. We believe that an international legal framework should be established to promote collaboration among nations, facilitate crime investigation, and support democratic governance. In this paper, we discuss the legal concerns of identity, crimes that could occur based on incidents in existing virtual worlds, and challenges to unified law enforcement in the metaverse.",
                        "Citation Paper Authors": "Authors:Hua Xuan Qin, Yuyang Wang, Pan Hui"
                    }
                },
                {
                    "Sentence ID": 68,
                    "Sentence": ".\nIn a metaverse ecosystem where crypto assets such as\nNFsT are widely used but legal regulation of metaverse\ntransactions is still immature ",
                    "Citation Text": "E. Hartwich, P. Ollig, G. Fridgen, and A. Rieger, \u201cProbably\nsomething: A multi-layer taxonomy of non-fungible tokens,\u201d arXiv\npreprint arXiv:2209.05456 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2209.05456",
                        "Citation Paper Title": "Title:Probably Something: A Multi-Layer Taxonomy of Non-Fungible Tokens",
                        "Citation Paper Abstract": "Abstract:Purpose: This paper aims to establish a fundamental and comprehensive understanding of Non-Fungible Tokens (NFTs) by identifying and structuring common characteristics within a taxonomy. NFTs are hyped and increasingly marketed as essential building blocks of the Metaverse. However, the dynamic evolution of the NFT space has posed challenges for those seeking to develop a deep and comprehensive understanding of NFTs, their features, and capabilities.\nDesign/methodology/approach: Utilizing common guidelines for the creation of taxonomies, we developed (over three iterations), a multi-layer taxonomy based on workshops and interviews with 11 academic and 15 industry experts. Through an evaluation of 25 NFTs, we demonstrate the usefulness of our taxonomy.\nFindings: The taxonomy has four layers, 14 dimensions and 42 characteristics, which describe NFTs in terms of reference object, token properties, token distribution, and realizable value.\nOriginality: Our framework is the first to systematically cover the emerging NFT phenomenon. It is concise yet extendible and presents many avenues for future research in a plethora of disciplines. The characteristics identified in our taxonomy are useful for NFT and Metaverse related research in Finance, Marketing, Law, and Information Systems. Additionally, the taxonomy can serve as an information source for policymakers as they consider NFT regulation.",
                        "Citation Paper Authors": "Authors:Eduard Hartwich, Philipp Ollig, Gilbert Fridgen, Alexander Rieger"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "proposed four characteristics of meta-\nverse, including universality, realism, scalability, and inter-\noperability. Lee et al. ",
                    "Citation Text": "L.-H. Lee, T. Braud, P. Zhou, L. Wang, D. Xu, Z. Lin, A. Kumar,\nC. Bermejo, and P. Hui, \u201cAll one needs to know about metaverse: A\ncomplete survey on technological singularity, virtual ecosystem, and\nresearch agenda,\u201d arXiv preprint arXiv:2110.05352 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.05352",
                        "Citation Paper Title": "Title:All One Needs to Know about Metaverse: A Complete Survey on Technological Singularity, Virtual Ecosystem, and Research Agenda",
                        "Citation Paper Abstract": "Abstract:Since the popularisation of the Internet in the 1990s, the cyberspace has kept evolving. We have created various computer-mediated virtual environments including social networks, video conferencing, virtual 3D worlds (e.g., VR Chat), augmented reality applications (e.g., Pokemon Go), and Non-Fungible Token Games (e.g., Upland). Such virtual environments, albeit non-perpetual and unconnected, have bought us various degrees of digital transformation. The term `metaverse' has been coined to further facilitate the digital transformation in every aspect of our physical lives. At the core of the metaverse stands the vision of an immersive Internet as a gigantic, unified, persistent, and shared realm. While the metaverse may seem futuristic, catalysed by emerging technologies such as Extended Reality, 5G, and Artificial Intelligence, the digital `big bang' of our cyberspace is not far away. This survey paper presents the first effort to offer a comprehensive framework that examines the latest metaverse development under the dimensions of state-of-the-art technologies and metaverse ecosystems, and illustrates the possibility of the digital `big bang'. First, technologies are the enablers that drive the transition from the current Internet to the metaverse. We thus examine eight enabling technologies rigorously - Extended Reality, User Interactivity (Human-Computer Interaction), Artificial Intelligence, Blockchain, Computer Vision, IoT and Robotics, Edge and Cloud computing, and Future Mobile Networks. In terms of applications, the metaverse ecosystem allows human users to live and play within a self-sustaining, persistent, and shared realm. Therefore, we discuss six user-centric factors -- Avatar, Content Creation, Virtual Economy, Social Acceptability, Security and Privacy, and Trust and Accountability. Finally, we propose a concrete research agenda for the development of the metaverse.",
                        "Citation Paper Authors": "Authors:Lik-Hang Lee, Tristan Braud, Pengyuan Zhou, Lin Wang, Dianlei Xu, Zijun Lin, Abhishek Kumar, Carlos Bermejo, Pan Hui"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "proposed a virtual classroom capable\nof teaching calculus that can help students learn ef\ufb01ciently.\nDuan et al. ",
                    "Citation Text": "H. Duan, J. Li, S. Fan, Z. Lin, X. Wu, and W. Cai, \u201cMetaverse for\nsocial good: A university campus prototype,\u201d in Proceedings of the\n29th ACM International Conference on Multimedia(MM) , 2021, pp.\n153\u2013161.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.08985",
                        "Citation Paper Title": "Title:Metaverse for Social Good: A University Campus Prototype",
                        "Citation Paper Abstract": "Abstract:In recent years, the metaverse has attracted enormous attention from around the world with the development of related technologies. The expected metaverse should be a realistic society with more direct and physical interactions, while the concepts of race, gender, and even physical disability would be weakened, which would be highly beneficial for society. However, the development of metaverse is still in its infancy, with great potential for improvement. Regarding metaverse's huge potential, industry has already come forward with advance preparation, accompanied by feverish investment, but there are few discussions about metaverse in academia to scientifically guide its development. In this paper, we highlight the representative applications for social good. Then we propose a three-layer metaverse architecture from a macro perspective, containing infrastructure, interaction, and ecosystem. Moreover, we journey toward both a historical and novel metaverse with a detailed timeline and table of specific attributes. Lastly, we illustrate our implemented blockchain-driven metaverse prototype of a university campus and discuss the prototype design and insights.",
                        "Citation Paper Authors": "Authors:Haihan Duan, Jiaye Li, Sizheng Fan, Zhonghao Lin, Xiao Wu, Wei Cai"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "discussed three com-\nponents of the metaverse, namely hardware, software and\ncontent. Yang et al. ",
                    "Citation Text": "Q. Yang, Y . Zhao, H. Huang, Z. Xiong, J. Kang, and Z. Zheng,\n\u201cFusing blockchain and AI with metaverse: A survey,\u201d IEEE Open\nJournal of the Computer Society , vol. 3, pp. 122\u2013136, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.03201",
                        "Citation Paper Title": "Title:Fusing Blockchain and AI with Metaverse: A Survey",
                        "Citation Paper Abstract": "Abstract:Metaverse as the latest buzzword has attracted great attention from both industry and academia. Metaverse seamlessly integrates the real world with the virtual world and allows avatars to carry out rich activities including creation, display, entertainment, social networking, and trading. Thus, it is promising to build an exciting digital world and to transform a better physical world through the exploration of the metaverse. In this survey, we dive into the metaverse by discussing how Blockchain and Artificial Intelligence (AI) fuse with it through investigating the state-of-the-art studies across the metaverse components, digital currencies, AI applications in the virtual world, and blockchain-empowered technologies. Further exploitation and interdisciplinary research on the fusion of AI and Blockchain towards metaverse will definitely require collaboration from both academia and industries. We wish that our survey can help researchers, engineers, and educators build an open, fair, and rational future metaverse.",
                        "Citation Paper Authors": "Authors:Qinglin Yang, Yetong Zhao, Huawei Huang, Zehui Xiong, Jiawen Kang, Zibin Zheng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.13353v1": {
            "Paper Title": "A Miniscule Survey on Blockchain Scalability",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.13312v1": {
            "Paper Title": "Users really do respond to smishing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.13228v1": {
            "Paper Title": "Packing Privacy Budget Efficiently",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.12178v2": {
            "Paper Title": "COVID Down Under: where did Australia's pandemic apps go wrong?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.08796v2": {
            "Paper Title": "A Survey on Password Guessing",
            "Sentences": [
                {
                    "Sentence ID": 33,
                    "Sentence": "proposed PASSGAN which is the first password guessing model using GAN to autonomously learn the\npassword distribution from a leaked password dataset. In that study, the authors followed the improved Wasserstein\nGANs (IWGAN) ",
                    "Citation Text": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. 2017. Improved training of wasserstein gans.\nAdvances in neural information processing systems 30 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.00028",
                        "Citation Paper Title": "Title:Improved Training of Wasserstein GANs",
                        "Citation Paper Abstract": "Abstract:Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.",
                        "Citation Paper Authors": "Authors:Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron Courville"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": "proposed G-Pass which is another GAN-based password generation model. G-Pass used LSTM network\nfor the generator, and multiple convolutional layers with multiple filter sizes for the discriminator. In addition, they\nused Gumbel-Softmax ",
                    "Citation Text": "Eric Jang, Shixiang Gu, and Ben Poole. 2016. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144 (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.01144",
                        "Citation Paper Title": "Title:Categorical Reparameterization with Gumbel-Softmax",
                        "Citation Paper Abstract": "Abstract:Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.",
                        "Citation Paper Authors": "Authors:Eric Jang, Shixiang Gu, Ben Poole"
                    }
                },
                {
                    "Sentence ID": 102,
                    "Sentence": "proposed pass2path , an LSTM deep network following the Encoder-Decoder architecture ",
                    "Citation Text": "Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. Advances in neural information processing\nsystems 27 (2014).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.3215",
                        "Citation Paper Title": "Title:Sequence to Sequence Learning with Neural Networks",
                        "Citation Paper Abstract": "Abstract:Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",
                        "Citation Paper Authors": "Authors:Ilya Sutskever, Oriol Vinyals, Quoc V. Le"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ". Thus, the model is capable of\nincorporating information from both the past and the future to produce more meaningful features at each step. Besides,\nthe authors leveraged and transformed the learned knowledge in BERT (Bidirectional Encoder Representations from\nTransformer) ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language\nunderstanding. arXiv preprint arXiv:1810.04805 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.13992v1": {
            "Paper Title": "Social-Aware Clustered Federated Learning with Customized Privacy\n  Preservation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.12785v1": {
            "Paper Title": "zkFaith: Soonami's Zero-Knowledge Identity Protocol",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": "This section describes the previous work to achieve an ef\ufb01cient zero-knowledge identity. We discuss the advantages and\ndisadvantages of their work and illustrate the differences between our solutions.\n2.1 Coconut\nSonnino et al. ",
                    "Citation Text": "Alberto Sonnino, Mustafa Al-Bassam, Shehar Bano, Sarah Meiklejohn, and George Danezis. Coconut: Threshold\nissuance selective disclosure credentials with applications to distributed ledgers. arXiv preprint arXiv:1802.07344 ,\n2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.07344",
                        "Citation Paper Title": "Title:Coconut: Threshold Issuance Selective Disclosure Credentials with Applications to Distributed Ledgers",
                        "Citation Paper Abstract": "Abstract:Coconut is a novel selective disclosure credential scheme supporting distributed threshold issuance, public and private attributes, re-randomization, and multiple unlinkable selective attribute revelations. Coconut integrates with blockchains to ensure confidentiality, authenticity and availability even when a subset of credential issuing authorities are malicious or offline. We implement and evaluate a generic Coconut smart contract library for Chainspace and Ethereum; and present three applications related to anonymous payments, electronic petitions, and distribution of proxies for censorship resistance. Coconut uses short and computationally efficient credentials, and our evaluation shows that most Coconut cryptographic primitives take just a few milliseconds on average, with verification taking the longest time (10 milliseconds).",
                        "Citation Paper Authors": "Authors:Alberto Sonnino, Mustafa Al-Bassam, Shehar Bano, Sarah Meiklejohn, George Danezis"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.03985v2": {
            "Paper Title": "How unfair is private learning ?",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": "instead\nof neural networks as in Section 3.1 and 3.2. For our implementation, we use the publicly available code\nin Holohan et al. ",
                    "Citation Text": "N. Holohan, S. Braghin, P. Mac Aonghusa, and K. Levacher. Di\u000bprivlib: the IBM di\u000berential privacy\nlibrary. 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.02444",
                        "Citation Paper Title": "Title:Diffprivlib: The IBM Differential Privacy Library",
                        "Citation Paper Abstract": "Abstract:Since its conception in 2006, differential privacy has emerged as the de-facto standard in data privacy, owing to its robust mathematical guarantees, generalised applicability and rich body of literature. Over the years, researchers have studied differential privacy and its applicability to an ever-widening field of topics. Mechanisms have been created to optimise the process of achieving differential privacy, for various data types and scenarios. Until this work however, all previous work on differential privacy has been conducted on a ad-hoc basis, without a single, unifying codebase to implement results.\nIn this work, we present the IBM Differential Privacy Library, a general purpose, open source library for investigating, experimenting and developing differential privacy applications in the Python programming language. The library includes a host of mechanisms, the building blocks of differential privacy, alongside a number of applications to machine learning and other data analytics tasks. Simplicity and accessibility has been prioritised in developing the library, making it suitable to a wide audience of users, from those using the library for their first investigations in data privacy, to the privacy experts looking to contribute their own models and mechanisms for others to use.",
                        "Citation Paper Authors": "Authors:Naoise Holohan, Stefano Braghin, P\u00f3l Mac Aonghusa, Killian Levacher"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "that has\npreviously been used in fairness-awareness studies like Quy et al. ",
                    "Citation Text": "T. L. Quy, A. Roy, V. Iosi\fdis, W. Zhang, and E. Ntoutsi. A survey on datasets for fairness-aware\nmachine learning. WIREs Data Mining and Knowledge Discovery , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.00530",
                        "Citation Paper Title": "Title:A survey on datasets for fairness-aware machine learning",
                        "Citation Paper Abstract": "Abstract:As decision-making increasingly relies on Machine Learning (ML) and (big) data, the issue of fairness in data-driven Artificial Intelligence (AI) systems is receiving increasing attention from both research and industry. A large variety of fairness-aware machine learning solutions have been proposed which involve fairness-related interventions in the data, learning algorithms and/or model outputs. However, a vital part of proposing new approaches is evaluating them empirically on benchmark datasets that represent realistic and diverse settings. Therefore, in this paper, we overview real-world datasets used for fairness-aware machine learning. We focus on tabular data as the most common data representation for fairness-aware machine learning. We start our analysis by identifying relationships between the different attributes, particularly w.r.t. protected attributes and class attribute, using a Bayesian network. For a deeper understanding of bias in the datasets, we investigate the interesting relationships using exploratory analysis.",
                        "Citation Paper Authors": "Authors:Tai Le Quy, Arjun Roy, Vasileios Iosifidis, Wenbin Zhang, Eirini Ntoutsi"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "shows extensively that this discrepancy is\nlarge between di\u000berent groups of people for popular facial recognition systems.\nDP-SGD ",
                    "Citation Text": "M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep\nlearning with di\u000berential privacy. In Proceedings of the ACM SIGSAC Conference on Computer and\nCommunications Security , 2016.\n13",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.00133",
                        "Citation Paper Title": "Title:Deep Learning with Differential Privacy",
                        "Citation Paper Abstract": "Abstract:Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.",
                        "Citation Paper Authors": "Authors:Mart\u00edn Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, Li Zhang"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "and has also been implicitly used in multiple works Du et al. ",
                    "Citation Text": "M. Du, S. Mukherjee, G. Wang, R. Tang, A. Awadallah, and X. Hu. Fairness via representation\nneutralization. In Conference on Neural Information Processing Systems , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.12674",
                        "Citation Paper Title": "Title:Fairness via Representation Neutralization",
                        "Citation Paper Abstract": "Abstract:Existing bias mitigation methods for DNN models primarily work on learning debiased encoders. This process not only requires a lot of instance-level annotations for sensitive attributes, it also does not guarantee that all fairness sensitive information has been removed from the encoder. To address these limitations, we explore the following research question: Can we reduce the discrimination of DNN models by only debiasing the classification head, even with biased representations as inputs? To this end, we propose a new mitigation technique, namely, Representation Neutralization for Fairness (RNF) that achieves fairness by debiasing only the task-specific classification head of DNN models. To this end, we leverage samples with the same ground-truth label but different sensitive attributes, and use their neutralized representations to train the classification head of the DNN model. The key idea of RNF is to discourage the classification head from capturing spurious correlation between fairness sensitive information in encoder representations with specific class labels. To address low-resource settings with no access to sensitive attribute annotations, we leverage a bias-amplified model to generate proxy annotations for sensitive attributes. Experimental results over several benchmark datasets demonstrate our RNF framework to effectively reduce discrimination of DNN models with minimal degradation in task-specific performance.",
                        "Citation Paper Authors": "Authors:Mengnan Du, Subhabrata Mukherjee, Guanchu Wang, Ruixiang Tang, Ahmed Hassan Awadallah, Xia Hu"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "provides some experimental evidence that DP-SGD can have disparate impact\non accuracy. Tran et al. ",
                    "Citation Text": "C. Tran, M. Dinh, and F. Fioretto. Di\u000berentially private empirical risk minimization under the fairness\nlens. In Conference on Neural Information Processing Systems , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.02674",
                        "Citation Paper Title": "Title:Differentially Empirical Risk Minimization under the Fairness Lens",
                        "Citation Paper Abstract": "Abstract:Differential Privacy (DP) is an important privacy-enhancing technology for private machine learning systems. It allows to measure and bound the risk associated with an individual participation in a computation. However, it was recently observed that DP learning systems may exacerbate bias and unfairness for different groups of individuals. This paper builds on these important observations and sheds light on the causes of the disparate impacts arising in the problem of differentially private empirical risk minimization. It focuses on the accuracy disparity arising among groups of individuals in two well-studied DP learning methods: output perturbation and differentially private stochastic gradient descent. The paper analyzes which data and model properties are responsible for the disproportionate impacts, why these aspects are affecting different groups disproportionately and proposes guidelines to mitigate these effects. The proposed approach is evaluated on several datasets and settings.",
                        "Citation Paper Authors": "Authors:Cuong Tran, My H. Dinh, Ferdinando Fioretto"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.12671v1": {
            "Paper Title": "MProtect: Operating System Memory Management without Access",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.12613v1": {
            "Paper Title": "Scalable and Secure Row-Swap: Efficient and Safe Row Hammer Mitigation\n  in Memory Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.12398v1": {
            "Paper Title": "Designing Autonomous Markets for Stablecoin Monetary Policy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.00940v4": {
            "Paper Title": "Maximal Extractable Value (MEV) Protection on a DAG",
            "Sentences": [
                {
                    "Sentence ID": 23,
                    "Sentence": "for the partial synchrony model, in which\nNarwhal is used as a \"mempool\". In order to drive Consensus deci-\nsions, Narwhal-HS adds messages outside Narwhal, using the DAG\nonly for spreading transactions.\nDAG-Rider ",
                    "Citation Text": "Idit Keidar, Eleftherios Kokoris-Kogias, Oded Naor, and Alexander Spiegelman.\n2021. All you need is dag. In ACM PODC .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.08325",
                        "Citation Paper Title": "Title:All You Need is DAG",
                        "Citation Paper Abstract": "Abstract:We present DAG-Rider, the first asynchronous Byzantine Atomic Broadcast protocol that achieves optimal resilience, optimal amortized communication complexity, and optimal time complexity. DAG-Rider is post-quantum safe and ensures that all messages proposed by correct processes eventually get decided. We construct DAG-Rider in two layers: In the first layer, processes reliably broadcast their proposals and build a structured Directed Acyclic Graph (DAG) of the communication among them. In the second layer, processes locally observe their DAGs and totally order all proposals with no extra communication.",
                        "Citation Paper Authors": "Authors:Idit Keidar, Eleftherios Kokoris-Kogias, Oded Naor, Alexander Spiegelman"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.01415v4": {
            "Paper Title": "Callee: Recovering Call Graphs for Binaries with Transfer and\n  Contrastive Learning",
            "Sentences": [
                {
                    "Sentence ID": 62,
                    "Sentence": "proposes a DNN-based cross-lingual basic-block embed-\nding model to measure the similarity of two blocks, which\nachieves cross-architecture similarity detection. By regard-\ning instructions as words and basic blocks as sentences, they\nuse word2vec ",
                    "Citation Text": "T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,\n\u201cDistributed representations of words and phrases and their compo-\nsitionality,\u201d in Advances in neural information processing systems ,\n2013, pp. 3111\u20133119.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1310.4546",
                        "Citation Paper Title": "Title:Distributed Representations of Words and Phrases and their Compositionality",
                        "Citation Paper Abstract": "Abstract:The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
                        "Citation Paper Authors": "Authors:Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.12151v1": {
            "Paper Title": "EarSpy: Spying Caller Speech and Identity through Tiny Vibrations of\n  Smartphone Ear Speakers",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.13990v1": {
            "Paper Title": "Detecting Exploit Primitives Automatically for Heap Vulnerabilities on\n  Binary Programs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.12101v1": {
            "Paper Title": "Security and Interpretability in Automotive Systems",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": "attempts to interpret model insights.\nHowever, these methods rely on models\u2019 parameters and\nweights, accessible by unwanted tampering with the warranty\nof the models. However, a solution based on non-intrusive\ntechniques by Petsiuk et al. ",
                    "Citation Text": "V . Petsiuk, A. Das, and K. Saenko, \u201cRISE: randomized input sampling\nfor explanation of black-box models,\u201d 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.07421",
                        "Citation Paper Title": "Title:RISE: Randomized Input Sampling for Explanation of Black-box Models",
                        "Citation Paper Abstract": "Abstract:Deep neural networks are being used increasingly to automate data analysis and decision making, yet their decision-making process is largely unclear and is difficult to explain to the end users. In this paper, we address the problem of Explainable AI for deep neural networks that take images as input and output a class probability. We propose an approach called RISE that generates an importance map indicating how salient each pixel is for the model's prediction. In contrast to white-box approaches that estimate pixel importance using gradients or other internal network state, RISE works on black-box models. It estimates importance empirically by probing the model with randomly masked versions of the input image and obtaining the corresponding outputs. We compare our approach to state-of-the-art importance extraction methods using both an automatic deletion/insertion metric and a pointing metric based on human-annotated object segments. Extensive experiments on several benchmark datasets show that our approach matches or exceeds the performance of other methods, including white-box approaches.\nProject page: this http URL",
                        "Citation Paper Authors": "Authors:Vitali Petsiuk, Abir Das, Kate Saenko"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.13416v3": {
            "Paper Title": "Attribute Inference Attack of Speech Emotion Recognition in Federated\n  Learning Settings",
            "Sentences": [
                {
                    "Sentence ID": 25,
                    "Sentence": ", which is designed to provide a standard and comprehensive\ntestbed for pre-trained models on various downstream speech\ntasks. We compute the deep speech representations from the pre-\ntrained models that are available in SUPERB including APC ",
                    "Citation Text": "Y .-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, \u201cAn unsupervised\nautoregressive model for speech representation learning,\u201d in Interspeech ,\n2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.03240",
                        "Citation Paper Title": "Title:An Unsupervised Autoregressive Model for Speech Representation Learning",
                        "Citation Paper Abstract": "Abstract:This paper proposes a novel unsupervised autoregressive neural model for learning generic speech representations. In contrast to other speech representation learning methods that aim to remove noise or speaker variabilities, ours is designed to preserve information for a wide range of downstream tasks. In addition, the proposed model does not require any phonetic or word boundary labels, allowing the model to benefit from large quantities of unlabeled data. Speech representations learned by our model significantly improve performance on both phone classification and speaker verification over the surface features and other supervised and unsupervised approaches. Further analysis shows that different levels of speech information are captured by our model at different layers. In particular, the lower layers tend to be more discriminative for speakers, while the upper layers provide more phonetic content.",
                        "Citation Paper Authors": "Authors:Yu-An Chung, Wei-Ning Hsu, Hao Tang, James Glass"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "for each utterance. In addition to the knowledge-basedspeech feature set, we propose to evaluate our framework on SU-\nPERB (Speech Processing Universal PERformance Benchmark) ",
                    "Citation Text": "S. wen Yang, P.-H. Chi, Y .-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y . Y . Lin,\nA. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang, W.-C. Tseng, K. tik\nLee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed,\nand H. yi Lee, \u201cSUPERB: Speech Processing Universal PERformance\nBenchmark,\u201d in Proc. Interspeech 2021 , 2021, pp. 1194\u20131198.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.01051",
                        "Citation Paper Title": "Title:SUPERB: Speech processing Universal PERformance Benchmark",
                        "Citation Paper Abstract": "Abstract:Self-supervised learning (SSL) has proven vital for advancing research in natural language processing (NLP) and computer vision (CV). The paradigm pretrains a shared model on large volumes of unlabeled data and achieves state-of-the-art (SOTA) for various tasks with minimal adaptation. However, the speech processing community lacks a similar setup to systematically explore the paradigm. To bridge this gap, we introduce Speech processing Universal PERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the performance of a shared model across a wide range of speech processing tasks with minimal architecture changes and labeled data. Among multiple usages of the shared model, we especially focus on extracting the representation learned from SSL due to its preferable re-usability. We present a simple framework to solve SUPERB tasks by learning task-specialized lightweight prediction heads on top of the frozen shared model. Our results demonstrate that the framework is promising as SSL representations show competitive generalizability and accessibility across SUPERB tasks. We release SUPERB as a challenge with a leaderboard and a benchmark toolkit to fuel the research in representation learning and general speech processing.",
                        "Citation Paper Authors": "Authors:Shu-wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal Lakhotia, Yist Y. Lin, Andy T. Liu, Jiatong Shi, Xuankai Chang, Guan-Ting Lin, Tzu-Hsien Huang, Wei-Cheng Tseng, Ko-tik Lee, Da-Rong Liu, Zili Huang, Shuyan Dong, Shang-Wen Li, Shinji Watanabe, Abdelrahman Mohamed, Hung-yi Lee"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2009.00203v3": {
            "Paper Title": "Efficient, Direct, and Restricted Black-Box Graph Evasion Attacks to\n  Any-Layer Graph Neural Networks via Influence Function",
            "Sentences": [
                {
                    "Sentence ID": 54,
                    "Sentence": "leveraged reinforcement\nlearning techniques to design non-targeted evasion attacks to both\ngraph classification and node classification. Z\u00fcgner et al . ",
                    "Citation Text": "Daniel Z\u00fcgner, Amir Akbarnejad, and Stephan G\u00fcnnemann. 2018. Adversarial\nattacks on neural networks for graph data. In KDD .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.07984",
                        "Citation Paper Title": "Title:Adversarial Attacks on Neural Networks for Graph Data",
                        "Citation Paper Abstract": "Abstract:Deep learning models for graphs have achieved strong performance for the task of node classification. Despite their proliferation, currently there is no study of their robustness to adversarial attacks. Yet, in domains where they are likely to be used, e.g. the web, adversaries are common. Can deep learning models for graphs be easily fooled? In this work, we introduce the first study of adversarial attacks on attributed graphs, specifically focusing on models exploiting ideas of graph convolutions. In addition to attacks at test time, we tackle the more challenging class of poisoning/causative attacks, which focus on the training phase of a machine learning model. We generate adversarial perturbations targeting the node's features and the graph structure, thus, taking the dependencies between instances in account. Moreover, we ensure that the perturbations remain unnoticeable by preserving important data characteristics. To cope with the underlying discrete domain we propose an efficient algorithm Nettack exploiting incremental computations. Our experimental study shows that accuracy of node classification significantly drops even when performing only few perturbations. Even more, our attacks are transferable: the learned attacks generalize to other state-of-the-art node classification models and unsupervised approaches, and likewise are successful even when only limited knowledge about the graph is given.",
                        "Citation Paper Authors": "Authors:Daniel Z\u00fcgner, Amir Akbarnejad, Stephan G\u00fcnnemann"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": ", a widely used type of\nGNN, and its special case Simple Graph Convolution (SGC) ",
                    "Citation Text": "Felix Wu, Tianyi Zhang, Amauri Holanda de Souza Jr, Christopher Fifty, Tao Yu,\nand Kilian Q Weinberger. 2019. Simplifying graph convolutional networks. In\nICML .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.07153",
                        "Citation Paper Title": "Title:Simplifying Graph Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:Graph Convolutional Networks (GCNs) and their variants have experienced significant attention and have become the de facto methods for learning graph representations. GCNs derive inspiration primarily from recent deep learning approaches, and as a result, may inherit unnecessary complexity and redundant computation. In this paper, we reduce this excess complexity through successively removing nonlinearities and collapsing weight matrices between consecutive layers. We theoretically analyze the resulting linear model and show that it corresponds to a fixed low-pass filter followed by a linear classifier. Notably, our experimental evaluation demonstrates that these simplifications do not negatively impact accuracy in many downstream applications. Moreover, the resulting model scales to larger datasets, is naturally interpretable, and yields up to two orders of magnitude speedup over FastGCN.",
                        "Citation Paper Authors": "Authors:Felix Wu, Tianyi Zhang, Amauri Holanda de Souza Jr., Christopher Fifty, Tao Yu, Kilian Q. Weinberger"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ", graph-based collective classification [ 35,37],\ngraph embedding [ 1,4,5,9,30], community detection ",
                    "Citation Text": "Jia Li, Honglei Zhang, Zhichao Han, Yu Rong, Hong Cheng, and Junzhou Huang.\n2020. Adversarial attack on community detection by hiding individuals. In\nWWW .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2001.07933",
                        "Citation Paper Title": "Title:Adversarial Attack on Community Detection by Hiding Individuals",
                        "Citation Paper Abstract": "Abstract:It has been demonstrated that adversarial graphs, i.e., graphs with imperceptible perturbations added, can cause deep graph models to fail on node/graph classification tasks. In this paper, we extend adversarial graphs to the problem of community detection which is much more difficult. We focus on black-box attack and aim to hide targeted individuals from the detection of deep graph community detection models, which has many applications in real-world scenarios, for example, protecting personal privacy in social networks and understanding camouflage patterns in transaction networks. We propose an iterative learning framework that takes turns to update two modules: one working as the constrained graph generator and the other as the surrogate community detection model. We also find that the adversarial graphs generated by our method can be transferred to other learning based community detection models.",
                        "Citation Paper Authors": "Authors:Jia Li, Honglei Zhang, Zhichao Han, Yu Rong, Hong Cheng, Junzhou Huang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2011.14159v4": {
            "Paper Title": "Adamastor: a New Low Latency and Scalable Decentralized Anonymous\n  Payment System",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.02130v5": {
            "Paper Title": "Classical Homomorphic Encryption for Quantum Circuits",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.11753v2": {
            "Paper Title": "Continuous Release of Data Streams under both Centralized and Local\n  Differential Privacy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2009.09213v5": {
            "Paper Title": "Dodging DeepFake Detection via Implicit Spatial-Domain Notch Filtering",
            "Sentences": []
        },
        "http://arxiv.org/abs/2011.02670v4": {
            "Paper Title": "A Black-Box Approach to Post-Quantum Zero-Knowledge in Constant Rounds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.10294v2": {
            "Paper Title": "Adaptive Webpage Fingerprinting from TLS Traces",
            "Sentences": []
        },
        "http://arxiv.org/abs/2002.02516v5": {
            "Paper Title": "Breaking the $O(\\sqrt n)$-Bit Barrier: Byzantine Agreement with Polylog\n  Bits Per Party",
            "Sentences": [
                {
                    "Sentence ID": 74,
                    "Sentence": "L. Lamport, R. E. Shostak, and M. C. Pease. The Byzantine generals problem. ACM Transactions on\nProgramming Languages and Systems , 4(3):382\u2013401, 1982. ",
                    "Citation Text": "K. Lee, D. H. Lee, and M. Yung. Sequential aggregate sign atures made shorter. In Proceedings of the\n11th International Conference on Applied Cryptography and Network Security (ACNS) , pages 202\u2013217,\n2013.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1502.06691",
                        "Citation Paper Title": "Title:Sequential Aggregate Signatures with Short Public Keys without Random Oracles",
                        "Citation Paper Abstract": "Abstract:The notion of aggregate signature has been motivated by applications and it enables any user to compress different signatures signed by different signers on different messages into a short signature. Sequential aggregate signature, in turn, is a special kind of aggregate signature that only allows a signer to add his signature into an aggregate signature in sequential order. This latter scheme has applications in diversified settings such as in reducing bandwidth of certificate chains and in secure routing protocols. Lu, Ostrovsky, Sahai, Shacham, and Waters (EUROCRYPT 2006) presented the first sequential aggregate signature scheme in the standard model. The size of their public key, however, is quite large (i.e., the number of group elements is proportional to the security parameter), and therefore, they suggested as an open problem the construction of such a scheme with short keys.\nIn this paper, we propose the first sequential aggregate signature schemes with short public keys (i.e., a constant number of group elements) in prime order (asymmetric) bilinear groups that are secure under static assumptions in the standard model. Furthermore, our schemes employ a constant number of pairing operations per message signing and message verification operation. Technically, we start with a public-key signature scheme based on the recent dual system encryption technique of Lewko and Waters (TCC 2010). This technique cannot directly provide an aggregate signature scheme since, as we observed, additional elements should be published in a public key to support aggregation. Thus, our constructions are careful augmentation techniques for the dual system technique to allow it to support sequential aggregate signature schemes. We also propose a multi-signature scheme with short public parameters in the standard model.",
                        "Citation Paper Authors": "Authors:Kwangsu Lee, Dong Hoon Lee, Moti Yung"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "R. Cohen, S. Coretti, J. A. Garay, and V. Zikas. Probabil istic termination and composability of\ncryptographic protocols. Journal of Cryptology , 32(3):690\u2013741, 2019. ",
                    "Citation Text": "R. Cohen, I. Haitner, N. Makriyannis, M. Orland, and A. S amorodnitsky. On the round complexity of\nrandomized Byzantine agreement. In Proceedings of the 33rd International Symposium on Distrib uted\nComputing (DISC) , pages 12:1\u201312:17, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.11329",
                        "Citation Paper Title": "Title:On the Round Complexity of Randomized Byzantine Agreement",
                        "Citation Paper Abstract": "Abstract:We prove lower bounds on the round complexity of randomized Byzantine agreement (BA) protocols, bounding the halting probability of such protocols after one and two rounds. In particular, we prove that:\n(1) BA protocols resilient against $n/3$ [resp., $n/4$] corruptions terminate (under attack) at the end of the first round with probability at most $o(1)$ [resp., $1/2+ o(1)$].\n(2) BA protocols resilient against a fraction of corruptions greater than $1/4$ terminate at the end of the second round with probability at most $1-\\Theta(1)$.\n(3) For a large class of protocols (including all BA protocols used in practice) and under a plausible combinatorial conjecture, BA protocols resilient against a fraction of corruptions greater than $1/3$ [resp., $1/4$] terminate at the end of the second round with probability at most $o(1)$ [resp., $1/2 + o(1)$].\nThe above bounds hold even when the parties use a trusted setup phase, e.g., a public-key infrastructure (PKI).\nThe third bound essentially matches the recent protocol of Micali (ITCS'17) that tolerates up to $n/3$ corruptions and terminates at the end of the third round with constant probability.",
                        "Citation Paper Authors": "Authors:Ran Cohen, Iftach Haitner, Nikolaos Makriyannis, Matan Orland, Alex Samorodnitsky"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "(and furthermore, the\nconnectivity of the underlying communication graph must be \u2126(n) [48,51]). This result extends\nto randomized BA protocols, in the special case of very strong adversarial (adaptive, strongly\nrushing3) capabilities ",
                    "Citation Text": "I. Abraham, T. H. Chan, D. Dolev, K. Nayak, R. Pass, L. Ren, and E. Shi. Communication complexity\nof Byzantine agreement, revisited. In Proceedings of the 38th Annual ACM Symposium on Principles\nof Distributed Computing (PODC) , pages 317\u2013326, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.03391",
                        "Citation Paper Title": "Title:Communication Complexity of Byzantine Agreement, Revisited",
                        "Citation Paper Abstract": "Abstract:As Byzantine Agreement (BA) protocols find application in large-scale decentralized cryptocurrencies, an increasingly important problem is to design BA protocols with improved communication complexity. A few existing works have shown how to achieve subquadratic BA under an {\\it adaptive} adversary. Intriguingly, they all make a common relaxation about the adaptivity of the attacker, that is, if an honest node sends a message and then gets corrupted in some round, the adversary {\\it cannot erase the message that was already sent} --- henceforth we say that such an adversary cannot perform \"after-the-fact removal\". By contrast, many (super-)quadratic BA protocols in the literature can tolerate after-the-fact removal. In this paper, we first prove that disallowing after-the-fact removal is necessary for achieving subquadratic-communication BA.\nNext, we show new subquadratic binary BA constructions (of course, assuming no after-the-fact removal) that achieves near-optimal resilience and expected constant rounds under standard cryptographic assumptions and a public-key infrastructure (PKI) in both synchronous and partially synchronous settings. In comparison, all known subquadratic protocols make additional strong assumptions such as random oracles or the ability of honest nodes to erase secrets from memory, and even with these strong assumptions, no prior work can achieve the above properties. Lastly, we show that some setup assumption is necessary for achieving subquadratic multicast-based BA.",
                        "Citation Paper Authors": "Authors:Ittai Abraham, T-H. Hubert Chan, Danny Dolev, Kartik Nayak, Rafael Pass, Ling Ren, Elaine Shi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2007.12412v3": {
            "Paper Title": "Model Checkers Are Cool: How to Model Check Voting Protocols in Uppaal",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.00422v3": {
            "Paper Title": "Distributed Multivariate Regression Modeling For Selecting Biomarkers\n  Under Data Protection Constraints",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": ".\nWe specifically choose componentwise likelihood-based boosting [24, 25] as a reg-\nularized regression approach, which can be adapted for estimation with distributed\ndata as will be shown below. Componentwise likelihood-based boosting is a stagewise\nregression approach ",
                    "Citation Text": "Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. Least angle\nregression. The Annals of Statistics , 32(2):407\u2013499, 2004.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:math/0406456",
                        "Citation Paper Title": "Title:Least Angle Regression",
                        "Citation Paper Abstract": "Abstract:  The purpose of model selection algorithms such as All Subsets, Forward\nSelection and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods.\nThree main properties are derived: (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of ordinary least squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods.\n(2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising new model selection method;",
                        "Citation Paper Authors": "Authors:Bradley Efron, Trevor Hastie, Iain Johnstone, Robert Tibshirani"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1809.06044v5": {
            "Paper Title": "BlockTag: Design and applications of a tagging system for blockchain\n  analysis",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": ". In what follows, we present\nthe background and related work and contrast it to ours.\nAnalysis systems. Blockchain analysis systems parse and analyze\nraw transaction data for many applications. Recently, Kalodner et al.\nproposed BlockSci ",
                    "Citation Text": "Harry Kalodner, Steven Goldfeder, Alishah Chator, Malte M\u00f6ser, and Arvind\nNarayanan. 2017. BlockSci: Design and applications of a blockchain analysis\nplatform. arXiv preprint arXiv:1709.02489 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.02489",
                        "Citation Paper Title": "Title:BlockSci: Design and applications of a blockchain analysis platform",
                        "Citation Paper Abstract": "Abstract:Analysis of blockchain data is useful for both scientific research and commercial applications. We present BlockSci, an open-source software platform for blockchain analysis. BlockSci is versatile in its support for different blockchains and analysis tasks. It incorporates an in-memory, analytical (rather than transactional) database, making it several hundred times faster than existing tools. We describe BlockSci's design and present four analyses that illustrate its capabilities.\nThis is a working paper that accompanies the first public release of BlockSci, available at this https URL. We seek input from the community to further develop the software and explore other potential applications.",
                        "Citation Paper Authors": "Authors:Harry Kalodner, Steven Goldfeder, Alishah Chator, Malte M\u00f6ser, Arvind Narayanan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.14185v2": {
            "Paper Title": "MALIGN: Explainable Static Raw-byte Based Malware Family Classification\n  using Sequence Alignment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.04741v4": {
            "Paper Title": "Is GitHub's Copilot as Bad as Humans at Introducing Vulnerabilities in\n  Code?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2005.13635v3": {
            "Paper Title": "Towards AI Forensics: Did the Artificial Intelligence System Do It?",
            "Sentences": [
                {
                    "Sentence ID": 33,
                    "Sentence": ",\ntrained on the COCO dataset. We trained the custom\nCNN being a VGG ",
                    "Citation Text": "Simonyan, K., and Zisserman, A. Very deep\nconvolutional networks for large-scale image recog-\nnition. Int. Conference on Learning Representa-\ntions (ICLR) (2014).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.1556",
                        "Citation Paper Title": "Title:Very Deep Convolutional Networks for Large-Scale Image Recognition",
                        "Citation Paper Abstract": "Abstract:In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.",
                        "Citation Paper Authors": "Authors:Karen Simonyan, Andrew Zisserman"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "showed that even when using data encryption, archi-\ntectural information and weights might be obtainable\nusing memory access patterns. XAI and other analytics\ntechniques[ 7,16,22,29,34], specifically for CNNs ",
                    "Citation Text": "Zhang, Q.-s., and Zhu, S.-c. Visual inter-\npretability for deep learning: a survey. Frontiers of\nInformation Technology & Electronic Engineering\n19, 1 (2018), 27\u201339.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.00614",
                        "Citation Paper Title": "Title:Visual Interpretability for Deep Learning: a Survey",
                        "Citation Paper Abstract": "Abstract:This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, the interpretability is always the Achilles' heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of low interpretability of their black-box representations. We believe that high model interpretability may help people to break several bottlenecks of deep learning, e.g., learning from very few annotations, learning via human-computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and we revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.",
                        "Citation Paper Authors": "Authors:Quanshi Zhang, Song-Chun Zhu"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": ". While relevant, this is not the focus\nof our work. On the other hand, AI Forensics can\nbe treated as a sub-discipline of digital forensics as\ndefined by ",
                    "Citation Text": "Baggili, I., and Behzadan, V. Founding\nThe Domain of AI Forensics. arXiv preprint\narXiv:1912.06497 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.06497",
                        "Citation Paper Title": "Title:Founding The Domain of AI Forensics",
                        "Citation Paper Abstract": "Abstract:With the widespread integration of AI in everyday and critical technologies, it seems inevitable to witness increasing instances of failure in AI systems. In such cases, there arises a need for technical investigations that produce legally acceptable and scientifically indisputable findings and conclusions on the causes of such failures. Inspired by the domain of cyber forensics, this paper introduces the need for the establishment of AI Forensics as a new discipline under AI safety. Furthermore, we propose a taxonomy of the subfields under this discipline, and present a discussion on the foundational challenges that lay ahead of this new research area.",
                        "Citation Paper Authors": "Authors:Ibrahim Baggili, Vahid Behzadan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2011.08960v3": {
            "Paper Title": "Deep Serial Number: Computational Watermarking for DNN Intellectual\n  Property Protection",
            "Sentences": []
        },
        "http://arxiv.org/abs/2003.11446v4": {
            "Paper Title": "Probabilistic Counters for Privacy Preserving Data Aggregation",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": ". A slightly modified version\nof Morris Counter called Morris+ was recently introduced in ",
                    "Citation Text": "J. Nelson, H. Yu, Optimal bounds for approximate counting (2022). arXiv:\n2010.02116 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.02116",
                        "Citation Paper Title": "Title:Optimal bounds for approximate counting",
                        "Citation Paper Abstract": "Abstract:Storing a counter incremented $N$ times would naively consume $O(\\log N)$ bits of memory. In 1978 Morris described the very first streaming algorithm: the \"Morris Counter\". His algorithm's space bound is a random variable, and it has been shown to be $O(\\log\\log N + \\log(1/\\varepsilon) + \\log(1/\\delta))$ bits in expectation to provide a $(1+\\varepsilon)$-approximation with probability $1-\\delta$ to the counter's value. We provide a new simple algorithm with a simple analysis showing that randomized space $O(\\log\\log N + \\log(1/\\varepsilon) + \\log\\log(1/\\delta))$ bits suffice for the same task, i.e. an exponentially improved dependence on the inverse failure probability. We then provide a new analysis showing that the original Morris Counter itself, after a minor but necessary tweak, actually also enjoys this same improved upper bound. Lastly, we prove a new lower bound for this task showing optimality of our upper bound. We thus completely resolve the asymptotic space complexity of approximate counting. Furthermore all our constants are explicit, and our lower bound and tightest upper bound differ by a multiplicative factor of at most $3+o(1)$.",
                        "Citation Paper Authors": "Authors:Jelani Nelson, Huacheng Yu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.11479v5": {
            "Paper Title": "Abelian varieties with prescribed embedding and full embedding degrees",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.08311v3": {
            "Paper Title": "Formal Verification of Robustness and Resilience of Learning-Enabled\n  State Estimation Systems",
            "Sentences": [
                {
                    "Sentence ID": 59,
                    "Sentence": ", a com-\npositional framework is developed for the falsification of\ntemporal logic properties of cyber-physical systems with\nML components. Their approaches are applied to an Au-\ntomatic Emergency Braking System. A simulation based\napproach ",
                    "Citation Text": "Cumhur Erkan Tuncali, James Kapinski, Hisahiro Ito, and Jy-\notirmoy V Deshmukh. Reasoning about safety of learning-\n21enabled components in autonomous cyber-physical systems.\narXiv preprint arXiv:1804.03973 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.03973",
                        "Citation Paper Title": "Title:Reasoning about Safety of Learning-Enabled Components in Autonomous Cyber-physical Systems",
                        "Citation Paper Abstract": "Abstract:We present a simulation-based approach for generating barrier certificate functions for safety verification of cyber-physical systems (CPS) that contain neural network-based controllers. A linear programming solver is utilized to find a candidate generator function from a set of simulation traces obtained by randomly selecting initial states for the CPS model. A level set of the generator function is then selected to act as a barrier certificate for the system, meaning it demonstrates that no unsafe system states are reachable from a given set of initial states. The barrier certificate properties are verified with an SMT solver. This approach is demonstrated on a case study in which a Dubins car model of an autonomous vehicle is controlled by a neural network to follow a given path.",
                        "Citation Paper Authors": "Authors:Cumhur Erkan Tuncali, James Kapinski, Hisahiro Ito, Jyotirmoy V. Deshmukh"
                    }
                },
                {
                    "Sentence ID": 58,
                    "Sentence": "for a recent survey on the progress of\nthis area.Research is sparse at the system level, and there is none\n(apparent) on state estimation systems. In ",
                    "Citation Text": "Tommaso Dreossi, Alexandre Donz\u00b4 e, and Sanjit A Seshia.\nCompositional falsification of cyber-physical systems with ma-\nchine learning components. Journal of Automated Reasoning ,\n63(4):1031\u20131053, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.00978",
                        "Citation Paper Title": "Title:Compositional Falsification of Cyber-Physical Systems with Machine Learning Components",
                        "Citation Paper Abstract": "Abstract:Cyber-physical systems (CPS), such as automotive systems, are starting to include sophisticated machine learning (ML) components. Their correctness, therefore, depends on properties of the inner ML modules. While learning algorithms aim to generalize from examples, they are only as good as the examples provided, and recent efforts have shown that they can produce inconsistent output under small adversarial perturbations. This raises the question: can the output from learning components can lead to a failure of the entire CPS? In this work, we address this question by formulating it as a problem of falsifying signal temporal logic (STL) specifications for CPS with ML components. We propose a compositional falsification framework where a temporal logic falsifier and a machine learning analyzer cooperate with the aim of finding falsifying executions of the considered model. The efficacy of the proposed technique is shown on an automatic emergency braking system model with a perception component based on deep neural networks.",
                        "Citation Paper Authors": "Authors:Tommaso Dreossi, Alexandre Donz\u00e9, Sanjit A. Seshia"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ". Currently, most safety verification and\nvalidation work focuses on the ML components, including\nformal verification [12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\nand coverage-guided testing [22, 23, 24, 25, 26, 27, 28, 29].\nPlease refer to ",
                    "Citation Text": "Xiaowei Huang, Daniel Kroening, Wenjie Ruan, James Sharp,\nYoucheng Sun, Emese Thamo, Min Wu, and Xinping Yi. A\nsurvey of safety and trustworthiness of deep neural networks.\narXiv preprint arXiv:1812.08342 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.08342",
                        "Citation Paper Title": "Title:A Survey of Safety and Trustworthiness of Deep Neural Networks: Verification, Testing, Adversarial Attack and Defence, and Interpretability",
                        "Citation Paper Abstract": "Abstract:In the past few years, significant progress has been made on deep neural networks (DNNs) in achieving human-level performance on several long-standing tasks. With the broader deployment of DNNs on various applications, the concerns over their safety and trustworthiness have been raised in public, especially after the widely reported fatal incidents involving self-driving cars. Research to address these concerns is particularly active, with a significant number of papers released in the past few years. This survey paper conducts a review of the current research effort into making DNNs safe and trustworthy, by focusing on four aspects: verification, testing, adversarial attack and defence, and interpretability. In total, we survey 202 papers, most of which were published after 2017.",
                        "Citation Paper Authors": "Authors:Xiaowei Huang, Daniel Kroening, Wenjie Ruan, James Sharp, Youcheng Sun, Emese Thamo, Min Wu, Xinping Yi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2011.05905v4": {
            "Paper Title": "ShadowNet: A Secure and Efficient On-device Model Inference System for\n  Convolutional Neural Networks",
            "Sentences": [
                {
                    "Sentence ID": 54,
                    "Sentence": "proposes TEE extension for GPU\nhardware, thus allowing GPU tasks to run securely. However, it\nrequires hardware changes to the GPU. Secloak ",
                    "Citation Text": "Matthew Lentz, Rijurekha Sen, Peter Druschel, and Bobby Bhattachar-\njee. Secloak: Arm trustzone-based mobile peripheral control. In\nProceedings of the 16th Annual International Conference on Mobile\nSystems, Applications, and Services , pages 1\u201313, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2001.08840",
                        "Citation Paper Title": "Title:SeCloak: ARM Trustzone-based Mobile Peripheral Control",
                        "Citation Paper Abstract": "Abstract:Reliable on-off control of peripherals on smart devices is a key to security and privacy in many scenarios. Journalists want to reliably turn off radios to protect their sources during investigative reporting. Users wish to ensure cameras and microphones are reliably off during private meetings. In this paper, we present SeCloak, an ARM TrustZone-based solution that ensures reliable on-off control of peripherals even when the platform software is compromised. We design a secure kernel that co-exists with software running on mobile devices (e.g., Android and Linux) without requiring any code modifications. An Android prototype demonstrates that mobile peripherals like radios, cameras, and microphones can be controlled reliably with a very small trusted computing base and with minimal performance overhead.",
                        "Citation Paper Authors": "Authors:Matthew Lentz, Rijurekha Sen, Peter Druschel, Bobby Bhattacharjee"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": ", on a victim\nmodel of a four layer fully-connected CNN trained on CIFAR-\n10 (Fig. 2). Knockoff is the state-of-the-art black-box attack\nwith an adaptive query strategy. A recent survey ",
                    "Citation Text": "Adam Dziedzic, Muhammad Ahmad Kaleem, Yu Shen Lu, and Nicolas\nPapernot. Increasing the cost of model extraction with calibrated proof of\nwork. In ICLR (International Conference on Learning Representations)\n[SPOTLIGTH] , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2201.09243",
                        "Citation Paper Title": "Title:Increasing the Cost of Model Extraction with Calibrated Proof of Work",
                        "Citation Paper Abstract": "Abstract:In model extraction attacks, adversaries can steal a machine learning model exposed via a public API by repeatedly querying it and adjusting their own model based on obtained predictions. To prevent model stealing, existing defenses focus on detecting malicious queries, truncating, or distorting outputs, thus necessarily introducing a tradeoff between robustness and model utility for legitimate users. Instead, we propose to impede model extraction by requiring users to complete a proof-of-work before they can read the model's predictions. This deters attackers by greatly increasing (even up to 100x) the computational effort needed to leverage query access for model extraction. Since we calibrate the effort required to complete the proof-of-work to each query, this only introduces a slight overhead for regular users (up to 2x). To achieve this, our calibration applies tools from differential privacy to measure the information revealed by a query. Our method requires no modification of the victim model and can be applied by machine learning practitioners to guard their publicly exposed models against being easily stolen.",
                        "Citation Paper Authors": "Authors:Adam Dziedzic, Muhammad Ahmad Kaleem, Yu Shen Lu, Nicolas Papernot"
                    }
                },
                {
                    "Sentence ID": 64,
                    "Sentence": ", and complicated Lambda\nlayers consisting of different non-linear operations, such as\nupsampling, concatenation. Third, the original model size is\n34MB which is large in the context of mobile environments.\nDatasets. MobileNet is evaluated on the ImageNet-2012\ndataset ",
                    "Citation Text": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev\nSatheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla,\nMichael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large\nScale Visual Recognition Challenge. International Journal of Computer\nVision (IJCV) , 115(3):211\u2013252, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.0575",
                        "Citation Paper Title": "Title:ImageNet Large Scale Visual Recognition Challenge",
                        "Citation Paper Abstract": "Abstract:The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions.\nThis paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.",
                        "Citation Paper Authors": "Authors:Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, Li Fei-Fei"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2007.15805v2": {
            "Paper Title": "vWitness: Certifying Web Page Interactions with Computer Vision",
            "Sentences": []
        },
        "http://arxiv.org/abs/2012.03586v2": {
            "Paper Title": "No Need to Know Physics: Resilience of Process-based Model-free Anomaly\n  Detection for Industrial Control Systems",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "eval-\nuates the impact of adversarial examples in ICS by applying\nwhite box FGSM ",
                    "Citation Text": "Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining\nand harnessing adversarial examples. CoRR , abs/1412.6572, 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.6572",
                        "Citation Paper Title": "Title:Explaining and Harnessing Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
                        "Citation Paper Authors": "Authors:Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "is weakened to demonstrate robustness to\nadversarial examples for the presented anomaly detector. An\niterative gradient-based attack is proposed. The work ",
                    "Citation Text": "Giulio Zizzo, Chris Hankin, Sergio Maffeis, and Kevin Jones. Intru-\nsion detection for industrial control systems: Evaluation analysis and\nadversarial attacks. arXiv preprint arXiv:1911.04278 , 2019.\nAPPENDIX A\nMETHODOLOGY\nIn order to evaluate the performance of the anomaly detector\nwe observe how Accuracy Eq. 2, Precision Eq. 3, Recall Eq. 4,\nand False Positive Rate Eq. 6 scores change when the spoofing\ntechnique is applied to the data.\nAccuracy=TP+TN\nTP+FP+TN+FN(2)\nPrecision=TP\nTP+FP(3)\nRecall=TP\nTP+FN(4)\nF1-Score=2\u00d7Precision\u00d7Recall\nPrecision+Recall(5)FPR=FP\nTN+FP(6)\nGiven the original classification scores (e.g., when no\nspoofing is applied to data), Synthetic Sensor Spoofing is\neffective if Precision and Recall score reduces substantially.\nWhen those two scores reduce, towards 0, it means that the\ninstances where the Synthetic Sensor Spoofing was applied\nwere misclassified moving them from being True Positives\nto False Negatives. Looking at False Positive Rate (FPR)\nscore we can also verify if the attacks are introducing False\nPositive in the Classification. If the FPR remains almost like\nthe original, it means that the Synthetic Sensor Spoofing did\nnot induce any wrong classification (as expected since we are\nnot spoofing data outside the boundaries of the attacks present\nin the dataset). Finally, since the datasets are unbalanced, with\nmore samples with of the negative class, Accuracy score will\nnot reach zero but at most the baseline where all the instances\nare labeled as the negative class.\nAPPENDIX B\nAOUDI ET AL . QUALITATIVE EVALUATION\nLike in the evaluation from the original paper, we show sen-\nsor LIT301 from SWaT dataset. In Figure 4(a), the departure\nscore obtained over original SWaT reflects the one presented\nin",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.04278",
                        "Citation Paper Title": "Title:Adversarial Attacks on Time-Series Intrusion Detection for Industrial Control Systems",
                        "Citation Paper Abstract": "Abstract:Neural networks are increasingly used for intrusion detection on industrial control systems (ICS). With neural networks being vulnerable to adversarial examples, attackers who wish to cause damage to an ICS can attempt to hide their attacks from detection by using adversarial example techniques. In this work we address the domain specific challenges of constructing such attacks against autoregressive based intrusion detection systems (IDS) in an ICS setting.\nWe model an attacker that can compromise a subset of sensors in a ICS which has a LSTM based IDS. The attacker manipulates the data sent to the IDS, and seeks to hide the presence of real cyber-physical attacks occurring in the ICS.\nWe evaluate our adversarial attack methodology on the Secure Water Treatment system when examining solely continuous data, and on data containing a mixture of discrete and continuous variables. In the continuous data domain our attack successfully hides the cyber-physical attacks requiring 2.87 out of 12 monitored sensors to be compromised on average. With both discrete and continuous data our attack required, on average, 3.74 out of 26 monitored sensors to be compromised.",
                        "Citation Paper Authors": "Authors:Giulio Zizzo, Chris Hankin, Sergio Maffeis, Kevin Jones"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2007.12674v2": {
            "Paper Title": "Controlling Privacy Loss in Sampling Schemes: an Analysis of Stratified\n  and Cluster Sampling",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": "provide lower and upper bounds on privacy amplification when sampling from a\nmultidimensional Bernoulli family, a task which has direct applications to Bayesian inference. Balle\n3et al. ",
                    "Citation Text": "Balle, B., Barthe, G., and Gaboardi, M. Privacy amplification by subsampling: Tight\nanalyses via couplings and divergences. In Advances in Neural Information Processing Systems\n31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8\nDecember 2018, Montr\u00b4 eal, Canada (2018), pp. 6280\u20136290.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.01647",
                        "Citation Paper Title": "Title:Privacy Amplification by Subsampling: Tight Analyses via Couplings and Divergences",
                        "Citation Paper Abstract": "Abstract:Differential privacy comes equipped with multiple analytical tools for the design of private data analyses. One important tool is the so-called \"privacy amplification by subsampling\" principle, which ensures that a differentially private mechanism run on a random subsample of a population provides higher privacy guarantees than when run on the entire population. Several instances of this principle have been studied for different random subsampling methods, each with an ad-hoc analysis. In this paper we present a general method that recovers and improves prior analyses, yields lower bounds and derives new instances of privacy amplification by subsampling. Our method leverages a characterization of differential privacy as a divergence which emerged in the program verification community. Furthermore, it introduces new tools, including advanced joint convexity and privacy profiles, which might be of independent interest.",
                        "Citation Paper Authors": "Authors:Borja Balle, Gilles Barthe, Marco Gaboardi"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "showed that applying Poisson sampling before running a differen-\ntially private mechanism improves its end-to-end privacy guarantee. Subsequently, Bun et al. ",
                    "Citation Text": "Bun, M., Nissim, K., Stemmer, U., and Vadhan, S. Differentially private release and\nlearning of threshold functions. In Proceedings of the 56th Annual IEEE Symposium on Foun-\ndations of Computer Science (Washington, DC, USA, 2015), FOCS \u201915, IEEE Computer\nSociety, pp. 634\u2013649.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1504.07553",
                        "Citation Paper Title": "Title:Differentially Private Release and Learning of Threshold Functions",
                        "Citation Paper Abstract": "Abstract:We prove new upper and lower bounds on the sample complexity of $(\\epsilon, \\delta)$ differentially private algorithms for releasing approximate answers to threshold functions. A threshold function $c_x$ over a totally ordered domain $X$ evaluates to $c_x(y) = 1$ if $y \\le x$, and evaluates to $0$ otherwise. We give the first nontrivial lower bound for releasing thresholds with $(\\epsilon,\\delta)$ differential privacy, showing that the task is impossible over an infinite domain $X$, and moreover requires sample complexity $n \\ge \\Omega(\\log^*|X|)$, which grows with the size of the domain. Inspired by the techniques used to prove this lower bound, we give an algorithm for releasing thresholds with $n \\le 2^{(1+ o(1))\\log^*|X|}$ samples. This improves the previous best upper bound of $8^{(1 + o(1))\\log^*|X|}$ (Beimel et al., RANDOM '13).\nOur sample complexity upper and lower bounds also apply to the tasks of learning distributions with respect to Kolmogorov distance and of properly PAC learning thresholds with differential privacy. The lower bound gives the first separation between the sample complexity of properly learning a concept class with $(\\epsilon,\\delta)$ differential privacy and learning without privacy. For properly learning thresholds in $\\ell$ dimensions, this lower bound extends to $n \\ge \\Omega(\\ell \\cdot \\log^*|X|)$.\nTo obtain our results, we give reductions in both directions from releasing and properly learning thresholds and the simpler interior point problem. Given a database $D$ of elements from $X$, the interior point problem asks for an element between the smallest and largest elements in $D$. We introduce new recursive constructions for bounding the sample complexity of the interior point problem, as well as further reductions and techniques for proving impossibility results for other basic problems in differential privacy.",
                        "Citation Paper Authors": "Authors:Mark Bun, Kobbi Nissim, Uri Stemmer, Salil Vadhan"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "Several works have studied the privacy amplification of simple sampling schemes. Kasiviswanathan\net al. ",
                    "Citation Text": "Kasiviswanathan, S. P., Lee, H. K., Nissim, K., Raskhodnikova, S., and Smith, A.\nWhat can we learn privately? SIAM Journal on Computing 40 , 3 (2011), 793\u2013826.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:0803.0924",
                        "Citation Paper Title": "Title:What Can We Learn Privately?",
                        "Citation Paper Abstract": "Abstract:  Learning problems form an important category of computational tasks that generalizes many of the computations researchers apply to large real-life data sets. We ask: what concept classes can be learned privately, namely, by an algorithm whose output does not depend too heavily on any one input or specific training example? More precisely, we investigate learning algorithms that satisfy differential privacy, a notion that provides strong confidentiality guarantees in contexts where aggregate information is released about a database containing sensitive information about individuals. We demonstrate that, ignoring computational constraints, it is possible to privately agnostically learn any concept class using a sample size approximately logarithmic in the cardinality of the concept class. Therefore, almost anything learnable is learnable privately: specifically, if a concept class is learnable by a (non-private) algorithm with polynomial sample complexity and output size, then it can be learned privately using a polynomial number of samples. We also present a computationally efficient private PAC learner for the class of parity functions. Local (or randomized response) algorithms are a practical class of private algorithms that have received extensive investigation. We provide a precise characterization of local private learning algorithms. We show that a concept class is learnable by a local algorithm if and only if it is learnable in the statistical query (SQ) model. Finally, we present a separation between the power of interactive and noninteractive local learning algorithms.",
                        "Citation Paper Authors": "Authors:Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya Raskhodnikova, Adam Smith"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.04554v4": {
            "Paper Title": "Replay-based Recovery for Autonomous Robotic Vehicles from Sensor\n  Deception Attacks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.12112v4": {
            "Paper Title": "Investigating Membership Inference Attacks under Data Dependencies",
            "Sentences": [
                {
                    "Sentence ID": 17,
                    "Sentence": ". Spurious correlations in ML datasets have also\nreceived a lot of attention in the ML literature as they can have\na detrimental effect on model accuracy ",
                    "Citation Text": "V . Nagarajan, A. Andreassen, and B. Neyshabur, \u201cUnderstanding the\nfailure modes of out-of-distribution generalization,\u201d https://arxiv.org/pdf/\n2010.15775.pdf, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.15775",
                        "Citation Paper Title": "Title:Understanding the Failure Modes of Out-of-Distribution Generalization",
                        "Citation Paper Abstract": "Abstract:Empirical studies suggest that machine learning models often rely on features, such as the background, that may be spuriously correlated with the label only during training time, resulting in poor accuracy during test-time. In this work, we identify the fundamental factors that give rise to this behavior, by explaining why models fail this way {\\em even} in easy-to-learn tasks where one would expect these models to succeed. In particular, through a theoretical study of gradient-descent-trained linear classifiers on some easy-to-learn tasks, we uncover two complementary failure modes. These modes arise from how spurious correlations induce two kinds of skews in the data: one geometric in nature, and another, statistical in nature. Finally, we construct natural modifications of image classification datasets to understand when these failure modes can arise in practice. We also design experiments to isolate the two failure modes when training modern neural networks on these datasets.",
                        "Citation Paper Authors": "Authors:Vaishnavh Nagarajan, Anders Andreassen, Behnam Neyshabur"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.15469v2": {
            "Paper Title": "Learning Failure-Inducing Models for Testing Software-Defined Networks",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": ". The research strands that most closely relate to our work are fuzzing\ntechniques based on ML for testing networked systems [ 7,47]. Chen et al . ",
                    "Citation Text": "Yuqi Chen, Christopher M. Poskitt, Jun Sun, Sridhar Adepu, and Fan Zhang. 2019. Learning-Guided Network Fuzzing\nfor Testing Cyber-Physical System Defences. In Proceedings of the 34th IEEE/ACM International Conference on Automated\nSoftware Engineering . 962\u2013973.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.05410",
                        "Citation Paper Title": "Title:Learning-Guided Network Fuzzing for Testing Cyber-Physical System Defences",
                        "Citation Paper Abstract": "Abstract:The threat of attack faced by cyber-physical systems (CPSs), especially when they play a critical role in automating public infrastructure, has motivated research into a wide variety of attack defence mechanisms. Assessing their effectiveness is challenging, however, as realistic sets of attacks to test them against are not always available. In this paper, we propose smart fuzzing, an automated, machine learning guided technique for systematically finding 'test suites' of CPS network attacks, without requiring any knowledge of the system's control programs or physical processes. Our approach uses predictive machine learning models and metaheuristic search algorithms to guide the fuzzing of actuators so as to drive the CPS into different unsafe physical states. We demonstrate the efficacy of smart fuzzing by implementing it for two real-world CPS testbeds---a water purification plant and a water distribution system---finding attacks that drive them into 27 different unsafe states involving water flow, pressure, and tank levels, including six that were not covered by an established attack benchmark. Finally, we use our approach to test the effectiveness of an invariant-based defence system for the water treatment plant, finding two attacks that were not detected by its physical invariant checks, highlighting a potential weakness that could be exploited in certain conditions.",
                        "Citation Paper Authors": "Authors:Yuqi Chen, Christopher M. Poskitt, Jun Sun, Sridhar Adepu, Fan Zhang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2007.06236v4": {
            "Paper Title": "Quality Inference in Federated Learning with Secure Aggregation",
            "Sentences": [
                {
                    "Sentence ID": 55,
                    "Sentence": "is also such an attack, which could tie the extracted information to specific participants of FL. However, it\ndoes not work with SA. Another property inference attack is the quantity composition attack ",
                    "Citation Text": "L. Wang, S. Xu, X. Wang, and Q. Zhu, \u201cEavesdrop the composition proportion of training labels in federated\nlearning,\u201d arXiv:1910.06044 [cs, stat] , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.06044",
                        "Citation Paper Title": "Title:Eavesdrop the Composition Proportion of Training Labels in Federated Learning",
                        "Citation Paper Abstract": "Abstract:Federated learning (FL) has recently emerged as a new form of collaborative machine learning, where a common model can be learned while keeping all the training data on local devices. Although it is designed for enhancing the data privacy, we demonstrated in this paper a new direction in inference attacks in the context of FL, where valuable information about training data can be obtained by adversaries with very limited power. In particular, we proposed three new types of attacks to exploit this vulnerability. The first type of attack, Class Sniffing, can detect whether a certain label appears in training. The other two types of attacks can determine the quantity of each label, i.e., Quantity Inference attack determines the composition proportion of the training label owned by the selected clients in a single round, while Whole Determination attack determines that of the whole training process. We evaluated our attacks on a variety of tasks and datasets with different settings, and the corresponding results showed that our attacks work well generally. Finally, we analyzed the impact of major hyper-parameters to our attacks and discussed possible defenses.",
                        "Citation Paper Authors": "Authors:Lixu Wang, Shichao Xu, Xiao Wang, Qi Zhu"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": ", which was designed to allocate goods to players proportionally\nto their contributions. A high-level summary of the role of the Shapley value within ML is presented in ",
                    "Citation Text": "B. Rozemberczki, L. Watson, P. Bayer, H.-T. Yang, O. Kiss, S. Nilsson, and R. Sarkar, \u201cThe shapley value in\nmachine learning,\u201d arXiv preprint arXiv:2202.05594 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.05594",
                        "Citation Paper Title": "Title:The Shapley Value in Machine Learning",
                        "Citation Paper Abstract": "Abstract:Over the last few years, the Shapley value, a solution concept from cooperative game theory, has found numerous applications in machine learning. In this paper, we first discuss fundamental concepts of cooperative game theory and axiomatic properties of the Shapley value. Then we give an overview of the most important applications of the Shapley value in machine learning: feature selection, explainability, multi-agent reinforcement learning, ensemble pruning, and data valuation. We examine the most crucial limitations of the Shapley value and point out directions for future research.",
                        "Citation Paper Authors": "Authors:Benedek Rozemberczki, Lauren Watson, P\u00e9ter Bayer, Hao-Tsung Yang, Oliv\u00e9r Kiss, Sebastian Nilsson, Rik Sarkar"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.09652v2": {
            "Paper Title": "Adversarial Color Projection: A Projector-based Physical Attack to DNNs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2010.09112v4": {
            "Paper Title": "BBB-Voting: 1-out-of-k Blockchain-Based Boardroom Voting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2004.12478v2": {
            "Paper Title": "Improved Image Wasserstein Attacks and Defenses",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.04772v5": {
            "Paper Title": "VAMS: Verifiable Auditing of Access to Confidential Data",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": ". However, they still impose trade-offs between\nprivacy and utility ",
                    "Citation Text": "M\u00e1rio S Alvim, Miguel E Andr\u00e9s, Konstantinos\nChatzikokolakis, Pierpaolo Degano, and Catuscia\nPalamidessi. Differential privacy: on the trade-off be-\ntween utility and information leakage. In International\nWorkshop on Formal Aspects in Security and Trust ,\npages 39\u201354. Springer, 2011.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1103.5188",
                        "Citation Paper Title": "Title:Differential Privacy: on the trade-off between Utility and Information Leakage",
                        "Citation Paper Abstract": "Abstract:Differential privacy is a notion of privacy that has become very popular in the database community. Roughly, the idea is that a randomized query mechanism provides sufficient privacy protection if the ratio between the probabilities that two adjacent datasets give the same answer is bound by e^epsilon. In the field of information flow there is a similar concern for controlling information leakage, i.e. limiting the possibility of inferring the secret information from the observables. In recent years, researchers have proposed to quantify the leakage in terms of R\u00e9nyi min mutual information, a notion strictly related to the Bayes risk. In this paper, we show how to model the query system in terms of an information-theoretic channel, and we compare the notion of differential privacy with that of mutual information. We show that differential privacy implies a bound on the mutual information (but not vice-versa). Furthermore, we show that our bound is tight. Then, we consider the utility of the randomization mechanism, which represents how close the randomized answers are, in average, to the real ones. We show that the notion of differential privacy implies a bound on utility, also tight, and we propose a method that under certain conditions builds an optimal randomization mechanism, i.e. a mechanism which provides the best utility while guaranteeing differential privacy.",
                        "Citation Paper Authors": "Authors:M\u00e1rio S. Alvim, Miguel E. Andr\u00e9s, Konstantinos Chatzikokolakis, Pierpaolo Degano, Catuscia Palamidessi"
                    }
                }
            ]
        }
    }
}