{
    "Neurons and Cognition": {
        "http://arxiv.org/abs/1808.10806v4": {
            "Paper Title": "Complete Inference of Causal Relations between Dynamical Systems",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": "stated that high values of CCM, which are independent of\nthe data length, are a sign of a hidden common cause. High CCM for short data\nseries is typical for highly correlated data series. Similarly, Harnack et al. ",
                    "Citation Text": "D. Harnack, E. Laminski, M. Sch\u00a8 unemann, K. R. Pawelzik, Topological\ncausality in dynamical systems, Physical Review Letters 119 (9) (2017)\n098301. doi:10.1103/PhysRevLett.119.098301 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1605.02570",
                        "Citation Paper Title": "Title:On Causality in Dynamical Systems",
                        "Citation Paper Abstract": "Abstract:Discovery of causal relations is fundamental for understanding the dynamics of complex systems. While causal interactions are well defined for acyclic systems that can be separated into causally effective subsystems, a mathematical definition of gradual causal interaction is still lacking for nonseparable dynamical systems. The solution proposed here is analytically tractable for time discrete chaotic maps and is shown to fulfill basic requirements for causality measures. It implies a method for determination of directed effective influences using pairs of measurements from dynamical systems. Applications to time series from systems of coupled differential equations and linear stochastic systems demonstrate its general utility.",
                        "Citation Paper Authors": "Authors:Daniel Harnack, Erik Laminski, Klaus Richard Pawelzik"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1805.10235v2": {
            "Paper Title": "Reconciliation of weak pairwise spike-train correlations and highly\n  coherent local field potentials across space",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.12231v3": {
            "Paper Title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias\n  improves accuracy and robustness",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.03881v4": {
            "Paper Title": "Feed-forward and noise-tolerant detection of feature homogeneity in\n  spiking networks with a latency code",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.09046v2": {
            "Paper Title": "Multiclass Common Spatial Pattern for EEG based Brain Computer Interface\n  with Adaptive Learning Classifier",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.03792v2": {
            "Paper Title": "Impact of axonal delay on structure development in a multi-layered\n  network",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.08750v3": {
            "Paper Title": "Generalisation in humans and deep neural networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.01279v3": {
            "Paper Title": "Built to Last: Functional and structural mechanisms in the moth\n  olfactory network mitigate effects of neural injury",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.08840v3": {
            "Paper Title": "Learning differentially reorganizes brain activity and connectivity",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.09431v3": {
            "Paper Title": "Dissociable neural representations of adversarially perturbed images in\n  convolutional neural networks and the human brain",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.00864v3": {
            "Paper Title": "FNS: an event-driven spiking neural network simulator based on the LIFL\n  neuron model",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.04738v2": {
            "Paper Title": "Inferring neuronal couplings from spiking data using a systematic\n  procedure with a statistical criterion",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.04191v2": {
            "Paper Title": "Using learning to control artificial avatars in human motor coordination\n  tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.02459v3": {
            "Paper Title": "Nonlinear Evolution via Spatially-Dependent Linear Dynamics for\n  Electrophysiology and Calcium Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.08476v3": {
            "Paper Title": "Human peripheral blur is optimal for object recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.11680v2": {
            "Paper Title": "Asymptotic and numerical analysis of a stochastic PDE model of volume\n  transmission",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.10974v2": {
            "Paper Title": "Decoding Brain Representations by Multimodal Learning of Neural Activity\n  and Visual Features",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.06108v3": {
            "Paper Title": "Modeling the Formation of Social Conventions from Embodied Real-Time\n  Interactions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.00786v2": {
            "Paper Title": "Caulking the Leakage Effect in MEEG Source Connectivity Analysis",
            "Sentences": [
                {
                    "Sentence ID": 92,
                    "Sentence": "Zhang, T. and Zou, H., 2014. Sparse precision matrix estimation via lasso penalized D -trace loss. \nBiometrika, 101(1), pp.103 -120.  \nhttps://doi.org/10.1093/biomet/ast059 ",
                    "Citation Text": "Zhang, Z. and Rao, B.D., 2011. Sparse signal recovery with temporally correlated source vectors using \nsparse Bayesian learning. IEEE Journal of Selected Topics in Signal Processing, 5(5), pp.912 -926.  \nhttps://doi.org/ 10.1109/JSTSP.2011.2159773",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1102.3949",
                        "Citation Paper Title": "Title:Sparse Signal Recovery with Temporally Correlated Source Vectors Using Sparse Bayesian Learning",
                        "Citation Paper Abstract": "Abstract:We address the sparse signal recovery problem in the context of multiple measurement vectors (MMV) when elements in each nonzero row of the solution matrix are temporally correlated. Existing algorithms do not consider such temporal correlations and thus their performance degrades significantly with the correlations. In this work, we propose a block sparse Bayesian learning framework which models the temporal correlations. In this framework we derive two sparse Bayesian learning (SBL) algorithms, which have superior recovery performance compared to existing algorithms, especially in the presence of high temporal correlations. Furthermore, our algorithms are better at handling highly underdetermined problems and require less row-sparsity on the solution matrix. We also provide analysis of the global and local minima of their cost function, and show that the SBL cost function has the very desirable property that the global minimum is at the sparsest solution to the MMV problem. Extensive experiments also provide some interesting results that motivate future theoretical research on the MMV model.",
                        "Citation Paper Authors": "Authors:Zhilin Zhang, Bhaskar D. Rao"
                    }
                },
                {
                    "Sentence ID": 90,
                    "Sentence": "Wu, W., Nagarajan, S. and Chen, Z., 2016. Bayesian Machine Learning: EEG \\/MEG signal processing \nmeasurements. IEEE Signal Processing Magazine, 33(1), pp.14 -36. \nhttps://doi.org/ 10.1109/MSP.2015.2481559 ",
                    "Citation Text": "Yuan, G., Tan, H. and Zheng, W.S., 2017. A Coordinate -wise Optimization Algorithm for Sparse Inverse \nCovariance Selection. arXiv preprint arXiv:1711.07038.  \nhttps://arxiv.org/abs/1711.07038",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.07038",
                        "Citation Paper Title": "Title:A Coordinate-wise Optimization Algorithm for Sparse Inverse Covariance Selection",
                        "Citation Paper Abstract": "Abstract:Sparse inverse covariance selection is a fundamental problem for analyzing dependencies in high dimensional data. However, such a problem is difficult to solve since it is NP-hard. Existing solutions are primarily based on convex approximation and iterative hard thresholding, which only lead to sub-optimal solutions. In this work, we propose a coordinate-wise optimization algorithm to solve this problem which is guaranteed to converge to a coordinate-wise minimum point. The algorithm iteratively and greedily selects one variable or swaps two variables to identify the support set, and then solves a reduced convex optimization problem over the support set to achieve the greatest descent. As a side contribution of this paper, we propose a Newton-like algorithm to solve the reduced convex sub-problem, which is proven to always converge to the optimal solution with global linear convergence rate and local quadratic convergence rate. Finally, we demonstrate the efficacy of our method on synthetic data and real-world data sets. As a result, the proposed method consistently outperforms existing solutions in terms of accuracy.",
                        "Citation Paper Authors": "Authors:Ganzhao Yuan, Haoxian Tan, Wei-Shi Zheng"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": "McLachlan, G. and Krishnan, T., 2007. The EM algorithm and extensi ons (Vol. 382). John Wiley & Sons. ",
                    "Citation Text": "McCoy, M.B. and Tropp, J.A., 2013. The achievable performance of convex demixing. arXiv preprint \narXiv:1309.7478.  \nhttps://arxiv.org/abs/1309.7478v1",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1309.7478",
                        "Citation Paper Title": "Title:The achievable performance of convex demixing",
                        "Citation Paper Abstract": "Abstract:Demixing is the problem of identifying multiple structured signals from a superimposed, undersampled, and noisy observation. This work analyzes a general framework, based on convex optimization, for solving demixing problems. When the constituent signals follow a generic incoherence model, this analysis leads to precise recovery guarantees. These results admit an attractive interpretation: each signal possesses an intrinsic degrees-of-freedom parameter, and demixing can succeed if and only if the dimension of the observation exceeds the total degrees of freedom present in the observation.",
                        "Citation Paper Authors": "Authors:Michael B. McCoy, Joel A. Tropp"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "Huizenga, H.M., De Munck, J.C., Waldorp, L.J. and Grasman, R.P., 2002. Spatiotemporal EEG/MEG \nsource analysis based on a parametric  noise covariance model. IEEE Transactions on Biomedical \nEngineering, 49(6), pp.533 -539.  \nhttps://doi.org/ 10.1109/TBME.2002.1001967 ",
                    "Citation Text": "Jankova, J. and Van De Geer, S., 2015. Confidence inter vals for high -dimensional inverse covariance \nestimation. Electronic Journal of Statistics, 9(1), pp.1205 -1229.  \nhttps://doi.org/ 10.1214/15 -EJS1031",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1403.6752",
                        "Citation Paper Title": "Title:Confidence intervals for high-dimensional inverse covariance estimation",
                        "Citation Paper Abstract": "Abstract:We propose methodology for statistical inference for low-dimensional parameters of sparse precision matrices in a high-dimensional setting. Our method leads to a non-sparse estimator of the precision matrix whose entries have a Gaussian limiting distribution. Asymptotic properties of the novel estimator are analyzed for the case of sub-Gaussian observations under a sparsity assumption on the entries of the true precision matrix and regularity conditions. Thresholding the de-sparsified estimator gives guarantees for edge selection in the associated graphical model. Performance of the proposed method is illustrated in a simulation study.",
                        "Citation Paper Authors": "Authors:Jana Jankova, Sara van de Geer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1807.05214v2": {
            "Paper Title": "Invertible generalized synchronization: A putative mechanism for\n  implicit learning in biological and artificial neural systems",
            "Sentences": [
                {
                    "Sentence ID": 71,
                    "Sentence": "M. E. J. Newman, Modularity and community struc-\nture in networks, Proc. Natl. Acad. Sci. USA 103, 8577\n(2006). ",
                    "Citation Text": "V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and\nE. Lefebvre, Fast unfolding of communities in large net-\nworks, J. Stat. Mech. , P10008 (2008).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:0803.0476",
                        "Citation Paper Title": "Title:Fast unfolding of communities in large networks",
                        "Citation Paper Abstract": "Abstract:  We propose a simple method to extract the community structure of large networks. Our method is a heuristic method that is based on modularity optimization. It is shown to outperform all other known community detection method in terms of computation time. Moreover, the quality of the communities detected is very good, as measured by the so-called modularity. This is shown first by identifying language communities in a Belgian mobile phone network of 2.6 million customers and by analyzing a web graph of 118 million nodes and more than one billion links. The accuracy of our algorithm is also verified on ad-hoc modular networks. .",
                        "Citation Paper Authors": "Authors:Vincent D. Blondel, Jean-Loup Guillaume, Renaud Lambiotte, Etienne Lefebvre"
                    }
                },
                {
                    "Sentence ID": 69,
                    "Sentence": "Y. Bengio, D.-H. Lee, J. Bornschein, T. Mesnard, and\nZ. Lin, Towards biologically plausible deep learning,\narXiv preprint arXiv:1502.04156 (2015). ",
                    "Citation Text": "M. A. Porter, J.-P. Onnela, and P. J. Mucha, Communi-\nties in networks, Notices of the American Mathematical\nSociety 56, 1082 (2009).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:0902.3788",
                        "Citation Paper Title": "Title:Communities in Networks",
                        "Citation Paper Abstract": "Abstract:  We survey some of the concepts, methods, and applications of community detection, which has become an increasingly important area of network science. To help ease newcomers into the field, we provide a guide to available methodology and open problems, and discuss why scientists from diverse backgrounds are interested in these problems. As a running theme, we emphasize the connections of community detection to problems in statistical physics and computational optimization.",
                        "Citation Paper Authors": "Authors:Mason A. Porter, Jukka-Pekka Onnela, Peter J. Mucha"
                    }
                },
                {
                    "Sentence ID": 68,
                    "Sentence": "G. Cybenko, Approximations by superpositions of a sig-\nmoidal function, Mathematics of Control, Signals and\nSystems 2, 183 (1989). ",
                    "Citation Text": "Y. Bengio, D.-H. Lee, J. Bornschein, T. Mesnard, and\nZ. Lin, Towards biologically plausible deep learning,\narXiv preprint arXiv:1502.04156 (2015).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1502.04156",
                        "Citation Paper Title": "Title:Towards Biologically Plausible Deep Learning",
                        "Citation Paper Abstract": "Abstract:Neuroscientists have long criticised deep learning algorithms as incompatible with current knowledge of neurobiology. We explore more biologically plausible versions of deep representation learning, focusing here mostly on unsupervised learning but developing a learning mechanism that could account for supervised, unsupervised and reinforcement learning. The starting point is that the basic learning rule believed to govern synaptic weight updates (Spike-Timing-Dependent Plasticity) arises out of a simple update rule that makes a lot of sense from a machine learning point of view and can be interpreted as gradient descent on some objective function so long as the neuronal dynamics push firing rates towards better values of the objective function (be it supervised, unsupervised, or reward-driven). The second main idea is that this corresponds to a form of the variational EM algorithm, i.e., with approximate rather than exact posteriors, implemented by neural dynamics. Another contribution of this paper is that the gradients required for updating the hidden states in the above variational interpretation can be estimated using an approximation that only requires propagating activations forward and backward, with pairs of layers learning to form a denoising auto-encoder. Finally, we extend the theory about the probabilistic interpretation of auto-encoders to justify improved sampling schemes based on the generative interpretation of denoising auto-encoders, and we validate all these ideas on generative learning tasks.",
                        "Citation Paper Authors": "Authors:Yoshua Bengio, Dong-Hyun Lee, Jorg Bornschein, Thomas Mesnard, Zhouhan Lin"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "N. Mesgarani and E. F. Chang, Selective cortical repre-\nsentation of attended speaker in multi-talker speech per-\nception, Nature 485, 233 (2012). ",
                    "Citation Text": "Z. Lu, B. R. Hunt, and E. Ott, Attractor recon-\nstruction by machine learning, Chaos: An Interdisci-\nplinary Journal of Nonlinear Science 28, 061104 (2018),\nhttps://doi.org/10.1063/1.5039508.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.03362",
                        "Citation Paper Title": "Title:Attractor Reconstruction by Machine Learning",
                        "Citation Paper Abstract": "Abstract:A machine-learning approach called \"reservoir computing\" has been used successfully for short-term prediction and attractor reconstruction of chaotic dynamical systems from time series data. We present a theoretical framework that describes conditions under which reservoir computing can create an empirical model capable of skillful short-term forecasts and accurate long-term ergodic behavior. We illustrate this theory through numerical experiments. We also argue that the theory applies to certain other machine learning methods for time series prediction.",
                        "Citation Paper Authors": "Authors:Zhixin Lu, Brian R. Hunt, Edward Ott"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "D. Sussillo and L. F. Abbott, Generating coherent pat-\nterns of activity from chaotic neural networks, Neuron\n63, 544 (2009). ",
                    "Citation Text": "J. Pathak, Z. Lu, B. R. Hunt, M. Girvan, and E. Ott, Us-\ning machine learning to replicate chaotic attractors and\ncalculate lyapunov exponents from data, Chaos: An In-\nterdisciplinary Journal of Nonlinear Science 27, 121102\n(2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.07313",
                        "Citation Paper Title": "Title:Using Machine Learning to Replicate Chaotic Attractors and Calculate Lyapunov Exponents from Data",
                        "Citation Paper Abstract": "Abstract:We use recent advances in the machine learning area known as 'reservoir computing' to formulate a method for model-free estimation from data of the Lyapunov exponents of a chaotic process. The technique uses a limited time series of measurements as input to a high-dimensional dynamical system called a 'reservoir'. After the reservoir's response to the data is recorded, linear regression is used to learn a large set of parameters, called the 'output weights'. The learned output weights are then used to form a modified autonomous reservoir designed to be capable of producing arbitrarily long time series whose ergodic properties approximate those of the input signal. When successful, we say that the autonomous reservoir reproduces the attractor's 'climate'. Since the reservoir equations and output weights are known, we can compute derivatives needed to determine the Lyapunov exponents of the autonomous reservoir, which we then use as estimates of the Lyapunov exponents for the original input generating system. We illustrate the effectiveness of our technique with two examples, the Lorenz system, and the Kuramoto-Sivashinsky (KS) equation. In particular, we use the Lorenz system to show that achieving climate reproduction may require tuning of the reservoir parameters. For the case of the KS equation, we note that as the system's spatial size is increased, the number of Lyapunov exponents increases, thus yielding a challenging test of our method, which we find the method successfully passes.",
                        "Citation Paper Authors": "Authors:Jaideep Pathak, Zhixin Lu, Brian R. Hunt, Michelle Girvan, Edward Ott"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "K. Rajan, C. D. Harvey, and D. W. Tank, Recurrent net-\nwork models of sequence generation and memory, Neuron\n90, 128 (2016). ",
                    "Citation Text": "A. Alemi, C. Machens, S. Den\u0012 eve, and J.-J. Slotine,\nLearning arbitrary dynamics in e\u000ecient, balanced spik-\ning networks using local plasticity rules, arXiv preprint\narXiv:1705.08026 (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.08026",
                        "Citation Paper Title": "Title:Learning arbitrary dynamics in efficient, balanced spiking networks using local plasticity rules",
                        "Citation Paper Abstract": "Abstract:Understanding how recurrent neural circuits can learn to implement dynamical systems is a fundamental challenge in neuroscience. The credit assignment problem, i.e. determining the local contribution of each synapse to the network's global output error, is a major obstacle in deriving biologically plausible local learning rules. Moreover, spiking recurrent networks implementing such tasks should not be hugely costly in terms of number of neurons and spikes, as they often are when adapted from rate models. Finally, these networks should be robust to noise and neural deaths in order to sustain these representations in the face of such naturally occurring perturbation. We approach this problem by fusing the theory of efficient, balanced spiking networks (EBN) with nonlinear adaptive control theory. Local learning rules are ensured by feeding back into the network its own error, resulting in a synaptic plasticity rule depending solely on presynaptic inputs and post-synaptic feedback. The spiking efficiency and robustness of the network are guaranteed by maintaining a tight excitatory/inhibitory balance, ensuring that each spike represents a local projection of the global output error and minimizes a loss function. The resulting networks can learn to implement complex dynamics with very small numbers of neurons and spikes, exhibit the same spike train variability as observed experimentally, and are extremely robust to noise and neuronal loss.",
                        "Citation Paper Authors": "Authors:Alireza Alemi, Christian Machens, Sophie Den\u00e8ve, Jean-Jacques Slotine"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "A. Hefny, C. Downey, and G. J. Gordon, Supervised\nlearning for dynamical system learning, in Advances in\nneural information processing systems (2015) pp. 1963{\n1971. ",
                    "Citation Text": "M. Raissi, P. Perdikaris, and G. E. Karniadakis, Multi-\nstep neural networks for data-driven discovery of nonlin-\near dynamical systems, arXiv preprint arXiv:1801.01236\n(2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1801.01236",
                        "Citation Paper Title": "Title:Multistep Neural Networks for Data-driven Discovery of Nonlinear Dynamical Systems",
                        "Citation Paper Abstract": "Abstract:The process of transforming observed data into predictive mathematical models of the physical world has always been paramount in science and engineering. Although data is currently being collected at an ever-increasing pace, devising meaningful models out of such observations in an automated fashion still remains an open problem. In this work, we put forth a machine learning approach for identifying nonlinear dynamical systems from data. Specifically, we blend classical tools from numerical analysis, namely the multi-step time-stepping schemes, with powerful nonlinear function approximators, namely deep neural networks, to distill the mechanisms that govern the evolution of a given data-set. We test the effectiveness of our approach for several benchmark problems involving the identification of complex, nonlinear and chaotic dynamics, and we demonstrate how this allows us to accurately learn the dynamics, forecast future states, and identify basins of attraction. In particular, we study the Lorenz system, the fluid flow behind a cylinder, the Hopf bifurcation, and the Glycoltic oscillator model as an example of complicated nonlinear dynamics typical of biological systems.",
                        "Citation Paper Authors": "Authors:Maziar Raissi, Paris Perdikaris, George Em Karniadakis"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1812.10050v2": {
            "Paper Title": "Statistical Model for Dynamically-Changing Correlation Matrices with\n  Application to Brain Connectivity",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.02479v2": {
            "Paper Title": "A metric model for the functional architecture of the visual cortex",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": ") to construct algorithms that keep trace of the geometry of the data, by means of\nparameterizations obtained through the eigenfunctions of L. In ",
                    "Citation Text": "D. Burago, S. Ivanov, Y. Kurylev, A graph discretization of the Laplace-Beltrami operator , J. Spectr.\nTheory 4, 675-714 (2014).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1301.2222",
                        "Citation Paper Title": "Title:A graph discretization of the Laplace-Beltrami operator",
                        "Citation Paper Abstract": "Abstract:We show that eigenvalues and eigenfunctions of the Laplace-Beltrami operator on a Riemannian manifold are approximated by eigenvalues and eigenvectors of a (suitably weighted) graph Laplace operator of a proximity graph on an epsilon-net.",
                        "Citation Paper Authors": "Authors:Dmitri Burago, Sergei Ivanov, Yaroslav Kurylev"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1809.02849v2": {
            "Paper Title": "Temporal sequences of brain activity at rest are constrained by white\n  matter structure and modulated by cognitive demands",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.01020v3": {
            "Paper Title": "Self-sustained activity of low firing rate in balanced networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.09874v2": {
            "Paper Title": "Learning Nonlinear Brain Dynamics: van der Pol Meets LSTM",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.02870v3": {
            "Paper Title": "The Role of Engagement, Honing, and Mindfulness in Creativity",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.06562v2": {
            "Paper Title": "A Robust Deep Learning Approach for Automatic Classification of Seizures\n  Against Non-seizures",
            "Sentences": [
                {
                    "Sentence ID": 4,
                    "Sentence": ", the\ntwo approaches extracted seizure features from the data on one\nchannel to conduct classi\ufb01cation. Each record in the Bonn EEG\ndata set is the data from only one channel.\nIn ",
                    "Citation Text": "P. Thodoroff, J. Pineau, A. Lim. Learning robust features using deep learn-\ning for automatic seizure detection. Proceedings of the 1st Machine Learn-\ning for Healthcare Conference; Los Angeles, CA, USA; 2016. Journal of\nMachine Learning Research, vol. 56, pp. 178-190, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1608.00220",
                        "Citation Paper Title": "Title:Learning Robust Features using Deep Learning for Automatic Seizure Detection",
                        "Citation Paper Abstract": "Abstract:We present and evaluate the capacity of a deep neural network to learn robust features from EEG to automatically detect seizures. This is a challenging problem because seizure manifestations on EEG are extremely variable both inter- and intra-patient. By simultaneously capturing spectral, temporal and spatial information our recurrent convolutional neural network learns a general spatially invariant representation of a seizure. The proposed approach exceeds significantly previous results obtained on cross-patient classifiers both in terms of sensitivity and false positive rate. Furthermore, our model proves to be robust to missing channel and variable electrode montage.",
                        "Citation Paper Authors": "Authors:Pierre Thodoroff, Joelle Pineau, Andrew Lim"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "presented seizure detection algorithm,\nwhich uses a nuclear norm regularization to convey spatial dis-\ntribution information of ictal patterns. The algorithm extracted\nfeatures from each channel, and then stacked them to analyze\nas one entity.\nTruong et al. ",
                    "Citation Text": "N. D. Truong, L. Kuhlmann, M. R. Bonyadi, J. Yang. Supervised\nlearning in automatic channel selection for epileptic seizure detec-\ntion. Expert Systems with Applications, vol. 86, pp. 199-207, 2017.\nhttps://doi.org/10.1016/j.eswa.2017.05.055.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1701.08968",
                        "Citation Paper Title": "Title:Supervised Learning in Automatic Channel Selection for Epileptic Seizure Detection",
                        "Citation Paper Abstract": "Abstract:Detecting seizure using brain neuroactivations recorded by intracranial electroencephalogram (iEEG) has been widely used for monitoring, diagnosing, and closed-loop therapy of epileptic patients, however, computational efficiency gains are needed if state-of-the-art methods are to be implemented in implanted devices. We present a novel method for automatic seizure detection based on iEEG data that outperforms current state-of-the-art seizure detection methods in terms of computational efficiency while maintaining the accuracy. The proposed algorithm incorporates an automatic channel selection (ACS) engine as a pre-processing stage to the seizure detection procedure. The ACS engine consists of supervised classifiers which aim to find iEEGchannelswhich contribute the most to a seizure. Seizure detection stage involves feature extraction and classification. Feature extraction is performed in both frequency and time domains where spectral power and correlation between channel pairs are calculated. Random Forest is used in classification of interictal, ictal and early ictal periods of iEEG signals. Seizure detection in this paper is retrospective and patient-specific. iEEG data is accessed via Kaggle, provided by International Epilepsy Electro-physiology Portal. The dataset includes a training set of 6.5 hours of interictal data and 41 minin ictal data and a test set of 9.14 hours. Compared to the state-of-the-art on the same dataset, we achieve 49.4% increase in computational efficiency and 400 mins better in average for detection delay. The proposed model is able to detect a seizure onset at 91.95% sensitivity and 94.05% specificity with a mean detection delay of 2.77 s. The area under the curve (AUC) is 96.44%, that is comparable to the current state-of-the-art with AUC of 96.29%.",
                        "Citation Paper Authors": "Authors:Nhan Truong, Levin Kuhlmann, Mohammad Reza Bonyadi, Jiawei Yang, Andrew Faulks, Omid Kavehei"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1804.10090v3": {
            "Paper Title": "Neuronal Synchronization Can Control the Energy Efficiency of\n  Inter-Spike Interval Coding",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.09603v3": {
            "Paper Title": "One step back, two steps forward: interference and learning in recurrent\n  neural networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.04975v2": {
            "Paper Title": "Multimodal Cross-registration and Quantification of Metric Distortions\n  in Whole Brain Histology of Marmoset using Diffeomorphic Mappings",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.05464v2": {
            "Paper Title": "Transfer Learning for Brain-Computer Interfaces: A Euclidean Space Data\n  Alignment Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.07965v2": {
            "Paper Title": "Deep learning with asymmetric connections and Hebbian updates",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.00105v2": {
            "Paper Title": "True Contextuality in a Psychophysical Experiment",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.00688v2": {
            "Paper Title": "Data-driven Perception of Neuron Point Process with Unknown Unknowns",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.10981v2": {
            "Paper Title": "Adaptive neural network classifier for decoding MEG signals",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.03142v2": {
            "Paper Title": "Fast and Efficient Information Transmission with Burst Spikes in Deep\n  Spiking Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.02304v3": {
            "Paper Title": "Hyperplane Neural Codes and the Polar Complex",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.02910v2": {
            "Paper Title": "Localization Algorithm with Circular Representation in 2D and its\n  Similarity to Mammalian Brains",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.11001v1": {
            "Paper Title": "Multivariate MR Biomarkers Better Predict Cognitive Dysfunction in Mouse\n  Models of Alzheimers Disease",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.07108v2": {
            "Paper Title": "Improving brain computer interface performance by data augmentation with\n  conditional Deep Convolutional Generative Adversarial Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.09574v4": {
            "Paper Title": "Long short-term memory and learning-to-learn in networks of spiking\n  neurons",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.01823v3": {
            "Paper Title": "Integrating Flexible Normalization into Mid-Level Representations of\n  Deep Convolutional Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.11851v3": {
            "Paper Title": "On the choice of metric in gradient-based theories of brain function",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.09123v1": {
            "Paper Title": "Statistical models of neural activity, criticality, and Zipf's law",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.07697v1": {
            "Paper Title": "Training on the test set? An analysis of Spampinato et al. [31]",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.07214v1": {
            "Paper Title": "Music and musical sonification for the rehabilitation of Parkinsonian\n  dysgraphia: Conceptual framework",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.06823v3": {
            "Paper Title": "Fast and Accurate Multiclass Inference for MI-BCIs Using Large\n  Multiscale Temporal and Spectral Features",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.04786v1": {
            "Paper Title": "Experimental Comparison of Hardware-Amenable Spike Detection Algorithms\n  for iBMIs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.12245v2": {
            "Paper Title": "Effect of time of day on reward circuitry: Further thoughts on methods,\n  prompted by Steel et al 2018",
            "Sentences": []
        },
        "http://arxiv.org/abs/1812.01342v1": {
            "Paper Title": "The Shift in brain-state induced by tDCS: an EEG study",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.05558v2": {
            "Paper Title": "Variational Bayesian Monte Carlo",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.11658v1": {
            "Paper Title": "Reduced structural connectivity between left auditory thalamus and the\n  motion-sensitive planum temporale in developmental dyslexia",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.10111v2": {
            "Paper Title": "Real-Time Sleep Staging using Deep Learning on a Smartphone for a\n  Wearable EEG",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.09739v1": {
            "Paper Title": "A probabilistic population code based on neural samples",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.00795v4": {
            "Paper Title": "Perceptual decision making: Biases in post-error reaction times\n  explained by attractor network dynamics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.04258v2": {
            "Paper Title": "Synchronization-Induced Spike Termination in Networks of Bistable\n  Neurons",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.11654v3": {
            "Paper Title": "3D MRI brain tumor segmentation using autoencoder regularization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.01552v3": {
            "Paper Title": "Normative atlases of neuroelectric brain activity and connectivity from\n  a large human cohort",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.07020v1": {
            "Paper Title": "Brain Connectivity Impairments and Categorization Disabilities in\n  Autism: A Theoretical Approach via Artificial Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.06866v1": {
            "Paper Title": "Optogenetic vision restoration with high resolution",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.07569v2": {
            "Paper Title": "Reliable counting of weakly labeled concepts by a single spiking neuron\n  model",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.01485v2": {
            "Paper Title": "PhoneMD: Learning to Diagnose Parkinson's Disease from Smartphone Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.05225v1": {
            "Paper Title": "Analysis of structure and dynamics in three-neuron motifs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.05058v1": {
            "Paper Title": "Electrophysiological indicators of gesture perception",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.02923v1": {
            "Paper Title": "Universal Spike Classifier",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.01701v1": {
            "Paper Title": "Role of Awareness and Universal Context in a Spiking Conscious Neural\n  Network (SCNN): A New Perspective and Future Directions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.01199v1": {
            "Paper Title": "Concerning the Neural Code",
            "Sentences": []
        },
        "http://arxiv.org/abs/1811.00941v1": {
            "Paper Title": "Replays of spatial memories suppress topological fluctuations in\n  cognitive map",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.01667v3": {
            "Paper Title": "Intracranial Error Detection via Deep Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.13302v1": {
            "Paper Title": "Evaluation of spike sorting algorithms: Simulations and application to\n  human Subthalamic Nucleus recordings",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.13034v1": {
            "Paper Title": "Theory for Inverse Stochastic Resonance in Nature",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.09042v2": {
            "Paper Title": "Generalisation of structural knowledge in the hippocampal-entorhinal\n  system",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.00053v2": {
            "Paper Title": "Task-Driven Convolutional Recurrent Models of the Visual System",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.06426v3": {
            "Paper Title": "Estimating scale-invariant future in continuous time",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.11393v1": {
            "Paper Title": "Dendritic cortical microcircuits approximate the backpropagation\n  algorithm",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.11248v1": {
            "Paper Title": "Sequence-dependent sensitivity explains the accuracy of decisions when\n  cues are separated with a gap",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.10941v1": {
            "Paper Title": "Alzheimer's Disease Diagnosis Based on Cognitive Methods in Virtual\n  Environments and Emotions Analysis",
            "Sentences": [
                {
                    "Sentence ID": 226,
                    "Sentence": "Vasileios Argyriou and Maria Petrou. Photometric stereo: An overview. Advances\nin Imaging and Electron Physics - ADV IMAG ELECTRON PHYS , 156:1{54, 12\n2009. ",
                    "Citation Text": "Aaron S Jackson, Adrian Bulat, Vasileios Argyriou, and Georgios Tzimiropoulos.\nLarge pose 3d face reconstruction from a single image via direct volumetric cnn\nregression. International Conference on Computer Vision , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.07834",
                        "Citation Paper Title": "Title:Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression",
                        "Citation Paper Abstract": "Abstract:3D face reconstruction is a fundamental Computer Vision problem of extraordinary difficulty. Current systems often assume the availability of multiple facial images (sometimes from the same subject) as input, and must address a number of methodological challenges such as establishing dense correspondences across large facial poses, expressions, and non-uniform illumination. In general these methods require complex and inefficient pipelines for model building and fitting. In this work, we propose to address many of these limitations by training a Convolutional Neural Network (CNN) on an appropriate dataset consisting of 2D images and 3D facial models or scans. Our CNN works with just a single 2D facial image, does not require accurate alignment nor establishes dense correspondence between images, works for arbitrary facial poses and expressions, and can be used to reconstruct the whole 3D facial geometry (including the non-visible parts of the face) bypassing the construction (during training) and fitting (during testing) of a 3D Morphable Model. We achieve this via a simple CNN architecture that performs direct regression of a volumetric representation of the 3D facial geometry from a single 2D image. We also demonstrate how the related task of facial landmark localization can be incorporated into the proposed framework and help improve reconstruction quality, especially for the cases of large poses and facial expressions. Testing code will be made available online, along with pre-trained models this http URL",
                        "Citation Paper Authors": "Authors:Aaron S. Jackson, Adrian Bulat, Vasileios Argyriou, Georgios Tzimiropoulos"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1811.02650v1": {
            "Paper Title": "Visual Attention is Beyond One Single Saliency Map",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.04122v2": {
            "Paper Title": "Evidence accumulation in a Laplace domain decision space",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.09391v1": {
            "Paper Title": "A neuro-inspired architecture for unsupervised continual learning based\n  on online clustering and hierarchical predictive coding",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.07429v1": {
            "Paper Title": "A Survey of Entorhinal Grid Cell Properties",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.00997v3": {
            "Paper Title": "A computational perspective of the role of Thalamus in cognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.10508v6": {
            "Paper Title": "Consciousness as a physical process caused by the organization of energy\n  in the brain",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.05077v1": {
            "Paper Title": "On the Brain Networks of Complex Problem Solving",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.02476v1": {
            "Paper Title": "The neural and cognitive architecture for learning from a small sample",
            "Sentences": []
        },
        "http://arxiv.org/abs/1810.00701v1": {
            "Paper Title": "Effect Of Site Selective Ion Channel Blocking on Action Potential",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.02103v2": {
            "Paper Title": "Making Sense of Consciousness as Integrated Information: Evolution and\n  Issues of IIT",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.11516v2": {
            "Paper Title": "Neural codes, decidability, and a new local obstruction to convexity",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.06899v1": {
            "Paper Title": "Testing Selective Influence Directly Using Trackball Movement Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.06303v1": {
            "Paper Title": "Psychiatric Illnesses as Disorders of Network Dynamics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.05196v1": {
            "Paper Title": "Statistical Data Assimilation: Formulation and Examples from\n  Neurobiology",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.03934v1": {
            "Paper Title": "Genetic and Neuroanatomical Support for Functional Brain Network\n  Dynamics in Epilepsy",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.03930v1": {
            "Paper Title": "Localization of Brain Activity from EEG/MEG Using MV-PURE Framework",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.02572v1": {
            "Paper Title": "The largest cognitive systems will be optoelectronic",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.01051v1": {
            "Paper Title": "Scaling Spike Detection and Sorting for Next Generation\n  Electrophysiology",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.05882v4": {
            "Paper Title": "A simple blind-denoising filter inspired by electrically coupled\n  photoreceptors in the retina",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.07692v1": {
            "Paper Title": "A Directionally Selective Neural Network with Separated ON and OFF\n  Pathways for Translational Motion Perception in a Visually Cluttered\n  Environment",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": ", which directly report pixel-wise brightness changes instead of traditional in-\ntensity images, have been used for motion detection and tracking with clustering\nand learning algorithms in robotics ",
                    "Citation Text": "V. Vasco, A. Glover, E. Mueggler, D. Scaramuzza, L. Natale, C. Bartolozzi,\nIndependent motion detection with event-driven cameras, in: International\nConference on Advanced Robotics (ICAR), 2017, pp. 530{536.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.08713",
                        "Citation Paper Title": "Title:Independent Motion Detection with Event-driven Cameras",
                        "Citation Paper Abstract": "Abstract:Unlike standard cameras that send intensity images at a constant frame rate, event-driven cameras asynchronously report pixel-level brightness changes, offering low latency and high temporal resolution (both in the order of micro-seconds). As such, they have great potential for fast and low power vision algorithms for robots. Visual tracking, for example, is easily achieved even for very fast stimuli, as only moving objects cause brightness changes. However, cameras mounted on a moving robot are typically non-stationary and the same tracking problem becomes confounded by background clutter events due to the robot ego-motion. In this paper, we propose a method for segmenting the motion of an independently moving object for event-driven cameras. Our method detects and tracks corners in the event stream and learns the statistics of their motion as a function of the robot's joint velocities when no independently moving objects are present. During robot operation, independently moving objects are identified by discrepancies between the predicted corner velocities from ego-motion and the measured corner velocities. We validate the algorithm on data collected from the neuromorphic iCub robot. We achieve a precision of ~ 90 % and show that the method is robust to changes in speed of both the head and the target.",
                        "Citation Paper Authors": "Authors:Valentina Vasco, Arren Glover, Elias Mueggler, Davide Scaramuzza, Lorenzo Natale, Chiara Bartolozzi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1808.08296v1": {
            "Paper Title": "Brain Biomarker Interpretation in ASD Using Deep Learning and fMRI",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.05002v2": {
            "Paper Title": "Complexity of human response delay in intermittent control: The case of\n  virtual stick balancing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1809.04414v1": {
            "Paper Title": "The Moon Illusion explained by the Projective Consciousness Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.09505v2": {
            "Paper Title": "Neural measures of the causal role of observers' facial mimicry on\n  visual working memory for facial expressions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.03630v1": {
            "Paper Title": "A Novel Method for Epileptic Seizure Detection Using Coupled Hidden\n  Markov Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.01642v1": {
            "Paper Title": "Multi-Objective Cognitive Model: a supervised approach for multi-subject\n  fMRI analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.04710v2": {
            "Paper Title": "A quantized physical framework for understanding the working mechanism\n  of ion channels",
            "Sentences": [
                {
                    "Sentence ID": 120,
                    "Sentence": "Teka, W., Marinov, T.M. and Santamaria, F., 2014. Neuronal spike timing adaptation described with \na fractional leaky integrate-and -fire model. PLoS computational biology, 10(3), p.e1003526. ",
                    "Citation Text": "Galves, A. and L\u00f6cherbach, E., 2013. Infinite systems of interacting chains with mem ory of variable \nlength \u2014a stochastic model for biological neural nets. Journal of Statistical Physics, 151(5), pp.896-921.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1212.5505",
                        "Citation Paper Title": "Title:Infinite systems of interacting chains with memory of variable length - a stochastic model for biological neural nets",
                        "Citation Paper Abstract": "Abstract:We consider a new class of non Markovian processes with a countable number of interacting components. At each time unit, each component can take two values, indicating if it has a spike or not at this precise moment. The system evolves as follows. For each component, the probability of having a spike at the next time unit depends on the entire time evolution of the system after the last spike time of the component. This class of systems extends in a non trivial way both the interacting particle systems, which are Markovian, and the stochastic chains with memory of variable length which have finite state space. These features make it suitable to describe the time evolution of biological neural systems. We construct a stationary version of the process by using a probabilistic tool which is a Kalikow-type decomposition either in random environment or in space-time. This construction implies uniqueness of the stationary process. Finally we consider the case where the interactions between components are given by a critical directed Erd\u00f6s-R\u00e9nyi-type random graph with a large but finite number of components. In this framework we obtain an explicit upper-bound for the correlation between successive inter-spike intervals which is compatible with previous empirical findings.",
                        "Citation Paper Authors": "Authors:Antonio Galves, Eva L\u00f6cherbach"
                    }
                },
                {
                    "Sentence ID": 80,
                    "Sentence": "Catterall, W.A., 2011. Voltage -gated calcium channels. Cold Spring Harbor perspectives in biology, \np.a003947. ",
                    "Citation Text": "Wang, H., Wang, J., Thow, X.Y., Lee, S., Peh, W.Y.X., Ng, K.A., He, T., Thakor, N.V. and Lee, C., \n2018. Unveiling Stimulation Secrets of Electrical Excitation of Neural Tissue Using a Circuit Probability \nTheory. arXiv preprint arXiv:1804.11310.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.11310",
                        "Citation Paper Title": "Title:Unveiling Stimulation Secrets of Electrical Excitation of Neural Tissue Using a Circuit Probability Theory",
                        "Citation Paper Abstract": "Abstract:A new theory, named the Circuit-Probability theory, is proposed to unveil the secret of electrical nerve stimulation, essentially explain the nonlinear and resonant phenomena observed when neural and non-neural tissues are electrically stimulated. For the explanation of frequency dependent response, an inductor is involved in the neural circuit model. Furthermore, predicted response to varied stimulation strength is calculated stochastically. Based on this theory, many empirical models, such as strength-duration relationship and LNP model, can be theoretically explained, derived, and amended. This theory can explain the complex nonlinear interactions in electrical stimulation and fit in vivo experiment data on stimulation-responses of many experiments. As such, the C-P theory should be able to guide novel experiments and more importantly, offer an in-depth physical understanding of the neural tissue. As a promising neural model, we can even further explore the more accurate circuit configuration and probability equation to better describe the electrical stimulation of neural tissues in the future.",
                        "Citation Paper Authors": "Authors:Hao Wang, Jiahui Wang, Xin Yuan Thow, Sanghoon Lee, Wendy Yen Xian Peh, Kian Ann Ng, Tianyiyi He, Nitish V. Thakor, Chengkuo Lee"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1804.00794v2": {
            "Paper Title": "Fixed points of competitive threshold-linear networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.01058v1": {
            "Paper Title": "Cortical Microcircuits from a Generative Vision Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/1808.01893v1": {
            "Paper Title": "Spiking Neural Networks modelled as Timed Automata with parameter\n  learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.11935v1": {
            "Paper Title": "Network models in neuroscience",
            "Sentences": [
                {
                    "Sentence ID": 150,
                    "Sentence": "Reimann, M. W. et al. Cliques of neurons bound into\ncavities provide a missing link between structure and\nfunction. Front Comput Neurosci 11, 48 (2017). ",
                    "Citation Text": "Curto, C., Itskov, V., Veliz-Cuba, A. & Youngs, N. The\nneural ring: an algebraic tool for analyzing the intrinsic\nstructure of neural codes. Bull Math Biol 75, 1571{1611\n(2013).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1212.4201",
                        "Citation Paper Title": "Title:The neural ring: an algebraic tool for analyzing the intrinsic structure of neural codes",
                        "Citation Paper Abstract": "Abstract:Neurons in the brain represent external stimuli via neural codes. These codes often arise from stereotyped stimulus-response maps, associating to each neuron a convex receptive field. An important problem confronted by the brain is to infer properties of a represented stimulus space without knowledge of the receptive fields, using only the intrinsic structure of the neural code. How does the brain do this? To address this question, it is important to determine what stimulus space features can - in principle - be extracted from neural codes. This motivates us to define the neural ring and a related neural ideal, algebraic objects that encode the full combinatorial data of a neural code. Our main finding is that these objects can be expressed in a \"canonical form\" that directly translates to a minimal description of the receptive field structure intrinsic to the code. We also find connections to Stanley-Reisner rings, and use ideas similar to those in the theory of monomial ideals to obtain an algorithm for computing the primary decomposition of pseudo-monomial ideals. This allows us to algorithmically extract the canonical form associated to any neural code, providing the groundwork for inferring stimulus space features from neural activity alone.",
                        "Citation Paper Authors": "Authors:Carina Curto, Vladimir Itskov, Alan Veliz-Cuba, Nora Youngs"
                    }
                },
                {
                    "Sentence ID": 147,
                    "Sentence": "Bassett, D. S., Wymbs, N. F., Porter, M. A., Mucha,\nP. J. & Grafton, S. T. Cross-linked structure of network\nevolution. Chaos 24, 013112 (2014). ",
                    "Citation Text": "Giusti, C., Ghrist, R. & Bassett, D. S. Two's company,\nthree (or more) is a simplex: Algebraic-topological tools\nfor understanding higher-order structure in neural data.\nJ Comput Neurosci 41, 1{14 (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1601.01704",
                        "Citation Paper Title": "Title:Two's company, three (or more) is a simplex: Algebraic-topological tools for understanding higher-order structure in neural data",
                        "Citation Paper Abstract": "Abstract:The language of graph theory, or network science, has proven to be an exceptional tool for addressing myriad problems in neuroscience. Yet, the use of networks is predicated on a critical simplifying assumption: that the quintessential unit of interest in a brain is a dyad -- two nodes (neurons or brain regions) connected by an edge. While rarely mentioned, this fundamental assumption inherently limits the types of neural structure and function that graphs can be used to model. Here, we describe a generalization of graphs that overcomes these limitations, thereby offering a broad range of new possibilities in terms of modeling and measuring neural phenomena. Specifically, we explore the use of \\emph{simplicial complexes}, a theoretical notion developed in the field of mathematics known as algebraic topology, which is now becoming applicable to real data due to a rapidly growing computational toolset. We review the underlying mathematical formalism as well as the budding literature applying simplicial complexes to neural data, from electrophysiological recordings in animal models to hemodynamic fluctuations in humans. Based on the exceptional flexibility of the tools and recent ground-breaking insights into neural function, we posit that this framework has the potential to eclipse graph theory in unraveling the fundamental mysteries of cognition.",
                        "Citation Paper Authors": "Authors:Chad Giusti, Robert Ghrist, Danielle S. Bassett"
                    }
                },
                {
                    "Sentence ID": 146,
                    "Sentence": "Petri, G. et al. Homological sca\u000bolds of brain functional\nnetworks. J R Soc Interface 11, 20140873 (2014). ",
                    "Citation Text": "Bassett, D. S., Wymbs, N. F., Porter, M. A., Mucha,\nP. J. & Grafton, S. T. Cross-linked structure of network\nevolution. Chaos 24, 013112 (2014).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1306.5479",
                        "Citation Paper Title": "Title:Cross-Linked Structure of Network Evolution",
                        "Citation Paper Abstract": "Abstract:We study the temporal co-variation of network co-evolution via the cross-link structure of networks, for which we take advantage of the formalism of hypergraphs to map cross-link structures back to network nodes. We investigate two sets of temporal network data in detail. In a network of coupled nonlinear oscillators, hyperedges that consist of network edges with temporally co-varying weights uncover the driving co-evolution patterns of edge weight dynamics both within and between oscillator communities. In the human brain, networks that represent temporal changes in brain activity during learning exhibit early co-evolution that then settles down with practice, and subsequent decreases in hyperedge size are consistent with emergence of an autonomous subgraph whose dynamics no longer depends on other parts of the network. Our results on real and synthetic networks give a poignant demonstration of the ability of cross-link structure to uncover unexpected co-evolution attributes in both real and synthetic dynamical systems. This, in turn, illustrates the utility of analyzing cross-links for investigating the structure of temporal networks.",
                        "Citation Paper Authors": "Authors:Danielle S. Bassett, Nicholas F. Wymbs, Mason A. Porter, Peter J. Mucha, Scott T. Grafton"
                    }
                },
                {
                    "Sentence ID": 144,
                    "Sentence": "Lord, L. D. et al. Insights into brain architectures from\nthe homological sca\u000bolds of functional connectivity net-\nworks. Front Syst Neurosci 10, 85 (2016). ",
                    "Citation Text": "Sizemore, A. E., Phillips-Cremins, J., Ghrist, R. & Bas-\nsett, D. S. The importance of the whole: topologi-\ncal data analysis for the network neuroscientist. arXiv\n1806 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.05167",
                        "Citation Paper Title": "Title:The importance of the whole: topological data analysis for the network neuroscientist",
                        "Citation Paper Abstract": "Abstract:The application of network techniques to the analysis of neural data has greatly improved our ability to quantify and describe these rich interacting systems. Among many important contributions, networks have proven useful in identifying sets of node pairs that are densely connected and that collectively support brain function. Yet the restriction to pairwise interactions prevents us from realizing intrinsic topological features such as cavities within the interconnection structure that may be just as crucial for proper function. To detect and quantify these topological features we must turn to methods from algebraic topology that encode data as a simplicial complex built of sets of interacting nodes called simplices. On this substrate, we can then use the relations between simplices and higher-order connectivity to expose cavities within the complex, thereby summarizing its topological nature. Here we provide an introduction to persistent homology, a fundamental method from applied topology that builds a global descriptor of system structure by chronicling the evolution of cavities as we move through a combinatorial object such as a weighted network. We detail the underlying mathematics and perform demonstrative calculations on the mouse structural connectome, electrical and chemical synapses in \\textit{C. elegans}, and genomic interaction data. Finally we suggest avenues for future work and highlight new advances in mathematics that appear ready for use in revealing the architecture and function of neural systems.",
                        "Citation Paper Authors": "Authors:Ann E. Sizemore, Jennifer Phillips-Cremins, Robert Ghrist, Danielle S. Bassett"
                    }
                },
                {
                    "Sentence ID": 138,
                    "Sentence": "Yu, Q. et al. Building an EEG-fMRI multi-modal brain\ngraph: A concurrent EEG-fMRI study. Front Hum Neu-\nrosci 10, 476 (2016). ",
                    "Citation Text": "Holme, P. & Saramaki, J. Temporal networks. Phys.\nRep.519, 97{125 (2012).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1108.1780",
                        "Citation Paper Title": "Title:Temporal Networks",
                        "Citation Paper Abstract": "Abstract:A great variety of systems in nature, society and technology -- from the web of sexual contacts to the Internet, from the nervous system to power grids -- can be modeled as graphs of vertices coupled by edges. The network structure, describing how the graph is wired, helps us understand, predict and optimize the behavior of dynamical systems. In many cases, however, the edges are not continuously active. As an example, in networks of communication via email, text messages, or phone calls, edges represent sequences of instantaneous or practically instantaneous contacts. In some cases, edges are active for non-negligible periods of time: e.g., the proximity patterns of inpatients at hospitals can be represented by a graph where an edge between two individuals is on throughout the time they are at the same ward. Like network topology, the temporal structure of edge activations can affect dynamics of systems interacting through the network, from disease contagion on the network of patients to information diffusion over an e-mail network. In this review, we present the emergent field of temporal networks, and discuss methods for analyzing topological and temporal structure and models for elucidating their relation to the behavior of dynamical systems. In the light of traditional network theory, one can see this framework as moving the information of when things happen from the dynamical system on the network, to the network itself. Since fundamental properties, such as the transitivity of edges, do not necessarily hold in temporal networks, many of these methods need to be quite different from those for static networks.",
                        "Citation Paper Authors": "Authors:Petter Holme, Jari Saram\u00e4ki"
                    }
                },
                {
                    "Sentence ID": 134,
                    "Sentence": "Newman, M. E. J. & Clauset, A. Structure and infer-\nence in annotated networks. Nature Communications 7,\n11863 (2016). ",
                    "Citation Text": "Murphy, A. C. et al. Explicitly linking regional activa-\ntion and function connectivity: Community structure of\nweighted networks with continuous annotation. arXiv\n1611 , 07962 (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.07962",
                        "Citation Paper Title": "Title:Explicitly Linking Regional Activation and Function Connectivity: Community Structure of Weighted Networks with Continuous Annotation",
                        "Citation Paper Abstract": "Abstract:A major challenge in neuroimaging is understanding the mapping of neurophysiological dynamics onto cognitive functions. Traditionally, these maps have been constructed by examining changes in the activity magnitude of regions related to task performance. Recently, network neuroscience has produced methods to map connectivity patterns among many regions to certain cognitive functions by drawing on tools from network science and graph theory. However, these two different views are rarely addressed simultaneously, largely because few tools exist that account for patterns between nodes while simultaneously considering activation of nodes. We address this gap by solving the problem of community detection on weighted networks with continuous (non-integer) annotations by deriving a generative probabilistic model. This model generates communities whose members connect densely to nodes within their own community, and whose members share similar annotation values. We demonstrate the utility of the model in the context of neuroimaging data gathered during a motor learning paradigm, where edges are task-based functional connectivity and annotations to each node are beta weights from a general linear model that encoded a linear decrease in blood-oxygen-level-dependent signal with practice. Interestingly, we observe that individuals who learn at a faster rate exhibit the greatest dissimilarity between functional connectivity and activation magnitudes, suggesting that activation and functional connectivity are distinct dimensions of neurophysiology that track behavioral change. More generally, the tool that we develop offers an explicit, mathematically principled link between functional activation and functional connectivity, and can readily be applied to a other similar problems in which one set of imaging data offers network data, and a second offers a regional attribute.",
                        "Citation Paper Authors": "Authors:Andrew C. Murphy, Shi Gu, Ankit N. Khambhati, Nicholas F. Wymbs, Scott T. Grafton, Theodore D. Satterthwaite, Danielle S. Bassett"
                    }
                },
                {
                    "Sentence ID": 133,
                    "Sentence": "Kaiser, M., Hilgetag, C. C. & van Ooyen, A. A simple\nrule for axon outgrowth and synaptic competition gen-erates realistic connection lengths and \flling fractions.\nCereb Cortex 19, 3001{3010 (2009). ",
                    "Citation Text": "Newman, M. E. J. & Clauset, A. Structure and infer-\nence in annotated networks. Nature Communications 7,\n11863 (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1507.04001",
                        "Citation Paper Title": "Title:Structure and inference in annotated networks",
                        "Citation Paper Abstract": "Abstract:For many networks of scientific interest we know both the connections of the network and information about the network nodes, such as the age or gender of individuals in a social network, geographic location of nodes in the Internet, or cellular function of nodes in a gene regulatory network. Here we demonstrate how this \"metadata\" can be used to improve our analysis and understanding of network structure. We focus in particular on the problem of community detection in networks and develop a mathematically principled approach that combines a network and its metadata to detect communities more accurately than can be done with either alone. Crucially, the method does not assume that the metadata are correlated with the communities we are trying to find. Instead the method learns whether a correlation exists and correctly uses or ignores the metadata depending on whether they contain useful information. The learned correlations are also of interest in their own right, allowing us to make predictions about the community membership of nodes whose network connections are unknown. We demonstrate our method on synthetic networks with known structure and on real-world networks, large and small, drawn from social, biological, and technological domains.",
                        "Citation Paper Authors": "Authors:M. E. J. Newman, Aaron Clauset"
                    }
                },
                {
                    "Sentence ID": 116,
                    "Sentence": "Muldoon, S. F. et al. Stimulation-based control of dy-\nnamic brain networks. PLoS Comput Biol 12, e1005076\n(2016). ",
                    "Citation Text": "Stiso, J. et al. White matter network architecture guides\ndirect electrical stimulation through optimal state tran-\nsitions. arXiv 1805 , 01260 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.01260",
                        "Citation Paper Title": "Title:White Matter Network Architecture Guides Direct Electrical Stimulation Through Optimal State Transitions",
                        "Citation Paper Abstract": "Abstract:Electrical brain stimulation is currently being investigated as a therapy for neurological disease. However, opportunities to optimize such therapies are challenged by the fact that the beneficial impact of focal stimulation on both neighboring and distant regions is not well understood. Here, we use network control theory to build a model of brain network function that makes predictions about how stimulation spreads through the brain's white matter network and influences large-scale dynamics. We test these predictions using combined electrocorticography (ECoG) and diffusion weighted imaging (DWI) data who volunteered to participate in an extensive stimulation regimen. We posit a specific model-based manner in which white matter tracts constrain stimulation, defining its capacity to drive the brain to new states, including states associated with successful memory encoding. In a first validation of our model, we find that the true pattern of white matter tracts can be used to more accurately predict the state transitions induced by direct electrical stimulation than the artificial patterns of null models. We then use a targeted optimal control framework to solve for the optimal energy required to drive the brain to a given state. We show that, intuitively, our model predicts larger energy requirements when starting from states that are farther away from a target memory state. We then suggest testable hypotheses about which structural properties will lead to efficient stimulation for improving memory based on energy requirements. Our work demonstrates that individual white matter architecture plays a vital role in guiding the dynamics of direct electrical stimulation, more generally offering empirical support for the utility of network control theoretic models of brain response to stimulation.",
                        "Citation Paper Authors": "Authors:Jennifer Stiso, Ankit N. Khambhati, Tommaso Menara, Ari E. Kahn, Joel M. Stein, Sandihitsu R. Das, Richard Gorniak, Joseph Tracy, Brian Litt, Kathryn A. Davis, Fabio Pasqualetti, Timothy Lucas, Danielle S. Bassett"
                    }
                },
                {
                    "Sentence ID": 89,
                    "Sentence": "Sautois, B., So\u000be, S. R., Li, W. C. & Roberts, A. Role\nof type-speci\fc neuron properties in a spinal cord motor\nnetwork. J Comput Neurosci 23, 59{77 (2007). ",
                    "Citation Text": "Teller, S. et al. Emergence of assortative mixing between\nclusters of cultured neurons. PLoS Comput Biol 10,\ne1003796 (2014).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1402.4824",
                        "Citation Paper Title": "Title:Emergence of assortative mixing between clusters of cultured neurons",
                        "Citation Paper Abstract": "Abstract:The analysis of the activity of neuronal cultures is considered to be a good proxy of the functional connectivity of in vivo neuronal tissues. Thus, the functional complex network inferred from activity patterns is a promising way to unravel the interplay between structure and functionality of neuronal systems. Here, we monitor the spontaneous self-sustained dynamics in neuronal cultures formed by interconnected aggregates of neurons (clusters). Dynamics is characterized by the fast activation of groups of clusters in sequences termed bursts. The analysis of the time delays between clusters' activations within the bursts allows the reconstruction of the directed functional connectivity of the network. We propose a method to statistically infer this connectivity and analyze the resulting properties of the associated complex networks. Surprisingly enough, in contrast to what has been reported for many biological networks, the clustered neuronal cultures present assortative mixing connectivity values, as well as a rich--club core, meaning that there is a preference for clusters to link to other clusters that share similar functional connectivity, which shapes a `connectivity backbone' in the network. These results point out that the grouping of neurons and the assortative connectivity between clusters are intrinsic survival mechanisms of the culture.",
                        "Citation Paper Authors": "Authors:Sara Teller, Clara Granell, Manlio De Domenico, Jordi Soriano, Sergio Gomez, Alex Arenas"
                    }
                },
                {
                    "Sentence ID": 87,
                    "Sentence": "Feldt, S., Bonifazi, P. & Cossart, R. Dissecting func-\ntional connectivity of neuronal microcircuits: experi-\nmental and theoretical insights. Trends Neurosci 34,\n225{236 (2011). ",
                    "Citation Text": "Kim, S. Y. & Lim, W. Fast sparsely synchronized brain\nrhythms in a scale-free neural network. Phys Rev E 92,\n022717 (2015).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1504.03063",
                        "Citation Paper Title": "Title:Fast Sparsely Synchronized Brain Rhythms in A Scale-Free Neural Network",
                        "Citation Paper Abstract": "Abstract:We consider a directed Barab\u00e1si-Albert scale-free network model with symmetric preferential attachment with the same in- and out-degrees, and study emergence of sparsely synchronized rhythms for a fixed attachment degree in an inhibitory population of fast spiking Izhikevich interneurons. For a study on the fast sparsely synchronized rhythms, we fix $J$ (synaptic inhibition strength) at a sufficiently large value, and investigate the population states by increasing $D$ (noise intensity). For small $D$, full synchronization with the same population-rhythm frequency $f_p$ and mean firing rate (MFR) $f_i$ of individual neurons occurs, while for sufficiently large $D$ partial synchronization with $f_p > {\\langle f_i \\rangle}$ ($\\langle f_i \\rangle$: ensemble-averaged MFR) appears due to intermittent discharge of individual neurons; particularly, the case of $f_p > 4 {\\langle f_i \\rangle}$ is referred to as sparse synchronization. Only for the partial and sparse synchronization, MFRs and contributions of individual neuronal dynamics to population synchronization change depending on their degrees, unlike the case of full synchronization. Consequently, dynamics of individual neurons reveal the inhomogeneous network structure for the case of partial and sparse synchronization, which is in contrast to the case of statistically homogeneous random graphs and small-world networks. Finally, we investigate the effect of network architecture on sparse synchronization in the following three cases: (1) variation in the degree of symmetric attachment (2) asymmetric preferential attachment of new nodes with different in- and out-degrees (3) preferential attachment between pre-existing nodes (without addition of new nodes). In these three cases, both relation between network topology and sparse synchronization and contributions of individual dynamics to the sparse synchronization are discussed.",
                        "Citation Paper Authors": "Authors:Sang-Yoon Kim, Woochang Lim"
                    }
                },
                {
                    "Sentence ID": 65,
                    "Sentence": "Micheloyannis, S. et al. Small-world networks and\ndisturbed functional connectivity in schizophrenia.\nSchizophr Res 87, 60{66 (2006). ",
                    "Citation Text": "Bettencourt, L. M., Stephens, G. J., Ham, M. I. &\nGross, G. W. Functional structure of cortical neuronal\nnetworks grown in vitro. Phys Rev E 75, 021915 (2007).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:q-bio/0703018",
                        "Citation Paper Title": "Title:The functional structure of cortical neuronal networks grown in vitro",
                        "Citation Paper Abstract": "Abstract:  We apply an information theoretic treatment of action potential time series measured with microelectrode arrays to estimate the connectivity of mammalian neuronal cell assemblies grown {\\it in vitro}. We infer connectivity between two neurons via the measurement of the mutual information between their spike trains. In addition we measure higher point multi-informations between any two spike trains conditional on the activity of a third cell, as a means to identify and distinguish classes of functional connectivity among three neurons. The use of a conditional three-cell measure removes some interpretational shortcomings of the pairwise mutual information and sheds light into the functional connectivity arrangements of any three cells. We analyze the resultant connectivity graphs in light of other complex networks and demonstrate that, despite their {\\it ex vivo} development, the connectivity maps derived from cultured neural assemblies are similar to other biological networks and display nontrivial structure in clustering coefficient, network diameter and assortative mixing. Specifically we show that these networks are weakly disassortative small world graphs, which differ significantly in their structure from randomized graphs with the same degree. We expect our analysis to be useful in identifying the computational motifs of a wide variety of complex networks, derived from time series data.",
                        "Citation Paper Authors": "Authors:Luis M. A. Bettencourt, Greg J. Stephens, Michael I. Ham, Guenter W. Gross"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "Simonsen, I., Buzna, L., Peters, K., Bornholdt, S. &\nHelbing, D. Transient dynamics increasing network vul-\nnerability to cascading failures. Phys Rev Lett 100,\n218701 (2008). ",
                    "Citation Text": "Albert, R., Jeong, H. & Barabasi, A. L. Error and attack\ntolerance of complex networks. Nature 406, 378{382\n(2000).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:cond-mat/0008064",
                        "Citation Paper Title": "Title:Error and attack tolerance of complex networks",
                        "Citation Paper Abstract": "Abstract:  Many complex systems, such as communication networks, display a surprising degree of robustness: while key components regularly malfunction, local failures rarely lead to the loss of the global information-carrying ability of the network. The stability of these complex systems is often attributed to the redundant wiring of the functional web defined by the systems' components. In this paper we demonstrate that error tolerance is not shared by all redundant systems, but it is displayed only by a class of inhomogeneously wired networks, called scale-free networks. We find that scale-free networks, describing a number of systems, such as the World Wide Web, Internet, social networks or a cell, display an unexpected degree of robustness, the ability of their nodes to communicate being unaffected by even unrealistically high failure rates. However, error tolerance comes at a high price: these networks are extremely vulnerable to attacks, i.e. to the selection and removal of a few nodes that play the most important role in assuring the network's connectivity.",
                        "Citation Paper Authors": "Authors:Reka Albert, Hawoong Jeong, Albert-Laszlo Barabasi (University of Notre Dame)"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "Cisneros, L., Jimenez, J., Cosenza, M. G. & Parravano,\nA. Information transfer and nontrivial collective behav-\nior in chaotic coupled map networks. Phys Rev E Stat\nNonlin Soft Matter Phys 65, 045204 (2002). ",
                    "Citation Text": "Simonsen, I., Buzna, L., Peters, K., Bornholdt, S. &\nHelbing, D. Transient dynamics increasing network vul-\nnerability to cascading failures. Phys Rev Lett 100,\n218701 (2008).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:0704.1952",
                        "Citation Paper Title": "Title:Dynamic Effects Increasing Network Vulnerability to Cascading Failures",
                        "Citation Paper Abstract": "Abstract:  We study cascading failures in networks using a dynamical flow model based on simple conservation and distribution laws to investigate the impact of transient dynamics caused by the rebalancing of loads after an initial network failure (triggering event). It is found that considering the flow dynamics may imply reduced network robustness compared to previous static overload failure models. This is due to the transient oscillations or overshooting in the loads, when the flow dynamics adjusts to the new (remaining) network structure. We obtain {\\em upper} and {\\em lower} limits to network robustness, and it is shown that {\\it two} time scales $\\tau$ and $\\tau_0$, defined by the network dynamics, are important to consider prior to accurately addressing network robustness or vulnerability. The robustness of networks showing cascading failures is generally determined by a complex interplay between the network topology and flow dynamics, where the ratio $\\chi=\\tau/\\tau_0$ determines the relative role of the two of them.",
                        "Citation Paper Authors": "Authors:Ingve Simonsen, Lubos Buzna, Karsten Peters, Stefan Bornholdt, Dirk Helbing"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": "Cohen, R. & Havlin, S. Complex Networks: Structure,\nRobustness and Function (Cambridge University Press,\n2010). ",
                    "Citation Text": "Gomez-Gardenes, J., Moreno, Y. & Arenas, A. Paths\nto synchronization on complex networks. Phys Rev Lett\n98, 034101 (2007).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:cond-mat/0608314",
                        "Citation Paper Title": "Title:Paths to Synchronization on Complex Networks",
                        "Citation Paper Abstract": "Abstract:  The understanding of emergent collective phenomena in natural and social systems has driven the interest of scientists from different disciplines during decades. Among these phenomena, the synchronization of a set of interacting individuals or units has been intensively studied because of its ubiquity in the natural world. In this paper, we show how for fixed coupling strengths local patterns of synchronization emerge differently in homogeneous and heterogeneous complex networks, driving the process towards a certain global synchronization degree following different paths. The dependence of the dynamics on the coupling strength and on the topology is unveiled. This study provides a new perspective and tools to understand this emerging phenomena.",
                        "Citation Paper Authors": "Authors:Jesus Gomez-Gardenes, Yamir Moreno, Alex Arenas"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "Bassett, D. S. & Sporns, O. Network neuroscience. Nat\nNeurosci 20, 353{364 (2017). ",
                    "Citation Text": "Bassett, D. S., Zurn, P. & Gold, J. I. On the nature and\nuse of models in network neuroscience. Nature Reviews\nNeuroscience In Press (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.11935",
                        "Citation Paper Title": "Title:Network models in neuroscience",
                        "Citation Paper Abstract": "Abstract:From interacting cellular components to networks of neurons and neural systems, interconnected units comprise a fundamental organizing principle of the nervous system. Understanding how their patterns of connections and interactions give rise to the many functions of the nervous system is a primary goal of neuroscience. Recently, this pursuit has begun to benefit from the development of new mathematical tools that can relate a system's architecture to its dynamics and function. These tools, which are known collectively as network science, have been used with increasing success to build models of neural systems across spatial scales and species. Here we discuss the nature of network models in neuroscience. We begin with a review of model theory from a philosophical perspective to inform our view of networks as models of complex systems in general, and of the brain in particular. We then summarize the types of models that are frequently studied in network neuroscience along three primary dimensions: from data representations to first-principles theory, from biophysical realism to functional phenomenology, and from elementary descriptions to coarse-grained approximations. We then consider ways to validate these models, focusing on approaches that perturb a system to probe its function. We close with a description of important frontiers in the construction of network models and their relevance for understanding increasingly complex functions of neural systems.",
                        "Citation Paper Authors": "Authors:Danielle S. Bassett, Perry Zurn, Joshua I. Gold"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "Betzel, R. F., Medaglia, J. D. & Bassett, D. S. Di-\nversity of meso-scale architecture in human and non-\nhuman connectomes. Nature Communications In Press\n(2018). ",
                    "Citation Text": "Albert, E. & Barabasi, A.-L. Statistical mechanics\nof complex networks. Reviews of Modern Physics 74\n(2002).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:cond-mat/0106096",
                        "Citation Paper Title": "Title:Statistical mechanics of complex networks",
                        "Citation Paper Abstract": "Abstract:  Complex networks describe a wide range of systems in nature and society, much quoted examples including the cell, a network of chemicals linked by chemical reactions, or the Internet, a network of routers and computers connected by physical links. While traditionally these systems were modeled as random graphs, it is increasingly recognized that the topology and evolution of real networks is governed by robust organizing principles. Here we review the recent advances in the field of complex networks, focusing on the statistical mechanics of network topology and dynamics. After reviewing the empirical data that motivated the recent interest in networks, we discuss the main models and analytical tools, covering random graphs, small-world and scale-free networks, as well as the interplay between topology and the network's robustness against failures and attacks.",
                        "Citation Paper Authors": "Authors:Reka Albert, Albert-Laszlo Barabasi"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": "Betzel, R. F. & Bassett, D. S. The speci\fcity and ro-\nbustness of long-distance connections in weighted, in-\nterareal connectomes. Proc Natl Acad Sci U S A Epub\nAhead of print (2018). ",
                    "Citation Text": "Betzel, R. F., Medaglia, J. D. & Bassett, D. S. Di-\nversity of meso-scale architecture in human and non-\nhuman connectomes. Nature Communications In Press\n(2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1702.02807",
                        "Citation Paper Title": "Title:Diversity of meso-scale architecture in human and non-human connectomes",
                        "Citation Paper Abstract": "Abstract:The brain's functional diversity is reflected in the meso-scale architecture of its connectome, i.e. its division into clusters and communities of topologically-related brain regions. The dominant view, and one that is reinforced by current analysis techniques, is that communities are strictly assortative and segregated from one another, purportedly for the purpose of carrying out specialized information processing. Such a view, however, precludes the possibility of non-assortative communities that could engender a richer functional repertoire by allowing for a more complex set of inter-community interactions. Here, we use weighted stochastic blockmodels to uncover the meso-scale architecture of \\emph{Drosophila}, mouse, rat, macaque, and human connectomes. We confirm that while many communities are assortative, others form core-periphery and disassortative structures, which in the human better recapitulate observed patterns of functional connectivity and in the mouse better recapitulate observed patterns of gene co-expression than other community detection techniques. We define a set of network measures for quantifying the diversity of community types in which brain regions participate. Finally, we show that diversity is peaked in control and subcortical systems in humans, and that individual differences in diversity within those systems predicts cognitive performance on Stroop and Navon tasks. In summary, our report paints a more diverse portrait of connectome meso-scale structure and demonstrates its relevance for cognitive performance.",
                        "Citation Paper Authors": "Authors:Richard F. Betzel, John D. Medaglia, Danielle S. Bassett"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1807.06398v2": {
            "Paper Title": "Dynamic reshaping of functional brain networks during visual object\n  recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.10269v1": {
            "Paper Title": "Ripple oscillations in the left temporal neocortex are associated with\n  impaired verbal episodic memory encoding",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.07669v1": {
            "Paper Title": "Modelling of consciousness and interpretation of quantum mechanics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.06203v1": {
            "Paper Title": "Penalized matrix decomposition for denoising, compression, and improved\n  demixing of functional imaging data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.04691v1": {
            "Paper Title": "Spatial Embedding Imposes Constraints on the Network Architectures of\n  Neural Systems",
            "Sentences": [
                {
                    "Sentence ID": 122,
                    "Sentence": "A. Sizemore, C. Giusti, and D. S. Bassett, Journal of\nComplex Networks 5, 245 (2017), arXiv:1512.06457. ",
                    "Citation Text": "D. Horak, S. Maleti, and M. Rajkovi, Stat. Mech\n(2009), 10.1088/1742-5468/2009/03/P03034.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:0811.2203",
                        "Citation Paper Title": "Title:Persistent Homology of Complex Networks",
                        "Citation Paper Abstract": "Abstract:  Long lived topological features are distinguished from short lived ones (considered as topological noise) in simplicial complexes constructed from complex networks. A new topological invariant, persistent homology, is determined and presented as a parametrized version of a Betti number. Complex networks with distinct degree distributions exhibit distinct persistent topological features. Persistent toplogical attributes, shown to be related to robust quality of networks, also reflect defficiency in certain connectivity properites of networks. Random networks, networks with exponential conectivity distribution and scale-free networks were considered for homological persistency analysis.",
                        "Citation Paper Authors": "Authors:Danijela Horak, Slobodan Maletic, Milan Rajkovic"
                    }
                },
                {
                    "Sentence ID": 121,
                    "Sentence": "G. Petri, M. Scolamiero, I. Donato, F. Vaccarino, and\nR. Lambiotte, PLoS ONE 8(2013), 10.1371/. ",
                    "Citation Text": "A. Sizemore, C. Giusti, and D. S. Bassett, Journal of\nComplex Networks 5, 245 (2017), arXiv:1512.06457.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.06457",
                        "Citation Paper Title": "Title:Classification of weighted networks through mesoscale homological features",
                        "Citation Paper Abstract": "Abstract:As complex networks find applications in a growing range of disciplines, the diversity of naturally occurring and model networks being studied is exploding. The adoption of a well-developed collection of network taxonomies is a natural method for both organizing this data and understanding deeper relationships between networks. Most existing metrics for network structure rely on classical graph-theoretic measures, extracting characteristics primarily related to individual vertices or paths between them, and thus classify networks from the perspective of local features. Here, we describe an alternative approach to studying structure in networks that relies on an algebraic-topological metric called persistent homology, which studies intrinsically mesoscale structures called cycles, constructed from cliques in the network. We present a classification of 14 commonly studied weighted network models into four groups or classes, and discuss the structural themes arising in each class. Finally, we compute the persistent homology of two real-world networks and one network constructed by a common dynamical systems model, and we compare the results with the three classes to obtain a better understanding of those networks.",
                        "Citation Paper Authors": "Authors:Ann Sizemore, Chad Giusti, Danielle Bassett"
                    }
                },
                {
                    "Sentence ID": 115,
                    "Sentence": "J. O. Garcia, A. Ashourvan, S. F. Muldoon, J. M. Vettel,\nand D. S. Bassett, Proceedings of the IEEE 106, 846\n(2018). ",
                    "Citation Text": "C. Giusti, R. Ghrist, and D. S. Bassett, J Comput\nNeurosci 41, 1 (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1601.01704",
                        "Citation Paper Title": "Title:Two's company, three (or more) is a simplex: Algebraic-topological tools for understanding higher-order structure in neural data",
                        "Citation Paper Abstract": "Abstract:The language of graph theory, or network science, has proven to be an exceptional tool for addressing myriad problems in neuroscience. Yet, the use of networks is predicated on a critical simplifying assumption: that the quintessential unit of interest in a brain is a dyad -- two nodes (neurons or brain regions) connected by an edge. While rarely mentioned, this fundamental assumption inherently limits the types of neural structure and function that graphs can be used to model. Here, we describe a generalization of graphs that overcomes these limitations, thereby offering a broad range of new possibilities in terms of modeling and measuring neural phenomena. Specifically, we explore the use of \\emph{simplicial complexes}, a theoretical notion developed in the field of mathematics known as algebraic topology, which is now becoming applicable to real data due to a rapidly growing computational toolset. We review the underlying mathematical formalism as well as the budding literature applying simplicial complexes to neural data, from electrophysiological recordings in animal models to hemodynamic fluctuations in humans. Based on the exceptional flexibility of the tools and recent ground-breaking insights into neural function, we posit that this framework has the potential to eclipse graph theory in unraveling the fundamental mysteries of cognition.",
                        "Citation Paper Authors": "Authors:Chad Giusti, Robert Ghrist, Danielle S. Bassett"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "D. S. Bassett and O. Sporns, Nature Neuroscience 20,\n353 (2017), arXiv:0106096v1 [arXiv:cond-mat]. ",
                    "Citation Text": "D. S. Bassett, P. Zurn, and J. I. Gold, Nature Reviews\nNeuroscience In Press (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.11935",
                        "Citation Paper Title": "Title:Network models in neuroscience",
                        "Citation Paper Abstract": "Abstract:From interacting cellular components to networks of neurons and neural systems, interconnected units comprise a fundamental organizing principle of the nervous system. Understanding how their patterns of connections and interactions give rise to the many functions of the nervous system is a primary goal of neuroscience. Recently, this pursuit has begun to benefit from the development of new mathematical tools that can relate a system's architecture to its dynamics and function. These tools, which are known collectively as network science, have been used with increasing success to build models of neural systems across spatial scales and species. Here we discuss the nature of network models in neuroscience. We begin with a review of model theory from a philosophical perspective to inform our view of networks as models of complex systems in general, and of the brain in particular. We then summarize the types of models that are frequently studied in network neuroscience along three primary dimensions: from data representations to first-principles theory, from biophysical realism to functional phenomenology, and from elementary descriptions to coarse-grained approximations. We then consider ways to validate these models, focusing on approaches that perturb a system to probe its function. We close with a description of important frontiers in the construction of network models and their relevance for understanding increasingly complex functions of neural systems.",
                        "Citation Paper Authors": "Authors:Danielle S. Bassett, Perry Zurn, Joshua I. Gold"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1803.06180v2": {
            "Paper Title": "Heterogeneity of Synaptic Input Connectivity Regulates Spike-based\n  Neuronal Avalanches",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.03238v1": {
            "Paper Title": "Exploring Brain-wide Development of Inhibition through Deep Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.02755v1": {
            "Paper Title": "An ALE meta-analytic comparison of verbal working memory tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.02741v1": {
            "Paper Title": "Algebraic signatures of convex and non-convex codes",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ". In\ngeneral, however, the absence of local obstructions does not guarantee thatCis convex ",
                    "Citation Text": "C. Lienkaemper, A. Shiu, and Z. Woodstock. Obstructions to convexity in neural codes. Advances\nin Appl. Math. , 85:31{59, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1509.03328",
                        "Citation Paper Title": "Title:Obstructions to convexity in neural codes",
                        "Citation Paper Abstract": "Abstract:How does the brain encode spatial structure? One way is through hippocampal neurons called place cells, which become associated to convex regions of space known as their receptive fields: each place cell fires at a high rate precisely when the animal is in the receptive field. The firing patterns of multiple place cells form what is known as a convex neural code. How can we tell when a neural code is convex? To address this question, Giusti and Itskov identified a local obstruction, defined via the topology of a code's simplicial complex, and proved that convex neural codes have no local obstructions. Curto et al. proved the converse for all neural codes on at most four neurons. Via a counterexample on five neurons, we show that this converse is false in general. Additionally, we classify all codes on five neurons with no local obstructions. This classification is enabled by our enumeration of connected simplicial complexes on 5 vertices up to isomorphism. Finally, we examine how local obstructions are related to maximal codewords (maximal sets of neurons that co-fire). Curto et al. proved that a code has no local obstructions if and only if it contains certain \"mandatory\" intersections of maximal codewords. We give a new criterion for an intersection of maximal codewords to be non-mandatory, and prove that it classifies all such non-mandatory codewords for codes on up to 5 neurons.",
                        "Citation Paper Authors": "Authors:Caitlin Lienkaemper, Anne Shiu, Zev Woodstock"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1807.02390v1": {
            "Paper Title": "On the identification of $k$-inductively pierced codes using toric\n  ideals",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.03174v1": {
            "Paper Title": "A quantum uncertainty entails entangled linguistic sequences",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.01469v1": {
            "Paper Title": "Shot Noise Neuron Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/1807.00509v2": {
            "Paper Title": "Ultrafast population coding and axo-somatic compartmentalization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.10428v1": {
            "Paper Title": "Static Internal Representation Of Dynamic Situations Reveals Time\n  Compaction In Human Cognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.04793v2": {
            "Paper Title": "A Connectome Based Hexagonal Lattice Convolutional Network Model of the\n  Drosophila Visual System",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.08634v1": {
            "Paper Title": "A probabilistic atlas of the human thalamic nuclei combining ex vivo MRI\n  and histology",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.01287v2": {
            "Paper Title": "Burst detection methods",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.06765v1": {
            "Paper Title": "Modularity Matters: Learning Invariant Relational Reasoning Tasks",
            "Sentences": [
                {
                    "Sentence ID": 42,
                    "Sentence": "a huge modular\nneural network is used to achieve state of the art performance on language modeling and machine\ntranslation tasks. The ResNeXt model ",
                    "Citation Text": "Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transforma-\ntions for deep neural networks. arXiv preprint arXiv:1611.05431 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.05431",
                        "Citation Paper Title": "Title:Aggregated Residual Transformations for Deep Neural Networks",
                        "Citation Paper Abstract": "Abstract:We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call \"cardinality\" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.",
                        "Citation Paper Authors": "Authors:Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, Kaiming He"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1806.06412v1": {
            "Paper Title": "Discrete structure of the brain rhythms",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.03047v1": {
            "Paper Title": "On sound-based interpretation of neonatal EEG",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.04037v1": {
            "Paper Title": "Neonatal EEG Interpretation and Decision Support Framework for Mobile\n  Platforms",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.02300v1": {
            "Paper Title": "Data-driven Probabilistic Atlases Capture Whole-brain Individual\n  Variation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.01979v1": {
            "Paper Title": "Spike Sorting by Convolutional Dictionary Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.01875v1": {
            "Paper Title": "EEG-GAN: Generative adversarial networks for electroencephalograhic\n  (EEG) brain signals",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.00546v2": {
            "Paper Title": "Spatially Localized Atlas Network Tiles Enables 3D Whole Brain\n  Segmentation from Limited Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1806.04704v1": {
            "Paper Title": "Sparse distributed representation, hierarchy, critical periods,\n  metaplasticity: the keys to lifelong fixed-time learning and best-match\n  retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.10734v2": {
            "Paper Title": "A neural network trained to predict future video frames mimics critical\n  properties of biological neuronal responses and perception",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.10958v1": {
            "Paper Title": "Discrete flow posteriors for variational inference in discrete dynamical\n  systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.03390v3": {
            "Paper Title": "Same-different problems strain convolutional neural networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.12005v1": {
            "Paper Title": "Combined MEG and fMRI Exponential Random Graph Modeling for inferring\n  functional Brain Connectivity",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.08195v3": {
            "Paper Title": "Adversarial Examples that Fool both Computer Vision and Time-Limited\n  Humans",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": "developed a model of object recognition in cortex that closely resembles\nmany aspects of modern CNNs. Kummerer et al. [ 21,22] showed that CNNs are predictive of human\ngaze \ufb01xation. Style transfer ",
                    "Citation Text": "Leon A Gatys, Alexander S Ecker, and Matthias Bethge. A neural algorithm of artistic style.\narXiv preprint arXiv:1508.06576 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1508.06576",
                        "Citation Paper Title": "Title:A Neural Algorithm of Artistic Style",
                        "Citation Paper Abstract": "Abstract:In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.",
                        "Citation Paper Authors": "Authors:Leon A. Gatys, Alexander S. Ecker, Matthias Bethge"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": ". Unlike the attack\nin a, the image contains features that seem semantically computer-like to humans. (c) An adversarial\npatch that causes images to be labeled as a toaster, optimized to cause misclassi\ufb01cation from multiple\nviewpoints, reproduced from ",
                    "Citation Text": "Tom B Brown, Dandelion Man\u00e9, Aurko Roy, Mart\u00edn Abadi, and Justin Gilmer. Adversarial\npatch. arXiv preprint arXiv:1712.09665 , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.09665",
                        "Citation Paper Title": "Title:Adversarial Patch",
                        "Citation Paper Abstract": "Abstract:We present a method to create universal, robust, targeted adversarial image patches in the real world. The patches are universal because they can be used to attack any scene, robust because they work under a wide variety of transformations, and targeted because they can cause a classifier to output any target class. These adversarial patches can be printed, added to any scene, photographed, and presented to image classifiers; even when the patches are small, they cause the classifiers to ignore the other items in the scene and report a chosen target class.\nTo reproduce the results from the paper, our code is available at this https URL",
                        "Citation Paper Authors": "Authors:Tom B. Brown, Dandelion Man\u00e9, Aurko Roy, Mart\u00edn Abadi, Justin Gilmer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1803.07352v2": {
            "Paper Title": "Disentangling top-down vs. bottom-up and low-level vs. high-level\n  influences on eye movements over time",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.03891v4": {
            "Paper Title": "Multifunctionality in embodied agents: Three levels of neural reuse",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.00840v2": {
            "Paper Title": "Preserved Structure Across Vector Space Representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.05036v1": {
            "Paper Title": "A Deep Learning Approach with an Attention Mechanism for Automatic Sleep\n  Stage Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.01385v4": {
            "Paper Title": "Effect of Inhibitory Spike-Timing-Dependent Plasticity on Fast Sparsely\n  Synchronized Rhythms in A Small-World Neuronal Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.04566v1": {
            "Paper Title": "WU-NEAT: A clinically validated, open- source MATLAB toolbox for\n  limited-channel neonatal EEG analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.04329v1": {
            "Paper Title": "Altered Modularity and Disproportional Integration in Functional\n  Networks are Markers of Abnormal Brain Organization in Schizophrenia",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.03337v1": {
            "Paper Title": "Multi-scale metrics and self-organizing maps: a computational approach\n  to the structure of sensory maps",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.02995v1": {
            "Paper Title": "A Spiking Neural Dynamical Drift-Diffusion Model on Collective Decision\n  Making with Self-Organized Criticality",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.07343v1": {
            "Paper Title": "The impact of binaural white noise with oscillations of 100 to 750hz in\n  the short-term visual working memory and the reactivity of alpha and beta\n  cerebral waves",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.02626v3": {
            "Paper Title": "Inferring health conditions from fMRI-graph data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.02609v1": {
            "Paper Title": "The Effect of In vivo-like Synaptic Inputs on Stellate Cells",
            "Sentences": []
        },
        "http://arxiv.org/abs/1805.01260v1": {
            "Paper Title": "White Matter Network Architecture Guides Direct Electrical Stimulation\n  Through Optimal State Transitions",
            "Sentences": [
                {
                    "Sentence ID": 73,
                    "Sentence": "M. Vinck, R. Oostenveld, M. Van Wingerden, F. Battaglia, and C. M. Pennartz. An improved index\nof phase-synchronization for electrophysiological data in the presence of volume-conduction, noise and\nsample-size bias. NeuroImage , 55(4):1548{1565, 2011. ",
                    "Citation Text": "E. Wu-Yan, R. F. Betzel, E. Tang, S. Gu, F. Pasqualetti, and D. S. Bassett. Benchmarking measures\nof network controllability on canonical graph models. 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.05117",
                        "Citation Paper Title": "Title:Benchmarking measures of network controllability on canonical graph models",
                        "Citation Paper Abstract": "Abstract:Many real-world systems are composed of many individual components that interact with one another in a complex pattern to produce diverse behaviors. Understanding how to intervene in these systems to guide behaviors is critically important to facilitate new discoveries and therapies in systems biology and neuroscience. A promising approach to optimizing interventions in complex systems is network control theory, an emerging conceptual framework and associated mathematics to understand how targeted input to nodes in a network system can predictably alter system dynamics. While network control theory is currently being applied to real-world data, the practical performance of these measures on simple networks with pre-specified structure is not well understood. In this study, we benchmark measures of network controllability on canonical graph models, providing an intuition for how control strategy, graph topology, and edge weight distribution mutually depend on one another. Our numerical studies motivate future analytical efforts to gain a mechanistic understanding of the relationship between graph topology and control, as well as efforts to design networks with specific control profiles.",
                        "Citation Paper Authors": "Authors:Elena Wu-Yan, Richard F. Betzel, Evelyn Tang, Shi Gu, Fabio Pasqualetti, Danielle S. Bassett"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1804.10861v1": {
            "Paper Title": "Neuroscientific User Models: The Source of Uncertain User Feedback and\n  Potentials for Improving Recommendation and Personalisation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.10093v1": {
            "Paper Title": "A GPP algorithm for hippocampal interneuron characterization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.00338v2": {
            "Paper Title": "Synthesizing realistic neural population activity patterns using\n  Generative Adversarial Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.05964v1": {
            "Paper Title": "Appropriate kernels for Divisive Normalization explained by Wilson-Cowan\n  equations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.04324v1": {
            "Paper Title": "Local reservoir model for choice-based learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.01961v2": {
            "Paper Title": "Machine learning of neuroimaging to diagnose cognitive impairment and\n  dementia: a systematic review and comparative analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.04604v1": {
            "Paper Title": "Discovery and usage of joint attention in images",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.02835v2": {
            "Paper Title": "A Community-Developed Open-Source Computational Ecosystem for Big Neuro\n  Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.03190v1": {
            "Paper Title": "Studying the Effects of Deep Brain Stimulation and Medication on the\n  Dynamics of STN-LFP Signals for Human Behavior Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.03142v1": {
            "Paper Title": "Markerless tracking of user-defined features with deep learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.01704v1": {
            "Paper Title": "Review: the development of neural stem cell biology and technology in\n  regenerative medicine",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.01487v1": {
            "Paper Title": "Predicting neural network dynamics via graphical analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.01296v1": {
            "Paper Title": "Gaussian Process Uncertainty in Age Estimation as a Measure of Brain\n  Abnormality",
            "Sentences": []
        },
        "http://arxiv.org/abs/1804.00970v1": {
            "Paper Title": "Eye movement simulation and detector creation to reduce laborious\n  parameter adjustments",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.10318v1": {
            "Paper Title": "Re-thinking EEG-based non-invasive brain interfaces: modeling and\n  analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.09018v1": {
            "Paper Title": "The Importance of Constraint Smoothness for Parameter Estimation in\n  Computational Cognitive Modeling",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.07883v1": {
            "Paper Title": "Scan transcription of two-dimensional shapes as an alternative\n  neuromorphic concept",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.07193v1": {
            "Paper Title": "Idiosyncratic choice bias in decision tasks naturally emerges from\n  neuronal network dynamics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.05897v1": {
            "Paper Title": "Contrasting information theoretic decompositions of modulatory and\n  arithmetic interactions in neural information processing systems",
            "Sentences": [
                {
                    "Sentence ID": 33,
                    "Sentence": "Gilbert, C. D. & Sigman, M. Brain States: Top-Down In\ufb02uences in Sensory Processing. Neu-\nron2007 ,54, 677-696. ",
                    "Citation Text": "Shwartz-Ziv, R. & Tishby, N. (2017) Opening the black box of deep neural networks via\ninformation. arXiv 2017 , arXiv:1703.00810.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.00810",
                        "Citation Paper Title": "Title:Opening the Black Box of Deep Neural Networks via Information",
                        "Citation Paper Abstract": "Abstract:Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work proposed to analyze DNNs in the \\textit{Information Plane}; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compression and prediction, successively, for each layer.\nIn this work we follow up on this idea and demonstrate the effectiveness of the Information-Plane visualization of DNNs. Our main results are: (i) most of the training epochs in standard DL are spent on {\\emph compression} of the input to efficient representation and not on fitting the training labels. (ii) The representation compression phase begins when the training errors becomes small and the Stochastic Gradient Decent (SGD) epochs change from a fast drift to smaller training error into a stochastic relaxation, or random diffusion, constrained by the training error value. (iii) The converged layers lie on or very close to the Information Bottleneck (IB) theoretical bound, and the maps from the input to any hidden layer and from this hidden layer to the output satisfy the IB self-consistent equations. This generalization through noise mechanism is unique to Deep Neural Networks and absent in one layer networks. (iv) The training time is dramatically reduced when adding more hidden layers. Thus the main advantage of the hidden layers is computational. This can be explained by the reduced relaxation time, as this it scales super-linearly (exponentially for simple diffusion) with the information compression from the previous layer.",
                        "Citation Paper Authors": "Authors:Ravid Shwartz-Ziv, Naftali Tishby"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1803.03692v1": {
            "Paper Title": "On the information in spike timing: neural codes derived from\n  polychronous groups",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.03204v1": {
            "Paper Title": "Strengthening Relationships between Neural Ideals and Receptive Fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/1803.01325v1": {
            "Paper Title": "Could Interaction with Social Robots Facilitate Joint Attention of\n  Children with Autism Spectrum Disorder?",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.10131v1": {
            "Paper Title": "Broadly heterogeneous network topology begets order-based representation\n  by privileged neurons",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.07905v1": {
            "Paper Title": "Closed-loop control of a modular neuromorphic biohybrid",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.06327v1": {
            "Paper Title": "Directional and Causal Information Flow in EEG for Assessing Perceived\n  Audio Quality",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.04255v2": {
            "Paper Title": "Systems of Global Governance in the Era of Human-Machine Convergence",
            "Sentences": [
                {
                    "Sentence ID": 122,
                    "Sentence": "\u201d, in Smart city ,\nCham, Switzerland: Springer, 2014, pp. 13\u2013\n43.doi:10.1007/978-3-319-06160-3 2. [On-\nline]. Available: http://link.springer.com/10.\n1007/978-3-319-06160-3%7B%5C %7D2. ",
                    "Citation Text": "J. Gubbi, R. Buyya, S. Marusic, and M.\nPalaniswami,\u201cInternet of Things (IoT): A vi-\nsion, architectural elements, and future direc-\ntions\u201d,Future Generation Computer Systems ,\nvol. 29, no. 7, pp. 1645\u20131660,Sep. 2013, issn:\n0167739X. doi:10.1016/j.future.2013.01.010 .\n[Online]. Available: http://linkinghub.elsevie\nr.com/retrieve/pii/S0167739X13000241 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1207.0203",
                        "Citation Paper Title": "Title:Internet of Things (IoT): A Vision, Architectural Elements, and Future Directions",
                        "Citation Paper Abstract": "Abstract:Ubiquitous sensing enabled by Wireless Sensor Network (WSN) technologies cuts across many areas of modern day living. This offers the ability to measure, infer and understand environmental indicators, from delicate ecologies and natural resources to urban environments. The proliferation of these devices in a communicating-actuating network creates the Internet of Things (IoT), wherein, sensors and actuators blend seamlessly with the environment around us, and the information is shared across platforms in order to develop a common operating picture (COP). Fuelled by the recent adaptation of a variety of enabling device technologies such as RFID tags and readers, near field communication (NFC) devices and embedded sensor and actuator nodes, the IoT has stepped out of its infancy and is the the next revolutionary technology in transforming the Internet into a fully integrated Future Internet. As we move from www (static pages web) to web2 (social networking web) to web3 (ubiquitous computing web), the need for data-on-demand using sophisticated intuitive queries increases significantly. This paper presents a cloud centric vision for worldwide implementation of Internet of Things. The key enabling technologies and application domains that are likely to drive IoT research in the near future are discussed. A cloud implementation using Aneka, which is based on interaction of private and public clouds is presented. We conclude our IoT vision by expanding on the need for convergence of WSN, the Internet and distributed computing directed at technological research community.",
                        "Citation Paper Authors": "Authors:Jayavardhana Gubbi, Rajkumar Buyya, Slaven Marusic, Marimuthu Palaniswami"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1802.04069v1": {
            "Paper Title": "Searchers adjust their eye movement dynamics to the target\n  characteristics in natural scenes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.04510v2": {
            "Paper Title": "Brain EEG Time Series Selection: A Novel Graph-Based Approach for\n  Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.02678v1": {
            "Paper Title": "Biological Mechanisms for Learning: A Computational Model of Olfactory\n  Learning in the Manduca sexta Moth, with Applications to Neural Nets",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.08116v2": {
            "Paper Title": "Psychlab: A Psychology Laboratory for Deep Reinforcement Learning Agents",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.01980v1": {
            "Paper Title": "Layered structure and leveled function of a human brain",
            "Sentences": []
        },
        "http://arxiv.org/abs/1802.00753v1": {
            "Paper Title": "Effect of time of day on reward circuitry. A discussion of Byrne et al.\n  2017",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.09024v2": {
            "Paper Title": "Moral attitudes and willingness to induce cognitive enhancement and\n  repair with brain stimulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.10186v1": {
            "Paper Title": "A Rational Distributed Process-level Account of Independence Judgment",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.09900v1": {
            "Paper Title": "Human Echolocation in Static Situations: Auditory Models of Detection\n  Thresholds for Distance, Pitch, Loudness and Timbre",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.09632v1": {
            "Paper Title": "In Praise of Artifice Reloaded: Caution with subjective image quality\n  databases",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.09589v1": {
            "Paper Title": "Coactivated Clique Based Multisource Overlapping Brain Subnetwork\n  Extraction",
            "Sentences": [
                {
                    "Sentence ID": 3,
                    "Sentence": ", we argue that the clique concept closely resembles groups of nodes which\nare the canonical network components . Based on the basic observation that typical\ncommunities consist of several cliques that tend to share many of their nodes ",
                    "Citation Text": "Palla, G., Der\u0013 enyi, I., Farkas, I., Vicsek, T.: Uncovering the overlapping com-\nmunity structure of complex networks in nature and society. arXiv preprint\nphysics/0506133 (2005)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:physics/0506133",
                        "Citation Paper Title": "Title:Uncovering the overlapping community structure of complex networks in nature and society",
                        "Citation Paper Abstract": "Abstract:  Many complex systems in nature and society can be described in terms of networks capturing the intricate web of connections among the units they are made of. A key question is how to interpret the global organization of such networks as the coexistence of their structural subunits (communities) associated with more highly interconnected parts. Identifying these a priori unknown building blocks (such as functionally related proteins, industrial sectors and groups of people) is crucial to the understanding of the structural and functional properties of networks. The existing deterministic methods used for large networks find separated communities, whereas most of the actual networks are made of highly overlapping cohesive groups of nodes. Here we introduce an approach to analysing the main statistical features of the interwoven sets of overlapping communities that makes a step towards uncovering the modular structure of complex systems. After defining a set of new characteristic quantities for the statistics of communities, we apply an efficient technique for exploring overlapping communities on a large scale. We find that overlaps are significant, and the distributions we introduce reveal universal features of networks. Our studies of collaboration, word-association and protein interaction graphs show that the web of communities has non-trivial correlations and specific scaling properties.",
                        "Citation Paper Authors": "Authors:Gergely Palla, Imre Derenyi, Illes Farkas, Tamas Vicsek"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1801.08585v1": {
            "Paper Title": "From Neuronal Models to Neuronal Dynamics and Image Processing",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.08108v1": {
            "Paper Title": "A neural model of the locust visual system for detection of object\n  approaches with real-world scenes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.07936v1": {
            "Paper Title": "Anticipating epileptic seizures through the analysis of EEG\n  synchronization as a data classification problem",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.07244v1": {
            "Paper Title": "To jump or not to jump: The Bereitschaftspotential required to jump into\n  192-meter abyss",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.05703v2": {
            "Paper Title": "Functional optimality of the sulcus pattern of the human brain",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.06226v1": {
            "Paper Title": "Computational model of avian nervous system nuclei governing learned\n  song",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.05151v1": {
            "Paper Title": "Constraint-free Natural Image Reconstruction from fMRI Signals Based on\n  Convolutional Neural Network",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": "H. Wen, J. Shi, Y. Zhang, K. -H. Lu, J. Cao, Z. Liu, Neural Encoding and Decoding with Deep \nLearning for Dynamic Natural Vision, Cerebral Cortex, (2017) 1 -25. ",
                    "Citation Text": "A. Mahendran, A. Vedaldi, Visualizing Deep Con volutional Neural Networks Using Natural \nPre-images, International Journal of Computer Vision, 120 (2016) 233 -255.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.02017",
                        "Citation Paper Title": "Title:Visualizing Deep Convolutional Neural Networks Using Natural Pre-Images",
                        "Citation Paper Abstract": "Abstract:Image representations, from SIFT and bag of visual words to Convolutional Neural Networks (CNNs) are a crucial component of almost all computer vision systems. However, our understanding of them remains limited. In this paper we study several landmark representations, both shallow and deep, by a number of complementary visualization techniques. These visualizations are based on the concept of \"natural pre-image\", namely a natural-looking image whose representation has some notable property. We study in particular three such visualizations: inversion, in which the aim is to reconstruct an image from its representation, activation maximization, in which we search for patterns that maximally stimulate a representation component, and caricaturization, in which the visual patterns that a representation detects in an image are exaggerated. We pose these as a regularized energy-minimization framework and demonstrate its generality and effectiveness. In particular, we show that this method can invert representations such as HOG more accurately than recent alternatives while being applicable to CNNs too. Among our findings, we show that several layers in CNNs retain photographically accurate information about the image, with different degrees of geometric and photometric invariance.",
                        "Citation Paper Authors": "Authors:Aravindh Mahendran, Andrea Vedaldi"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "T. Horikawa, Y. Kamitani, Hierarchical Neural Representation of Dreamed Objects Revealed by \nBrain Decoding with Deep Neural Network Features, Frontiers in Computational Neuroscience, 11 \n(2017). ",
                    "Citation Text": "C. Du, C. Du, H. He, Sharing deep generative representation for perceived image reconstruction \nfrom human brain activity,  arXiv preprint arXiv:1704.07575, (2017).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1704.07575",
                        "Citation Paper Title": "Title:Sharing deep generative representation for perceived image reconstruction from human brain activity",
                        "Citation Paper Abstract": "Abstract:Decoding human brain activities via functional magnetic resonance imaging (fMRI) has gained increasing attention in recent years. While encouraging results have been reported in brain states classification tasks, reconstructing the details of human visual experience still remains difficult. Two main challenges that hinder the development of effective models are the perplexing fMRI measurement noise and the high dimensionality of limited data instances. Existing methods generally suffer from one or both of these issues and yield dissatisfactory results. In this paper, we tackle this problem by casting the reconstruction of visual stimulus as the Bayesian inference of missing view in a multiview latent variable model. Sharing a common latent representation, our joint generative model of external stimulus and brain response is not only \"deep\" in extracting nonlinear features from visual images, but also powerful in capturing correlations among voxel activities of fMRI recordings. The nonlinearity and deep structure endow our model with strong representation ability, while the correlations of voxel activities are critical for suppressing noise and improving prediction. We devise an efficient variational Bayesian method to infer the latent variables and the model parameters. To further improve the reconstruction accuracy, the latent representations of testing instances are enforced to be close to that of their neighbours from the training set via posterior regularization. Experiments on three fMRI recording datasets demonstrate that our approach can more accurately reconstruct visual stimuli.",
                        "Citation Paper Authors": "Authors:Changde Du, Changying Du, Huiguang He"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "D.L. Yamins, J.J. DiCarlo, Using goal -driven deep learning models to understand sensory \ncortex, Nature neuroscience, 19 (2016) 356 -365. ",
                    "Citation Text": "P. Agrawal, D. Stansbury, J. Malik, J.L. Gallant, Pixels to Voxels: Modeling Visual \nRepresentation in the Human Brain, arXiv preprint arXiv:1407.5104, (2014).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1407.5104",
                        "Citation Paper Title": "Title:Pixels to Voxels: Modeling Visual Representation in the Human Brain",
                        "Citation Paper Abstract": "Abstract:The human brain is adept at solving difficult high-level visual processing problems such as image interpretation and object recognition in natural scenes. Over the past few years neuroscientists have made remarkable progress in understanding how the human brain represents categories of objects and actions in natural scenes. However, all current models of high-level human vision operate on hand annotated images in which the objects and actions have been assigned semantic tags by a human operator. No current models can account for high-level visual function directly in terms of low-level visual input (i.e., pixels). To overcome this fundamental limitation we sought to develop a new class of models that can predict human brain activity directly from low-level visual input (i.e., pixels). We explored two classes of models drawn from computer vision and machine learning. The first class of models was based on Fisher Vectors (FV) and the second was based on Convolutional Neural Networks (ConvNets). We find that both classes of models accurately predict brain activity in high-level visual areas, directly from pixels and without the need for any semantic tags or hand annotation of images. This is the first time that such a mapping has been obtained. The fit models provide a new platform for exploring the functional principles of human vision, and they show that modern methods of computer vision and machine learning provide important tools for characterizing brain function.",
                        "Citation Paper Authors": "Authors:Pulkit Agrawal, Dustin Stansbury, Jitendra Malik, Jack L. Gallant"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1802.02523v1": {
            "Paper Title": "Plasma Brain Dynamics (PBD): A Mechanism for EEG Waves Under Human\n  Consciousness",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.05017v1": {
            "Paper Title": "Hypergraph based Subnetwork Extraction using Fusion of Task and Rest\n  Functional Connectivity",
            "Sentences": [
                {
                    "Sentence ID": 33,
                    "Sentence": ". We also explore combining the two by applying\na multislice community detection approach ",
                    "Citation Text": "Mucha, P.J., Richardson, T., Macon, K., Porter, M.A., Onnela, J.P.: Commu-\nnity structure in time-dependent, multiscale, and multiplex networks. science\n328(5980) (2010) 876{878",
                    "Citation": {
                        "Citation Paper ID": "arXiv:0911.1824",
                        "Citation Paper Title": "Title:Community Structure in Time-Dependent, Multiscale, and Multiplex Networks",
                        "Citation Paper Abstract": "Abstract:Network science is an interdisciplinary endeavor, with methods and applications drawn from across the natural, social, and information sciences. A prominent problem in network science is the algorithmic detection of tightly-connected groups of nodes known as communities. We developed a generalized framework of network quality functions that allowed us to study the community structure of arbitrary multislice networks, which are combinations of individual networks coupled through links that connect each node in one network slice to itself in other slices. This framework allows one to study community structure in a very general setting encompassing networks that evolve over time, have multiple types of links (multiplexity), and have multiple scales.",
                        "Citation Paper Authors": "Authors:Peter J. Mucha, Thomas Richardson, Kevin Macon, Mason A. Porter, Jukka-Pekka Onnela"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": ". Hypergraphs have been used to identify\nnon-random structure in structural connectivity of the cortical microcircuits ",
                    "Citation Text": "Dotko, P., Hess, K., Levi, R., Nolte, M., Reimann, M., Scolamiero, M., Turner,\nK., Muller, E., Markram, H.: Topological analysis of the connectome of digital\nreconstructions of neural microcircuits. arXiv preprint arXiv:1601.01580 (2016)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1601.01580",
                        "Citation Paper Title": "Title:Topological analysis of the connectome of digital reconstructions of neural microcircuits",
                        "Citation Paper Abstract": "Abstract:A recent publication provides the network graph for a neocortical microcircuit comprising 8 million connections between 31,000 neurons (H. Markram, et al., Reconstruction and simulation of neocortical microcircuitry, Cell, 163 (2015) no. 2, 456-492). Since traditional graph-theoretical methods may not be sufficient to understand the immense complexity of such a biological network, we explored whether methods from algebraic topology could provide a new perspective on its structural and functional organization. Structural topological analysis revealed that directed graphs representing connectivity among neurons in the microcircuit deviated significantly from different varieties of randomized graph. In particular, the directed graphs contained in the order of $10^7$ simplices \u00d0 groups of neurons with all-to-all directed connectivity. Some of these simplices contained up to 8 neurons, making them the most extreme neuronal clustering motif ever reported. Functional topological analysis of simulated neuronal activity in the microcircuit revealed novel spatio-temporal metrics that provide an effective classification of functional responses to qualitatively different stimuli. This study represents the first algebraic topological analysis of structural connectomics and connectomics-based spatio-temporal activity in a biologically realistic neural microcircuit. The methods used in the study show promise for more general applications in network science.",
                        "Citation Paper Authors": "Authors:Pawe Dotko, Kathryn Hess, Ran Levi, Max Nolte, Michael Reimann, Martina Scolamiero, Katharine Turner, Eilif Muller, Henry Markram"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1801.04623v1": {
            "Paper Title": "Sex differences in network controllability as a predictor of executive\n  function in youth",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.04515v1": {
            "Paper Title": "Solving constraint-satisfaction problems with distributed\n  neocortical-like neuronal networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.02549v1": {
            "Paper Title": "Electroencephalography source connectivity: toward high time/space\n  resolution brain networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.05753v2": {
            "Paper Title": "Continual Developmental Neurosimulation Using Embodied Computational\n  Agents",
            "Sentences": [
                {
                    "Sentence ID": 88,
                    "Sentence": ".\nThe\nother\nmethodologies\nof\ninterest\nare\nclass-incr emental\nlearning\nand \nforwar d\ntransfer\n.\nClass-incremental\nlearning\nassumes\nthat\nthe\nagent\u2019s\nenvironment\nis \nnon-stationary ,\nand\ndisallows\nfor\nclasses\nto\nbe\nreused\nacross\nexperiential\ncontexts ",
                    "Citation Text": "Maltoni,\nD.\nand\nLomonaco,\nV.\n(2019).\nContinuous\nlearning\nin\nsingle \nincremental-task\nscenarios.\nNeural\nNetworks\n,\n116,\n56\u201373.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.08568",
                        "Citation Paper Title": "Title:Continuous Learning in Single-Incremental-Task Scenarios",
                        "Citation Paper Abstract": "Abstract:It was recently shown that architectural, regularization and rehearsal strategies can be used to train deep models sequentially on a number of disjoint tasks without forgetting previously acquired knowledge. However, these strategies are still unsatisfactory if the tasks are not disjoint but constitute a single incremental task (e.g., class-incremental learning). In this paper we point out the differences between multi-task and single-incremental-task scenarios and show that well-known approaches such as LWF, EWC and SI are not ideal for incremental task scenarios. A new approach, denoted as AR1, combining architectural and regularization strategies is then specifically proposed. AR1 overhead (in term of memory and computation) is very small thus making it suitable for online learning. When tested on CORe50 and iCIFAR-100, AR1 outperformed existing regularization strategies by a good margin.",
                        "Citation Paper Authors": "Authors:Davide Maltoni, Vincenzo Lomonaco"
                    }
                },
                {
                    "Sentence ID": 86,
                    "Sentence": ".\nStatistically ,\nthis\ntranslates\ninto\nhow\nthe\ndistribution\nof\nevents\nchanges \nover\ntime ",
                    "Citation Text": "Cossu,\nA.,\nGraffieti,\nG.,\nPellegrini,\nL.,\nMaltoni,\nD.,\nBacciu,\nD.,\nCarta,\nA.\nand \nLomonaco,\nV.\n(2022).\nIs\nClass-Incremental\nEnough\nfor\nContinual\nLearning?\nFrontiers \nin\nArtificial\nIntelligence\n,\n5,\n829842.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.02925",
                        "Citation Paper Title": "Title:Is Class-Incremental Enough for Continual Learning?",
                        "Citation Paper Abstract": "Abstract:The ability of a model to learn continually can be empirically assessed in different continual learning scenarios. Each scenario defines the constraints and the opportunities of the learning environment. Here, we challenge the current trend in the continual learning literature to experiment mainly on class-incremental scenarios, where classes present in one experience are never revisited. We posit that an excessive focus on this setting may be limiting for future research on continual learning, since class-incremental scenarios artificially exacerbate catastrophic forgetting, at the expense of other important objectives like forward transfer and computational efficiency. In many real-world environments, in fact, repetition of previously encountered concepts occurs naturally and contributes to softening the disruption of previous knowledge. We advocate for a more in-depth study of alternative continual learning scenarios, in which repetition is integrated by design in the stream of incoming information. Starting from already existing proposals, we describe the advantages such class-incremental with repetition scenarios could offer for a more comprehensive assessment of continual learning models.",
                        "Citation Paper Authors": "Authors:Andrea Cossu, Gabriele Graffieti, Lorenzo Pellegrini, Davide Maltoni, Davide Bacciu, Antonio Carta, Vincenzo Lomonaco"
                    }
                },
                {
                    "Sentence ID": 55,
                    "Sentence": "Gordon,\nN.K.\nand\nGordon,\nR.\n(2016).\nEmbryogenesis\nExplained.\nWorld \nScientific\nPublishing,\nSingapore. ",
                    "Citation Text": "Haber,\nN.,\nMrowca,\nD.,\nWang,\nS.,\nFei-Fei,\nL.,\nand\nYamins,\nD.L.K.\n(2018). \nLearning\nto\nPlay\nWith\nIntrinsically-Motivated,\nSelf-Aware\nAgents.\nAdvances\nin \nNeural\nInformation\nProcessing\nSystems\n,\n31,\n8398\u20138409.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.07442",
                        "Citation Paper Title": "Title:Learning to Play with Intrinsically-Motivated Self-Aware Agents",
                        "Citation Paper Abstract": "Abstract:Infants are experts at playing, with an amazing ability to generate novel structured behaviors in unstructured environments that lack clear extrinsic reward signals. We seek to mathematically formalize these abilities using a neural network that implements curiosity-driven intrinsic motivation. Using a simple but ecologically naturalistic simulated environment in which an agent can move and interact with objects it sees, we propose a \"world-model\" network that learns to predict the dynamic consequences of the agent's actions. Simultaneously, we train a separate explicit \"self-model\" that allows the agent to track the error map of its own world-model, and then uses the self-model to adversarially challenge the developing world-model. We demonstrate that this policy causes the agent to explore novel and informative interactions with its environment, leading to the generation of a spectrum of complex behaviors, including ego-motion prediction, object attention, and object gathering. Moreover, the world-model that the agent learns supports improved performance on object dynamics prediction, detection, localization and recognition tasks. Taken together, our results are initial steps toward creating flexible autonomous agents that self-supervise in complex novel physical environments.",
                        "Citation Paper Authors": "Authors:Nick Haber, Damian Mrowca, Li Fei-Fei, Daniel L. K. Yamins"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "von\nOswald,\nJ.,\nHenning,\nC.,\nGrewe,\nB.F.,\nand\nSacramento,\nJ.\n(2022).\nContinual \nlearning\nwith\nhypernetworks.\narXiv\n,\n1906.00695. ",
                    "Citation Text": "Zenke,\nF.,\nPoole,\nB.,\nand\nGanguli,\nS.\n(2017).\nContinual\nlearning\nthrough\nsynaptic \nintelligence.\nProceedings\nof\nthe\nInternational\nConfer ence\non\nMachine\nLearning\n, \n34(70),\n3987\u20133995.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.04200",
                        "Citation Paper Title": "Title:Continual Learning Through Synaptic Intelligence",
                        "Citation Paper Abstract": "Abstract:While deep learning has led to remarkable advances across diverse applications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biological neural networks continually adapt to changing domains, possibly by leveraging complex molecular machinery to solve many tasks simultaneously. In this study, we introduce intelligent synapses that bring some of this biological complexity into artificial neural networks. Each synapse accumulates task relevant information over time, and exploits this information to rapidly store new memories without forgetting old ones. We evaluate our approach on continual learning of classification tasks, and show that it dramatically reduces forgetting while maintaining computational efficiency.",
                        "Citation Paper Authors": "Authors:Friedemann Zenke, Ben Poole, Surya Ganguli"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "Van\nde\nVen,\nG.M.\nand\nTolias,\nA.S.\n(2019).\nThree\nscenarios\nfor\ncontinual \nlearning.\narXiv\n,\n1904.07734.\n28 ",
                    "Citation Text": "Shin,\nH.,\nLee,\nJ.K.,\nKim,\nJ.,\nand\nKim,\nJ.\n(2017).\nContinual\nlearning\nwith\ndeep \ngenerative\nreplay.\narXiv\n,\n1705.08690.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.08690",
                        "Citation Paper Title": "Title:Continual Learning with Deep Generative Replay",
                        "Citation Paper Abstract": "Abstract:Attempts to train a comprehensive artificial intelligence capable of solving multiple tasks have been impeded by a chronic problem called catastrophic forgetting. Although simply replaying all previous data alleviates the problem, it requires large memory and even worse, often infeasible in real world applications where the access to past data is limited. Inspired by the generative nature of hippocampus as a short-term memory system in primate brain, we propose the Deep Generative Replay, a novel framework with a cooperative dual model architecture consisting of a deep generative model (\"generator\") and a task solving model (\"solver\"). With only these two models, training data for previous tasks can easily be sampled and interleaved with those for a new task. We test our methods in several sequential learning settings involving image classification tasks.",
                        "Citation Paper Authors": "Authors:Hanul Shin, Jung Kwon Lee, Jaehong Kim, Jiwon Kim"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "Okray,\nZ.,\nJacob,\nP.F.,\nStern,\nC.,\nDesmond,\nK.,\nOtto,\nN.,\nTalbot,\nC.B., \nVargas-Gutierrez,\nP.,\nand\nWaddell,\nS.\n(2023).\nMultisensory\nlearning\nbinds\nneurons \ninto\na\ncross-modal\nmemory\nengram.\nNatur e\n,\n617,\n777\u2013784. ",
                    "Citation Text": "Parisi,\nG.I.,\nKemker,\nR.,\nPart,\nJ.L.,\nKanan,\nC.,\nand\nWermter,\nS.\n(2019).\nContinual \nlifelong\nlearning\nwith\nneural\nnetworks:\na\nreview.\nNeural\nNetworks\n,\n113,\n54\u201371.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.07569",
                        "Citation Paper Title": "Title:Continual Lifelong Learning with Neural Networks: A Review",
                        "Citation Paper Abstract": "Abstract:Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.",
                        "Citation Paper Authors": "Authors:German I. Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan, Stefan Wermter"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "Kudithipudi,\nD.,\nAguilar-Simon,\nM.,\nBabb,\nJ.,\nBazhenov,\nM.,\nBlackiston,\nD., \nBongard,\nJ.,\nBrna,\nA.P.,\nRaja,\nS.C.,\nCheney,\nN.,\nClune,\nJ.\n(2022).\nBiological \nunderpinnings\nfor\nlifelong\nlearning\nmachines.\nNatur e\nMachine\nIntelligence\n,\n4(3), \n196\u2013210. ",
                    "Citation Text": "Kim,\nS.,\nNoci,\nL.,\nOrvieto,\nA.,\nHofmann,\nT.\n(2023).\nAchieving\na\nBetter \nStability-Plasticity\nTrade-off\nvia\nAuxiliary\nNetworks\nin\nContinual\nLearning.\narXiv\n, \n2303.09483.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2303.09483",
                        "Citation Paper Title": "Title:Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks in Continual Learning",
                        "Citation Paper Abstract": "Abstract:In contrast to the natural capabilities of humans to learn new tasks in a sequential fashion, neural networks are known to suffer from catastrophic forgetting, where the model's performances on old tasks drop dramatically after being optimized for a new task. Since then, the continual learning (CL) community has proposed several solutions aiming to equip the neural network with the ability to learn the current task (plasticity) while still achieving high accuracy on the previous tasks (stability). Despite remarkable improvements, the plasticity-stability trade-off is still far from being solved and its underlying mechanism is poorly understood. In this work, we propose Auxiliary Network Continual Learning (ANCL), a novel method that applies an additional auxiliary network which promotes plasticity to the continually learned model which mainly focuses on stability. More concretely, the proposed framework materializes in a regularizer that naturally interpolates between plasticity and stability, surpassing strong baselines on task incremental and class incremental scenarios. Through extensive analyses on ANCL solutions, we identify some essential principles beneath the stability-plasticity trade-off.",
                        "Citation Paper Authors": "Authors:Sanghwan Kim, Lorenzo Noci, Antonio Orvieto, Thomas Hofmann"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "provides\na\nway\nforward.\nThe\ncollective \nbehavior\nof\ndBVs\nmust\nalso\nbe\ninterpreted,\nand\nhas\nbeen\nclassified\nfor\nthe\ndBV \ninstantiation\nBV\ncollectives\nin ",
                    "Citation Text": "Dvoretskii,\nS.,\nGong,\nZ.,\nGupta,\nA.,\nParent,\nJ.,\nand\nAlicea,\nB.\n(2020).\nBraitenberg \nVehicles\nas\nDevelopmental\nNeurosimulation.\narXiv\n,\n2003.07689.\n27",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.07689",
                        "Citation Paper Title": "Title:Braitenberg Vehicles as Developmental Neurosimulation",
                        "Citation Paper Abstract": "Abstract:Connecting brain and behavior is a longstanding issue in the areas of behavioral science, artificial intelligence, and neurobiology. As is standard among models of artificial and biological neural networks, an analogue of the fully mature brain is presented as a blank slate. However, this does not consider the realities of biological development and developmental learning. Our purpose is to model the development of an artificial organism that exhibits complex behaviors. We introduce three alternate approaches to demonstrate how developmental embodied agents can be implemented. The resulting developmental BVs (dBVs) will generate behaviors ranging from stimulus responses to group behavior that resembles collective motion. We will situate this work in the domain of artificial brain networks along with broader themes such as embodied cognition, feedback, and emergence. Our perspective is exemplified by three software instantiations that demonstrate how a BV-genetic algorithm hybrid model, multisensory Hebbian learning model, and multi-agent approaches can be used to approach BV development. We introduce use cases such as optimized spatial cognition (vehicle-genetic algorithm hybrid model), hinges connecting behavioral and neural models (multisensory Hebbian learning model), and cumulative classification (multi-agent approaches). In conclusion, we consider future applications of the developmental neurosimulation approach.",
                        "Citation Paper Authors": "Authors:Stefan Dvoretskii, Ziyi Gong, Ankit Gupta, Jesse Parent, Bradly Alicea"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.13238v3": {
            "Paper Title": "Conductance-based dendrites perform Bayes-optimal cue integration",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.07292v4": {
            "Paper Title": "Sign and Relevance Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.00371v2": {
            "Paper Title": "Macaque's Cortical Functional Connectivity Dynamics at the Onset of\n  Propofol-Induced Anesthesia",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.03580v3": {
            "Paper Title": "One-shot learning of paired association navigation with biologically\n  plausible schemas",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.12482v3": {
            "Paper Title": "Self-Supervised Graph Representation Learning for Neuronal Morphologies",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.06174v2": {
            "Paper Title": "Does the Brain Infer Invariance Transformations from Graph Symmetries?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.14735v2": {
            "Paper Title": "Successes and failures of simple statistical physics models for a\n  network of real neurons",
            "Sentences": [
                {
                    "Sentence ID": 12,
                    "Sentence": "E Granot-Atedgi, G Tka\u0014 cik, R Segev, and E Schneidman.\nStimulus-dependent maximum entropy models of neural\npopulation codes. PLoS Comput. Biol ., 9(3):e1002922,\n2013. ",
                    "Citation Text": "W Bialek, A Cavagna, I Giardina, T Mora, O Pohl, E Sil-\nvestri, M Viale, and AM Walczak. Social interactions\ndominate speed control in poising natural \rocks near crit-\nicality. Proc. Natl. Acad. Sci. (USA) , 111(20):7212{7217,\n2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1107.0604",
                        "Citation Paper Title": "Title:Statistical mechanics for natural flocks of birds",
                        "Citation Paper Abstract": "Abstract:Interactions among neighboring birds in a flock cause an alignment of their flight directions. We show that the minimally structured (maximum entropy) model consistent with these local correlations correctly predicts the propagation of order throughout entire flocks of starlings, with no free parameters. These models are mathematically equivalent to the Heisenberg model of magnetism, and define an \"energy\" for each configuration of flight directions in the flock. Comparing flocks of different densities, the range of interactions that contribute to the energy involves a fixed number of (topological) neighbors, rather than a fixed (metric) spatial range. Comparing flocks of different sizes, the model correctly accounts for the observed scale invariance of long ranged correlations among the fluctuations in flight direction.",
                        "Citation Paper Authors": "Authors:William Bialek, Andrea Cavagna, Irene Giardina, Thierry Mora, Edmondo Silvestri, Massimiliano Viale, Aleksandra M Walczak"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "W Bialek, A Cavagna, I Giardina, T Mora, E Silvestri,\nM Viale, and AM Walczak. Statistical mechanics for\nnatural \rocks of birds. Proc. Natl. Acad. Sci. (USA) ,\n109(13):4786{4791, 2012. ",
                    "Citation Text": "E Granot-Atedgi, G Tka\u0014 cik, R Segev, and E Schneidman.\nStimulus-dependent maximum entropy models of neural\npopulation codes. PLoS Comput. Biol ., 9(3):e1002922,\n2013.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1205.6438",
                        "Citation Paper Title": "Title:Stimulus-dependent maximum entropy models of neural population codes",
                        "Citation Paper Abstract": "Abstract:Neural populations encode information about their stimulus in a collective fashion, by joint activity patterns of spiking and silence. A full account of this mapping from stimulus to neural activity is given by the conditional probability distribution over neural codewords given the sensory input. To be able to infer a model for this distribution from large-scale neural recordings, we introduce a stimulus-dependent maximum entropy (SDME) model---a minimal extension of the canonical linear-nonlinear model of a single neuron, to a pairwise-coupled neural population. The model is able to capture the single-cell response properties as well as the correlations in neural spiking due to shared stimulus and due to effective neuron-to-neuron connections. Here we show that in a population of 100 retinal ganglion cells in the salamander retina responding to temporal white-noise stimuli, dependencies between cells play an important encoding role. As a result, the SDME model gives a more accurate account of single cell responses and in particular outperforms uncoupled models in reproducing the distributions of codewords emitted in response to a stimulus. We show how the SDME model, in conjunction with static maximum entropy models of population vocabulary, can be used to estimate information-theoretic quantities like surprise and information transmission in a neural population.",
                        "Citation Paper Authors": "Authors:Einat Granot-Atedgi, Ga\u0161per Tka\u010dik, Ronen Segev, Elad Schneidman"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "A Lapedes, B Giraud, and C Jarzynski. Using sequence\nalignments to predict protein structure and stability with\nhigh accuracy. arXiv preprint arXiv:1207.2484 , 2012. ",
                    "Citation Text": "W Bialek, A Cavagna, I Giardina, T Mora, E Silvestri,\nM Viale, and AM Walczak. Statistical mechanics for\nnatural \rocks of birds. Proc. Natl. Acad. Sci. (USA) ,\n109(13):4786{4791, 2012.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1107.0604",
                        "Citation Paper Title": "Title:Statistical mechanics for natural flocks of birds",
                        "Citation Paper Abstract": "Abstract:Interactions among neighboring birds in a flock cause an alignment of their flight directions. We show that the minimally structured (maximum entropy) model consistent with these local correlations correctly predicts the propagation of order throughout entire flocks of starlings, with no free parameters. These models are mathematically equivalent to the Heisenberg model of magnetism, and define an \"energy\" for each configuration of flight directions in the flock. Comparing flocks of different densities, the range of interactions that contribute to the energy involves a fixed number of (topological) neighbors, rather than a fixed (metric) spatial range. Comparing flocks of different sizes, the model correctly accounts for the observed scale invariance of long ranged correlations among the fluctuations in flight direction.",
                        "Citation Paper Authors": "Authors:William Bialek, Andrea Cavagna, Irene Giardina, Thierry Mora, Edmondo Silvestri, Massimiliano Viale, Aleksandra M Walczak"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "DS Marks, LJ Colwell, R Sheridan, TA Hopf, A Pag-\nnani, R Zecchina, and C Sander. Protein 3d structure\ncomputed from evolutionary sequence variation. PLoS\nOne, 6(12):e28766, 2011. ",
                    "Citation Text": "A Lapedes, B Giraud, and C Jarzynski. Using sequence\nalignments to predict protein structure and stability with\nhigh accuracy. arXiv preprint arXiv:1207.2484 , 2012.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1207.2484",
                        "Citation Paper Title": "Title:Using Sequence Alignments to Predict Protein Structure and Stability With High Accuracy",
                        "Citation Paper Abstract": "Abstract:We present a sequence-based probabilistic formalism that directly addresses co-operative effects in networks of interacting positions in proteins, providing significantly improved contact prediction, as well as accurate quantitative prediction of free energy changes due to non-additive effects of multiple mutations. In addition to these practical considerations, the agreement of our sequence-based calculations with experimental data for both structure and stability demonstrates a strong relation between the statistical distribution of protein sequences produced by natural evolutionary processes, and the thermodynamic stability of the structures to which these sequences fold.",
                        "Citation Paper Authors": "Authors:Alan Lapedes, Bertrand Giraud, Christopher Jarzynski"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.13860v3": {
            "Paper Title": "Active Inference Tree Search in Large POMDPs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.04995v2": {
            "Paper Title": "Wheels: A New Criterion for Non-convexity of Neural Codes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.08928v6": {
            "Paper Title": "RNNs of RNNs: Recursive Construction of Stable Assemblies of Recurrent\n  Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.08118v4": {
            "Paper Title": "Brain Functional Connectivity Estimation Utilizing Diffusion Kernels on\n  a Structural Connectivity Graph",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.02786v3": {
            "Paper Title": "LGGNet: Learning from Local-Global-Graph Representations for\n  Brain-Computer Interface",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": ", we use the logarithmic\nof the average pooled square of the representations as ",
                    "Citation Text": "R. T. Schirrmeister, J. T. Springenberg, L. D. J. Fiederer, M. Glasstetter,\nK. Eggensperger, M. Tangermann, F. Hutter, W. Burgard, and T. Ball,\n\u201cDeep learning with convolutional neural networks for EEG decoding\nand visualization,\u201d Human Brain Mapping , vol. 38, no. 11, pp. 5391\u2013\n5420, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.05051",
                        "Citation Paper Title": "Title:Deep learning with convolutional neural networks for EEG decoding and visualization",
                        "Citation Paper Abstract": "Abstract:PLEASE READ AND CITE THE REVISED VERSION at Human Brain Mapping: this http URL\nCode available here: this https URL",
                        "Citation Paper Authors": "Authors:Robin Tibor Schirrmeister, Jost Tobias Springenberg, Lukas Dominique Josef Fiederer, Martin Glasstetter, Katharina Eggensperger, Michael Tangermann, Frank Hutter, Wolfram Burgard, Tonio Ball"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "that use GNNs to decode\nEEG signals. Jang et al. ",
                    "Citation Text": "S. Jang, S. Moon, and J. Lee, \u201cEEG-based video identi\ufb01cation using\ngraph signal modeling and graph convolutional neural network,\u201d in\n2018 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) , 2018, pp. 3066\u20133070.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.04229",
                        "Citation Paper Title": "Title:EEG-based video identification using graph signal modeling and graph convolutional neural network",
                        "Citation Paper Abstract": "Abstract:This paper proposes a novel graph signal-based deep learning method for electroencephalography (EEG) and its application to EEG-based video identification. We present new methods to effectively represent EEG data as signals on graphs, and learn them using graph convolutional neural networks. Experimental results for video identification using EEG responses obtained while watching videos show the effectiveness of the proposed approach in comparison to existing methods. Effective schemes for graph signal representation of EEG are also discussed.",
                        "Citation Paper Authors": "Authors:Soobeom Jang, Seong-Eun Moon, Jong-Seok Lee"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2104.07445v3": {
            "Paper Title": "Simulations Approaching Data: Cortical Slow Waves in Inferred Models of\n  the Whole Hemisphere of Mouse",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.16606v2": {
            "Paper Title": "Functional and spatial rewiring jointly generate convergent-divergent\n  units in self-organizing networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.04892v4": {
            "Paper Title": "Local Field Potential Journey into the Basal Ganglia",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.07873v3": {
            "Paper Title": "Phase transitions in when feedback is useful",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.07760v2": {
            "Paper Title": "The impact of aging on human brain network target controllability",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.01677v3": {
            "Paper Title": "A contrastive rule for meta-learning",
            "Sentences": [
                {
                    "Sentence ID": 58,
                    "Sentence": "reported in the main text (Tabs. 2 and 3) are taken\nfrom the original papers, except for Omniglot \ufb01rst-order MAML which is reported in ref. ",
                    "Citation Text": "Alex Nichol, Joshua Achiam, and John Schulman. On \ufb01rst-order meta-learning algorithms.\narXiv preprint arXiv:1803.02999 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.02999",
                        "Citation Paper Title": "Title:On First-Order Meta-Learning Algorithms",
                        "Citation Paper Abstract": "Abstract:This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only first-order derivatives for the meta-learning updates. This family includes and generalizes first-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.",
                        "Citation Paper Authors": "Authors:Alex Nichol, Joshua Achiam, John Schulman"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2103.02339v3": {
            "Paper Title": "Deep Recurrent Encoder: A scalable end-to-end network to model brain\n  signals",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.03535v2": {
            "Paper Title": "Graph Neural Networks in Network Neuroscience",
            "Sentences": [
                {
                    "Sentence ID": 111,
                    "Sentence": ". Furthermore, it is\nnecessary to develop GNNs that are trained with a frugal\nsetting where a few brain graphs will be used in the learning\nstep of the model as in ",
                    "Citation Text": "U. Guvercin, M. A. Gharsallaoui, and I. Rekik, \u201cOne\nrepresentative-shot learning using a population-driven template\nwith application to brain connectivity classi\ufb01cation and evolutionprediction,\u201d International Workshop on PRedictive Intelligence In\nMEdicine , pp. 25\u201336, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.11238",
                        "Citation Paper Title": "Title:One Representative-Shot Learning Using a Population-Driven Template with Application to Brain Connectivity Classification and Evolution Prediction",
                        "Citation Paper Abstract": "Abstract:Few-shot learning presents a challenging paradigm for training discriminative models on a few training samples representing the target classes to discriminate. However, classification methods based on deep learning are ill-suited for such learning as they need large amounts of training data --let alone one-shot learning. Recently, graph neural networks (GNNs) have been introduced to the field of network neuroscience, where the brain connectivity is encoded in a graph. However, with scarce neuroimaging datasets particularly for rare diseases and low-resource clinical facilities, such data-devouring architectures might fail in learning the target task. In this paper, we take a very different approach in training GNNs, where we aim to learn with one sample and achieve the best performance --a formidable challenge to tackle. Specifically, we present the first one-shot paradigm where a GNN is trained on a single population-driven template --namely a connectional brain template (CBT). A CBT is a compact representation of a population of brain graphs capturing the unique connectivity patterns shared across individuals. It is analogous to brain image atlases for neuroimaging datasets. Using a one-representative CBT as a training sample, we alleviate the training load of GNN models while boosting their performance across a variety of classification and regression tasks. We demonstrate that our method significantly outperformed benchmark one-shot learning methods with downstream classification and time-dependent brain graph data forecasting tasks while competing with the train-on-all conventional training strategy. Our source code can be found at this https URL.",
                        "Citation Paper Authors": "Authors:Umut Guvercin, Mohammed Amine Gharsallaoui, Islem Rekik"
                    }
                },
                {
                    "Sentence ID": 81,
                    "Sentence": ". Assuming that a brain graph does not\ncapture the high-order relation between brain regions, ",
                    "Citation Text": "A. Banka, I. Buzi, and I. Rekik, \u201cMulti-view brain hyperconnec-\ntome autoencoder for brain state classi\ufb01cation,\u201d in International\nWorkshop on PRedictive Intelligence In MEdicine . Springer, 2020,\npp. 101\u2013110.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.11553",
                        "Citation Paper Title": "Title:Multi-View Brain HyperConnectome AutoEncoder For Brain State Classification",
                        "Citation Paper Abstract": "Abstract:Graph embedding is a powerful method to represent graph neurological data (e.g., brain connectomes) in a low dimensional space for brain connectivity mapping, prediction and classification. However, existing embedding algorithms have two major limitations. First, they primarily focus on preserving one-to-one topological relationships between nodes (i.e., regions of interest (ROIs) in a connectome), but they have mostly ignored many-to-many relationships (i.e., set to set), which can be captured using a hyperconnectome structure. Second, existing graph embedding techniques cannot be easily adapted to multi-view graph data with heterogeneous distributions. In this paper, while cross-pollinating adversarial deep learning with hypergraph theory, we aim to jointly learn deep latent embeddings of subject0specific multi-view brain graphs to eventually disentangle different brain states. First, we propose a new simple strategy to build a hyperconnectome for each brain view based on nearest neighbour algorithm to preserve the connectivities across pairs of ROIs. Second, we design a hyperconnectome autoencoder (HCAE) framework which operates directly on the multi-view hyperconnectomes based on hypergraph convolutional layers to better capture the many-to-many relationships between brain regions (i.e., nodes). For each subject, we further regularize the hypergraph autoencoding by adversarial regularization to align the distribution of the learned hyperconnectome embeddings with that of the input hyperconnectomes. We formalize our hyperconnectome embedding within a geometric deep learning framework to optimize for a given subject, thereby designing an individual-based learning framework. Our experiments showed that the learned embeddings by HCAE yield to better results for brain state classification compared with other deep graph embedding methods methods.",
                        "Citation Paper Authors": "Authors:Alin Banka, Inis Buzi, Islem Rekik"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.10109v2": {
            "Paper Title": "Correlation-invariant synaptic plasticity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.01312v2": {
            "Paper Title": "Lonely individuals process the world in idiosyncratic ways",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.10244v4": {
            "Paper Title": "Sequential attractors in combinatorial threshold-linear networks",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": ", we have seen a close correspondence between certain minimal \ufb01xed\npoints, called core motifs , and the attractors of a network ",
                    "Citation Text": "C. Parmelee, S. Moore, K. Morrison, and C. Curto. Core motifs predict dynamic attractors in combi-\nnatorial threshold-linear networks. In preparation.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.03198",
                        "Citation Paper Title": "Title:Core motifs predict dynamic attractors in combinatorial threshold-linear networks",
                        "Citation Paper Abstract": "Abstract:Combinatorial threshold-linear networks (CTLNs) are a special class of inhibition-dominated TLNs defined from directed graphs. Like more general TLNs, they display a wide variety of nonlinear dynamics including multistability, limit cycles, quasiperiodic attractors, and chaos. In prior work, we have developed a detailed mathematical theory relating stable and unstable fixed points of CTLNs to graph-theoretic properties of the underlying network. Here we find that a special type of fixed points, corresponding to core motifs, are predictive of both static and dynamic attractors. Moreover, the attractors can be found by choosing initial conditions that are small perturbations of these fixed points. This motivates us to hypothesize that dynamic attractors of a network correspond to unstable fixed points supported on core motifs. We tested this hypothesis on a large family of directed graphs of size $n=5$, and found remarkable agreement. Furthermore, we discovered that core motifs with similar embeddings give rise to nearly identical attractors. This allowed us to classify attractors based on structurally-defined graph families. Our results suggest that graphical properties of the connectivity can be used to predict a network's complex repertoire of nonlinear dynamics.",
                        "Citation Paper Authors": "Authors:Caitlyn Parmelee, Samantha Moore, Katherine Morrison, Carina Curto"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.11067v2": {
            "Paper Title": "A dynamical scan path model for task-dependence during scene viewing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.09165v2": {
            "Paper Title": "A reservoir of timescales in random neural networks",
            "Sentences": [
                {
                    "Sentence ID": 37,
                    "Sentence": "H. K. Inagaki, L. Fontolan, S. Romani, and K. Svoboda,\nDiscrete attractor dynamics underlies persistent activity\nin the frontal cortex , Nature 566, 212 (2019). ",
                    "Citation Text": "S. Recanatesi, U. Pereira-Obilinovic, M. Murakami,\nZ. Mainen, and L. Mazzucato, Metastable attractors ex-\nplain the variable timing of stable behavioral action se-\nquences , Neuron (2022).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2001.09600",
                        "Citation Paper Title": "Title:Metastable attractors explain the variable timing of stable behavioral action sequences",
                        "Citation Paper Abstract": "Abstract:Natural animal behavior displays rich lexical and temporal dynamics, even in a stable environment. This implies that behavioral variability arises from sources within the brain, but the origin and mechanics of these processes remain largely unknown. Here, we focus on the observation that the timing of self-initiated actions shows large variability even when they are executed in stable, well-learned sequences. Could this mix of reliability and stochasticity arise within the same circuit? We trained rats to perform a stereotyped sequence of self-initiated actions and recorded neural ensemble activity in secondary motor cortex (M2), which is known to reflect trial-by-trial action timing fluctuations. Using hidden Markov models we established a robust and accurate dictionary between ensemble activity patterns and actions. We then showed that metastable attractors, representing activity patterns with the requisite combination of reliable sequential structure and high transition timing variability, could be produced by reciprocally coupling a high dimensional recurrent network and a low dimensional feedforward one. Transitions between attractors were generated by correlated variability arising from the feedback loop between the two networks. This mechanism predicted a specific structure of low-dimensional noise correlations that were empirically verified in M2 ensemble dynamics. This work suggests a robust network motif as a novel mechanism to support critical aspects of animal behavior and establishes a framework for investigating its circuit origins via correlated variability.",
                        "Citation Paper Authors": "Authors:Stefano Recanatesi, Ulises Pereira, Masayoshi Murakami, Zachary Mainen, Luca Mazzucato"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "M. Helias and D. Dahmen, Statistical \feld theory for neu-\nral networks (Springer, 2020). ",
                    "Citation Text": "Y. Ahmadian, F. Fumarola, and K. D. Miller, Properties\nof networks with partially structured and partially random\nconnectivity , Physical Review E 91, 012820 (2015).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1311.4672",
                        "Citation Paper Title": "Title:Properties of networks with partially structured and partially random connectivity",
                        "Citation Paper Abstract": "Abstract:We provide a general formula for the eigenvalue density of large random $N\\times N$ matrices of the form $A = M + LJR$, where $M$, $L$ and $R$ are arbitrary deterministic matrices and $J$ is a random matrix of zero-mean independent and identically distributed elements. For $A$ nonnormal, the eigenvalues do not suffice to specify the dynamics induced by $A$, so we also provide general formulae for the transient evolution of the magnitude of activity and frequency power spectrum in an $N$-dimensional linear dynamical system with a coupling matrix given by $A$. These quantities can also be thought of as characterizing the stability and the magnitude of the linear response of a nonlinear network to small perturbations about a fixed point. We derive these formulae and work them out analytically for some examples of $M$, $L$ and $R$ motivated by neurobiological models. We also argue that the persistence as $N\\rightarrow\\infty$ of a finite number of randomly distributed outlying eigenvalues outside the support of the eigenvalue density of $A$, as previously observed, arises in regions of the complex plane $\\Omega$ where there are nonzero singular values of $L^{-1} (z\\mathbf{1} - M) R^{-1}$ (for $z\\in\\Omega$) that vanish as $N\\rightarrow\\infty$. When such singular values do not exist and $L$ and $R$ are equal to the identity, there is a correspondence in the normalized Frobenius norm (but not in the operator norm) between the support of the spectrum of $A$ for $J$ of norm $\\sigma$ and the $\\sigma$-pseudospectrum of $M$.",
                        "Citation Paper Authors": "Authors:Yashar Ahmadian, Francesco Fumarola, Kenneth D. Miller"
                    }
                },
                {
                    "Sentence ID": 2,
                    "Sentence": "J.-P. Bouchaud, Weak ergodicity breaking and aging in\ndisordered systems , Journal de Physique I 2, 1705 (1992). ",
                    "Citation Text": "E. Almaas, B. Kovacs, T. Vicsek, Z. N. Oltvai, and A.-L.\nBarab\u0013 asi, Global organization of metabolic \ruxes in the\nbacterium escherichia coli , Nature 427, 839 (2004).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:q-bio/0403001",
                        "Citation Paper Title": "Title:Global organization of metabolic fluxes in the bacterium, Escherichia coli",
                        "Citation Paper Abstract": "Abstract:  Cellular metabolism, the integrated interconversion of thousands of metabolic substrates through enzyme-catalyzed biochemical reactions, is the most investigated complex intercellular web of molecular interactions. While the topological organization of individual reactions into metabolic networks is increasingly well understood, the principles governing their global functional utilization under different growth conditions pose many open questions. We implement a flux balance analysis of the E. coli MG1655 metabolism, finding that the network utilization is highly uneven: while most metabolic reactions have small fluxes, the metabolism's activity is dominated by several reactions with very high fluxes. E. coli responds to changes in growth conditions by reorganizing the rates of selected fluxes predominantly within this high flux backbone. The identified behavior likely represents a universal feature of metabolic activity in all cells, with potential implications to metabolic engineering.",
                        "Citation Paper Authors": "Authors:E. Almaas, B. Kovacs, T. Vicsek, Z. N. Oltvai, A.-L. Barabasi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.12979v4": {
            "Paper Title": "Predictive Coding: a Theoretical and Experimental Review",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.05539v5": {
            "Paper Title": "BioLCNet: Reward-modulated Locally Connected Spiking Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.12067v2": {
            "Paper Title": "Quasi-universal scaling in mouse-brain neuronal activity stems from\n  edge-of-instability critical dynamics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.01279v3": {
            "Paper Title": "The mean field approach for populations of spiking neurons",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.14108v2": {
            "Paper Title": "Efficient and robust multi-task learning in the brain with modular\n  latent primitives",
            "Sentences": [
                {
                    "Sentence ID": 2,
                    "Sentence": "also use so-called primitive policies\nin order to solve (sometimes multiple ",
                    "Citation Text": "Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with\npolicy sketches, 2016. URL https://arxiv.org/abs/1611.01796 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.01796",
                        "Citation Paper Title": "Title:Modular Multitask Reinforcement Learning with Policy Sketches",
                        "Citation Paper Abstract": "Abstract:We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate tasks with sequences of named subtasks, providing information about high-level structural relationships among tasks but not how to implement them---specifically not providing the detailed guidance used by much previous work on learning policy abstractions for RL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations). To learn from sketches, we present a model that associates every subtask with a modular subpolicy, and jointly maximizes reward over full task-specific policies by tying parameters across shared subpolicies. Optimization is accomplished via a decoupled actor--critic training objective that facilitates learning common behaviors from multiple dissimilar reward functions. We evaluate the effectiveness of our approach in three environments featuring both discrete and continuous control, and with sparse rewards that can be obtained only after completing a number of high-level subgoals. Experiments show that using our approach to learn policies guided by sketches gives better performance than existing techniques for learning task-specific or shared policies, while naturally inducing a library of interpretable primitive behaviors that can be recombined to rapidly adapt to new tasks.",
                        "Citation Paper Authors": "Authors:Jacob Andreas, Dan Klein, Sergey Levine"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.03220v2": {
            "Paper Title": "Joint Embedding of Structural and Functional Brain Networks with Graph\n  Neural Networks for Mental Illness Diagnosis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.05922v3": {
            "Paper Title": "Trivial or impossible -- dichotomous data difficulty masks model\n  differences (on ImageNet and beyond)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.08706v3": {
            "Paper Title": "How and When Random Feedback Works: A Case Study of Low-Rank Matrix\n  Factorization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.09979v5": {
            "Paper Title": "Artificial Perception Meets Psychophysics, Revealing a Fundamental Law\n  of Illusory Motion",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.06634v3": {
            "Paper Title": "End-to-end translation of human neural activity to speech with a\n  dual-dual generative adversarial network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.05113v3": {
            "Paper Title": "More Than Meets the Eye: Self-Supervised Depth Reconstruction From Brain\n  Activity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.07356v3": {
            "Paper Title": "Hippocampal formation-inspired probabilistic generative model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.04427v4": {
            "Paper Title": "On the relation between statistical learning and perceptual distances",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.02865v3": {
            "Paper Title": "Spike-inspired Rank Coding for Fast and Accurate Recurrent Neural\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.04784v3": {
            "Paper Title": "Interdigitated Columnar Representation of Personal Space and Visual\n  Space in Human Parietal Cortex",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.04730v2": {
            "Paper Title": "The contribution of local variations in hue or contrast to symmetry of\n  things in a thing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.04261v3": {
            "Paper Title": "Learning cortical representations through perturbed and adversarial\n  dreaming",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.05912v5": {
            "Paper Title": "Functional Connectivity of the Brain Across Rodents and Humans",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.10851v2": {
            "Paper Title": "Witten-type topological field theory of self-organized criticality for\n  stochastic neural networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.08145v2": {
            "Paper Title": "Active Observer Visual Problem-Solving Methods are Dynamically\n  Hypothesized, Deployed and Tested",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.04162v2": {
            "Paper Title": "Symmetry Perception by Deep Networks: Inadequacy of Feed-Forward\n  Architectures and Improvements with Recurrent Connections",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.04463v4": {
            "Paper Title": "Neural Latents Benchmark '21: Evaluating latent variable models of\n  neural population activity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.13031v2": {
            "Paper Title": "Towards Biologically Plausible Convolutional Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.04643v2": {
            "Paper Title": "Normative brain mapping of interictal intracranial EEG to localise\n  epileptogenic tissue",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.10471v2": {
            "Paper Title": "Multi-Phase Locking Value: A Generalized Method for Determining\n  Instantaneous Multi-frequency Phase Coupling",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": "use quadratic nonlinearity to de-\ntect cross-frequency coupling between theta and gamma\nwaves in the hippocampus. Recent works focus on recon-\nstructing coupling functions ",
                    "Citation Text": "T. Stankovski, T. Pereira, P. V. E. McClintock, and\nA. Stefanovska, Coupling functions: dynamical interac-\ntion mechanisms in the physical, biological and social\nsciences, Philosophical Transactions of the Royal Society\nA377: 20190039 , 10.1098/rsta.2019.0039 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.01810",
                        "Citation Paper Title": "Title:Coupling functions: Universal insights into dynamical interaction mechanisms",
                        "Citation Paper Abstract": "Abstract:The dynamical systems found in Nature are rarely isolated. Instead they interact and influence each other. The coupling functions that connect them contain detailed information about the functional mechanisms underlying the interactions and prescribe the physical rule specifying how an interaction occurs. Here, we aim to present a coherent and comprehensive review encompassing the rapid progress made recently in the analysis, understanding and applications of coupling functions. The basic concepts and characteristics of coupling functions are presented through demonstrative examples of different domains, revealing the mechanisms and emphasizing their multivariate nature. The theory of coupling functions is discussed through gradually increasing complexity from strong and weak interactions to globally-coupled systems and networks. A variety of methods that have been developed for the detection and reconstruction of coupling functions from measured data is described. These methods are based on different statistical techniques for dynamical inference. Stemming from physics, such methods are being applied in diverse areas of science and technology, including chemistry, biology, physiology, neuroscience, social sciences, mechanics and secure communications. This breadth of application illustrates the universality of coupling functions for studying the interaction mechanisms of coupled dynamical systems.",
                        "Citation Paper Authors": "Authors:Tomislav Stankovski, Tiago Pereira, Peter V. E. McClintock, Aneta Stefanovska"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2201.04229v1": {
            "Paper Title": "Brain Signals Analysis Based Deep Learning Methods: Recent advances in\n  the study of non-invasive brain signals",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.10994v2": {
            "Paper Title": "Detection of Anticipatory Dynamics Between a Pair of Zebrafish",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.14734v1": {
            "Paper Title": "Sequential Episodic Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.13523v1": {
            "Paper Title": "Interpreting Dynamical Systems as Bayesian Reasoners",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.12147v2": {
            "Paper Title": "Generative Models of Brain Dynamics -- A review",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.00622v1": {
            "Paper Title": "Learning shared neural manifolds from multi-subject FMRI data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.07617v2": {
            "Paper Title": "Algorithmic insights on continual learning from fruit flies",
            "Sentences": [
                {
                    "Sentence ID": 75,
                    "Sentence": "network (learning rate = 0 :001, batch size = 64, number of epochs = 25, with\nbatch normalization and Adam) on KMNIST ",
                    "Citation Text": "Clanuwat, T., Bober-Irizar, M., Kitamoto, A., et al. Deep Learning for Classical Japanese\nLiterature . Dec. 3, 2018. arXiv: cs.CV/1812.01718 [cs.CV] .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.01718",
                        "Citation Paper Title": "Title:Deep Learning for Classical Japanese Literature",
                        "Citation Paper Abstract": "Abstract:Much of machine learning research focuses on producing models which perform well on benchmark tasks, in turn improving our understanding of the challenges associated with those tasks. From the perspective of ML researchers, the content of the task itself is largely irrelevant, and thus there have increasingly been calls for benchmark tasks to more heavily focus on problems which are of social or cultural relevance. In this work, we introduce Kuzushiji-MNIST, a dataset which focuses on Kuzushiji (cursive Japanese), as well as two larger, more challenging datasets, Kuzushiji-49 and Kuzushiji-Kanji. Through these datasets, we wish to engage the machine learning community into the world of classical Japanese literature. Dataset available at this https URL",
                        "Citation Paper Authors": "Authors:Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, David Ha"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": ") uses the Fisher information criterion to identify\nweights that are important for previously learned tasks, and then introduces a penalty if\nthese weights are modi\fed when learning a new task.\n2.Gradient episodic memory (GEM ",
                    "Citation Text": "Lopez-Paz, D. and Ranzato, M. \\Gradient episodic memory for continual learning\". Advances\nin neural information processing systems . 2017, pp. 6467{6476.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.08840",
                        "Citation Paper Title": "Title:Gradient Episodic Memory for Continual Learning",
                        "Citation Paper Abstract": "Abstract:One major obstacle towards AI is the poor ability of models to solve new problems quicker, and without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient Episodic Memory (GEM) that alleviates forgetting, while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of the MNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when compared to the state-of-the-art.",
                        "Citation Paper Authors": "Authors:David Lopez-Paz, Marc'Aurelio Ranzato"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.12798v2": {
            "Paper Title": "From internal models toward metacognitive AI",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.10882v1": {
            "Paper Title": "Imagery agnosia and its phenomenology",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.13911v2": {
            "Paper Title": "Modeling Category-Selective Cortical Regions with Topographic\n  Variational Autoencoders",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.12434v2": {
            "Paper Title": "Emergent behavior and neural dynamics in artificial agents tracking\n  turbulent plumes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.03214v2": {
            "Paper Title": "Piano Timbre Development Analysis using Machine Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.00620v1": {
            "Paper Title": "Overview of the EEG Pilot Subtask at MediaEval 2021: Predicting Media\n  Memorability",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.08085v2": {
            "Paper Title": "Natural continual learning: success is a journey, not (just) a\n  destination",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.10503v1": {
            "Paper Title": "Periodically kicked feedforward chains of simple excitable\n  FitzHugh-Nagumo neurons",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.07408v1": {
            "Paper Title": "Towards a Network Control Theory of Electroconvulsive Therapy Response",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.04936v3": {
            "Paper Title": "Using Information Theory to Measure Psychophysical Performance",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.05091v1": {
            "Paper Title": "Network mechanisms of working memory: the role of neuronal\n  nonlinearities",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.04909v1": {
            "Paper Title": "A MEMS-based optical scanning system for precise, high-speed neural\n  interfacing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.10645v2": {
            "Paper Title": "Combining Different V1 Brain Model Variants to Improve Robustness to\n  Image Corruptions in CNNs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.03879v2": {
            "Paper Title": "Emergence of memory manifolds",
            "Sentences": [
                {
                    "Sentence ID": 22,
                    "Sentence": "H. Sompolinsky, A. Crisanti, and H.-J. Sommers, Phys-\nical review letters 61, 259 (1988). ",
                    "Citation Text": "J. Schuecker, S. Goedeke, and M. Helias, Physical Re-\nview X 8, 041029 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.01880",
                        "Citation Paper Title": "Title:Optimal sequence memory in driven random networks",
                        "Citation Paper Abstract": "Abstract:Autonomous randomly coupled neural networks display a transition to chaos at a critical coupling strength. We here investigate the effect of a time-varying input on the onset of chaos and the resulting consequences for information processing. Dynamic mean-field theory yields the statistics of the activity, the maximum Lyapunov exponent, and the memory capacity of the network. We find an exact condition that determines the transition from stable to chaotic dynamics and the sequential memory capacity in closed form. The input suppresses chaos by a dynamic mechanism, shifting the transition to significantly larger coupling strengths than predicted by local stability analysis. Beyond linear stability, a regime of coexistent locally expansive, but non-chaotic dynamics emerges that optimizes the capacity of the network to store sequential input.",
                        "Citation Paper Authors": "Authors:Jannis Schuecker, Sven Goedeke, Moritz Helias"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2112.01987v1": {
            "Paper Title": "Leaving Flatland: Advances in 3D behavioral measurement",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.01081v1": {
            "Paper Title": "Matrix metalloproteinases as new targets in Alzheimer's disease:\n  Opportunities and Challenges",
            "Sentences": []
        },
        "http://arxiv.org/abs/2112.00119v1": {
            "Paper Title": "Forgetting leads to chaos in attractor networks",
            "Sentences": [
                {
                    "Sentence ID": 71,
                    "Sentence": "Y. Amit and Y. Huang, Precise capacity analysis in bi-\nnary networks with multiple coding level inputs, Neural\ncomputation 22, 660 (2010). ",
                    "Citation Text": "F. Schuessler, A. Dubreuil, F. Mastrogiuseppe, S. Osto-\njic, and O. Barak, Dynamics of random recurrent net-\nworks with correlated low-rank structure, Physical Re-\nview Research 2, 013111 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.04358",
                        "Citation Paper Title": "Title:Dynamics of random recurrent networks with correlated low-rank structure",
                        "Citation Paper Abstract": "Abstract:A given neural network in the brain is involved in many different tasks. This implies that, when considering a specific task, the network's connectivity contains a component which is related to the task and another component which can be considered random. Understanding the interplay between the structured and random components, and their effect on network dynamics and functionality is an important open question. Recent studies addressed the co-existence of random and structured connectivity, but considered the two parts to be uncorrelated. This constraint limits the dynamics and leaves the random connectivity non-functional. Algorithms that train networks to perform specific tasks typically generate correlations between structure and random connectivity. Here we study nonlinear networks with correlated structured and random components, assuming the structure to have a low rank. We develop an analytic framework to establish the precise effect of the correlations on the eigenvalue spectrum of the joint connectivity. We find that the spectrum consists of a bulk and multiple outliers, whose location is predicted by our theory. Using mean-field theory, we show that these outliers directly determine both the fixed points of the system and their stability. Taken together, our analysis elucidates how correlations allow structured and random connectivity to synergistically extend the range of computations available to networks.",
                        "Citation Paper Authors": "Authors:Friedrich Schuessler, Alexis Dubreuil, Francesca Mastrogiuseppe, Srdjan Ostojic, Omri Barak"
                    }
                },
                {
                    "Sentence ID": 68,
                    "Sentence": "B. Poole, S. Lahiri, M. Raghu, J. Sohl-Dickstein, and\nS. Ganguli, Exponential expressivity in deep neural net-\nworks through transient chaos, Advances in neural infor-\nmation processing systems 29, 3360 (2016). ",
                    "Citation Text": "C. Keup, T. K\u007f uhn, D. Dahmen, and M. Helias, Transient\nchaotic dimensionality expansion by recurrent networks,\nPhysical Review X 11, 021064 (2021).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.11006",
                        "Citation Paper Title": "Title:Transient chaotic dimensionality expansion by recurrent networks",
                        "Citation Paper Abstract": "Abstract:Neurons in the brain communicate with spikes, which are discrete events in time and value. Functional network models often employ rate units that are continuously coupled by analog signals. Is there a qualitative difference implied by these two forms of signaling? We develop a unified mean-field theory for large random networks to show that first- and second-order statistics in rate and binary networks are in fact identical if rate neurons receive the right amount of noise. Their response to presented stimuli, however, can be radically different. We quantify these differences by studying how nearby state trajectories evolve over time, asking to what extent the dynamics is chaotic. Chaos in the two models is found to be qualitatively different. In binary networks we find a network-size-dependent transition to chaos and a chaotic submanifold whose dimensionality expands stereotypically with time, while rate networks with matched statistics are nonchaotic. Dimensionality expansion in chaotic binary networks aids classification in reservoir computing and optimal performance is reached within about a single activation per neuron; a fast mechanism for computation that we demonstrate also in spiking networks. A generalization of this mechanism extends to rate networks in their respective chaotic regimes.",
                        "Citation Paper Authors": "Authors:Christian Keup, Tobias K\u00fchn, David Dahmen, Moritz Helias"
                    }
                },
                {
                    "Sentence ID": 67,
                    "Sentence": "D. Sussillo and L. F. Abbott, Generating coherent pat-\nterns of activity from chaotic neural networks, Neuron\n63, 544 (2009). ",
                    "Citation Text": "B. Poole, S. Lahiri, M. Raghu, J. Sohl-Dickstein, and\nS. Ganguli, Exponential expressivity in deep neural net-\nworks through transient chaos, Advances in neural infor-\nmation processing systems 29, 3360 (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.05340",
                        "Citation Paper Title": "Title:Exponential expressivity in deep neural networks through transient chaos",
                        "Citation Paper Abstract": "Abstract:We combine Riemannian geometry with the mean field theory of high dimensional chaos to study the nature of signal propagation in generic, deep neural networks with random weights. Our results reveal an order-to-chaos expressivity phase transition, with networks in the chaotic phase computing nonlinear functions whose global curvature grows exponentially with depth but not width. We prove this generic class of deep random functions cannot be efficiently computed by any shallow network, going beyond prior work restricted to the analysis of single functions. Moreover, we formalize and quantitatively demonstrate the long conjectured idea that deep networks can disentangle highly curved manifolds in input space into flat manifolds in hidden space. Our theoretical analysis of the expressive power of deep networks broadly applies to arbitrary nonlinearities, and provides a quantitative underpinning for previously abstract notions about the geometry of deep functions.",
                        "Citation Paper Authors": "Authors:Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, Surya Ganguli"
                    }
                },
                {
                    "Sentence ID": 61,
                    "Sentence": "K. Rajan, L. Abbott, and H. Sompolinsky, Stimulus-\ndependent suppression of chaos in recurrent neural net-\nworks, Physical Review E 82, 011903 (2010). ",
                    "Citation Text": "J. Schuecker, S. Goedeke, and M. Helias, Optimal se-\nquence memory in driven random networks, Physical Re-\nview X 8, 041029 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.01880",
                        "Citation Paper Title": "Title:Optimal sequence memory in driven random networks",
                        "Citation Paper Abstract": "Abstract:Autonomous randomly coupled neural networks display a transition to chaos at a critical coupling strength. We here investigate the effect of a time-varying input on the onset of chaos and the resulting consequences for information processing. Dynamic mean-field theory yields the statistics of the activity, the maximum Lyapunov exponent, and the memory capacity of the network. We find an exact condition that determines the transition from stable to chaotic dynamics and the sequential memory capacity in closed form. The input suppresses chaos by a dynamic mechanism, shifting the transition to significantly larger coupling strengths than predicted by local stability analysis. Beyond linear stability, a regime of coexistent locally expansive, but non-chaotic dynamics emerges that optimizes the capacity of the network to store sequential input.",
                        "Citation Paper Authors": "Authors:Jannis Schuecker, Sven Goedeke, Moritz Helias"
                    }
                },
                {
                    "Sentence ID": 60,
                    "Sentence": "M. Stern, H. Sompolinsky, and L. Abbott, Dynamics of\nrandom neural networks with bistable units, Physical Re-\nview E 90, 062710 (2014). ",
                    "Citation Text": "K. Rajan, L. Abbott, and H. Sompolinsky, Stimulus-\ndependent suppression of chaos in recurrent neural net-\nworks, Physical Review E 82, 011903 (2010).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:0912.3513",
                        "Citation Paper Title": "Title:Stimulus-Dependent Suppression of Chaos in Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:Neuronal activity arises from an interaction between ongoing firing generated spontaneously by neural circuits and responses driven by external stimuli. Using mean-field analysis, we ask how a neural network that intrinsically generates chaotic patterns of activity can remain sensitive to extrinsic input. We find that inputs not only drive network responses, they also actively suppress ongoing activity, ultimately leading to a phase transition in which chaos is completely eliminated. The critical input intensity at the phase transition is a non-monotonic function of stimulus frequency, revealing a \"resonant\" frequency at which the input is most effective at suppressing chaos even though the power spectrum of the spontaneous activity peaks at zero and falls exponentially. A prediction of our analysis is that the variance of neural responses should be most strongly suppressed at frequencies matching the range over which many sensory systems operate.",
                        "Citation Paper Authors": "Authors:Kanaka Rajan, L F Abbott, Haim Sompolinsky"
                    }
                },
                {
                    "Sentence ID": 56,
                    "Sentence": "G. Wainrib and J. Touboul, Topological and dynamical\ncomplexity of random neural networks, Physical review\nletters 110, 118101 (2013). ",
                    "Citation Text": "R. Engelken, F. Wolf, and L. Abbott, Lyapunov spec-\ntra of chaotic recurrent neural networks, arXiv preprint\narXiv:2006.02427 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.02427",
                        "Citation Paper Title": "Title:Lyapunov spectra of chaotic recurrent neural networks",
                        "Citation Paper Abstract": "Abstract:Brains process information through the collective dynamics of large neural networks. Collective chaos was suggested to underlie the complex ongoing dynamics observed in cerebral cortical circuits and determine the impact and processing of incoming information streams. In dissipative systems, chaotic dynamics takes place on a subset of phase space of reduced dimensionality and is organized by a complex tangle of stable, neutral and unstable manifolds. Key topological invariants of this phase space structure such as attractor dimension, and Kolmogorov-Sinai entropy so far remained elusive.\nHere we calculate the complete Lyapunov spectrum of recurrent neural networks. We show that chaos in these networks is extensive with a size-invariant Lyapunov spectrum and characterized by attractor dimensions much smaller than the number of phase space dimensions. We find that near the onset of chaos, for very intense chaos, and discrete-time dynamics, random matrix theory provides analytical approximations to the full Lyapunov spectrum. We show that a generalized time-reversal symmetry of the network dynamics induces a point-symmetry of the Lyapunov spectrum reminiscent of the symplectic structure of chaotic Hamiltonian systems. Fluctuating input reduces both the entropy rate and the attractor dimension. For trained recurrent networks, we find that Lyapunov spectrum analysis provides a quantification of error propagation and stability achieved. Our methods apply to systems of arbitrary connectivity, and we describe a comprehensive set of controls for the accuracy and convergence of Lyapunov exponents.\nOur results open a novel avenue for characterizing the complex dynamics of recurrent neural networks and the geometry of the corresponding chaotic attractors. They also highlight the potential of Lyapunov spectrum analysis as a diagnostic for machine learning applications of recurrent networks.",
                        "Citation Paper Authors": "Authors:Rainer Engelken, Fred Wolf, L.F. Abbott"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "O. Barak and M. Tsodyks, Working models of working\nmemory, Curr. Opin. Neurobiol. 25, 20 (2014). ",
                    "Citation Text": "D. Kobak, W. Brendel, C. Constantinidis, C. E. Feier-\nstein, A. Kepecs, Z. F. Mainen, X. L. Qi, R. Romo,\nN. Uchida, and C. K. Machens, Demixed principal com-\nponent analysis of neural population data, Elife 5(2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1410.6031",
                        "Citation Paper Title": "Title:Demixed principal component analysis of population activity in higher cortical areas reveals independent representation of task parameters",
                        "Citation Paper Abstract": "Abstract:Neurons in higher cortical areas, such as the prefrontal cortex, are known to be tuned to a variety of sensory and motor variables. The resulting diversity of neural tuning often obscures the represented information. Here we introduce a novel dimensionality reduction technique, demixed principal component analysis (dPCA), which automatically discovers and highlights the essential features in complex population activities. We reanalyze population data from the prefrontal areas of rats and monkeys performing a variety of working memory and decision-making tasks. In each case, dPCA summarizes the relevant features of the population response in a single figure. The population activity is decomposed into a few demixed components that capture most of the variance in the data and that highlight dynamic tuning of the population to various task parameters, such as stimuli, decisions, rewards, etc. Moreover, dPCA reveals strong, condition-independent components of the population activity that remain unnoticed with conventional approaches.",
                        "Citation Paper Authors": "Authors:Dmitry Kobak, Wieland Brendel, Christos Constantinidis, Claudia E. Feierstein, Adam Kepecs, Zachary F. Mainen, Ranulfo Romo, Xue-Lian Qi, Naoshige Uchida, Christian K. Machens"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.09780v3": {
            "Paper Title": "Locally Learned Synaptic Dropout for Complete Bayesian Inference",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.02913v2": {
            "Paper Title": "Interference suppression techniques for OPM-based MEG: Opportunities and\n  challenges",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.14367v2": {
            "Paper Title": "OpenSync: An opensource platform for synchronizing multiple measures in\n  neuroscience experiments",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.13537v1": {
            "Paper Title": "A model of semantic completion in generative episodic memory",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.09006v2": {
            "Paper Title": "Natural Image Reconstruction from fMRI using Deep Learning: A Survey",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.11152v1": {
            "Paper Title": "Clustering based method for finding spikes in insect neurons",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.10530v1": {
            "Paper Title": "Kalman filters as the steady-state solution of gradient descent on\n  variational free energy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.09356v1": {
            "Paper Title": "Charting and navigating the space of solutions for recurrent neural\n  networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.06979v1": {
            "Paper Title": "Neural Population Geometry Reveals the Role of Stochasticity in Robust\n  Perception",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": ", to their geometrical properties. Here we provide\na brief description of the key quantities (See SM 1 and ",
                    "Citation Text": "SueYeon Chung, Daniel D Lee, and Haim Sompolinsky. Classi\ufb01cation and geometry of general\nperceptual manifolds. Physical Review X , 8(3):031003, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.06487",
                        "Citation Paper Title": "Title:Classification and Geometry of General Perceptual Manifolds",
                        "Citation Paper Abstract": "Abstract:Perceptual manifolds arise when a neural population responds to an ensemble of sensory signals associated with different physical features (e.g., orientation, pose, scale, location, and intensity) of the same perceptual object. Object recognition and discrimination requires classifying the manifolds in a manner that is insensitive to variability within a manifold. How neuronal systems give rise to invariant object classification and recognition is a fundamental problem in brain theory as well as in machine learning. Here we study the ability of a readout network to classify objects from their perceptual manifold representations. We develop a statistical mechanical theory for the linear classification of manifolds with arbitrary geometry revealing a remarkable relation to the mathematics of conic decomposition. Novel geometrical measures of manifold radius and manifold dimension are introduced which can explain the classification capacity for manifolds of various geometries. The general theory is demonstrated on a number of representative manifolds, including L2 ellipsoids prototypical of strictly convex manifolds, L1 balls representing polytopes consisting of finite sample points, and orientation manifolds which arise from neurons tuned to respond to a continuous angle variable, such as object orientation. The effects of label sparsity on the classification capacity of manifolds are elucidated, revealing a scaling relation between label sparsity and manifold radius. Theoretical predictions are corroborated by numerical simulations using recently developed algorithms to compute maximum margin solutions for manifold dichotomies. Our theory and its extensions provide a powerful and rich framework for applying statistical mechanics of linear classification to data arising from neuronal responses to object stimuli, as well as to artificial deep networks trained for object recognition tasks.",
                        "Citation Paper Authors": "Authors:SueYeon Chung, Daniel D. Lee, Haim Sompolinsky"
                    }
                },
                {
                    "Sentence ID": 31,
                    "Sentence": ", and\nthat more structural studies are needed for investigating the internal layers of networks ",
                    "Citation Text": "Anna A Ivanova, John Hewitt, and Noga Zaslavsky. Probing arti\ufb01cial neural networks: insights\nfrom neuroscience. arXiv preprint arXiv:2104.08197 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.08197",
                        "Citation Paper Title": "Title:Probing artificial neural networks: insights from neuroscience",
                        "Citation Paper Abstract": "Abstract:A major challenge in both neuroscience and machine learning is the development of useful tools for understanding complex information processing systems. One such tool is probes, i.e., supervised models that relate features of interest to activation patterns arising in biological or artificial neural networks. Neuroscience has paved the way in using such models through numerous studies conducted in recent decades. In this work, we draw insights from neuroscience to help guide probing research in machine learning. We highlight two important design choices for probes $-$ direction and expressivity $-$ and relate these choices to research goals. We argue that specific research goals play a paramount role when designing a probe and encourage future probing studies to be explicit in stating these goals.",
                        "Citation Paper Authors": "Authors:Anna A. Ivanova, John Hewitt, Noga Zaslavsky"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "used random noise applied to pixels and intermediate layers\nto improve adversarial robustness, and Cohen et al. ",
                    "Citation Text": "Jeremy M Cohen, Elan Rosenfeld, and J Zico Kolter. Certi\ufb01ed adversarial robustness via\nrandomized smoothing. February 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.02918",
                        "Citation Paper Title": "Title:Certified Adversarial Robustness via Randomized Smoothing",
                        "Citation Paper Abstract": "Abstract:We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the $\\ell_2$ norm. This \"randomized smoothing\" technique has been proposed recently in the literature, but existing guarantees are loose. We prove a tight robustness guarantee in $\\ell_2$ norm for smoothing with Gaussian noise. We use randomized smoothing to obtain an ImageNet classifier with e.g. a certified top-1 accuracy of 49% under adversarial perturbations with $\\ell_2$ norm less than 0.5 (=127/255). No certified defense has been shown feasible on ImageNet except for smoothing. On smaller-scale datasets where competing approaches to certified $\\ell_2$ robustness are viable, smoothing delivers higher certified accuracies. Our strong empirical results suggest that randomized smoothing is a promising direction for future research into adversarially robust classification. Code and models are available at this http URL.",
                        "Citation Paper Authors": "Authors:Jeremy M Cohen, Elan Rosenfeld, J. Zico Kolter"
                    }
                },
                {
                    "Sentence ID": 22,
                    "Sentence": "Previous work in machine learning has reported that additive noise can improve the adversarial\nrobustness of a model: Liu et al. ",
                    "Citation Text": "Xuanqing Liu, Minhao Cheng, Huan Zhang, and Cho-Jui Hsieh. Towards robust neural networks\nvia random self-ensemble. December 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.00673",
                        "Citation Paper Title": "Title:Towards Robust Neural Networks via Random Self-ensemble",
                        "Citation Paper Abstract": "Abstract:Recent studies have revealed the vulnerability of deep neural networks: A small adversarial perturbation that is imperceptible to human can easily make a well-trained deep neural network misclassify. This makes it unsafe to apply neural networks in security-critical applications. In this paper, we propose a new defense algorithm called Random Self-Ensemble (RSE) by combining two important concepts: {\\bf randomness} and {\\bf ensemble}. To protect a targeted model, RSE adds random noise layers to the neural network to prevent the strong gradient-based attacks, and ensembles the prediction over random noises to stabilize the performance. We show that our algorithm is equivalent to ensemble an infinite number of noisy models $f_\\epsilon$ without any additional memory overhead, and the proposed training procedure based on noisy stochastic gradient descent can ensure the ensemble model has a good predictive capability. Our algorithm significantly outperforms previous defense techniques on real data sets. For instance, on CIFAR-10 with VGG network (which has 92\\% accuracy without any attack), under the strong C\\&W attack within a certain distortion tolerance, the accuracy of unprotected model drops to less than 10\\%, the best previous defense technique has $48\\%$ accuracy, while our method still has $86\\%$ prediction accuracy under the same level of attack. Finally, our method is simple and easy to integrate into any neural network.",
                        "Citation Paper Authors": "Authors:Xuanqing Liu, Minhao Cheng, Huan Zhang, Cho-Jui Hsieh"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.06920v1": {
            "Paper Title": "Neural optimal feedback control with local learning rules",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": "has been suggested to generalize the Kalman \ufb01lter. The inputs could also be processed\nusing additional neural network layers to obtain a representation that renders the dynamics linear ",
                    "Citation Text": "B. Lusch, J. N. Kutz, and S. L. Brunton. Deep learning for universal linear embeddings of nonlinear\ndynamics. Nature Communications, 9(1):1\u201310, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.09707",
                        "Citation Paper Title": "Title:Deep learning for universal linear embeddings of nonlinear dynamics",
                        "Citation Paper Abstract": "Abstract:Identifying coordinate transformations that make strongly nonlinear dynamics approximately linear is a central challenge in modern dynamical systems. These transformations have the potential to enable prediction, estimation, and control of nonlinear systems using standard linear theory. The Koopman operator has emerged as a leading data-driven embedding, as eigenfunctions of this operator provide intrinsic coordinates that globally linearize the dynamics. However, identifying and representing these eigenfunctions has proven to be mathematically and computationally challenging. This work leverages the power of deep learning to discover representations of Koopman eigenfunctions from trajectory data of dynamical systems. Our network is parsimonious and interpretable by construction, embedding the dynamics on a low-dimensional manifold that is of the intrinsic rank of the dynamics and parameterized by the Koopman eigenfunctions. In particular, we identify nonlinear coordinates on which the dynamics are globally linear using a modified auto-encoder. We also generalize Koopman representations to include a ubiquitous class of systems that exhibit continuous spectra, ranging from the simple pendulum to nonlinear optics and broadband turbulence. Our framework parametrizes the continuous frequency using an auxiliary network, enabling a compact and efficient embedding at the intrinsic rank, while connecting our models to half a century of asymptotics. In this way, we benefit from the power and generality of deep learning, while retaining the physical interpretability of Koopman embeddings.",
                        "Citation Paper Authors": "Authors:Bethany Lusch, J. Nathan Kutz, Steven L. Brunton"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.06518v1": {
            "Paper Title": "Greater than the parts: A review of the information decomposition\n  approach to causal emergence",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.05902v1": {
            "Paper Title": "Heteroclinic cycling and extinction in May-Leonard models with\n  demographic stochasticity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.05299v1": {
            "Paper Title": "Can Information Flows Suggest Targets for Interventions in Neural\n  Circuits?",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": ", which has also been\nused in many previous works ",
                    "Citation Text": "A. Agarwal, A. Beygelzimer, M. Dud\u00edk, J. Langford, and H. Wallach, \u201cA reductions approach to fair\nclassi\ufb01cation,\u201d in International Conference on Machine Learning . PMLR, 2018, pp. 60\u201369.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.02453",
                        "Citation Paper Title": "Title:A Reductions Approach to Fair Classification",
                        "Citation Paper Abstract": "Abstract:We present a systematic approach for achieving fairness in a binary classification setting. While we focus on two well-known quantitative definitions of fairness, our approach encompasses many other previously studied definitions as special cases. The key idea is to reduce fair classification to a sequence of cost-sensitive classification problems, whose solutions yield a randomized classifier with the lowest (empirical) error subject to the desired constraints. We introduce two reductions that work for any representation of the cost-sensitive classifier and compare favorably to prior baselines on a variety of data sets, while overcoming several of their disadvantages.",
                        "Citation Paper Authors": "Authors:Alekh Agarwal, Alina Beygelzimer, Miroslav Dud\u00edk, John Langford, Hanna Wallach"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.02953v2": {
            "Paper Title": "Visual Search Asymmetry: Deep Nets and Humans Share Similar Inherent\n  Biases",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.03864v1": {
            "Paper Title": "Realism is almost true: A critique of the interface theory of perception",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.05075v2": {
            "Paper Title": "Statistical Neuroscience in the Single Trial Limit",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.02749v2": {
            "Paper Title": "Predify: Augmenting deep neural networks with brain-inspired predictive\n  coding dynamics",
            "Sentences": [
                {
                    "Sentence ID": 54,
                    "Sentence": "for both networks; although it would prove computationally\nprohibitive to systematically explore all standard types of adversarial attacks, we also evaluated\nrandom Projected Gradient Descent attacks (RPGD, with L2norm) ",
                    "Citation Text": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.\nTowards deep learning models resistant to adversarial attacks. In International Conference on\nLearning Representations , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06083",
                        "Citation Paper Title": "Title:Towards Deep Learning Models Resistant to Adversarial Attacks",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL.",
                        "Citation Paper Authors": "Authors:Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "further in the Appendix A.5, together with our own detailed exploration of their network\u2019s behavior.\nOther approaches to predictive coding for object recognition include Boutin et al. ",
                    "Citation Text": "Victor Boutin, Angelo Franciosini, Frederic Chavane, Franck Ruf\ufb01er, and Laurent Perrinet.\nSparse deep predictive coding captures contour integration capabilities of the early visual system.\narXiv preprint arXiv:1902.07651 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.07651",
                        "Citation Paper Title": "Title:Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system",
                        "Citation Paper Abstract": "Abstract:Both neurophysiological and psychophysical experiments have pointed out the crucial role of recurrent and feedback connections to process context-dependent information in the early visual cortex. While numerous models have accounted for feedback effects at either neural or representational level, none of them were able to bind those two levels of analysis. Is it possible to describe feedback effects at both levels using the same model? We answer this question by combining Predictive Coding (PC) and Sparse Coding (SC) into a hierarchical and convolutional framework. In this Sparse Deep Predictive Coding (SDPC) model, the SC component models the internal recurrent processing within each layer, and the PC component describes the interactions between layers using feedforward and feedback connections. Here, we train a 2-layered SDPC on two different databases of images, and we interpret it as a model of the early visual system (V1 & V2). We first demonstrate that once the training has converged, SDPC exhibits oriented and localized receptive fields in V1 and more complex features in V2. Second, we analyze the effects of feedback on the neural organization beyond the classical receptive field of V1 neurons using interaction maps. These maps are similar to association fields and reflect the Gestalt principle of good continuation. We demonstrate that feedback signals reorganize interaction maps and modulate neural activity to promote contour integration. Third, we demonstrate at the representational level that the SDPC feedback connections are able to overcome noise in input images. Therefore, the SDPC captures the association field principle at the neural level which results in better disambiguation of blurred images at the representational level.",
                        "Citation Paper Authors": "Authors:Victor Boutin, Angelo Franciosini, Frederic Chavane, Franck Ruffier, Laurent Perrinet"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.13637v4": {
            "Paper Title": "When are Deep Networks really better than Decision Forests at small\n  sample sizes, and how?",
            "Sentences": [
                {
                    "Sentence ID": 39,
                    "Sentence": ". MLPClassifier from the scikit-learn (BSD 3-Clause) package was used with one hidden\nlayer ",
                    "Citation Text": "F . Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P . Pretten-\nhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and\nE. Duchesnay. Scikit-learn: Machine learning in Python. Journal ofMachine Learning Research,\n12:2825\u20132830, 2011.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1201.0490",
                        "Citation Paper Title": "Title:Scikit-learn: Machine Learning in Python",
                        "Citation Paper Abstract": "Abstract:Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from this http URL.",
                        "Citation Paper Authors": "Authors:Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Andreas M\u00fcller, Joel Nothman, Gilles Louppe, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, \u00c9douard Duchesnay"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": ".\nFor optimizing hyperparameters in a network, we essentially followed the guidance of Bouthillier\net al. ",
                    "Citation Text": "Xavier Bouthillier, Pierre Delaunay, Mirko Bronzi, Assya Tro\ufb01mov, Brennan Nichyporuk, Justin\nSzeto, Naz Sepah, Edward Raff, Kanika Madan, Vikram Voleti, Samira Ebrahimi Kahou, Vincent\nMichalski, Dmitriy Serdyuk, Tal Arbel, Chris Pal, Ga\u00ebl Varoquaux, and Pascal Vincent. Accounting\nfor variance in machine learning benchmarks. arXiv preprint at http://arxiv.org/abs/2103.03098,\n2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.03098",
                        "Citation Paper Title": "Title:Accounting for Variance in Machine Learning Benchmarks",
                        "Citation Paper Abstract": "Abstract:Strong empirical evidence that one machine-learning algorithm A outperforms another one B ideally calls for multiple trials optimizing the learning pipeline over sources of variation such as data sampling, data augmentation, parameter initialization, and hyperparameters choices. This is prohibitively expensive, and corners are cut to reach conclusions. We model the whole benchmarking process, revealing that variance due to data sampling, parameter initialization and hyperparameter choice impact markedly the results. We analyze the predominant comparison methods used today in the light of this variance. We show a counter-intuitive result that adding more sources of variation to an imperfect estimator approaches better the ideal estimator at a 51 times reduction in compute cost. Building on these results, we study the error rate of detecting improvements, on five different deep-learning tasks/architectures. This study leads us to propose recommendations for performance comparisons.",
                        "Citation Paper Authors": "Authors:Xavier Bouthillier, Pierre Delaunay, Mirko Bronzi, Assya Trofimov, Brennan Nichyporuk, Justin Szeto, Naz Sepah, Edward Raff, Kanika Madan, Vikram Voleti, Samira Ebrahimi Kahou, Vincent Michalski, Dmitriy Serdyuk, Tal Arbel, Chris Pal, Ga\u00ebl Varoquaux, Pascal Vincent"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": ". We used 500 trees\nwith no depth limit, only varying the number of features selected per split (\u201cmax-features\u201d). Letting d\nequal the number of features in the dataset, we varied max-features to be one of:p\nd,d=4,d=3,d=1:5,\nandd ",
                    "Citation Text": "Philipp Probst, Marvin N. Wright, and Anne-Laure Boulesteix. Hyperparameters and tuning strate-\ngies for random forest. WIREs Data Mining andKnowledge Discovery, 9(3):e1301, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.03515",
                        "Citation Paper Title": "Title:Hyperparameters and Tuning Strategies for Random Forest",
                        "Citation Paper Abstract": "Abstract:The random forest algorithm (RF) has several hyperparameters that have to be set by the user, e.g., the number of observations drawn randomly for each tree and whether they are drawn with or without replacement, the number of variables drawn randomly for each split, the splitting rule, the minimum number of samples that a node must contain and the number of trees. In this paper, we first provide a literature review on the parameters' influence on the prediction performance and on variable importance measures.\nIt is well known that in most cases RF works reasonably well with the default values of the hyperparameters specified in software packages. Nevertheless, tuning the hyperparameters can improve the performance of RF. In the second part of this paper, after a brief overview of tuning strategies we demonstrate the application of one of the most established tuning strategies, model-based optimization (MBO). To make it easier to use, we provide the tuneRanger R package that tunes RF with MBO automatically. In a benchmark study on several datasets, we compare the prediction performance and runtime of tuneRanger with other tuning implementations in R and RF with default hyperparameters.",
                        "Citation Paper Authors": "Authors:Philipp Probst, Marvin Wright, Anne-Laure Boulesteix"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.02716v2": {
            "Paper Title": "Noether's Learning Dynamics: Role of Symmetry Breaking in Neural\n  Networks",
            "Sentences": [
                {
                    "Sentence ID": 62,
                    "Sentence": ". An important bene\ufb01t of normalization layers is to enable stable training with large learning rates ",
                    "Citation Text": "Nils Bjorck, Carla P Gomes, Bart Selman, and Kilian Q Weinberger. Understanding batch\nnormalization. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,\nand R. Garnett, editors, Advances in Neural Information Processing Systems , volume 31.\nCurran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/\nfile/36072923bfc3cf47745d704feb489480-Paper.pdf .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.02375",
                        "Citation Paper Title": "Title:Understanding Batch Normalization",
                        "Citation Paper Abstract": "Abstract:Batch normalization (BN) is a technique to normalize activations in intermediate layers of deep neural networks. Its tendency to improve accuracy and speed up training have established BN as a favorite technique in deep learning. Yet, despite its enormous success, there remains little consensus on the exact reason and mechanism behind these improvements. In this paper we take a step towards a better understanding of BN, following an empirical approach. We conduct several experiments, and show that BN primarily enables training with larger learning rates, which is the cause for faster convergence and better generalization. For networks without BN we demonstrate how large gradient updates can result in diverging loss and activations growing uncontrollably with network depth, which limits possible learning rates. BN avoids this problem by constantly correcting activations to be zero-mean and of unit standard deviation, which enables larger gradient steps, yields faster convergence and may help bypass sharp local minima. We further show various ways in which gradients and activations of deep unnormalized networks are ill-behaved. We contrast our results against recent findings in random matrix theory, shedding new light on classical initialization schemes and their consequences.",
                        "Citation Paper Authors": "Authors:Johan Bjorck, Carla Gomes, Bart Selman, Kilian Q. Weinberger"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": ". A recent work identi\ufb01ed symmetry-induced geometric structures of the loss and studied\ntheir roles under gradient \ufb02ow dynamics with modi\ufb01ed loss predicting the parameter dynamics ",
                    "Citation Text": "Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel LK Yamins, and Hidenori Tanaka.\nNeural mechanics: Symmetry and broken conservation laws in deep learning dynamics. In\nInternational Conference on Learning Representations , 2021. URL https://openreview.\nnet/forum?id=q8qLAbQBupm .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.04728",
                        "Citation Paper Title": "Title:Neural Mechanics: Symmetry and Broken Conservation Laws in Deep Learning Dynamics",
                        "Citation Paper Abstract": "Abstract:Understanding the dynamics of neural network parameters during training is one of the key challenges in building a theoretical foundation for deep learning. A central obstacle is that the motion of a network in high-dimensional parameter space undergoes discrete finite steps along complex stochastic gradients derived from real-world datasets. We circumvent this obstacle through a unifying theoretical framework based on intrinsic symmetries embedded in a network's architecture that are present for any dataset. We show that any such symmetry imposes stringent geometric constraints on gradients and Hessians, leading to an associated conservation law in the continuous-time limit of stochastic gradient descent (SGD), akin to Noether's theorem in physics. We further show that finite learning rates used in practice can actually break these symmetry induced conservation laws. We apply tools from finite difference methods to derive modified gradient flow, a differential equation that better approximates the numerical trajectory taken by SGD at finite learning rates. We combine modified gradient flow with our framework of symmetries to derive exact integral expressions for the dynamics of certain parameter combinations. We empirically validate our analytic expressions for learning dynamics on VGG-16 trained on Tiny ImageNet. Overall, by exploiting symmetry, our work demonstrates that we can analytically describe the learning dynamics of various parameter combinations at finite learning rates and batch sizes for state of the art architectures trained on any dataset.",
                        "Citation Paper Authors": "Authors:Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel L.K. Yamins, Hidenori Tanaka"
                    }
                },
                {
                    "Sentence ID": 54,
                    "Sentence": ". An array of adaptive optimization methods [ 36,37,38] have been proposed partially\nmotivated as diagonal approximations of the Fisher Information Matrix (see recent discussions\nquestioning the validity of the approximations ",
                    "Citation Text": "Frederik Kunstner, Lukas Balles, and Philipp Hennig. Limitations of the empirical \ufb01sher\napproximation for natural gradient descent. arXiv preprint arXiv:1905.12558 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.12558",
                        "Citation Paper Title": "Title:Limitations of the Empirical Fisher Approximation for Natural Gradient Descent",
                        "Citation Paper Abstract": "Abstract:Natural gradient descent, which preconditions a gradient descent update with the Fisher information matrix of the underlying statistical model, is a way to capture partial second-order information. Several highly visible works have advocated an approximation known as the empirical Fisher, drawing connections between approximate second-order methods and heuristics like Adam. We dispute this argument by showing that the empirical Fisher---unlike the Fisher---does not generally capture second-order information. We further argue that the conditions under which the empirical Fisher approaches the Fisher (and the Hessian) are unlikely to be met in practice, and that, even on simple optimization problems, the pathologies of the empirical Fisher can have undesirable effects.",
                        "Citation Paper Authors": "Authors:Frederik Kunstner, Lukas Balles, Philipp Hennig"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "and its approximations [ 24,48] are invariant under af\ufb01ne\ntransformations including scale transformation due to normalization layers ",
                    "Citation Text": "Guodong Zhang, Chaoqi Wang, Bowen Xu, and Roger Grosse. Three mechanisms of weight\ndecay regularization. In International Conference on Learning Representations , 2019. URL\nhttps://openreview.net/forum?id=B1lz-3Rct7 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.12281",
                        "Citation Paper Title": "Title:Three Mechanisms of Weight Decay Regularization",
                        "Citation Paper Abstract": "Abstract:Weight decay is one of the standard tricks in the neural network toolbox, but the reasons for its regularization effect are poorly understood, and recent results have cast doubt on the traditional interpretation in terms of $L_2$ regularization. Literal weight decay has been shown to outperform $L_2$ regularization for optimizers for which they differ. We empirically investigate weight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a variety of network architectures. We identify three distinct mechanisms by which weight decay exerts a regularization effect, depending on the particular optimization algorithm and architecture: (1) increasing the effective learning rate, (2) approximately regularizing the input-output Jacobian norm, and (3) reducing the effective damping coefficient for second-order optimization. Our results provide insight into how to improve the regularization of neural networks.",
                        "Citation Paper Authors": "Authors:Guodong Zhang, Chaoqi Wang, Bowen Xu, Roger Grosse"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2111.00599v1": {
            "Paper Title": "Bayesian optimization of distributed neurodynamical controller models\n  for spatial navigation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2111.00070v1": {
            "Paper Title": "Deep inference of latent dynamics with spatio-temporal super-resolution\n  using selective backpropagation through time",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.04089v2": {
            "Paper Title": "Credit Assignment Through Broadcasting a Global Error Vector",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.14853v1": {
            "Paper Title": "Targeted Neural Dynamical Modeling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.14092v1": {
            "Paper Title": "BioGrad: Biologically Plausible Gradient-Based Learning for Spiking\n  Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.15685v3": {
            "Paper Title": "Tensor decomposition of higher-order correlations by nonlinear Hebbian\n  plasticity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.11130v1": {
            "Paper Title": "Inverse Optimal Control Adapted to the Noise Characteristics of the\n  Human Sensorimotor System",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.13495v2": {
            "Paper Title": "Learning Dynamic Graph Representation of Brain Connectome with\n  Spatio-Temporal Attention",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.13666v2": {
            "Paper Title": "Asynchronous and coherent dynamics in balanced excitatory-inhibitory\n  spiking networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.09919v1": {
            "Paper Title": "ToFFi -- Toolbox for Frequency-based Fingerprinting of Brain Signals",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.13611v1": {
            "Paper Title": "Dendritic Self-Organizing Maps for Continual Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.09336v1": {
            "Paper Title": "Preterm neonates distinguish rhythm violation through a hierarchy of\n  cortical processing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.09521v1": {
            "Paper Title": "Quantitative relations among causality measures with applications to\n  nonlinear pulse-output network reconstruction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.14598v1": {
            "Paper Title": "A logical and topological proof of the irreducibility of consciousness\n  to physical data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.12873v1": {
            "Paper Title": "Godot is not coming: when we will let innovations enter psychiatry?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.08621v1": {
            "Paper Title": "When heart beats differently in depression: a review of HRV measures",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.11064v2": {
            "Paper Title": "An Electroencephalography connectome predictive model of major\n  depressive disorder severity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.06871v2": {
            "Paper Title": "Two-argument activation functions learn soft XOR operations like\n  cortical neurons",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.05158v1": {
            "Paper Title": "Can the brain use waves to solve planning problems?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.04954v1": {
            "Paper Title": "Recurrent Attention Models with Object-centric Capsule Representation\n  for Multi-object Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.03452v1": {
            "Paper Title": "Inter-Domain Alignment for Predicting High-Resolution Brain Networks\n  Using Teacher-Student Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.01503v1": {
            "Paper Title": "Emotionally-Informed Decisions: Bringing Gut's Feelings into\n  Self-adaptive and Co-adaptive Software Systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.00889v1": {
            "Paper Title": "A Minimal Intervention Definition of Reverse Engineering a Neural\n  Circuit",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.00609v1": {
            "Paper Title": "Calcium imaging and analysis of the mouse hippocampus or neocortex using\n  miniature microendoscopes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.09844v2": {
            "Paper Title": "Assessing clinical utility of Machine Learning and Artificial\n  Intelligence approaches to analyze speech recordings in Multiple Sclerosis: A\n  Pilot Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/2110.03517v1": {
            "Paper Title": "Representation of probability distributions with implied volatility and\n  biological rationale",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.02726v2": {
            "Paper Title": "Popular individuals process the world in particularly normative ways",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.11437v3": {
            "Paper Title": "Nerve theorems for fixed points of neural networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.07416v1": {
            "Paper Title": "Mapping Input Noise to Escape Noise in Integrate-and-fire neurons: A\n  Level-Crossing Approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.06544v1": {
            "Paper Title": "I wanna draw like you: Inter-and intra-individual differences in\n  orang-utan drawings",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.04137v3": {
            "Paper Title": "The backpropagation-based recollection hypothesis: Backpropagated action\n  potentials mediate recall, imagination, language understanding and naming",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.05688v1": {
            "Paper Title": "Thematic analysis of multiple sclerosis research by enhanced strategic\n  diagram",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.05545v1": {
            "Paper Title": "An interdisciplinary approach to high school curriculum development:\n  Swarming Powered by Neuroscience",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.01854v2": {
            "Paper Title": "Predicting isocitrate dehydrogenase mutation status in glioma using\n  structural brain networks and graph neural networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.07053v2": {
            "Paper Title": "Combining the Projective Consciousness Model and Virtual Humans to\n  assess ToM capacity in Virtual Reality: a proof-of-concept",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.03723v1": {
            "Paper Title": "Disentangling Alzheimer's disease neurodegeneration from typical brain\n  aging using machine learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.03429v1": {
            "Paper Title": "Computing on Functions Using Randomized Vector Representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.03351v1": {
            "Paper Title": "Capturing the objects of vision with neural networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.03115v1": {
            "Paper Title": "Improving Phenotype Prediction using Long-Range Spatio-Temporal Dynamics\n  of Functional Connectivity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.05181v2": {
            "Paper Title": "Condition Integration Memory Network: An Interpretation of the Meaning\n  of the Neuronal Design",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.01347v1": {
            "Paper Title": "Account for Neuronal Representations from the Perspective of Neurons",
            "Sentences": []
        },
        "http://arxiv.org/abs/2109.04361v1": {
            "Paper Title": "MutualGraphNet: A novel model for motor imagery classification",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": ". Two CNN models were specially designed for motor imagery classi\fcation called\nShallow ConvNets and Deep ConvNets ",
                    "Citation Text": "R. T. Schirrmeister, J. T. Springenberg, Ldj Fiederer, M. Glasstetter, K. Eggensperger, M. Tanger-\nmann, F. Hutter, W. Burgard, and T. Ball. Deep learning with convolutional neural networks for\neeg decoding and visualization. Human Brain Mapping , 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.05051",
                        "Citation Paper Title": "Title:Deep learning with convolutional neural networks for EEG decoding and visualization",
                        "Citation Paper Abstract": "Abstract:PLEASE READ AND CITE THE REVISED VERSION at Human Brain Mapping: this http URL\nCode available here: this https URL",
                        "Citation Paper Authors": "Authors:Robin Tibor Schirrmeister, Jost Tobias Springenberg, Lukas Dominique Josef Fiederer, Martin Glasstetter, Katharina Eggensperger, Michael Tangermann, Frank Hutter, Wolfram Burgard, Tonio Ball"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": ", graph convolutional networks have been proven to have better performance\non the graph structure data. Many works have been done to improve the performance of the graph\nconvolutional networks. For example, The GraphSAGE model ",
                    "Citation Text": "William Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs.\n06 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.02216",
                        "Citation Paper Title": "Title:Inductive Representation Learning on Large Graphs",
                        "Citation Paper Abstract": "Abstract:Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.",
                        "Citation Paper Authors": "Authors:William L. Hamilton, Rex Ying, Jure Leskovec"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.13611v1": {
            "Paper Title": "EEG-connectivity: A fundamental guide and checklist for optimal study\n  design and evaluation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.00899v2": {
            "Paper Title": "How do we generalize?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.07030v2": {
            "Paper Title": "The Backpropagation Algorithm Implemented on Spiking Neuromorphic\n  Hardware",
            "Sentences": [
                {
                    "Sentence ID": 67,
                    "Sentence": "and represent the activation function by a surrogate (or\nstraight-through estimator ",
                    "Citation Text": "Bengio, Y., L\u0013 eonard, N. & Courville, A. Estimat-\ning or propagating gradients through stochastic neu-\nrons for conditional computation. arXiv preprint\narXiv:1308.3432 (2013).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1308.3432",
                        "Citation Paper Title": "Title:Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation",
                        "Citation Paper Abstract": "Abstract:Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we \"back-propagate\" through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of {\\em conditional computation}, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.",
                        "Citation Paper Authors": "Authors:Yoshua Bengio, Nicholas L\u00e9onard, Aaron Courville"
                    }
                },
                {
                    "Sentence ID": 35,
                    "Sentence": "Shrestha et al. (2021) Loihi EMSTDP FA/DFA CNN-CNN-100-10 8.4 20 94.7 ",
                    "Citation Text": "Frenkel, C., Legat, J.-D. & Bol, D. A 28-nm convo-\nlutional neuromorphic processor enabling online learn-\ning with spike-based retinas. In 2020 IEEE Interna-\ntional Symposium on Circuits and Systems (ISCAS) ,\n1{5 (IEEE, 2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.06318",
                        "Citation Paper Title": "Title:A 28-nm Convolutional Neuromorphic Processor Enabling Online Learning with Spike-Based Retinas",
                        "Citation Paper Abstract": "Abstract:In an attempt to follow biological information representation and organization principles, the field of neuromorphic engineering is usually approached bottom-up, from the biophysical models to large-scale integration in silico. While ideal as experimentation platforms for cognitive computing and neuroscience, bottom-up neuromorphic processors have yet to demonstrate an efficiency advantage compared to specialized neural network accelerators for real-world problems. Top-down approaches aim at answering this difficulty by (i) starting from the applicative problem and (ii) investigating how to make the associated algorithms hardware-efficient and biologically-plausible. In order to leverage the data sparsity of spike-based neuromorphic retinas for adaptive edge computing and vision applications, we follow a top-down approach and propose SPOON, a 28-nm event-driven CNN (eCNN). It embeds online learning with only 16.8-% power and 11.8-% area overheads with the biologically-plausible direct random target projection (DRTP) algorithm. With an energy per classification of 313nJ at 0.6V and a 0.32-mm$^2$ area for accuracies of 95.3% (on-chip training) and 97.5% (off-chip training) on MNIST, we demonstrate that SPOON reaches the efficiency of conventional machine learning accelerators while embedding on-chip learning and being compatible with event-based sensors, a point that we further emphasize with N-MNIST benchmarking.",
                        "Citation Paper Authors": "Authors:Charlotte Frenkel, Jean-Didier Legat, David Bol"
                    }
                },
                {
                    "Sentence ID": 48,
                    "Sentence": "to achieve self-\nalignment of the backward weights to the forward weights\nor to use feedback alignment to approximately align the\nfeedforward weights to random feedback weights ",
                    "Citation Text": "Lillicrap, T. P., Cownden, D., Tweed, D. B. & Aker-\nman, C. J. Random synaptic feedback weights support\nerror backpropagation for deep learning. Nature com-\nmunications 7, 1{10 (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1411.0247",
                        "Citation Paper Title": "Title:Random feedback weights support learning in deep neural networks",
                        "Citation Paper Abstract": "Abstract:The brain processes information through many layers of neurons. This deep architecture is representationally powerful, but it complicates learning by making it hard to identify the responsible neurons when a mistake is made. In machine learning, the backpropagation algorithm assigns blame to a neuron by computing exactly how it contributed to an error. To do this, it multiplies error signals by matrices consisting of all the synaptic weights on the neuron's axon and farther downstream. This operation requires a precisely choreographed transport of synaptic weight information, which is thought to be impossible in the brain. Here we present a surprisingly simple algorithm for deep learning, which assigns blame by multiplying error signals by random synaptic weights. We show that a network can learn to extract useful information from signals sent through these random feedback connections. In essence, the network learns to learn. We demonstrate that this new mechanism performs as quickly and accurately as backpropagation on a variety of problems and describe the principles which underlie its function. Our demonstration provides a plausible basis for how a neuron can be adapted using error signals generated at distal locations in the brain, and thus dispels long-held assumptions about the algorithmic constraints on learning in neural circuits.",
                        "Citation Paper Authors": "Authors:Timothy P. Lillicrap, Daniel Cownden, Douglas B. Tweed, Colin J. Akerman"
                    }
                },
                {
                    "Sentence ID": 84,
                    "Sentence": "Lee et al. (2016) Simulation BP 784-800-10 - - 98.64 ",
                    "Citation Text": "O'Connor, P. & Welling, M. Deep spiking networks.15\narXiv preprint arXiv:1602.08323 (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1602.08323",
                        "Citation Paper Title": "Title:Deep Spiking Networks",
                        "Citation Paper Abstract": "Abstract:We introduce an algorithm to do backpropagation on a spiking network. Our network is \"spiking\" in the sense that our neurons accumulate their activation into a potential over time, and only send out a signal (a \"spike\") when this potential crosses a threshold and the neuron is reset. Neurons only update their states when receiving signals from other neurons. Total computation of the network thus scales with the number of spikes caused by an input rather than network size. We show that the spiking Multi-Layer Perceptron behaves identically, during both prediction and training, to a conventional deep network of rectified-linear units, in the limiting case where we run the spiking network for a long time. We apply this architecture to a conventional classification problem (MNIST) and achieve performance very close to that of a conventional Multi-Layer Perceptron with the same architecture. Our network is a natural architecture for learning based on streaming event-based data, and is a stepping stone towards using spiking neural networks to learn efficiently on streaming data.",
                        "Citation Paper Authors": "Authors:Peter O'Connor, Max Welling"
                    }
                },
                {
                    "Sentence ID": 81,
                    "Sentence": "Shrestha et al. (2019) Simulation EM-STDP 784-500-10 - - 97 ",
                    "Citation Text": "Tavanaei, A. & Maida, A. Bp-stdp: Approximating\nbackpropagation using spike timing dependent plastic-\nity.Neurocomputing 330, 39{47 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.04214",
                        "Citation Paper Title": "Title:BP-STDP: Approximating Backpropagation using Spike Timing Dependent Plasticity",
                        "Citation Paper Abstract": "Abstract:The problem of training spiking neural networks (SNNs) is a necessary precondition to understanding computations within the brain, a field still in its infancy. Previous work has shown that supervised learning in multi-layer SNNs enables bio-inspired networks to recognize patterns of stimuli through hierarchical feature acquisition. Although gradient descent has shown impressive performance in multi-layer (and deep) SNNs, it is generally not considered biologically plausible and is also computationally expensive. This paper proposes a novel supervised learning approach based on an event-based spike-timing-dependent plasticity (STDP) rule embedded in a network of integrate-and-fire (IF) neurons. The proposed temporally local learning rule follows the backpropagation weight change updates applied at each time step. This approach enjoys benefits of both accurate gradient descent and temporally local, efficient STDP. Thus, this method is able to address some open questions regarding accurate and efficient computations that occur in the brain. The experimental results on the XOR problem, the Iris data, and the MNIST dataset demonstrate that the proposed SNN performs as successfully as the traditional NNs. Our approach also compares favorably with the state-of-the-art multi-layer SNNs.",
                        "Citation Paper Authors": "Authors:Amirhossein Tavanaei, Anthony S. Maida"
                    }
                },
                {
                    "Sentence ID": 78,
                    "Sentence": "Stromatias et al. (2015) SpiNNaker inference 784-500-500-10 3.3 11 95\nNeuromorphic sBP in simulated SNN ",
                    "Citation Text": "Jin, Y., Zhang, W. & Li, P. Hybrid macro/micro level\nbackpropagation for training deep spiking neural net-\nworks. arXiv preprint arXiv:1805.07866 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.07866",
                        "Citation Paper Title": "Title:Hybrid Macro/Micro Level Backpropagation for Training Deep Spiking Neural Networks",
                        "Citation Paper Abstract": "Abstract:Spiking neural networks (SNNs) are positioned to enable spatio-temporal information processing and ultra-low power event-driven neuromorphic hardware. However, SNNs are yet to reach the same performances of conventional deep artificial neural networks (ANNs), a long-standing challenge due to complex dynamics and non-differentiable spike events encountered in training. The existing SNN error backpropagation (BP) methods are limited in terms of scalability, lack of proper handling of spiking discontinuities, and/or mismatch between the rate-coded loss function and computed gradient. We present a hybrid macro/micro level backpropagation (HM2-BP) algorithm for training multi-layer SNNs. The temporal effects are precisely captured by the proposed spike-train level post-synaptic potential (S-PSP) at the microscopic level. The rate-coded errors are defined at the macroscopic level, computed and back-propagated across both macroscopic and microscopic levels. Different from existing BP methods, HM2-BP directly computes the gradient of the rate-coded loss function w.r.t tunable parameters. We evaluate the proposed HM2-BP algorithm by training deep fully connected and convolutional SNNs based on the static MNIST [14] and dynamic neuromorphic N-MNIST [26]. HM2-BP achieves an accuracy level of 99.49% and 98.88% for MNIST and N-MNIST, respectively, outperforming the best reported performances obtained from the existing SNN BP algorithms. Furthermore, the HM2-BP produces the highest accuracies based on SNNs for the EMNIST [3] dataset, and leads to high recognition accuracy for the 16-speaker spoken English letters of TI46 Corpus [16], a challenging patio-temporal speech recognition benchmark for which no prior success based on SNNs was reported. It also achieves competitive performances surpassing those of conventional deep learning models when dealing with asynchronous spiking streams.",
                        "Citation Paper Authors": "Authors:Yingyezhe Jin, Wenrui Zhang, Peng Li"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2109.06011v1": {
            "Paper Title": "Online Optimization of Stimulation Speed in an Auditory Brain-Computer\n  Interface under Time Constraints",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.08975v1": {
            "Paper Title": "Assessing Cerebellar Disorders With Wearable Inertial Sensor Data Using\n  Time-Frequency and Autoregressive Hidden Markov Model Approaches",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.08214v1": {
            "Paper Title": "Distinguishing Healthy Ageing from Dementia: a Biomechanical Simulation\n  of Brain Atrophy using Deep Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.06264v1": {
            "Paper Title": "Bridging the gap between emotion and joint action",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.05827v1": {
            "Paper Title": "Fluctuation in background synaptic activity controls synaptic plasticity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.12187v2": {
            "Paper Title": "Frequency Superposition -- A Multi-Frequency Stimulation Method in\n  SSVEP-based BCIs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.00134v2": {
            "Paper Title": "Structural Characterization of Oscillations in Brain Networks with Rate\n  Dynamics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.13461v2": {
            "Paper Title": "Marine Vehicles Localization Using Grid Cells for Path Integration",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": ". One promising area of SLAM research is the utilization\nof neural networks to obtain a reliable position estimation. In ",
                    "Citation Text": "Sen Wang, Ronald Clark, Hongkai Wen, and Niki Trigoni. Deepvo:\nTowards end-to-end visual odometry with deep recurrent convolutional\nneural networks. In 2017 IEEE International Conference on Robotics\nand Automation (ICRA) , pages 2043\u20132050, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1709.08429",
                        "Citation Paper Title": "Title:DeepVO: Towards End-to-End Visual Odometry with Deep Recurrent Convolutional Neural Networks",
                        "Citation Paper Abstract": "Abstract:This paper studies monocular visual odometry (VO) problem. Most of existing VO algorithms are developed under a standard pipeline including feature extraction, feature matching, motion estimation, local optimisation, etc. Although some of them have demonstrated superior performance, they usually need to be carefully designed and specifically fine-tuned to work well in different environments. Some prior knowledge is also required to recover an absolute scale for monocular VO. This paper presents a novel end-to-end framework for monocular VO by using deep Recurrent Convolutional Neural Networks (RCNNs). Since it is trained and deployed in an end-to-end manner, it infers poses directly from a sequence of raw RGB images (videos) without adopting any module in the conventional VO pipeline. Based on the RCNNs, it not only automatically learns effective feature representation for the VO problem through Convolutional Neural Networks, but also implicitly models sequential dynamics and relations using deep Recurrent Neural Networks. Extensive experiments on the KITTI VO dataset show competitive performance to state-of-the-art methods, verifying that the end-to-end Deep Learning technique can be a viable complement to the traditional VO systems.",
                        "Citation Paper Authors": "Authors:Sen Wang, Ronald Clark, Hongkai Wen, Niki Trigoni"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": ". Recent improvement in computer\ntechnology has allowed for research on SLAM to focus on\nmore robust methods, as the real world application of SLAM\ntechnology expands into a variety of real-world applications ",
                    "Citation Text": "Cesar Cadena, L. Carlone, Henry Carrillo, Y . Latif, D. Scaramuzza, Jos \u00b4e\nNeira, I. Reid, and J. Leonard. Past, present, and future of simultaneous\nlocalization and mapping: Toward the robust-perception age. IEEE\nTransactions on Robotics , 32:1309\u20131332, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.05830",
                        "Citation Paper Title": "Title:Past, Present, and Future of Simultaneous Localization And Mapping: Towards the Robust-Perception Age",
                        "Citation Paper Abstract": "Abstract:Simultaneous Localization and Mapping (SLAM)consists in the concurrent construction of a model of the environment (the map), and the estimation of the state of the robot moving within it. The SLAM community has made astonishing progress over the last 30 years, enabling large-scale real-world applications, and witnessing a steady transition of this technology to industry. We survey the current state of SLAM. We start by presenting what is now the de-facto standard formulation for SLAM. We then review related work, covering a broad set of topics including robustness and scalability in long-term mapping, metric and semantic representations for mapping, theoretical performance guarantees, active SLAM and exploration, and other new frontiers. This paper simultaneously serves as a position paper and tutorial to those who are users of SLAM. By looking at the published research with a critical eye, we delineate open challenges and new research issues, that still deserve careful scientific investigation. The paper also contains the authors' take on two questions that often animate discussions during robotics conferences: Do robots need SLAM? and Is SLAM solved?",
                        "Citation Paper Authors": "Authors:Cesar Cadena, Luca Carlone, Henry Carrillo, Yasir Latif, Davide Scaramuzza, Jose Neira, Ian Reid, John J. Leonard"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2108.04289v1": {
            "Paper Title": "ACE: A Novel Approach for the Statistical Analysis of Pairwise\n  Connectivity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.03532v1": {
            "Paper Title": "Learning Orientations: a Discrete Geometry Model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.03518v1": {
            "Paper Title": "Spatial representability of neuronal activity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.01229v1": {
            "Paper Title": "Taking Cognition Seriously: A generalised physics of cognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.00758v1": {
            "Paper Title": "Neuronal Network Inference and Membrane Potential Model using\n  Multivariate Hawkes Processes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.09943v2": {
            "Paper Title": "The principle of weight divergence facilitation for unsupervised pattern\n  recognition in spiking neural networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.01665v1": {
            "Paper Title": "Efficient Neural Network Approximation of Robust PCA for Automated\n  Analysis of Calcium Imaging Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.14235v1": {
            "Paper Title": "EEG multipurpose eye blink detector using convolutional neural network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.12609v1": {
            "Paper Title": "Tracking Fast Neural Adaptation by Globally Adaptive Point Process\n  Estimation for Brain-Machine Interface",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.10178v1": {
            "Paper Title": "A Network Control Theory Approach to Longitudinal Symptom Dynamics in\n  Major Depressive Disorder",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.10169v1": {
            "Paper Title": "Genetic, Individual, and Familial Risk Correlates of Brain Network\n  Controllability in Major Depressive Disorder",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.09384v1": {
            "Paper Title": "An induction proof of the backpropagation algorithm in matrix notation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.09360v1": {
            "Paper Title": "A biophysical network model reveals the link between deficient\n  inhibitory cognitive control and major neurotransmitter and neural\n  connectivity hypotheses in schizophrenia",
            "Sentences": []
        },
        "http://arxiv.org/abs/2108.06397v1": {
            "Paper Title": "Cognitive factor forming an individual constituent in a driver model\n  inferred from multiplicatory relationships between cognitive sub-factors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.00814v2": {
            "Paper Title": "A computational theory for the production of limb movements",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.08514v1": {
            "Paper Title": "Classification of Upper Arm Movements from EEG signals using Machine\n  Learning with ICA Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.04316v2": {
            "Paper Title": "Exploration and preference satisfaction trade-off in reward-free\n  learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.05097v1": {
            "Paper Title": "BrainNNExplainer: An Interpretable Graph Neural Network Framework for\n  Brain Network based Disease Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.01807v4": {
            "Paper Title": "Building population models for large-scale neural recordings:\n  opportunities and pitfalls",
            "Sentences": [
                {
                    "Sentence ID": 97,
                    "Sentence": "Zhao, Y. and Park, I. M. (2017). Variational latent gaussian process for recovering single-\ntrial dynamics from population spike trains. Neural Comput , 29(5):1293\u20131316. ",
                    "Citation Text": "Zhou, D. and Wei, X.-X. (2020). Learning identiable and interpretable latent models of\nhigh-dimensional neural activity using pi-vae. arXiv preprint arXiv:2011.04798 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.04798",
                        "Citation Paper Title": "Title:Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-VAE",
                        "Citation Paper Abstract": "Abstract:The ability to record activities from hundreds of neurons simultaneously in the brain has placed an increasing demand for developing appropriate statistical techniques to analyze such data. Recently, deep generative models have been proposed to fit neural population responses. While these methods are flexible and expressive, the downside is that they can be difficult to interpret and identify. To address this problem, we propose a method that integrates key ingredients from latent models and traditional neural encoding models. Our method, pi-VAE, is inspired by recent progress on identifiable variational auto-encoder, which we adapt to make appropriate for neuroscience applications. Specifically, we propose to construct latent variable models of neural activity while simultaneously modeling the relation between the latent and task variables (non-neural variables, e.g. sensory, motor, and other externally observable states). The incorporation of task variables results in models that are not only more constrained, but also show qualitative improvements in interpretability and identifiability. We validate pi-VAE using synthetic data, and apply it to analyze neurophysiological datasets from rat hippocampus and macaque motor cortex. We demonstrate that pi-VAE not only fits the data better, but also provides unexpected novel insights into the structure of the neural codes.",
                        "Citation Paper Authors": "Authors:Ding Zhou, Xue-Xin Wei"
                    }
                },
                {
                    "Sentence ID": 96,
                    "Sentence": "Zhao, Y. and Park, I. M. (2016). Interpretable nonlinear dynamic modeling of neural trajec-\ntories. arXiv preprint arXiv:1608.06546 . ",
                    "Citation Text": "Zhao, Y. and Park, I. M. (2017). Variational latent gaussian process for recovering single-\ntrial dynamics from population spike trains. Neural Comput , 29(5):1293\u20131316.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1604.03053",
                        "Citation Paper Title": "Title:Variational Latent Gaussian Process for Recovering Single-Trial Dynamics from Population Spike Trains",
                        "Citation Paper Abstract": "Abstract:When governed by underlying low-dimensional dynamics, the interdependence of simultaneously recorded population of neurons can be explained by a small number of shared factors, or a low-dimensional trajectory. Recovering these latent trajectories, particularly from single-trial population recordings, may help us understand the dynamics that drive neural computation. However, due to the biophysical constraints and noise in the spike trains, inferring trajectories from data is a challenging statistical problem in general. Here, we propose a practical and efficient inference method, called the variational latent Gaussian process (vLGP). The vLGP combines a generative model with a history-dependent point process observation together with a smoothness prior on the latent trajectories. The vLGP improves upon earlier methods for recovering latent trajectories, which assume either observation models inappropriate for point processes or linear dynamics. We compare and validate vLGP on both simulated datasets and population recordings from the primary visual cortex. In the V1 dataset, we find that vLGP achieves substantially higher performance than previous methods for predicting omitted spike trains, as well as capturing both the toroidal topology of visual stimuli space, and the noise-correlation. These results show that vLGP is a robust method with a potential to reveal hidden neural dynamics from large-scale neural recordings.",
                        "Citation Paper Authors": "Authors:Yuan Zhao, Il Memming Park"
                    }
                },
                {
                    "Sentence ID": 95,
                    "Sentence": "Wu, A., Roy, N. A., Keeley, S., and Pillow, J. W. (2017). Gaussian process based nonlinear\nlatent structure discovery in multivariate spike train data. Adv Neur In , 30:3496.\n15 ",
                    "Citation Text": "Zhao, Y. and Park, I. M. (2016). Interpretable nonlinear dynamic modeling of neural trajec-\ntories. arXiv preprint arXiv:1608.06546 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1608.06546",
                        "Citation Paper Title": "Title:Interpretable Nonlinear Dynamic Modeling of Neural Trajectories",
                        "Citation Paper Abstract": "Abstract:A central challenge in neuroscience is understanding how neural system implements computation through its dynamics. We propose a nonlinear time series model aimed at characterizing interpretable dynamics from neural trajectories. Our model assumes low-dimensional continuous dynamics in a finite volume. It incorporates a prior assumption about globally contractional dynamics to avoid overly enthusiastic extrapolation outside of the support of observed trajectories. We show that our model can recover qualitative features of the phase portrait such as attractors, slow points, and bifurcations, while also producing reliable long-term future predictions in a variety of dynamical models and in real neural data.",
                        "Citation Paper Authors": "Authors:Yuan Zhao, Il Memming Park"
                    }
                },
                {
                    "Sentence ID": 86,
                    "Sentence": "Smith, A. C. and Brown, E. N. (2003). Estimating a state-space model from point process\nobservations. Neural Comput , 15(5):965\u2013991. ",
                    "Citation Text": "Spreemann, G., Dunn, B., Botnan, M. B., and Baas, N. A. (2018). Using persistent homology\nto reveal hidden covariates in systems governed by the kinetic ising model. Phys Rev E ,\n97(3):032313.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1510.06629",
                        "Citation Paper Title": "Title:Using persistent homology to reveal hidden information in neural data",
                        "Citation Paper Abstract": "Abstract:We propose a method, based on persistent homology, to uncover topological properties of a priori unknown covariates of neuron activity. Our input data consist of spike train measurements of a set of neurons of interest, a candidate list of the known stimuli that govern neuron activity, and the corresponding state of the animal throughout the experiment performed. Using a generalized linear model for neuron activity and simple assumptions on the effects of the external stimuli, we infer away any contribution to the observed spike trains by the candidate stimuli. Persistent homology then reveals useful information about any further, unknown, covariates.",
                        "Citation Paper Authors": "Authors:Gard Spreemann, Benjamin Dunn, Magnus Bakke Botnan, Nils A. Baas"
                    }
                },
                {
                    "Sentence ID": 81,
                    "Sentence": "Schneidman, E., Berry, M. J., Segev, R., and Bialek, W. (2006). Weak pairwise correlations\nimply strongly correlated network states in a neural population. Nature , 440. ",
                    "Citation Text": "She, Q. and Wu, A. (2020). Neural dynamics discovery via gaussian process recurrent neural\nnetworks. In Uncertainty in Articial Intelligence , pages 454\u2013464. PMLR.\n* This paper extends RNN-based dynamical modelling approaches with a Gaussian Process\nobservation model. They show that their more expressive observation model is able to extract\ninsightful low-dimensional dynamics and better t neural datasets.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.00650",
                        "Citation Paper Title": "Title:Neural Dynamics Discovery via Gaussian Process Recurrent Neural Networks",
                        "Citation Paper Abstract": "Abstract:Latent dynamics discovery is challenging in extracting complex dynamics from high-dimensional noisy neural data. Many dimensionality reduction methods have been widely adopted to extract low-dimensional, smooth and time-evolving latent trajectories. However, simple state transition structures, linear embedding assumptions, or inflexible inference networks impede the accurate recovery of dynamic portraits. In this paper, we propose a novel latent dynamic model that is capable of capturing nonlinear, non-Markovian, long short-term time-dependent dynamics via recurrent neural networks and tackling complex nonlinear embedding via non-parametric Gaussian process. Due to the complexity and intractability of the model and its inference, we also provide a powerful inference network with bi-directional long short-term memory networks that encode both past and future information into posterior distributions. In the experiment, we show that our model outperforms other state-of-the-art methods in reconstructing insightful latent dynamics from both simulated and experimental neural datasets with either Gaussian or Poisson observations, especially in the low-sample scenario. Our codes and additional materials are available at this https URL.",
                        "Citation Paper Authors": "Authors:Qi She, Anqi Wu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2107.03971v1": {
            "Paper Title": "The classical mean negative asynchrony in sensorimotor synchronization\n  is not universal in humans. A cross-cultural study",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.02704v1": {
            "Paper Title": "Unsupervised learning of MRI tissue properties using MRI physics models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2107.06762v1": {
            "Paper Title": "Modelling Neuronal Behaviour with Time Series Regression: Recurrent\n  Neural Networks on C. Elegans Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.16219v1": {
            "Paper Title": "Do grid codes afford generalization and flexible decision-making?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.16061v1": {
            "Paper Title": "Reasoning about conscious experience with axiomatic and graphical\n  mathematics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.12993v1": {
            "Paper Title": "Evaluation of deep lift pose models for 3D rodent pose estimation based\n  on geometrically triangulated data",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": ". View-invariant human pose estimation\nfrom embedding spaces was recently proposed by Sun et al. ",
                    "Citation Text": "Jennifer J. Sun, Jiaping Zhao, Liang-Chieh Chen, Florian\nSchroff, Hartwig Adam, and Ting Liu. View-Invariant Prob-\nabilistic Embedding for Human Pose. arXiv:1912.01001\n[cs], Oct. 2020. arXiv: 1912.01001. 1, 2, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.01001",
                        "Citation Paper Title": "Title:View-Invariant Probabilistic Embedding for Human Pose",
                        "Citation Paper Abstract": "Abstract:Depictions of similar human body configurations can vary with changing viewpoints. Using only 2D information, we would like to enable vision algorithms to recognize similarity in human body poses across multiple views. This ability is useful for analyzing body movements and human behaviors in images and videos. In this paper, we propose an approach for learning a compact view-invariant embedding space from 2D joint keypoints alone, without explicitly predicting 3D poses. Since 2D poses are projected from 3D space, they have an inherent ambiguity, which is difficult to represent through a deterministic mapping. Hence, we use probabilistic embeddings to model this input uncertainty. Experimental results show that our embedding model achieves higher accuracy when retrieving similar poses across different camera views, in comparison with 2D-to-3D pose lifting models. We also demonstrate the effectiveness of applying our embeddings to view-invariant action recognition and video alignment. Our code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Jennifer J. Sun, Jiaping Zhao, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Ting Liu"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "for 3D pose estimation in freely moving monkeys\nas well as rodents in a constrained behavioral setup. Pose\nlifting of human data using temporal 1-dimensional dilated\nconvolutional neural networks was previously proposed by\nPavllo et al. ",
                    "Citation Text": "Dario Pavllo, Christoph Feichtenhofer, David Grangier, and\nMichael Auli. 3d human pose estimation in video with tem-\nporal convolutions and semi-supervised training, 2019. 1, 2,\n3, 4",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.11742",
                        "Citation Paper Title": "Title:3D human pose estimation in video with temporal convolutions and semi-supervised training",
                        "Citation Paper Abstract": "Abstract:In this work, we demonstrate that 3D poses in video can be effectively estimated with a fully convolutional model based on dilated temporal convolutions over 2D keypoints. We also introduce back-projection, a simple and effective semi-supervised training method that leverages unlabeled video data. We start with predicted 2D keypoints for unlabeled video, then estimate 3D poses and finally back-project to the input 2D keypoints. In the supervised setting, our fully-convolutional model outperforms the previous best result from the literature by 6 mm mean per-joint position error on Human3.6M, corresponding to an error reduction of 11%, and the model also shows significant improvements on HumanEva-I. Moreover, experiments with back-projection show that it comfortably outperforms previous state-of-the-art results in semi-supervised settings where labeled data is scarce. Code and models are available at this https URL",
                        "Citation Paper Authors": "Authors:Dario Pavllo, Christoph Feichtenhofer, David Grangier, Michael Auli"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "Previously 3D pose estimation on humans was proposed\nby Martinez et al. ",
                    "Citation Text": "Julieta Martinez, Rayat Hossain, Javier Romero, and\nJames J. Little. A simple yet effective baseline for 3d hu-\nman pose estimation, 2017. 1, 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.03098",
                        "Citation Paper Title": "Title:A simple yet effective baseline for 3d human pose estimation",
                        "Citation Paper Abstract": "Abstract:Following the success of deep convolutional networks, state-of-the-art methods for 3d human pose estimation have focused on deep end-to-end systems that predict 3d joint locations given raw image pixels. Despite their excellent performance, it is often not easy to understand whether their remaining error stems from a limited 2d pose (visual) understanding, or from a failure to map 2d poses into 3-dimensional positions. With the goal of understanding these sources of error, we set out to build a system that given 2d joint locations predicts 3d positions. Much to our surprise, we have found that, with current technology, \"lifting\" ground truth 2d joint locations to 3d space is a task that can be solved with a remarkably low error rate: a relatively simple deep feed-forward network outperforms the best reported result by about 30\\% on Human3.6M, the largest publicly available 3d pose estimation benchmark. Furthermore, training our system on the output of an off-the-shelf state-of-the-art 2d detector (\\ie, using images as input) yields state of the art results -- this includes an array of systems that have been trained end-to-end specifically for this task. Our results indicate that a large portion of the error of modern deep 3d pose estimation systems stems from their visual analysis, and suggests directions to further advance the state of the art in 3d human pose estimation.",
                        "Citation Paper Authors": "Authors:Julieta Martinez, Rayat Hossain, Javier Romero, James J. Little"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.11951v1": {
            "Paper Title": "A platform for cognitive monitoring of neurosurgical patients during\n  hospitalization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.08143v2": {
            "Paper Title": "Constrained plasticity reserve as a natural way to control frequency and\n  weights in spiking neural networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.10627v1": {
            "Paper Title": "Experimentally testable whole brain manifolds that recapitulate behavior",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.10112v1": {
            "Paper Title": "Deep Reinforcement Learning Models Predict Visual Responses in the\n  Brain: A Preliminary Result",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.01955v2": {
            "Paper Title": "Predictive coding feedback results in perceived illusory contours in a\n  recurrent neural network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.03230v3": {
            "Paper Title": "Barlow Twins: Self-Supervised Learning via Redundancy Reduction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.07369v1": {
            "Paper Title": "A Self-Supervised Framework for Function Learning and Extrapolation",
            "Sentences": [
                {
                    "Sentence ID": 9,
                    "Sentence": ", an appropriate kernel is found by\n\ufb01tting a Boltzmann machine. Neural Processes go further and ",
                    "Citation Text": "M. Garnelo, J. Schwarz, D. Rosenbaum, F. Viola, D. J. Rezende, S. A. Eslami, and Y . W. Teh.\nNeural processes. arXiv preprint arXiv:1807.01622 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.01622",
                        "Citation Paper Title": "Title:Neural Processes",
                        "Citation Paper Abstract": "Abstract:A neural network (NN) is a parameterised function that can be tuned via gradient descent to approximate a labelled collection of data with high precision. A Gaussian process (GP), on the other hand, is a probabilistic model that defines a distribution over possible functions, and is updated in light of data via the rules of probabilistic inference. GPs are probabilistic, data-efficient and flexible, however they are also computationally intensive and thus limited in their applicability. We introduce a class of neural latent variable models which we call Neural Processes (NPs), combining the best of both worlds. Like GPs, NPs define distributions over functions, are capable of rapid adaptation to new observations, and can estimate the uncertainty in their predictions. Like NNs, NPs are computationally efficient during training and evaluation but also learn to adapt their priors to data. We demonstrate the performance of NPs on a range of learning tasks, including regression and optimisation, and compare and contrast with related models in the literature.",
                        "Citation Paper Authors": "Authors:Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J. Rezende, S.M. Ali Eslami, Yee Whye Teh"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": "implement a\nsimilar idea, except by building up a family of kernels using operations of a small number of atomic\nkernels, and performing a search over the resulting combinatorial space. ",
                    "Citation Text": "S. Sun, G. Zhang, C. Wang, W. Zeng, J. Li, and R. Grosse. Differentiable compositional kernel\nlearning for gaussian processes. International Conference on Machine Learning , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.04326",
                        "Citation Paper Title": "Title:Differentiable Compositional Kernel Learning for Gaussian Processes",
                        "Citation Paper Abstract": "Abstract:The generalization properties of Gaussian processes depend heavily on the choice of kernel, and this choice remains a dark art. We present the Neural Kernel Network (NKN), a flexible family of kernels represented by a neural network. The NKN architecture is based on the composition rules for kernels, so that each unit of the network corresponds to a valid kernel. It can compactly approximate compositional kernel structures such as those used by the Automatic Statistician (Lloyd et al., 2014), but because the architecture is differentiable, it is end-to-end trainable with gradient-based optimization. We show that the NKN is universal for the class of stationary kernels. Empirically we demonstrate pattern discovery and extrapolation abilities of NKN on several tasks that depend crucially on identifying the underlying structure, including time series and texture extrapolation, as well as Bayesian optimization.",
                        "Citation Paper Authors": "Authors:Shengyang Sun, Guodong Zhang, Chaoqi Wang, Wenyuan Zeng, Jiaman Li, Roger Grosse"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "do so by introducing nonparametric families\n8of kernels that can approximate arbitrary kernels to an arbitrary level of precision. ",
                    "Citation Text": "D. Duvenaud, J. R. Lloyd, R. Grosse, J. B. Tenenbaum, and Z. Ghahramani. Structure discovery\nin nonparametric regression through compositional kernel search. In International Conference\non Machine Learning , 2013.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1302.4922",
                        "Citation Paper Title": "Title:Structure Discovery in Nonparametric Regression through Compositional Kernel Search",
                        "Citation Paper Abstract": "Abstract:Despite its importance, choosing the structural form of the kernel in nonparametric regression remains a black art. We define a space of kernel structures which are built compositionally by adding and multiplying a small number of base kernels. We present a method for searching over this space of structures which mirrors the scientific discovery process. The learned structures can often decompose functions into interpretable components and enable long-range extrapolation on time-series datasets. Our structure search method outperforms many widely used kernels and kernel combination methods on a variety of prediction tasks.",
                        "Citation Paper Authors": "Authors:David Duvenaud, James Robert Lloyd, Roger Grosse, Joshua B. Tenenbaum, Zoubin Ghahramani"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "used Gaussian processes to capture a\nwide range of empirical function learning phenomena, while ",
                    "Citation Text": "A. G. Wilson, C. Dann, C. G. Lucas, and E. P. Xing. The human kernel. In Conference on\nNeural Information Processing Systems , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1510.07389",
                        "Citation Paper Title": "Title:The Human Kernel",
                        "Citation Paper Abstract": "Abstract:Bayesian nonparametric models, such as Gaussian processes, provide a compelling framework for automatic statistical modelling: these models have a high degree of flexibility, and automatically calibrated complexity. However, automating human expertise remains elusive; for example, Gaussian processes with standard kernels struggle on function extrapolation problems that are trivial for human learners. In this paper, we create function extrapolation problems and acquire human responses, and then design a kernel learning framework to reverse engineer the inductive biases of human learners across a set of behavioral experiments. We use the learned kernels to gain psychological insights and to extrapolate in human-like ways that go beyond traditional stationary and polynomial kernels. Finally, we investigate Occam's razor in human and Gaussian process based function learning.",
                        "Citation Paper Authors": "Authors:Andrew Gordon Wilson, Christoph Dann, Christopher G. Lucas, Eric P. Xing"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "it is used in tandem with\na siamese network architecture, as we described in Section 2.1, while ",
                    "Citation Text": "A. Aberdam, R. Litman, S. Tsiper, O. Anschel, R. Slossberg, S. Mazor, R. Manmatha, and\nP. Perona. Sequence-to-sequence contrastive learning for text recognition. arXiv:2012.10873v1 ,\n2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.10873",
                        "Citation Paper Title": "Title:Sequence-to-Sequence Contrastive Learning for Text Recognition",
                        "Citation Paper Abstract": "Abstract:We propose a framework for sequence-to-sequence contrastive learning (SeqCLR) of visual representations, which we apply to text recognition. To account for the sequence-to-sequence structure, each feature map is divided into different instances over which the contrastive loss is computed. This operation enables us to contrast in a sub-word level, where from each image we extract several positive pairs and multiple negative examples. To yield effective visual representations for text recognition, we further suggest novel augmentation heuristics, different encoder architectures and custom projection heads. Experiments on handwritten text and on scene text show that when a text decoder is trained on the learned representations, our method outperforms non-sequential contrastive methods. In addition, when the amount of supervision is reduced, SeqCLR significantly improves performance compared with supervised training, and when fine-tuned with 100% of the labels, our method achieves state-of-the-art results on standard handwritten text recognition benchmarks.",
                        "Citation Paper Authors": "Authors:Aviad Aberdam, Ron Litman, Shahar Tsiper, Oron Anschel, Ron Slossberg, Shai Mazor, R. Manmatha, Pietro Perona"
                    }
                },
                {
                    "Sentence ID": 28,
                    "Sentence": ", except with a memory bank used to sample negative examples,\nwith this approach extended to videos in ",
                    "Citation Text": "T. Pan, Y . Song, T. Yang, W. Jiang, and W. Liu. Videomoco: Contrastive video representation\nlearning with temporally adversarial examples. In Computer Vision and Pattern Recognition ,\n2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2103.05905",
                        "Citation Paper Title": "Title:VideoMoCo: Contrastive Video Representation Learning with Temporally Adversarial Examples",
                        "Citation Paper Abstract": "Abstract:MoCo is effective for unsupervised image representation learning. In this paper, we propose VideoMoCo for unsupervised video representation learning. Given a video sequence as an input sample, we improve the temporal feature representations of MoCo from two perspectives. First, we introduce a generator to drop out several frames from this sample temporally. The discriminator is then learned to encode similar feature representations regardless of frame removals. By adaptively dropping out different frames during training iterations of adversarial learning, we augment this input sample to train a temporally robust encoder. Second, we use temporal decay to model key attenuation in the memory queue when computing the contrastive loss. As the momentum encoder updates after keys enqueue, the representation ability of these keys degrades when we use the current input sample for contrastive learning. This degradation is reflected via temporal decay to attend the input sample to recent keys in the queue. As a result, we adapt MoCo to learn video representations without empirically designing pretext tasks. By empowering the temporal robustness of the encoder and modeling the temporal decay of the keys, our VideoMoCo improves MoCo temporally based on contrastive learning. Experiments on benchmark datasets including UCF101 and HMDB51 show that VideoMoCo stands as a state-of-the-art video representation learning method.",
                        "Citation Paper Authors": "Authors:Tian Pan, Yibing Song, Tianyu Yang, Wenhao Jiang, Wei Liu"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "integrates the contrastive objective with a reconstructive one. A similar\ncontrastive objective is used in ",
                    "Citation Text": "K. He, H. Fan, Y . Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual\nrepresentation learning. In Computer Visision and Pattern Recognition , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.05722",
                        "Citation Paper Title": "Title:Momentum Contrast for Unsupervised Visual Representation Learning",
                        "Citation Paper Abstract": "Abstract:We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.",
                        "Citation Paper Authors": "Authors:Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.09007v1": {
            "Paper Title": "Fungi anaesthesia",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.04651v1": {
            "Paper Title": "The whole prefrontal cortex is premotor cortex",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.04540v1": {
            "Paper Title": "Object Based Attention Through Internal Gating",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": ", potentially\novercoming some of the texture-based biases that are known in traditional CNNs ",
                    "Citation Text": "Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland\nBrendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and\nrobustness. arXiv preprint arXiv:1811.12231 , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.12231",
                        "Citation Paper Title": "Title:ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness",
                        "Citation Paper Abstract": "Abstract:Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on \"Stylized-ImageNet\", a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.",
                        "Citation Paper Authors": "Authors:Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, Wieland Brendel"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "use region-proposal networks to generate bounding boxes around where\nan object could be. Mask-RCNN ",
                    "Citation Text": "Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE\nInternational Conference on Computer Vision , pages 2961\u20132969, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.06870",
                        "Citation Paper Title": "Title:Mask R-CNN",
                        "Citation Paper Abstract": "Abstract:We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: this https URL",
                        "Citation Paper Authors": "Authors:Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, Ross Girshick"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.03688v1": {
            "Paper Title": "A Computational Model of Representation Learning in the Brain Cortex,\n  Integrating Unsupervised and Reinforcement Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.14612v1": {
            "Paper Title": "A Theory of Language Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.02948v1": {
            "Paper Title": "Neural dSCA: demixing multimodal interaction among brain areas during\n  naturalistic experiments",
            "Sentences": [
                {
                    "Sentence ID": 37,
                    "Sentence": "to extract a one-dimensional envelope,\nmel-frequency cepstral coef\ufb01cients, and chroma, resulting in a 33-dimensional feature vector. For the\nmotion features, we used 17 COCO-format three-dimensional (3D) human joint locations, which\nresulted in a 51-dimensional feature vector ",
                    "Citation Text": "Ruilong Li, Shan Yang, David A Ross, and Angjoo Kanazawa. Learn to Dance with AIST++:\nMusic Conditioned 3D Dance Generation. arXiv preprint arXiv:2101.08779 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2101.08779",
                        "Citation Paper Title": "Title:AI Choreographer: Music Conditioned 3D Dance Generation with AIST++",
                        "Citation Paper Abstract": "Abstract:We present AIST++, a new multi-modal dataset of 3D dance motion and music, along with FACT, a Full-Attention Cross-modal Transformer network for generating 3D dance motion conditioned on music. The proposed AIST++ dataset contains 5.2 hours of 3D dance motion in 1408 sequences, covering 10 dance genres with multi-view videos with known camera poses -- the largest dataset of this kind to our knowledge. We show that naively applying sequence models such as transformers to this dataset for the task of music conditioned 3D motion generation does not produce satisfactory 3D motion that is well correlated with the input music. We overcome these shortcomings by introducing key changes in its architecture design and supervision: FACT model involves a deep cross-modal transformer block with full-attention that is trained to predict $N$ future motions. We empirically show that these changes are key factors in generating long sequences of realistic dance motion that are well-attuned to the input music. We conduct extensive experiments on AIST++ with user studies, where our method outperforms recent state-of-the-art methods both qualitatively and quantitatively.",
                        "Citation Paper Authors": "Authors:Ruilong Li, Shan Yang, David A. Ross, Angjoo Kanazawa"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2106.01684v1": {
            "Paper Title": "Language Independent Speech Emotion and Non-invasive Early Detection of\n  Neurocognitive Disorder",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.00790v1": {
            "Paper Title": "Statistical Mechanics of Neural Processing of Object Manifolds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.00637v1": {
            "Paper Title": "A Blueprint for the Study of the Brain's Spatiotemporal Patterns",
            "Sentences": []
        },
        "http://arxiv.org/abs/2106.00172v1": {
            "Paper Title": "Population and Individual Firing Behaviors in Sparsely Synchronized\n  Rhythms in The Hippocampal Dentate Gyrus",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.04132v2": {
            "Paper Title": "Replay in Deep Learning: Current Approaches and Missing Biological\n  Elements",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.01127v2": {
            "Paper Title": "Microwave amplitude reflecting instability of LFP electrode ground field\n  is useful for consciousness state identification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.12916v1": {
            "Paper Title": "Robust learning from corrupted EEG with dynamic spatial filtering",
            "Sentences": [
                {
                    "Sentence ID": 43,
                    "Sentence": ") or to be highly correlated with simpler spectral power\nrepresentations ",
                    "Citation Text": "R. Schirrmeister, L. Gemein, K. Eggensperger, F. Hutter, and T. Ball. Deep learning with\nconvolutional neural networks for decoding and visualization of EEG pathology. In 2017\nIEEE Signal Processing in Medicine and Biology Symposium (SPMB) , pages 1{7, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.05051",
                        "Citation Paper Title": "Title:Deep learning with convolutional neural networks for EEG decoding and visualization",
                        "Citation Paper Abstract": "Abstract:PLEASE READ AND CITE THE REVISED VERSION at Human Brain Mapping: this http URL\nCode available here: this https URL",
                        "Citation Paper Authors": "Authors:Robin Tibor Schirrmeister, Jost Tobias Springenberg, Lukas Dominique Josef Fiederer, Martin Glasstetter, Katharina Eggensperger, Michael Tangermann, Frank Hutter, Wolfram Burgard, Tonio Ball"
                    }
                },
                {
                    "Sentence ID": 104,
                    "Sentence": "to recombine input channels into a \fxed number of virtual channels and allow models to be\ntransferred to di\u000berent montages. In the same vein, Saeed et al. presented a Transformer-like\nspatial attention module to dynamically re-order input channels ",
                    "Citation Text": "Aaqib Saeed, David Grangier, Olivier Pietquin, and Neil Zeghidour. Learning from heteroge-\nneous EEG signals with di\u000berentiable channel reordering. arXiv preprint arXiv:2010.13694 ,\n2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.13694",
                        "Citation Paper Title": "Title:Learning from Heterogeneous EEG Signals with Differentiable Channel Reordering",
                        "Citation Paper Abstract": "Abstract:We propose CHARM, a method for training a single neural network across inconsistent input channels. Our work is motivated by Electroencephalography (EEG), where data collection protocols from different headsets result in varying channel ordering and number, which limits the feasibility of transferring trained systems across datasets. Our approach builds upon attention mechanisms to estimate a latent reordering matrix from each input signal and map input channels to a canonical order. CHARM is differentiable and can be composed further with architectures expecting a consistent channel ordering to build end-to-end trainable classifiers. We perform experiments on four EEG classification datasets and demonstrate the efficacy of CHARM via simulated shuffling and masking of input channels. Moreover, our method improves the transfer of pre-trained representations between datasets collected with different protocols.",
                        "Citation Paper Authors": "Authors:Aaqib Saeed, David Grangier, Olivier Pietquin, Neil Zeghidour"
                    }
                },
                {
                    "Sentence ID": 91,
                    "Sentence": ". Combined with\ndropout, this approach substantially improved performance over previous state-of-the-art models.\nThe development of noise-invariant representations of speech signals was also investigated ",
                    "Citation Text": "Dmitriy Serdyuk, Kartik Audhkhasi, Phil\u0013 emon Brakel, Bhuvana Ramabhadran, Samuel\nThomas, and Yoshua Bengio. Invariant representations for noisy speech recognition. arXiv\npreprint arXiv:1612.01928 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1612.01928",
                        "Citation Paper Title": "Title:Invariant Representations for Noisy Speech Recognition",
                        "Citation Paper Abstract": "Abstract:Modern automatic speech recognition (ASR) systems need to be robust under acoustic variability arising from environmental, speaker, channel, and recording conditions. Ensuring such robustness to variability is a challenge in modern day neural network-based ASR systems, especially when all types of variability are not seen during training. We attempt to address this problem by encouraging the neural network acoustic model to learn invariant feature representations. We use ideas from recent research on image generation using Generative Adversarial Networks and domain adaptation ideas extending adversarial gradient-based training. A recent work from Ganin et al. proposes to use adversarial training for image domain adaptation by using an intermediate representation from the main target classification network to deteriorate the domain classifier performance through a separate neural network. Our work focuses on investigating neural architectures which produce representations invariant to noise conditions for ASR. We evaluate the proposed architecture on the Aurora-4 task, a popular benchmark for noise robust ASR. We show that our method generalizes better than the standard multi-condition training especially when only a few noise categories are seen during training.",
                        "Citation Paper Authors": "Authors:Dmitriy Serdyuk, Kartik Audhkhasi, Phil\u00e9mon Brakel, Bhuvana Ramabhadran, Samuel Thomas, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 62,
                    "Sentence": "with\f1= 0:9,\f2= 0:999, a learning rate of 10\u00003and cosine annealing. The parameters of\nall neural networks were randomly initialized using uniform He initialization ",
                    "Citation Text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into recti\fers:\nSurpassing human-level performance on imagenet classi\fcation. In ICCV , pages 1026{1034,\n2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1502.01852",
                        "Citation Paper Title": "Title:Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
                        "Citation Paper Abstract": "Abstract:Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.",
                        "Citation Paper Authors": "Authors:Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2105.10461v2": {
            "Paper Title": "Edelman's Steps Toward a Conscious Artifact",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.08111v1": {
            "Paper Title": "Livewired Neural Networks: Making Neurons That Fire Together Wire\n  Together",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.09124v2": {
            "Paper Title": "Self-Organized Criticality in the Brain",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.07140v1": {
            "Paper Title": "NeuroGen: activation optimized image synthesis for discovery\n  neuroscience",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.06418v1": {
            "Paper Title": "SCAU: Modeling spectral causality for multivariate time series with\n  applications to electroencephalograms",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.05592v1": {
            "Paper Title": "Image interpretation by iterative bottom-up top-down processing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.03961v2": {
            "Paper Title": "Is response priming based on surface color? Response to Skrzypulec\n  (2021)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.01395v1": {
            "Paper Title": "Neural Optimization: Understanding Trade-offs with Pareto Theory",
            "Sentences": []
        },
        "http://arxiv.org/abs/2105.02811v1": {
            "Paper Title": "The Reconfiguration Pattern of Individual Brain Metabolic Connectome for\n  Parkinson's Disease Identification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.06592v2": {
            "Paper Title": "A Philosophical Understanding of Representation for Neuroscience",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.09743v1": {
            "Paper Title": "Neural tuning and representational geometry",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.09549v1": {
            "Paper Title": "Adaptive Nonparametric Psychophysics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.09218v1": {
            "Paper Title": "Activity stabilization in a population model of working memory by\n  sinusoidal and noisy inputs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.07062v1": {
            "Paper Title": "Decoding of the Walking States and Step Rates from Cortical\n  Electrocorticogram Signals",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.01489v2": {
            "Paper Title": "Explanatory models in neuroscience: Part 2 -- constraint-based\n  intelligibility",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.06937v1": {
            "Paper Title": "Shared memories driven by the intrinsic memorability of items",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.03438v2": {
            "Paper Title": "Collective dynamics in the presence of finite-width pulses",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.01490v2": {
            "Paper Title": "Explanatory models in neuroscience: Part 1 -- taking mechanistic\n  abstraction seriously",
            "Sentences": []
        },
        "http://arxiv.org/abs/2104.02097v1": {
            "Paper Title": "Geodesic Fiber Tracking in White Matter using Activation Function",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.15182v1": {
            "Paper Title": "Verifying Design through Generative Visualization of Neural Activities",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.12649v1": {
            "Paper Title": "A synapse-centric account of the free energy principle",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.09709v1": {
            "Paper Title": "Do Picardy thirds smile? Tonal hierarchy and tonal valence: explicit and\n  implicit measures",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.09507v1": {
            "Paper Title": "A New Method for Features Normalization in Motor Imagery Few-Shot\n  Learning using Resting-State",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.08203v1": {
            "Paper Title": "Computational timbre and tonal system similarity analysis of the music\n  of Northern Myanmar-based Kachin compared to Xinjiang-based Uyghur ethnic\n  groups",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.06887v2": {
            "Paper Title": "Can a Fruit Fly Learn Word Embeddings?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.07966v1": {
            "Paper Title": "Active Dynamical Prospection: Modeling Mental Simulation as Particle\n  Filtering for Sensorimotor Control during Pathfinding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.06562v1": {
            "Paper Title": "A learning rule balancing energy consumption and information\n  maximization in a feed-forward neuronal network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.04636v1": {
            "Paper Title": "Prefrontal cortex functional connectivity based on simultaneous record\n  of electrical and hemodynamic responses associated with mental stress",
            "Sentences": []
        },
        "http://arxiv.org/abs/2103.02163v1": {
            "Paper Title": "To Deconvolve, or Not to Deconvolve: Inferences of Neuronal Activities\n  using Calcium Imaging Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.11812v1": {
            "Paper Title": "Mesoscale microscopy for micromammals: image analysis tools for\n  understanding the rodent brain",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.07036v2": {
            "Paper Title": "EEGs disclose significant brain activity correlated with synaptic\n  fickleness",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.05501v1": {
            "Paper Title": "A Neural Network with Local Learning Rules for Minor Subspace Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.01597v1": {
            "Paper Title": "Optimal allocation of finite sampling capacity in accumulator models of\n  multi-alternative decision making",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.11249v1": {
            "Paper Title": "Efficient Video Summarization Framework using EEG and Eye-tracking\n  Signals",
            "Sentences": [
                {
                    "Sentence ID": 27,
                    "Sentence": "uses highly complex architectures which are\nhardly explainable. Thus, these frameworks are used as black\nboxes. Recently, explainable architectures ",
                    "Citation Text": "R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and\nD. Batra, \u201cGrad-cam: Visual explanations from deep networks via\ngradient-based localization,\u201d in Proceedings of the IEEE international\nconference on computer vision , 2017, pp. 618\u2013626.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.02391",
                        "Citation Paper Title": "Title:Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization",
                        "Citation Paper Abstract": "Abstract:We propose a technique for producing \"visual explanations\" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at this https URL, along with a demo at this http URL, and a video at this http URL.",
                        "Citation Paper Authors": "Authors:Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.10617v1": {
            "Paper Title": "Identification of brain states, transitions, and communities using\n  functional MRI",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.10568v1": {
            "Paper Title": "Galvanic vestibular stimulation produces cross-modal improvements in\n  visual thresholds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.09774v1": {
            "Paper Title": "What If Memory Information is Stored Inside the Neuron, Instead of in\n  the Synapse?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.10331v1": {
            "Paper Title": "Separating Stimulus-Induced and Background Components of Dynamic\n  Functional Connectivity in Naturalistic fMRI",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": ". Many ef\ufb01cient\nand scalable algorithms such as the augmented Lagrange\nmultipliers (ALM) method ",
                    "Citation Text": "Z. Lin, M. Chen and Y . Ma, \u201cThe augmented lagrange multiplier method\nfor exact recovery of corrupted low-rank matrices,\u201d arXiv preprint\narXiv:1009.5055 , 2010.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1009.5055",
                        "Citation Paper Title": "Title:The Augmented Lagrange Multiplier Method for Exact Recovery of Corrupted Low-Rank Matrices",
                        "Citation Paper Abstract": "Abstract:This paper proposes scalable and fast algorithms for solving the Robust PCA problem, namely recovering a low-rank matrix with an unknown fraction of its entries being arbitrarily corrupted. This problem arises in many applications, such as image processing, web data ranking, and bioinformatic data analysis. It was recently shown that under surprisingly broad conditions, the Robust PCA problem can be exactly solved via convex optimization that minimizes a combination of the nuclear norm and the $\\ell^1$-norm . In this paper, we apply the method of augmented Lagrange multipliers (ALM) to solve this convex program. As the objective function is non-smooth, we show how to extend the classical analysis of ALM to such new objective functions and prove the optimality of the proposed algorithms and characterize their convergence rate. Empirically, the proposed new algorithms can be more than five times faster than the previous state-of-the-art algorithms for Robust PCA, such as the accelerated proximal gradient (APG) algorithm. Moreover, the new algorithms achieve higher precision, yet being less storage/memory demanding. We also show that the ALM technique can be used to solve the (related but somewhat simpler) matrix completion problem and obtain rather promising results too. We further prove the necessary and sufficient condition for the inexact ALM to converge globally. Matlab code of all algorithms discussed are available at this http URL",
                        "Citation Paper Authors": "Authors:Zhouchen Lin, Minming Chen, Yi Ma"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2101.09453v1": {
            "Paper Title": "Improved Training of Sparse Coding Variational Autoencoder via Weight\n  Normalization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2102.02908v1": {
            "Paper Title": "Simultaneous Confidence Corridors for neuroimaging data analysis:\n  applications to Alzheimer's Disease diagnosis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.08532v1": {
            "Paper Title": "Desynchrony and synchronisation underpinning sleep-wake cycles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.05091v2": {
            "Paper Title": "MRI Images, Brain Lesions and Deep Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.03609v1": {
            "Paper Title": "Neurocognitive Informatics Manifesto",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.01533v1": {
            "Paper Title": "On the Control of Attentional Processes in Vision",
            "Sentences": []
        },
        "http://arxiv.org/abs/2101.01538v1": {
            "Paper Title": "Is Brain-Mind Quantum? A theory and supporting evidence",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.17255v6": {
            "Paper Title": "A Cognitive Architecture for Machine Consciousness and Artificial\n  Superintelligence: Thought Is Structured by the Iterative Updating of Working\n  Memory",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.07751v6": {
            "Paper Title": "Dynamics of stochastic integrate-and-fire networks",
            "Sentences": [
                {
                    "Sentence ID": 137,
                    "Sentence": "S. Coleman and E. Weinberg, Radiative Corrections as\nthe Origin of Spontaneous Symmetry Breaking, Phys.\nRev. D 7, 1888 (1973). ",
                    "Citation Text": "M. A. Buice, J. D. Cowan, and C. C. Chow, System-\natic Fluctuation Expansion for Neural Network Activity\nEquations, Neural Comput. 22, 377 (2010).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:0902.3925",
                        "Citation Paper Title": "Title:Systematic fluctuation expansion for neural network activity equations",
                        "Citation Paper Abstract": "Abstract:  Population rate or activity equations are the foundation of a common approach to modeling for neural networks. These equations provide mean field dynamics for the firing rate or activity of neurons within a network given some connectivity. The shortcoming of these equations is that they take into account only the average firing rate while leaving out higher order statistics like correlations between firing. A stochastic theory of neural networks which includes statistics at all orders was recently formulated. We describe how this theory yields a systematic extension to population rate equations by introducing equations for correlations and appropriate coupling terms. Each level of the approximation yields closed equations, i.e. they depend only upon the mean and specific correlations of interest, without an {\\it ad hoc} criterion for doing so. We show in an example of an all-to-all connected network how our system of generalized activity equations captures phenomena missed by the mean fieldrate equations alone.",
                        "Citation Paper Authors": "Authors:Michael A. Buice, Jack D. Cowan, Carson C. Chow"
                    }
                },
                {
                    "Sentence ID": 131,
                    "Sentence": "C. C. Chow and M. A. Buice, Path Integral Methods for\nStochastic Differential Equations, J. Math. Neurosci. 5,\n1 (2015). ",
                    "Citation Text": "J. A. Hertz, Y. Roudi, and P. Sollich, Path integral\nmethods for the dynamics of stochastic and disordered\nsystems, J. Phys. A 50, 033001 (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1604.05775",
                        "Citation Paper Title": "Title:Path integral methods for the dynamics of stochastic and disordered systems",
                        "Citation Paper Abstract": "Abstract:We review some of the techniques used to study the dynamics of disordered systems subject to both quenched and fast (thermal) noise. Starting from the Martin-Siggia-Rose path integral formalism for a single variable stochastic dynamics, we provide a pedagogical survey of the perturbative, i.e. diagrammatic, approach to dynamics and how this formalism can be used for studying soft spin models. We review the supersymmetric formulation of the Langevin dynamics of these models and discuss the physical implications of the supersymmetry. We also describe the key steps involved in studying the disorder-averaged dynamics. Finally, we discuss the path integral approach for the case of hard Ising spins and review some recent developments in the dynamics of such kinetic Ising models.",
                        "Citation Paper Authors": "Authors:John A. Hertz, Yasser Roudi, Peter Sollich"
                    }
                },
                {
                    "Sentence ID": 126,
                    "Sentence": "Q. Cormier, E. Tanr\u00b4 e, and R. Veltz, Long time behav-\nior of a mean-field model of interacting neurons, Stoch.\nProc.Appl. 130, 2553 (2020),. ",
                    "Citation Text": "V. Schmutz, E. L\u00a8 ocherbach, and T. Schwalger,\nOn a finite-size neuronal population equation,\narXiv:2106.14721 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.14721",
                        "Citation Paper Title": "Title:On a finite-size neuronal population equation",
                        "Citation Paper Abstract": "Abstract:Population equations for infinitely large networks of spiking neurons have a long tradition in theoretical neuroscience. In this work, we analyze a recent generalization of these equations to populations of finite size, which takes the form of a nonlinear stochastic integral equation. We prove that, in the case of leaky integrate-and-fire (LIF) neurons with escape noise and for a slightly simplified version of the model, the equation is well-posed and stable in the sense of Br\u00e9maud-Massouli\u00e9. The proof combines methods from Markov processes taking values in the space of positive measures and nonlinear Hawkes processes. For applications, we also provide efficient simulation algorithms.",
                        "Citation Paper Authors": "Authors:Valentin Schmutz, Eva L\u00f6cherbach, Tilo Schwalger"
                    }
                },
                {
                    "Sentence ID": 122,
                    "Sentence": "R. Rosenbaum and B. Doiron, Balanced Networks of\nSpiking Neurons with Spatially Dependent Recurrent\nConnections, Phys. Rev. X 4, 021039 (2014). ",
                    "Citation Text": "A. De Masi, A. Galves, E. L\u00a8 ocherbach, and E. Presutti,\nHydrodynamic Limit for Interacting Neurons, J. Stat.\nPhys. 158, 866 (2015).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1401.4264",
                        "Citation Paper Title": "Title:Hydrodynamic limit for interacting neurons",
                        "Citation Paper Abstract": "Abstract:  This paper studies the hydrodynamic limit of a stochastic process describing the time evolution of a system with N neurons with mean-field interactions produced both by chemical and by electrical synapses. This system can be informally described as follows. Each neuron spikes randomly following a point process with rate depending on its membrane potential. At its spiking time, the membrane potential of the spiking neuron is reset to the value 0 and, simultaneously, the membrane potentials of the other neurons are increased by an amount of potential 1/N . This mimics the effect of chemical synapses. Additionally, the effect of electrical synapses is represented by a deterministic drift of all the membrane potentials towards the average value of the system.\nWe show that, as the system size N diverges, the distribution of membrane potentials becomes deterministic and is described by a limit density which obeys a non linear PDE which is a conservation law of hyperbolic type.",
                        "Citation Paper Authors": "Authors:Anna De Masi, Antonio Galves, Eva L\u00f6cherbach, Errico Presutti"
                    }
                },
                {
                    "Sentence ID": 111,
                    "Sentence": "C. Curto, C. Langdon, and K. Morrison, Combina-\ntorial Geometry of Threshold-Linear Networks , Tech.\nRep. arXiv:2008.01032 (arXiv, 2020) arXiv:2008.01032\n[math, q-bio] type: article. ",
                    "Citation Text": "D. E. Santander, S. Ebli, A. Patania, N. Sanderson,\nF. Burtscher, K. Morrison, and C. Curto, Nerve The-\norems for Fixed Points of Neural Networks, in Re-\nsearch in Computational Topology 2 , Association for\nWomen in Mathematics Series, edited by E. Gasparovic,\nV. Robins, and K. Turner (Springer International Pub-\nlishing, Cham, 2022) pp. 129\u2013165.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.11437",
                        "Citation Paper Title": "Title:Nerve theorems for fixed points of neural networks",
                        "Citation Paper Abstract": "Abstract:Nonlinear network dynamics are notoriously difficult to understand. Here we study a class of recurrent neural networks called combinatorial threshold-linear networks (CTLNs) whose dynamics are determined by the structure of a directed graph. They are a special case of TLNs, a popular framework for modeling neural activity in computational neuroscience. In prior work, CTLNs were found to be surprisingly tractable mathematically. For small networks, the fixed points of the network dynamics can often be completely determined via a series of graph rules that can be applied directly to the underlying graph. For larger networks, it remains a challenge to understand how the global structure of the network interacts with local properties. In this work, we propose a method of covering graphs of CTLNs with a set of smaller directional graphs that reflect the local flow of activity. While directional graphs may or may not have a feedforward architecture, their fixed point structure is indicative of feedforward dynamics. The combinatorial structure of the graph cover is captured by the nerve of the cover. The nerve is a smaller, simpler graph that is more amenable to graphical analysis. We present three nerve theorems that provide strong constraints on the fixed points of the underlying network from the structure of the nerve. We then illustrate the power of these theorems with some examples. Remarkably, we find that the nerve not only constrains the fixed points of CTLNs, but also gives insight into the transient and asymptotic dynamics. This is because the flow of activity in the network tends to follow the edges of the nerve.",
                        "Citation Paper Authors": "Authors:Daniela Egas Santander, Stefania Ebli, Alice Patania, Nicole Sanderson, Felicia Burtscher, Katherine Morrison, Carina Curto"
                    }
                },
                {
                    "Sentence ID": 110,
                    "Sentence": "C. Curto, J. Geneson, and K. Morrison, Fixed Points of\nCompetitive Threshold-Linear Networks, Neural Com-\nputation 31, 94 (2019). ",
                    "Citation Text": "C. Curto, C. Langdon, and K. Morrison, Combina-\ntorial Geometry of Threshold-Linear Networks , Tech.\nRep. arXiv:2008.01032 (arXiv, 2020) arXiv:2008.01032\n[math, q-bio] type: article.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.01032",
                        "Citation Paper Title": "Title:Combinatorial Geometry of Threshold-Linear Networks",
                        "Citation Paper Abstract": "Abstract:The architecture of a neural network constrains the potential dynamics that can emerge. Some architectures may only allow for a single dynamic regime, while others display a great deal of flexibility with qualitatively different dynamics that can be reached by modulating connection strengths. In this work, we develop novel mathematical techniques to study the dynamic constraints imposed by different network architectures in the context of competitive threshold-linear networks (TLNs). Any given TLN is naturally characterized by a hyperplane arrangement in $\\mathbb{R}^n$, and the combinatorial properties of this arrangement determine the pattern of fixed points of the dynamics. This observation enables us to recast the question of network flexibility in the language of oriented matroids, allowing us to employ tools and results from this theory in order to characterize the different dynamic regimes a given architecture can support. In particular, fixed points of a TLN correspond to cocircuits of an associated oriented matroid; and mutations of the matroid correspond to bifurcations in the collection of fixed points. As an application, we provide a complete characterization of all possible sets of fixed points that can arise in networks through size $n=3$, together with descriptions of how to modulate synaptic strengths of the network in order to access the different dynamic regimes. These results provide a framework for studying the possible computational roles of various motifs observed in real neural networks.",
                        "Citation Paper Authors": "Authors:Carina Curto, Christopher Langdon, Katherine Morrison"
                    }
                },
                {
                    "Sentence ID": 109,
                    "Sentence": "A. Vishwanathan et al. ,Predicting modular functions\nand neural coding of behavior from a synaptic wiring\ndiagram , bioRxiv, 359620 (2021). ",
                    "Citation Text": "C. Curto, J. Geneson, and K. Morrison, Fixed Points of\nCompetitive Threshold-Linear Networks, Neural Com-\nputation 31, 94 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.00794",
                        "Citation Paper Title": "Title:Fixed points of competitive threshold-linear networks",
                        "Citation Paper Abstract": "Abstract:Threshold-linear networks (TLNs) are models of neural networks that consist of simple, perceptron-like neurons and exhibit nonlinear dynamics that are determined by the network's connectivity. The fixed points of a TLN, including both stable and unstable equilibria, play a critical role in shaping its emergent dynamics. In this work, we provide two novel characterizations for the set of fixed points of a competitive TLN: the first is in terms of a simple sign condition, while the second relies on the concept of domination. We apply these results to a special family of TLNs, called combinatorial threshold-linear networks (CTLNs), whose connectivity matrices are defined from directed graphs. This leads us to prove a series of graph rules that enable one to determine fixed points of a CTLN by analyzing the underlying graph. Additionally, we study larger networks composed of smaller \"building block\" subnetworks, and prove several theorems relating the fixed points of the full network to those of its components. Our results provide the foundation for a kind of \"graphical calculus\" to infer features of the dynamics from a network's connectivity.",
                        "Citation Paper Authors": "Authors:Carina Curto, Jesse Geneson, Katherine Morrison"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": "F. Mastrogiuseppe and S. Ostojic, Linking Connectivity,\nDynamics, and Computations in Low-Rank Recurrent\nNeural Networks, Neuron 99, 609 (2018). ",
                    "Citation Text": "F. Schuessler, A. Dubreuil, F. Mastrogiuseppe, S. Osto-\njic, and O. Barak, Dynamics of random recurrent net-\nworks with correlated low-rank structure, Phys. Rev.\nRes.2, 013111 (2020),.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.04358",
                        "Citation Paper Title": "Title:Dynamics of random recurrent networks with correlated low-rank structure",
                        "Citation Paper Abstract": "Abstract:A given neural network in the brain is involved in many different tasks. This implies that, when considering a specific task, the network's connectivity contains a component which is related to the task and another component which can be considered random. Understanding the interplay between the structured and random components, and their effect on network dynamics and functionality is an important open question. Recent studies addressed the co-existence of random and structured connectivity, but considered the two parts to be uncorrelated. This constraint limits the dynamics and leaves the random connectivity non-functional. Algorithms that train networks to perform specific tasks typically generate correlations between structure and random connectivity. Here we study nonlinear networks with correlated structured and random components, assuming the structure to have a low rank. We develop an analytic framework to establish the precise effect of the correlations on the eigenvalue spectrum of the joint connectivity. We find that the spectrum consists of a bulk and multiple outliers, whose location is predicted by our theory. Using mean-field theory, we show that these outliers directly determine both the fixed points of the system and their stability. Taken together, our analysis elucidates how correlations allow structured and random connectivity to synergistically extend the range of computations available to networks.",
                        "Citation Paper Authors": "Authors:Friedrich Schuessler, Alexis Dubreuil, Francesca Mastrogiuseppe, Srdjan Ostojic, Omri Barak"
                    }
                },
                {
                    "Sentence ID": 47,
                    "Sentence": "J. Schuecker, S. Goedeke, and M. Helias, Optimal Se-\nquence Memory in Driven Random Networks, Phys.\nRev. X 8, 041029 (2018). ",
                    "Citation Text": "J. Aljadeff, M. Stern, and T. Sharpee, Transition to\nChaos in Random Networks with Cell-Type-Specific\nConnectivity, Phys. Rev. Lett. 114, 088101 (2015).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1407.2297",
                        "Citation Paper Title": "Title:Transition to chaos in random networks with cell-type-specific connectivity",
                        "Citation Paper Abstract": "Abstract:In neural circuits, statistical connectivity rules strongly depend on neuronal type. Here we study dynamics of neural networks with cell-type specific connectivity by extending the dynamic mean field method, and find that these networks exhibit a phase transition between silent and chaotic activity. By analyzing the locus of this transition, we derive a new result in random matrix theory: the spectral radius of a random connectivity matrix with block-structured variances. We apply our results to show how a small group of hyper-excitable neurons within the network can significantly increase the network's computational capacity.",
                        "Citation Paper Authors": "Authors:Johnatan Aljadeff, Merav Stern, Tatyana O. Sharpee"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "U. Pereira and N. Brunel, Attractor Dynamics in Net-\nworks with Learning Rules Inferred from In Vivo Data,\nNeuron 99, 227 (2018). ",
                    "Citation Text": "J. Schuecker, S. Goedeke, and M. Helias, Optimal Se-\nquence Memory in Driven Random Networks, Phys.\nRev. X 8, 041029 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.01880",
                        "Citation Paper Title": "Title:Optimal sequence memory in driven random networks",
                        "Citation Paper Abstract": "Abstract:Autonomous randomly coupled neural networks display a transition to chaos at a critical coupling strength. We here investigate the effect of a time-varying input on the onset of chaos and the resulting consequences for information processing. Dynamic mean-field theory yields the statistics of the activity, the maximum Lyapunov exponent, and the memory capacity of the network. We find an exact condition that determines the transition from stable to chaotic dynamics and the sequential memory capacity in closed form. The input suppresses chaos by a dynamic mechanism, shifting the transition to significantly larger coupling strengths than predicted by local stability analysis. Beyond linear stability, a regime of coexistent locally expansive, but non-chaotic dynamics emerges that optimizes the capacity of the network to store sequential input.",
                        "Citation Paper Authors": "Authors:Jannis Schuecker, Sven Goedeke, Moritz Helias"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "P. C. Bressloff, Path-Integral Methods for Analyzing the\nEffects of Fluctuations in Stochastic Hybrid Neural Net-\nworks, J. Math. Neurosci. 5, 4 (2015). ",
                    "Citation Text": "A. van Meegen, T. K\u00a8 uhn, and M. Helias, Large-\nDeviation Approach to Random Recurrent Neuronal\nNetworks: Parameter Inference and Fluctuation-\nInduced Transitions, Phys. Rev. Lett. 127, 158302\n(2021).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2009.08889",
                        "Citation Paper Title": "Title:Large Deviations Approach to Random Recurrent Neuronal Networks: Parameter Inference and Fluctuation-Induced Transitions",
                        "Citation Paper Abstract": "Abstract:We here unify the field theoretical approach to neuronal networks with large deviations theory. For a prototypical random recurrent network model with continuous-valued units, we show that the effective action is identical to the rate function and derive the latter using field theory. This rate function takes the form of a Kullback-Leibler divergence which enables data-driven inference of model parameters and calculation of fluctuations beyond mean-field theory. Lastly, we expose a regime with fluctuation-induced transitions between mean-field solutions.",
                        "Citation Paper Authors": "Authors:Alexander van Meegen, Tobias K\u00fchn, Moritz Helias"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "H. Sompolinsky, A. Crisanti, and H. J. Sommers, Chaos\nin Random Neural Networks, Phys. Rev. Lett. 61, 259\n(1988). ",
                    "Citation Text": "M. Stern, H. Sompolinsky, and L. F. Abbott, Dynamics\nof random neural networks with bistable units, Phys.\nRev. E 90, 062710 (2014).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1409.2535",
                        "Citation Paper Title": "Title:Dynamics of Random Neural Networks with Bistable Units",
                        "Citation Paper Abstract": "Abstract:We construct and analyze a rate-based neural network model in which self-interacting units represent clusters of neurons with strong local connectivity and random inter-unit connections reflect long-range interactions. When sufficiently strong, the self-interactions make the individual units bistable. Simulation results, mean-field calculations and stability analysis reveal the different dynamic regimes of this network and identify the locations in parameter space of its phase transitions. We identify an interesting dynamical regime exhibiting transient but long-lived chaotic activity that combines features of chaotic and multiple fixed-point attractors.",
                        "Citation Paper Authors": "Authors:Merav Stern, Haim Sompolinsky, L. F. Abbott"
                    }
                },
                {
                    "Sentence ID": 36,
                    "Sentence": "M. Mattia and P. Del Giudice, Population dynamics of\ninteracting spiking neurons, Phys. Rev. E 66, 051917\n(2002). ",
                    "Citation Text": "B. Pietras, N. Gallice, and T. Schwalger, Low-\ndimensional firing-rate dynamics for populations of\nrenewal-type spiking neurons, Phys. Rev. E 102, 022407\n(2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.06038",
                        "Citation Paper Title": "Title:Low-dimensional firing-rate dynamics for populations of renewal-type spiking neurons",
                        "Citation Paper Abstract": "Abstract:The macroscopic dynamics of large populations of neurons can be mathematically analyzed using low-dimensional firing-rate or neural-mass models. However, these models fail to capture spike synchronization effects of stochastic spiking neurons such as the non-stationary population response to rapidly changing stimuli. Here, we derive low-dimensional firing-rate models for homogeneous populations of general renewal-type neurons, including integrate-and-fire models driven by white noise. Renewal models account for neuronal refractoriness and spike synchronization dynamics. The derivation is based on an eigenmode expansion of the associated refractory density equation, which generalizes previous spectral methods for Fokker-Planck equations to arbitrary renewal models. We find a simple relation between the eigenvalues, which determine the characteristic time scales of the firing rate dynamics, and the Laplace transform of the interspike interval density or the survival function of the renewal process. Analytical expressions for the Laplace transforms are readily available for many renewal models including the leaky integrate-and-fire model. Retaining only the first eigenmode yields already an adequate low-dimensional approximation of the firing-rate dynamics that captures spike synchronization effects and fast transient dynamics at stimulus onset. We explicitly demonstrate the validity of our model for a large homogeneous population of Poisson neurons with absolute refractoriness, and other renewal models that admit an explicit analytical calculation of the eigenvalues. The here presented eigenmode expansion provides a systematic framework for novel firing-rate models in computational neuroscience based on spiking neuron dynamics with refractoriness.",
                        "Citation Paper Authors": "Authors:Bastian Pietras, No\u00e9 Gallice, Tilo Schwalger"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.00384v2": {
            "Paper Title": "Gap junctions and synchronization clusters in the Thalamic Reticular\n  Nuclei",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.08094v2": {
            "Paper Title": "Joint processing of linguistic properties in brains and language models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.01087v3": {
            "Paper Title": "Neural modulation enhancement using connectivity-based EEG neurofeedback\n  with simultaneous fMRI for emotion regulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.10107v4": {
            "Paper Title": "Physical Time and Human Time",
            "Sentences": [
                {
                    "Sentence ID": 56,
                    "Sentence": ", all taking place in \none direction of time . This is based in the various secondary arrows of time just discussed.  \nLynn  et al ",
                    "Citation Text": "Lynn,  C. W., Holmes,  C. M., Bialek,  W., and Schwab,  D. J. (2022).  Decomposing  the local  arrow  of time  in \ninteracting systems . Physical Review Letters  129:118101.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.14721",
                        "Citation Paper Title": "Title:Decomposing the local arrow of time in interacting systems",
                        "Citation Paper Abstract": "Abstract:We show that the evidence for a local arrow of time, which is equivalent to the entropy production in thermodynamic systems, can be decomposed. In a system with many degrees of freedom, there is a term that arises from the irreversible dynamics of the individual variables, and then a series of non--negative terms contributed by correlations among pairs, triplets, and higher--order combinations of variables. We illustrate this decomposition on simple models of noisy logical computations, and then apply it to the analysis of patterns of neural activity in the retina as it responds to complex dynamic visual scenes. We find that neural activity breaks detailed balance even when the visual inputs do not, and that this irreversibility arises primarily from interactions between pairs of neurons.",
                        "Citation Paper Authors": "Authors:Christopher W. Lynn, Caroline M. Holmes, William Bialek, David J. Schwab"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "so precise \noutcomes from detailed initial data , which  is necessarily  subject to the He isenberg un certainty principle  \nrelating position and momentum , cannot occur . Arguably the same is true for structure formation in the \ncosmos ",
                    "Citation Text": "Neyrinck,  M., Genel,  S., and St\u00fccker,  J. (2022 ). Boundaries  of chaos  and determinism  in the cosmos . arXiv  \npreprint  arXiv:2206.10666 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2206.10666",
                        "Citation Paper Title": "Title:Boundaries of chaos and determinism in the cosmos",
                        "Citation Paper Abstract": "Abstract:According to the standard model of cosmology, the arrangement of matter in the cosmos on scales much larger than galaxies is entirely specified by the initial conditions laid down during inflation. But zooming in by dozens of orders of magnitude to microscopic (and human?) scales, quantum randomness reigns, independent of the initial conditions. Where is the boundary of determinism, and how does that interplay with chaos? Here, we make a first attempt at answering this question in an astronomical context, including currently understood processes. The boundary is a function, at least, of length scale, position, and matter type (dark matter being more simply predictable). In intergalactic voids, the primordial pattern of density fluctuations is largely preserved. But we argue that within galaxies, the conditions are at minimum chaotic, and may even be influenced by non-primordial information, or randomness independent of the initial conditions. Randomness could be supplied by events such as supernovae and jets from active galactic nuclei (AGN) and other accretion disks, which, with the help of chaotic dynamics, could broadcast any possible microscopic randomness to larger scales, eventually throughout a galaxy. This may be generated or amplified by a recently investigated process called spontaneous stochasticity, or effective randomness in turbulent systems arising from arbitrarily small perturbations.",
                        "Citation Paper Authors": "Authors:Mark Neyrinck, Shy Genel, Jens St\u00fccker"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.12421v6": {
            "Paper Title": "Data-Driven Network Neuroscience: On Data Collection and Benchmark",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": "compared to other preprocessing tools. Some work converts preprocessed neuroimages to brain\nnetwork datasets which, however, are predominately binarized, i.e., the edge weight can take only two\nvalues 0or1. For example, in ",
                    "Citation Text": "T. Lanciano, F. Bonchi, and A. Gionis. Explainable classification of brain networks via contrast\nsubgraphs. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge\nDiscovery & Data Mining , pages 3308\u20133318, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.05176",
                        "Citation Paper Title": "Title:Explainable Classification of Brain Networks via Contrast Subgraphs",
                        "Citation Paper Abstract": "Abstract:Mining human-brain networks to discover patterns that can be used to discriminate between healthy individuals and patients affected by some neurological disorder, is a fundamental task in neuroscience. Learning simple and interpretable models is as important as mere classification accuracy. In this paper we introduce a novel approach for classifying brain networks based on extracting contrast subgraphs, i.e., a set of vertices whose induced subgraphs are dense in one class of graphs and sparse in the other. We formally define the problem and present an algorithmic solution for extracting contrast subgraphs. We then apply our method to a brain-network dataset consisting of children affected by Autism Spectrum Disorder and children Typically Developed. Our analysis confirms the interestingness of the discovered patterns, which match background knowledge in the neuroscience literature. Further analysis on other classification tasks confirm the simplicity, soundness, and high explainability of our proposal, which also exhibits superior classification accuracy, to more complex state-of-the-art methods.",
                        "Citation Paper Authors": "Authors:Tommaso Lanciano, Francesco Bonchi, Aristides Gionis"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": ". To compare the\neffectiveness of brain networks with that of BOLD signals in disease classification, we test on two\nmethods based on BOLD signals: GRU ",
                    "Citation Text": "J. Chung, C. Gulcehre, K. Cho, and Y . Bengio. Empirical evaluation of gated recurrent neural\nnetworks on sequence modeling. arXiv preprint arXiv:1412.3555 , 2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.3555",
                        "Citation Paper Title": "Title:Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
                        "Citation Paper Abstract": "Abstract:In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.",
                        "Citation Paper Authors": "Authors:Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.07161v3": {
            "Paper Title": "Spatio-temporally graded causality: a model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.10370v2": {
            "Paper Title": "A new generation of reduction methods for networks of neurons with\n  complex dynamic phenotypes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.13001v6": {
            "Paper Title": "Deep Discriminative to Kernel Density Networks for Calibrated Inference",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "as performance statistics. While measuring in-distribution calibration for the\ndatasets in OpenML-CC18 data suite, as we do not know the true distribution, we used maximum cal-\nibration error as defined by Guo et al. ",
                    "Citation Text": "Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural\nnetworks. In International conference onmachine learning, pages 1321\u20131330. PMLR, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.04599",
                        "Citation Paper Title": "Title:On Calibration of Modern Neural Networks",
                        "Citation Paper Abstract": "Abstract:Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.",
                        "Citation Paper Authors": "Authors:Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q. Weinberger"
                    }
                },
                {
                    "Sentence ID": 5,
                    "Sentence": "modified the training loss so that the downstream projected features follow a Gaussian distribution.\nHowever, there is no guarantee of performance for OOD detection for the above methods. To the best of\nour knowledge, only Meinke et al. ",
                    "Citation Text": "Alexander Meinke, Julian Bitterwolf, and Matthias Hein. Provably robust detection of out-of-\ndistribution data (almost) for free. arXiv preprint arXiv:2106.04260, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.04260",
                        "Citation Paper Title": "Title:Provably Robust Detection of Out-of-distribution Data (almost) for free",
                        "Citation Paper Abstract": "Abstract:The application of machine learning in safety-critical systems requires a reliable assessment of uncertainty. However, deep neural networks are known to produce highly overconfident predictions on out-of-distribution (OOD) data. Even if trained to be non-confident on OOD data, one can still adversarially manipulate OOD data so that the classifier again assigns high confidence to the manipulated samples. We show that two previously published defenses can be broken by better adapted attacks, highlighting the importance of robustness guarantees around OOD data. Since the existing method for this task is hard to train and significantly limits accuracy, we construct a classifier that can simultaneously achieve provably adversarially robust OOD detection and high clean accuracy. Moreover, by slightly modifying the classifier's architecture our method provably avoids the asymptotic overconfidence problem of standard neural networks. We provide code for all our experiments.",
                        "Citation Paper Authors": "Authors:Alexander Meinke, Julian Bitterwolf, Matthias Hein"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "employed likelihood ratio test for detecting OOD samples. Wan et al. ",
                    "Citation Text": "Weitao Wan, Yuanyi Zhong, Tianpeng Li, and Jiansheng Chen. Rethinking feature distribution for\n10loss functions in image classification. In Proceedings oftheIEEE conference oncomputer vision\nandpattern recognition, pages 9117\u20139126, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.02988",
                        "Citation Paper Title": "Title:Rethinking Feature Distribution for Loss Functions in Image Classification",
                        "Citation Paper Abstract": "Abstract:We propose a large-margin Gaussian Mixture (L-GM) loss for deep neural networks in classification tasks. Different from the softmax cross-entropy loss, our proposal is established on the assumption that the deep features of the training set follow a Gaussian Mixture distribution. By involving a classification margin and a likelihood regularization, the L-GM loss facilitates both a high classification performance and an accurate modeling of the training feature distribution. As such, the L-GM loss is superior to the softmax loss and its major variants in the sense that besides classification, it can be readily used to distinguish abnormal inputs, such as the adversarial examples, based on their features' likelihood to the training feature distribution. Extensive experiments on various recognition benchmarks like MNIST, CIFAR, ImageNet and LFW, as well as on adversarial examples demonstrate the effectiveness of our proposal.",
                        "Citation Paper Authors": "Authors:Weitao Wan, Yuanyi Zhong, Tianpeng Li, Jiansheng Chen"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "and Our Contributions There are a number of approaches in the literature which\nattempt to learn a generative model and control the likelihoods far away from the training data. For\nexample, Ren et al. ",
                    "Citation Text": "Jie Ren, Peter J Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark Depristo, Joshua Dillon, and\nBalaji Lakshminarayanan. Likelihood ratios for out-of-distribution detection. Advances inneural\ninformation processing systems, 32, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.02845",
                        "Citation Paper Title": "Title:Likelihood Ratios for Out-of-Distribution Detection",
                        "Citation Paper Abstract": "Abstract:Discriminative neural networks offer little or no performance guarantees when deployed on data not generated by the same process as the training distribution. On such out-of-distribution (OOD) inputs, the prediction may not only be erroneous, but confidently so, limiting the safe deployment of classifiers in real-world applications. One such challenging application is bacteria identification based on genomic sequences, which holds the promise of early detection of diseases, but requires a model that can output low confidence predictions on OOD genomic sequences from new bacteria that were not present in the training data. We introduce a genomics dataset for OOD detection that allows other researchers to benchmark progress on this important problem. We investigate deep generative model based approaches for OOD detection and observe that the likelihood score is heavily affected by population level background statistics. We propose a likelihood ratio method for deep generative models which effectively corrects for these confounding background statistics. We benchmark the OOD detection performance of the proposed method against existing approaches on the genomics dataset and show that our method achieves state-of-the-art performance. We demonstrate the generality of the proposed method by showing that it significantly improves OOD detection when applied to deep generative models of images.",
                        "Citation Paper Authors": "Authors:Jie Ren, Peter J. Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark A. DePristo, Joshua V. Dillon, Balaji Lakshminarayanan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.06348v4": {
            "Paper Title": "Can Brain Signals Reveal Inner Alignment with Human Languages?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.07455v2": {
            "Paper Title": "A Geometric Framework for Odor Representation",
            "Sentences": [
                {
                    "Sentence ID": 28,
                    "Sentence": "interference and sampling error but lack-\ning explicit consequential regions ",
                    "Citation Text": "Imam, N. & Cleland, T. A. Rapid online learning and robust recall in a neuromor-\nphic olfactory circuit. Nature Machine Intelligence 2, 181\u2013191 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.07067",
                        "Citation Paper Title": "Title:Rapid online learning and robust recall in a neuromorphic olfactory circuit",
                        "Citation Paper Abstract": "Abstract:We present a neural algorithm for the rapid online learning and identification of odorant samples under noise, based on the architecture of the mammalian olfactory bulb and implemented on the Intel Loihi neuromorphic system. As with biological olfaction, the spike timing-based algorithm utilizes distributed, event-driven computations and rapid (one-shot) online learning. Spike timing-dependent plasticity rules operate iteratively over sequential gamma-frequency packets to construct odor representations from the activity of chemosensor arrays mounted in a wind tunnel. Learned odorants then are reliably identified despite strong destructive interference. Noise resistance is further enhanced by neuromodulation and contextual priming. Lifelong learning capabilities are enabled by adult neurogenesis. The algorithm is applicable to any signal identification problem in which high-dimensional signals are embedded in unknown backgrounds.",
                        "Citation Paper Authors": "Authors:Nabil Imam, Thomas A. Cleland"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.06071v7": {
            "Paper Title": "Noise Perturbation for Saliency Prediction with Psychophysical Synthetic\n  Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.08478v2": {
            "Paper Title": "\"Task-relevant autoencoding\" enhances machine learning for human\n  neuroscience",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.04643v2": {
            "Paper Title": "Critical Learning Periods for Multisensory Integration in Deep Networks",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ", such a pre-training strategy has been successfully\napplied to text ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805 , 2018. 3, 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "found that\nearly periods of training were critical for determining the\nasymptotic network behavior. Additionally, it was found\nthat the timing of regularization was important for deter-\nmining asymptotic performance ",
                    "Citation Text": "Aditya Sharad Golatkar, Alessandro Achille, and Stefano\nSoatto. Time matters in regularizing deep networks: Weight\ndecay and data augmentation affect early learning dynamics,\nmatter little near convergence. In Advances in Neural Infor-\nmation Processing Systems 32 , pages 10677\u201310687. Curran\nAssociates, Inc., 2019. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.13277",
                        "Citation Paper Title": "Title:Time Matters in Regularizing Deep Networks: Weight Decay and Data Augmentation Affect Early Learning Dynamics, Matter Little Near Convergence",
                        "Citation Paper Abstract": "Abstract:Regularization is typically understood as improving generalization by altering the landscape of local extrema to which the model eventually converges. Deep neural networks (DNNs), however, challenge this view: We show that removing regularization after an initial transient period has little effect on generalization, even if the final loss landscape is the same as if there had been no regularization. In some cases, generalization even improves after interrupting regularization. Conversely, if regularization is applied only after the initial transient, it has no effect on the final solution, whose generalization gap is as bad as if regularization never happened. This suggests that what matters for training deep networks is not just whether or how, but when to regularize. The phenomena we observe are manifest in different datasets (CIFAR-10, CIFAR-100), different architectures (ResNet-18, All-CNN), different regularization methods (weight decay, data augmentation), different learning rate schedules (exponential, piece-wise constant). They collectively suggest that there is a ``critical period'' for regularizing deep networks that is decisive of the final performance. More analysis should, therefore, focus on the transient rather than asymptotic behavior of learning.",
                        "Citation Paper Authors": "Authors:Aditya Golatkar, Alessandro Achille, Stefano Soatto"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": ", and information-\ntheoretic approaches [20, 21].\nCritical periods in animals and deep networks: Such\narchitectural considerations often neglect the impact com-\ning from multisensory learning dynamics, where informa-\ntion can be learned at different speeds from each sensor ",
                    "Citation Text": "Nan Wu, Stanislaw Jastrzebski, Kyunghyun Cho, and\nKrzysztof J Geras. Characterizing and overcoming the\ngreedy nature of learning in multi-modal deep neural net-\nworks. In International Conference on Machine Learning ,\npages 24043\u201324055. PMLR, 2022. 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2202.05306",
                        "Citation Paper Title": "Title:Characterizing and overcoming the greedy nature of learning in multi-modal deep neural networks",
                        "Citation Paper Abstract": "Abstract:We hypothesize that due to the greedy nature of learning in multi-modal deep neural networks, these models tend to rely on just one modality while under-fitting the other modalities. Such behavior is counter-intuitive and hurts the models' generalization, as we observe empirically. To estimate the model's dependence on each modality, we compute the gain on the accuracy when the model has access to it in addition to another modality. We refer to this gain as the conditional utilization rate. In the experiments, we consistently observe an imbalance in conditional utilization rates between modalities, across multiple tasks and architectures. Since conditional utilization rate cannot be computed efficiently during training, we introduce a proxy for it based on the pace at which the model learns from each modality, which we refer to as the conditional learning speed. We propose an algorithm to balance the conditional learning speeds between modalities during training and demonstrate that it indeed addresses the issue of greedy learning. The proposed algorithm improves the model's generalization on three datasets: Colored MNIST, ModelNet40, and NVIDIA Dynamic Hand Gesture.",
                        "Citation Paper Authors": "Authors:Nan Wu, Stanis\u0142aw Jastrz\u0119bski, Kyunghyun Cho, Krzysztof J. Geras"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.08618v2": {
            "Paper Title": "The combinatorial code and the graph rules of Dale networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.11750v2": {
            "Paper Title": "Reconstructing high-order sequence features of dynamic functional\n  connectivity networks based on diversified covert attention patterns for\n  Alzheimer's disease classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.00435v2": {
            "Paper Title": "Leveraging Tendon Vibration to Enhance Pseudo-Haptic Perceptions in VR",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.13378v2": {
            "Paper Title": "Novel Machine Learning Approaches for Improving the Reproducibility and\n  Reliability of Functional and Effective Connectivity from Functional MRI",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.10634v2": {
            "Paper Title": "Interneurons accelerate learning dynamics in recurrent neural networks\n  for statistical adaptation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.09398v3": {
            "Paper Title": "Aligning individual brains with Fused Unbalanced Gromov-Wasserstein",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": ".\nAn alternative functional alignment framework has followed another path ",
                    "Citation Text": "Alexandre Gramfort, Gabriel Peyr\u00e9, and Marco Cuturi. Fast Optimal Transport Averaging of\nNeuroimaging Data . 2015. DOI:10.48550/ARXIV.1503.08596 .URL:https://arxiv.\norg/abs/1503.08596 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1503.08596",
                        "Citation Paper Title": "Title:Fast Optimal Transport Averaging of Neuroimaging Data",
                        "Citation Paper Abstract": "Abstract:Knowing how the Human brain is anatomically and functionally organized at the level of a group of healthy individuals or patients is the primary goal of neuroimaging research. Yet computing an average of brain imaging data defined over a voxel grid or a triangulation remains a challenge. Data are large, the geometry of the brain is complex and the between subjects variability leads to spatially or temporally non-overlapping effects of interest. To address the problem of variability, data are commonly smoothed before group linear averaging. In this work we build on ideas originally introduced by Kantorovich to propose a new algorithm that can average efficiently non-normalized data defined over arbitrary discrete domains using transportation metrics. We show how Kantorovich means can be linked to Wasserstein barycenters in order to take advantage of an entropic smoothing approach. It leads to a smooth convex optimization problem and an algorithm with strong convergence guarantees. We illustrate the versatility of this tool and its empirical behavior on functional neuroimaging data, functional MRI and magnetoencephalography (MEG) source estimates, defined on voxel grids and triangulations of the folded cortical surface.",
                        "Citation Paper Authors": "Authors:Alexandre Gramfort, Gabriel Peyr\u00e9, Marco Cuturi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2201.08941v3": {
            "Paper Title": "Uncovering the System Vulnerability and Criticality of Human Brain under\n  Dynamical Neuropathological Events in Alzheimer's Disease",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.11474v4": {
            "Paper Title": "Sense-making and Intensification of Affect in the Production and\n  Apprehension of Sound. The Idea of Exo-brain",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.08288v2": {
            "Paper Title": "Time-domain Classification of the Brain Reward System: Analysis of\n  Natural- and Drug-Reward Driven Local Field Potential Signals in Hippocampus\n  and Nucleus Accumbens",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.11883v2": {
            "Paper Title": "Hebbian Deep Learning Without Feedback",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.04442v4": {
            "Paper Title": "A Theory of Visibility Measures in the Dissociation Paradigm",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.13544v3": {
            "Paper Title": "Meta-Learning the Inductive Biases of Simple Neural Circuits",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.03771v4": {
            "Paper Title": "Expressive architectures enhance interpretability of dynamics-based\n  neural population models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.02972v2": {
            "Paper Title": "Decomposed Linear Dynamical Systems (dLDS) for learning the latent\n  components of neural dynamics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.04827v2": {
            "Paper Title": "Cortical origins of MacKay-type visual illusions: A case for the\n  non-linearity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.11600v2": {
            "Paper Title": "Chronic iEEG recordings and interictal spike rate reveal multiscale\n  temporal modulations in seizure states",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.06884v2": {
            "Paper Title": "Multi-tasking via baseline control in recurrent neural networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.13426v2": {
            "Paper Title": "Bayesian Active Learning for Discrete Latent Variable Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.03523v2": {
            "Paper Title": "Winning the lottery with neural connectivity constraints: faster\n  learning across cognitive tasks with spatially constrained sparse RNNs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.01960v2": {
            "Paper Title": "FingerFlex: Inferring Finger Trajectories from ECoG signals",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.10414v3": {
            "Paper Title": "Quantifying Extrinsic Curvature in Neural Manifolds",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.01201v4": {
            "Paper Title": "Human alignment of neural network representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.06129v3": {
            "Paper Title": "A Synapse-Threshold Synergistic Learning Approach for Spiking Neural\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.11478v2": {
            "Paper Title": "Neural Co-Processors for Restoring Brain Function: Results from a\n  Cortical Model of Grasping",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.01685v2": {
            "Paper Title": "Toward a realistic model of speech processing in the brain with\n  self-supervised learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.02842v4": {
            "Paper Title": "Open-Source Tools for Behavioral Video Analysis: Setup, Methods, and\n  Development",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.10571v2": {
            "Paper Title": "Neuromorphic Networks as Revealed by Features Similarity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.15752v2": {
            "Paper Title": "Constrained Predictive Coding as a Biologically Plausible Model of the\n  Cortical Hierarchy",
            "Sentences": [
                {
                    "Sentence ID": 78,
                    "Sentence": ",\nhas provided many insights into the learning dynamics of deep networks. Some of the properties\ndiscovered in the deep linear network, like \u201cbalancedness\" of weights [77, 78], generalize for certain\nnonlinear networks ",
                    "Citation Text": "Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homoge-\nneous models: Layers are automatically balanced. Advances in Neural Information Processing\nSystems , 31, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.00900",
                        "Citation Paper Title": "Title:Algorithmic Regularization in Learning Deep Homogeneous Models: Layers are Automatically Balanced",
                        "Citation Paper Abstract": "Abstract:We study the implicit regularization imposed by gradient descent for learning multi-layer homogeneous functions including feed-forward fully connected and convolutional deep neural networks with linear, ReLU or Leaky ReLU activation. We rigorously prove that gradient flow (i.e. gradient descent with infinitesimal step size) effectively enforces the differences between squared norms across different layers to remain invariant without any explicit regularization. This result implies that if the weights are initially small, gradient flow automatically balances the magnitudes of all layers. Using a discretization argument, we analyze gradient descent with positive step size for the non-convex low-rank asymmetric matrix factorization problem without any regularization. Inspired by our findings for gradient flow, we prove that gradient descent with step sizes $\\eta_t = O\\left(t^{-\\left( \\frac12+\\delta\\right)} \\right)$ ($0<\\delta\\le\\frac12$) automatically balances two low-rank factors and converges to a bounded global optimum. Furthermore, for rank-$1$ asymmetric matrix factorization we give a finer analysis showing gradient descent with constant step size converges to the global minimum at a globally linear rate. We believe that the idea of examining the invariance imposed by first order algorithms in learning homogeneous models could serve as a fundamental building block for studying optimization for learning deep models.",
                        "Citation Paper Authors": "Authors:Simon S. Du, Wei Hu, Jason D. Lee"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.06122v2": {
            "Paper Title": "Modeling the Shape of the Brain Connectome via Deep Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.10898v2": {
            "Paper Title": "Training language models to summarize narratives improves brain\n  alignment",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.05354v2": {
            "Paper Title": "Phenomenological modeling of diverse and heterogeneous synaptic dynamics\n  at natural density",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.03155v2": {
            "Paper Title": "Understanding Neural Coding on Latent Manifolds by Sharing Features and\n  Dividing Ensembles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.14751v3": {
            "Paper Title": "Low-dimensional models of single neurons: A review",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.05081v2": {
            "Paper Title": "A Macrocolumn Architecture Implemented with Spiking Neurons",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.10891v2": {
            "Paper Title": "Generative Sampling in Bundle Tractography using Autoencoders (GESTA)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.13620v2": {
            "Paper Title": "Reconstruction-guided attention improves the robustness and shape\n  processing of neural networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.16414v5": {
            "Paper Title": "Meta-Learning Biologically Plausible Plasticity Rules with Random\n  Feedback Pathways",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "rely on weight decay to approximately align forward and\nbackward weights ",
                    "Citation Text": "M. Akrout, C. Wilson, P. Humphreys, T. Lillicrap, and D. B. Tweed. Deep learning\nwithout weight transport. Advances in neural information processing systems , 32, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.05391",
                        "Citation Paper Title": "Title:Deep Learning without Weight Transport",
                        "Citation Paper Abstract": "Abstract:Current algorithms for deep learning probably cannot run in the brain because they rely on weight transport, where forward-path neurons transmit their synaptic weights to a feedback path, in a way that is likely impossible biologically. An algorithm called feedback alignment achieves deep learning without weight transport by using random feedback weights, but it performs poorly on hard visual-recognition tasks. Here we describe two mechanisms - a neural circuit called a weight mirror and a modification of an algorithm proposed by Kolen and Pollack in 1994 - both of which let the feedback path learn appropriate synaptic weights quickly and accurately even in large networks, without weight transport or complex wiring.Tested on the ImageNet visual-recognition task, these mechanisms outperform both feedback alignment and the newer sign-symmetry method, and nearly match backprop, the standard algorithm of deep learning, which uses weight transport.",
                        "Citation Paper Authors": "Authors:Mohamed Akrout, Collin Wilson, Peter C. Humphreys, Timothy Lillicrap, Douglas Tweed"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "uses learnable auxiliary feedback and lateral\nconnections to facilitate error propagation during training and meta-learns the plasticity rules\nto update these connections. Finally, Sandler et al. ",
                    "Citation Text": "M. Sandler, M. Vladymyrov, A. Zhmoginov, N. Miller, T. Madams, A. Jackson, and\nB. A. Y. Arcas. Meta-learning bidirectional update rules. In International Conference\non Machine Learning , pages 9288{9300. PMLR, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.04657",
                        "Citation Paper Title": "Title:Meta-Learning Bidirectional Update Rules",
                        "Citation Paper Abstract": "Abstract:In this paper, we introduce a new type of generalized neural network where neurons and synapses maintain multiple states. We show that classical gradient-based backpropagation in neural networks can be seen as a special case of a two-state network where one state is used for activations and another for gradients, with update rules derived from the chain rule. In our generalized framework, networks have neither explicit notion of nor ever receive gradients. The synapses and neurons are updated using a bidirectional Hebb-style update rule parameterized by a shared low-dimensional \"genome\". We show that such genomes can be meta-learned from scratch, using either conventional optimization techniques, or evolutionary strategies, such as CMA-ES. Resulting update rules generalize to unseen tasks and train faster than gradient descent based optimizers for several standard computer vision and synthetic tasks.",
                        "Citation Paper Authors": "Authors:Mark Sandler, Max Vladymyrov, Andrey Zhmoginov, Nolan Miller, Andrew Jackson, Tom Madams, Blaise Aguera y Arcas"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.11665v2": {
            "Paper Title": "Representational dissimilarity metric spaces for stochastic neural\n  networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.15283v2": {
            "Paper Title": "A library of quantitative markers of seizure severity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.01848v2": {
            "Paper Title": "Explaining Patterns in Data with Language Models via Interpretable\n  Autoprompting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.06862v6": {
            "Paper Title": "Deep learning in a bilateral brain with hemispheric specialization",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": "which generates class-hierarchy-aware episodes of data and the Tiered-ImageNet of\nCaccia et al. ",
                    "Citation Text": "Massimo Caccia, Pau Rodriguez, Oleksiy Ostapenko, Fabrice Normandin, Min Lin, Lucas Page-Caccia, Is-\nsam Hadj Laradji, Irina Rish, Alexandre Lacoste, David V\u00e1zquez, and Laurent Charlin. Online fast adaptation\nand knowledge accumulation (osaka): a new approach to continual learning. Advances in Neural Information\nProcessing Systems , 33:16532\u201316545, 2020. URL https://github.com/ElementAI/osaka .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.05856",
                        "Citation Paper Title": "Title:Online Fast Adaptation and Knowledge Accumulation: a New Approach to Continual Learning",
                        "Citation Paper Abstract": "Abstract:Continual learning studies agents that learn from streams of tasks without forgetting previous ones while adapting to new ones. Two recent continual-learning scenarios have opened new avenues of research. In meta-continual learning, the model is pre-trained to minimize catastrophic forgetting of previous tasks. In continual-meta learning, the aim is to train agents for faster remembering of previous tasks through adaptation. In their original formulations, both methods have limitations. We stand on their shoulders to propose a more general scenario, OSAKA, where an agent must quickly solve new (out-of-distribution) tasks, while also requiring fast remembering. We show that current continual learning, meta-learning, meta-continual learning, and continual-meta learning techniques fail in this new scenario. We propose Continual-MAML, an online extension of the popular MAML algorithm as a strong baseline for this scenario. We empirically show that Continual-MAML is better suited to the new scenario than the aforementioned methodologies, as well as standard continual learning and meta-learning approaches.",
                        "Citation Paper Authors": "Authors:Massimo Caccia, Pau Rodriguez, Oleksiy Ostapenko, Fabrice Normandin, Min Lin, Lucas Caccia, Issam Laradji, Irina Rish, Alexandre Lacoste, David Vazquez, Laurent Charlin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.09224v2": {
            "Paper Title": "Self-Supervised Learning Through Efference Copies",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.01338v4": {
            "Paper Title": "Biologically-plausible backpropagation through arbitrary timespans via\n  local neuromodulators",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.00823v3": {
            "Paper Title": "Beyond accuracy: generalization properties of bio-plausible temporal\n  credit assignment rules",
            "Sentences": [
                {
                    "Sentence ID": 5,
                    "Sentence": ". Recently, many\nempirical studies have consistently supported the usefulness of this predictor [ 109\u2013116]. In particular,\nthe authors of ",
                    "Citation Text": "Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fan-\ntastic generalization measures and where to \ufb01nd them. arXiv preprint arXiv:1912.02178 ,\n2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.02178",
                        "Citation Paper Title": "Title:Fantastic Generalization Measures and Where to Find Them",
                        "Citation Paper Abstract": "Abstract:Generalization of deep networks has been of great interest in recent years, resulting in a number of theoretically and empirically motivated complexity measures. However, most papers proposing such measures study only a small set of models, leaving open the question of whether the conclusion drawn from those experiments would remain valid in other settings. We present the first large scale study of generalization in deep networks. We investigate more then 40 complexity measures taken from both theoretical bounds and empirical studies. We train over 10,000 convolutional networks by systematically varying commonly used hyperparameters. Hoping to uncover potentially causal relationships between each measure and generalization, we analyze carefully controlled experiments and show surprising failures of some measures as well as promising measures for further research.",
                        "Citation Paper Authors": "Authors:Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, Samy Bengio"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.02249v3": {
            "Paper Title": "Rapid Learning of Spatial Representations for Goal-Directed Navigation\n  Based on a Novel Model of Hippocampal Place Fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.05922v2": {
            "Paper Title": "Internal feedback in the cortical perception-action loop enables fast\n  and accurate behavior",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.13493v2": {
            "Paper Title": "Mesoscopic modeling of hidden spiking neurons",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": "is restricted to stationary input whereas we\nuse \u2018renewal-type\u2019 in the broader sense that also includes time-dependent input.\n3t\u00001. The \ufb01nite-size correction term stabilizes the model by enforcing the approximate neuronal\nmass conservationP\na\u00151St;ant\u0000a\u0019N(see ",
                    "Citation Text": "Valentin Schmutz, Eva L\u00f6cherbach, and Tilo Schwalger. On a \ufb01nite-size neuronal population\nequation. arXiv preprint arXiv:2106.14721 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.14721",
                        "Citation Paper Title": "Title:On a finite-size neuronal population equation",
                        "Citation Paper Abstract": "Abstract:Population equations for infinitely large networks of spiking neurons have a long tradition in theoretical neuroscience. In this work, we analyze a recent generalization of these equations to populations of finite size, which takes the form of a nonlinear stochastic integral equation. We prove that, in the case of leaky integrate-and-fire (LIF) neurons with escape noise and for a slightly simplified version of the model, the equation is well-posed and stable in the sense of Br\u00e9maud-Massouli\u00e9. The proof combines methods from Markov processes taking values in the space of positive measures and nonlinear Hawkes processes. For applications, we also provide efficient simulation algorithms.",
                        "Citation Paper Authors": "Authors:Valentin Schmutz, Eva L\u00f6cherbach, Tilo Schwalger"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.06865v6": {
            "Paper Title": "Sketch of a novel approach to a neural model",
            "Sentences": [
                {
                    "Sentence ID": 35,
                    "Sentence": ".\nThey may mimic it by pattern generation for images or text. Certain modi\ufb01cations of weight-adjusting neural networks\nwere proposed on technical grounds ",
                    "Citation Text": "A. Graves, G. Wayne, and I. Danihelka. Neural Turing Machines . arXiv. Oct. 2014. DOI:10.48550/ARXIV.\n1410.5401 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1410.5401",
                        "Citation Paper Title": "Title:Neural Turing Machines",
                        "Citation Paper Abstract": "Abstract:We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.",
                        "Citation Paper Authors": "Authors:Alex Graves, Greg Wayne, Ivo Danihelka"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.09829v2": {
            "Paper Title": "Capturing cross-session neural population variability through\n  self-supervised identification of consistent neuron ensembles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.06889v2": {
            "Paper Title": "NREM and REM: cognitive and energetic gains in thalamo-cortical sleeping\n  and awake spiking model",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.05436v4": {
            "Paper Title": "Coherence resonance and stochastic synchronization in a small-world\n  neural network: An interplay in the presence of spike-timing-dependent\n  plasticity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.13260v2": {
            "Paper Title": "Novel Reinforcement Learning Algorithm for Suppressing Synchronization\n  in Closed Loop Deep Brain Stimulators",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.14787v1": {
            "Paper Title": "Integrated information theory (IIT) 4.0: Formulating the properties of\n  phenomenal existence in physical terms",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.13571v2": {
            "Paper Title": "Cortical Xi-Alpha model for resting state electric neuronal activity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.14746v1": {
            "Paper Title": "MindBigData 2022 A Large Dataset of Brain Signals",
            "Sentences": [
                {
                    "Sentence ID": 72,
                    "Sentence": ".  \nhttps://dms.cs.ucy.ac.cy/op/op.Download.php?docum\nentid=16735&version=1  \n- Object classification from randomized EEG trials ",
                    "Citation Text": "Hamad Ahmed, Ronnie B Wilbur,Hari M \nBharadwaj and Jeffrey Mark, Object classification \nfrom randomized EEG trials , Purdue University USA  \n2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.06046",
                        "Citation Paper Title": "Title:Object classification from randomized EEG trials",
                        "Citation Paper Abstract": "Abstract:New results suggest strong limits to the feasibility of classifying human brain activity evoked from image stimuli, as measured through EEG. Considerable prior work suffers from a confound between the stimulus class and the time since the start of the experiment. A prior attempt to avoid this confound using randomized trials was unable to achieve results above chance in a statistically significant fashion when the data sets were of the same size as the original experiments. Here, we again attempt to replicate these experiments with randomized trials on a far larger (20x) dataset of 1,000 stimulus presentations of each of forty classes, all from a single subject. To our knowledge, this is the largest such EEG data collection effort from a single subject and is at the bounds of feasibility. We obtain classification accuracy that is marginally above chance and above chance in a statistically significant fashion, and further assess how accuracy depends on the classifier used, the amount of training data used, and the number of classes. Reaching the limits of data collection without substantial improvement in classification accuracy suggests limits to the feasibility of this enterprise.",
                        "Citation Paper Authors": "Authors:Hamad Ahmed, Ronnie B Wilbur, Hari M Bharadwaj, Jeffrey Mark Siskind"
                    }
                },
                {
                    "Sentence ID": 68,
                    "Sentence": ".  \nhttps://www.emerald.com/insight/content/doi/10.110\n8/IJQRM -07-2021 -0237/full/html  \n- Toward reliable signals decoding for \nelect roencephalogram: a benchmark study to eegnex ",
                    "Citation Text": "Xia Chen, Xiangbin Tengb, Han Chenc, Yafeng \nPanc & Philipp Geyerd , Toward reliable signals \ndecoding for electroencephalogram: a benchmark \nstudy to eegnex,  Leibniz University Hannover, \nGermany, Department of Education and Psychology, \nFreie Universit\u00e4t Berlin, Germany & Department of \nPsychology and Behavioral Sciences, Zhejiang \nUniversity, China   Nov-2022 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2207.12369",
                        "Citation Paper Title": "Title:Toward reliable signals decoding for electroencephalogram: A benchmark study to EEGNeX",
                        "Citation Paper Abstract": "Abstract:This study examines the efficacy of various neural network (NN) models in interpreting mental constructs via electroencephalogram (EEG) signals. Through the assessment of 16 prevalent NN models and their variants across four brain-computer interface (BCI) paradigms, we gauged their information representation capability. Rooted in comprehensive literature review findings, we proposed EEGNeX, a novel, purely ConvNet-based architecture. We pitted it against both existing cutting-edge strategies and the Mother of All BCI Benchmarks (MOABB) involving 11 distinct EEG motor imagination (MI) classification tasks and revealed that EEGNeX surpasses other state-of-the-art methods. Notably, it shows up to 2.1%-8.5% improvement in the classification accuracy in different scenarios with statistical significance (p < 0.05) compared to its competitors. This study not only provides deeper insights into designing efficient NN models for EEG data but also lays groundwork for future explorations into the relationship between bioelectric brain signals and NN architectures. For the benefit of broader scientific collaboration, we have made all benchmark models, including EEGNeX, publicly available at (this https URL).",
                        "Citation Paper Authors": "Authors:Xia Chen, Xiangbin Teng, Han Chen, Yafeng Pan, Philipp Geyer"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.13285v1": {
            "Paper Title": "On the Level Sets and Invariance of Neural Tuning Landscapes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.10367v1": {
            "Paper Title": "Modeling Human Eye Movements with Neural Networks in a Maze-Solving Task",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.09729v1": {
            "Paper Title": "Bistable perception, precision and neuromodulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.08211v1": {
            "Paper Title": "Sequence generation in inhibition-dominated neural networks",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": ".\nCyclic unions. The most straightforward generalization of a cycle is the cyclic union , an architecture \ufb01rst\nintroduced in ",
                    "Citation Text": "C. Curto, J. Geneson, and K. Morrison. Fixed points of competitive threshold-linear networks. Neural Comput. , 31(1):94\u2013\n155, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1804.00794",
                        "Citation Paper Title": "Title:Fixed points of competitive threshold-linear networks",
                        "Citation Paper Abstract": "Abstract:Threshold-linear networks (TLNs) are models of neural networks that consist of simple, perceptron-like neurons and exhibit nonlinear dynamics that are determined by the network's connectivity. The fixed points of a TLN, including both stable and unstable equilibria, play a critical role in shaping its emergent dynamics. In this work, we provide two novel characterizations for the set of fixed points of a competitive TLN: the first is in terms of a simple sign condition, while the second relies on the concept of domination. We apply these results to a special family of TLNs, called combinatorial threshold-linear networks (CTLNs), whose connectivity matrices are defined from directed graphs. This leads us to prove a series of graph rules that enable one to determine fixed points of a CTLN by analyzing the underlying graph. Additionally, we study larger networks composed of smaller \"building block\" subnetworks, and prove several theorems relating the fixed points of the full network to those of its components. Our results provide the foundation for a kind of \"graphical calculus\" to infer features of the dynamics from a network's connectivity.",
                        "Citation Paper Authors": "Authors:Carina Curto, Jesse Geneson, Katherine Morrison"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2301.07171v1": {
            "Paper Title": "fMRI-based Static and Dynamic Functional Connectivity Analysis for\n  Post-stroke Motor Dysfunction Patient: A Review",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.07790v1": {
            "Paper Title": "Population Template-Based Brain Graph Augmentation for Improving\n  One-Shot Learning Classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.05316v1": {
            "Paper Title": "Graph-Regularized Manifold-Aware Conditional Wasserstein GAN for Brain\n  Functional Connectivity Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2301.04570v1": {
            "Paper Title": "Barriers and Solutions to the Adoption of Clinical Tools for\n  Computational Psychiatry",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.04377v1": {
            "Paper Title": "Models Developed for Spiking Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.04316v1": {
            "Paper Title": "Bio-Inspired, Task-Free Continual Learning through Activity\n  Regularization",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.04951v1": {
            "Paper Title": "EEG-NeXt: A Modernized ConvNet for The Classification of Cognitive\n  Activity from EEG",
            "Sentences": []
        },
        "http://arxiv.org/abs/2301.05397v1": {
            "Paper Title": "The problem with AI consciousness: A neurogenetic case against synthetic\n  sentience",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.13413v2": {
            "Paper Title": "Reinforcement Learning with Non-Exponential Discounting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2301.01619v1": {
            "Paper Title": "Self-assembled neuromorphic networks at self-organized criticality in\n  Ag-hBN platform",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.03329v1": {
            "Paper Title": "Enhancing Low-Density EEG-Based Brain-Computer Interfaces with\n  Similarity-Keeping Knowledge Distillation",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": ".\nB. Feature-based knowledge distillation\nThe \ufb01rst feature-based KD was proposed as the FitNets ",
                    "Citation Text": "A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y . Ben-\ngio, \u201cFitnets: Hints for thin deep nets,\u201d arXiv preprint arXiv:1412.6550 ,\n2014.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1412.6550",
                        "Citation Paper Title": "Title:FitNets: Hints for Thin Deep Nets",
                        "Citation Paper Abstract": "Abstract:While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network.",
                        "Citation Paper Authors": "Authors:Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, Yoshua Bengio"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": ". As shown in\nFig. 4, we chose the two 4-electrode montages to mimic the\nmontages of headphone-like and headband-like low-density\nEEG devices.\nNeural network architecture for EEG decoding. We\nemployed representative EEG decoding neural networks, Shal-\nlowConvNet ",
                    "Citation Text": "R. T. Schirrmeister, J. T. Springenberg, L. D. J. Fiederer, M. Glasstetter,\nK. Eggensperger, M. Tangermann, F. Hutter, W. Burgard, and T. Ball,\n\u201cDeep learning with convolutional neural networks for eeg decoding\nand visualization,\u201d Hum. Brain Mapp. , aug 2017. [Online]. Available:\nhttp://dx.doi.org/10.1002/hbm.23730",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.05051",
                        "Citation Paper Title": "Title:Deep learning with convolutional neural networks for EEG decoding and visualization",
                        "Citation Paper Abstract": "Abstract:PLEASE READ AND CITE THE REVISED VERSION at Human Brain Mapping: this http URL\nCode available here: this https URL",
                        "Citation Paper Authors": "Authors:Robin Tibor Schirrmeister, Jost Tobias Springenberg, Lukas Dominique Josef Fiederer, Martin Glasstetter, Katharina Eggensperger, Michael Tangermann, Frank Hutter, Wolfram Burgard, Tonio Ball"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "allows one\nsingle small model to be much faster but with only negligible\nperformance loss by learning a large-scale dataset labeled by\na complex model ensemble.\nLater on, Hinton et al. ",
                    "Citation Text": "G. Hinton, O. Vinyals, J. Dean et al. , \u201cDistilling the knowledge in a\nneural network,\u201d arXiv preprint arXiv:1503.02531 , vol. 2, no. 7, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1503.02531",
                        "Citation Paper Title": "Title:Distilling the Knowledge in a Neural Network",
                        "Citation Paper Abstract": "Abstract:A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.",
                        "Citation Paper Authors": "Authors:Geoffrey Hinton, Oriol Vinyals, Jeff Dean"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "to facilitate multi-step model training in a multi-layer\nLSTM model for EEG-based emotion recognition. Joshi et al. ",
                    "Citation Text": "V . Joshi, S. Vijayarangan, P. SP, and M. Sivaprakasam, \u201cA deep knowl-\nedge distillation framework for eeg assisted enhancement of single-lead\necg based sleep staging,\u201d arXiv preprint arXiv:2112.07252 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2112.07252",
                        "Citation Paper Title": "Title:A Deep Knowledge Distillation framework for EEG assisted enhancement of single-lead ECG based sleep staging",
                        "Citation Paper Abstract": "Abstract:Automatic Sleep Staging study is presently done with the help of Electroencephalogram (EEG) signals. Recently, Deep Learning (DL) based approaches have enabled significant progress in this area, allowing for near-human accuracy in automated sleep staging. However, EEG based sleep staging requires an extensive as well as an expensive clinical setup. Moreover, the requirement of an expert for setup and the added inconvenience to the subject under study renders it unfavourable in a point of care context. Electrocardiogram (ECG), an unobtrusive alternative to EEG, is more suitable, but its performance, unsurprisingly, remains sub-par compared to EEG-based sleep staging. Naturally, it would be helpful to transfer knowledge from EEG to ECG, ultimately enhancing the model's performance on ECG based inputs. Knowledge Distillation (KD) is a renowned concept in DL that looks to transfer knowledge from a better but potentially more cumbersome teacher model to a compact student model. Building on this concept, we propose a cross-modal KD framework to improve ECG-based sleep staging performance with assistance from features learned through models trained on EEG. Additionally, we also conducted multiple experiments on the individual components of the proposed model to get better insight into the distillation approach. Data of 200 subjects from the Montreal Archive of Sleep Studies (MASS) was utilized for our study. The proposed model showed a 14.3\\% and 13.4\\% increase in weighted-F1-score in 4-class and 3-class sleep staging, respectively. This demonstrates the viability of KD for performance improvement of single-channel ECG based sleep staging in 4-class(W-L-D-R) and 3-class(W-N-R) classification.",
                        "Citation Paper Authors": "Authors:Vaibhav Joshi, Sricharan Vijayarangan, Preejith SP, Mohanasankar Sivaprakasam"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "took the correlation congruence be-\ntween data samples into account and minimized the pairwise\ncorrelation difference between the teacher and the student.\nRelational knowledge distillation (RKD) ",
                    "Citation Text": "W. Park, D. Kim, Y . Lu, and M. Cho, \u201cRelational knowledge distilla-\ntion,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , 2019, pp. 3967\u20133976.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.05068",
                        "Citation Paper Title": "Title:Relational Knowledge Distillation",
                        "Citation Paper Abstract": "Abstract:Knowledge distillation aims at transferring knowledge acquired in one model (a teacher) to another model (a student) that is typically smaller. Previous approaches can be expressed as a form of training the student to mimic output activations of individual data examples represented by the teacher. We introduce a novel approach, dubbed relational knowledge distillation (RKD), that transfers mutual relations of data examples instead. For concrete realizations of RKD, we propose distance-wise and angle-wise distillation losses that penalize structural differences in relations. Experiments conducted on different tasks show that the proposed method improves educated student models with a significant margin. In particular for metric learning, it allows students to outperform their teachers' performance, achieving the state of the arts on standard benchmark datasets.",
                        "Citation Paper Authors": "Authors:Wonpyo Park, Dongju Kim, Yan Lu, Minsu Cho"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2212.03300v1": {
            "Paper Title": "Supervised Tractogram Filtering using Geometric Deep Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.08718v2": {
            "Paper Title": "Topology of cognitive maps",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.08601v2": {
            "Paper Title": "Comparative study of machine learning and deep learning methods on ASD\n  classification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.00596v1": {
            "Paper Title": "Language models and brain alignment: beyond word-level semantics and\n  prediction",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.00555v1": {
            "Paper Title": "A Structure-guided Effective and Temporal-lag Connectivity Network for\n  Revealing Brain Disorder Mechanisms",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.00450v1": {
            "Paper Title": "The relationship between the interdisciplinary activation of children's\n  scientific concepts and their mastery of basic knowledges: a pre study based\n  on reaction times",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.00081v1": {
            "Paper Title": "Efficient multi-scale representation of visual objects using a\n  biologically plausible spike-latency code and winner-take-all inhibition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.16599v1": {
            "Paper Title": "Compression supports low-dimensional representations of behavior across\n  neural circuits",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.15963v1": {
            "Paper Title": "The human digital twin brain in the resting state and in action",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.02226v1": {
            "Paper Title": "Inferring latent neural sources via deep transcoding of simultaneously\n  acquired EEG and fMRI",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.05242v2": {
            "Paper Title": "Neural Circuit Architectural Priors for Embodied Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.12935v1": {
            "Paper Title": "Functional Connectome: Approximating Brain Networks with Artificial\n  Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.14194v2": {
            "Paper Title": "Free energy and inference in living systems",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.16993v3": {
            "Paper Title": "STN: a new tensor network method to identify stimulus category from\n  brain activity pattern",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.10713v1": {
            "Paper Title": "A privacy-preserving data storage and service framework based on deep\n  learning and blockchain for construction workers' wearable IoT sensors",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.09058v1": {
            "Paper Title": "Stability of topological descriptors for neuronal morphology",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.08497v1": {
            "Paper Title": "Characterizing the structure of mouse behavior using Motion Sequencing",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.05701v2": {
            "Paper Title": "Diffusion Tensor Estimation with Transformer Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.07507v1": {
            "Paper Title": "High-Accuracy Machine Learning Techniques for Functional Connectome\n  Fingerprinting and Cognitive State Decoding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.07374v1": {
            "Paper Title": "New Interpretable Patterns and Discriminative Features from Brain\n  Functional Network Connectivity Using Dictionary Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.04764v1": {
            "Paper Title": "Quantitative Susceptibility Mapping in Cognitive Decline: A Review of\n  Technical Aspects and Applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.02700v2": {
            "Paper Title": "Achieving mouse-level strategic evasion performance using real-time\n  computational planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.00416v2": {
            "Paper Title": "Higher-order mutual information reveals synergistic sub-networks for\n  multi-neuron importance",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": ".\n6 Conclusion\nIn this work, to the best of our knowledge, we proposed the \ufb01rst method for quantifying multi-neuron\nfeature importance ",
                    "Citation Text": "Matthew L. Leavitt, M. Leavitt, Matthew L. Leavitt, and A. S. Morcos. Towards falsi\ufb01able\ninterpretability research. . MAG ID: 3094509718.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.12016",
                        "Citation Paper Title": "Title:Towards falsifiable interpretability research",
                        "Citation Paper Abstract": "Abstract:Methods for understanding the decisions of and mechanisms underlying deep neural networks (DNNs) typically rely on building intuition by emphasizing sensory or semantic features of individual examples. For instance, methods aim to visualize the components of an input which are \"important\" to a network's decision, or to measure the semantic properties of single neurons. Here, we argue that interpretability research suffers from an over-reliance on intuition-based approaches that risk-and in some cases have caused-illusory progress and misleading conclusions. We identify a set of limitations that we argue impede meaningful progress in interpretability research, and examine two popular classes of interpretability methods-saliency and single-neuron-based approaches-that serve as case studies for how overreliance on intuition and lack of falsifiability can undermine interpretability research. To address these concerns, we propose a strategy to address these impediments in the form of a framework for strongly falsifiable interpretability research. We encourage researchers to use their intuitions as a starting point to develop and test clear, falsifiable hypotheses, and hope that our framework yields robust, evidence-based interpretability methods that generate meaningful advances in our understanding of DNNs.",
                        "Citation Paper Authors": "Authors:Matthew L. Leavitt, Ari Morcos"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.02868v1": {
            "Paper Title": "Lightweight 3D Convolutional Neural Network for Schizophrenia diagnosis\n  using MRI Images and Ensemble Bagging Classifier",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.10189v2": {
            "Paper Title": "Neural Topic Modeling of Psychotherapy Sessions",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": "is an unsupervised text modeling\napproach based on variational auto-encoder. ",
                    "Citation Text": "Yishu Miao, Edward Grefenstette, and Phil Blunsom,\n\u201cDiscovering discrete latent topics with neural varia-\ntional inference,\u201d in International Conference on Ma-\nchine Learning . PMLR, 2017, pp. 2410\u20132419.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.00359",
                        "Citation Paper Title": "Title:Discovering Discrete Latent Topics with Neural Variational Inference",
                        "Citation Paper Abstract": "Abstract:Topic models have been widely explored as probabilistic generative models of documents. Traditional inference methods have sought closed-form derivations for updating the models, however as the expressiveness of these models grows, so does the difficulty of performing fast and accurate inference over their parameters. This paper presents alternative neural approaches to topic modelling by providing parameterisable distributions over topics which permit training by backpropagation in the framework of neural variational inference. In addition, with the help of a stick-breaking construction, we propose a recurrent network that is able to discover a notionally unbounded number of topics, analogous to Bayesian non-parametric topic models. Experimental results on the MXM Song Lyrics, 20NewsGroups and Reuters News datasets demonstrate the effectiveness and efficiency of these neural topic models.",
                        "Citation Paper Authors": "Authors:Yishu Miao, Edward Grefenstette, Phil Blunsom"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2211.01236v1": {
            "Paper Title": "Isometric Representations in Neural Networks Improve Robustness",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.00302v1": {
            "Paper Title": "Electroencephalography and mild cognitive impairment research: A scoping\n  review and bibliometric analysis (ScoRBA)",
            "Sentences": []
        },
        "http://arxiv.org/abs/2211.00245v1": {
            "Paper Title": "Pairing cellular and synaptic dynamics into building blocks of rhythmic\n  neural circuits",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.14683v2": {
            "Paper Title": "Computer-aided diagnosis and prediction in brain disorders",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.09058v3": {
            "Paper Title": "Mapping Husserlian phenomenology onto active inference",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.12025v2": {
            "Paper Title": "Integrating Statistical and Machine Learning Approaches to Identify\n  Receptive Field Structure in Neural Populations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2212.01337v1": {
            "Paper Title": "Vizaj -- An interactive javascript tool for visualizing spatial networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.15207v1": {
            "Paper Title": "Towards the convergent therapeutic potential of GPCRs in autism spectrum\n  disorders",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.13564v1": {
            "Paper Title": "Preparing fMRI Data for Statistical Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.13526v1": {
            "Paper Title": "Computational Inference in Cognitive Science: Operational, Societal and\n  Ethical Considerations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.08933v2": {
            "Paper Title": "A theory of learning with constrained weight-distribution",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.11769v2": {
            "Paper Title": "Single-phase deep learning in cortico-cortical networks",
            "Sentences": [
                {
                    "Sentence ID": 6,
                    "Sentence": "As previously mentioned, BurstCCN takes inspiration from two prior models: EDNs ",
                    "Citation Text": "Jo\u00e3o Sacramento, Rui Ponte Costa, Yoshua Bengio, and Walter Senn. Dendritic cortical\nmicrocircuits approximate the backpropagation algorithm. In Advances in Neural Information\nProcessing Systems , pages 8721\u20138732, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.11393",
                        "Citation Paper Title": "Title:Dendritic cortical microcircuits approximate the backpropagation algorithm",
                        "Citation Paper Abstract": "Abstract:Deep learning has seen remarkable developments over the last years, many of them inspired by neuroscience. However, the main learning mechanism behind these advances - error backpropagation - appears to be at odds with neurobiology. Here, we introduce a multilayer neuronal network model with simplified dendritic compartments in which error-driven synaptic plasticity adapts the network towards a global desired output. In contrast to previous work our model does not require separate phases and synaptic learning is driven by local dendritic prediction errors continuously in time. Such errors originate at apical dendrites and occur due to a mismatch between predictive input from lateral interneurons and activity from actual top-down feedback. Through the use of simple dendritic compartments and different cell-types our model can represent both error and normal activity within a pyramidal neuron. We demonstrate the learning capabilities of the model in regression and classification tasks, and show analytically that it approximates the error backpropagation algorithm. Moreover, our framework is consistent with recent observations of learning between brain areas and the architecture of cortical microcircuits. Overall, we introduce a novel view of learning on dendritic cortical circuits and on how the brain may solve the long-standing synaptic credit assignment problem.",
                        "Citation Paper Authors": "Authors:Jo\u00e3o Sacramento, Rui Ponte Costa, Yoshua Bengio, Walter Senn"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.07069v2": {
            "Paper Title": "Biologically plausible solutions for spiking networks with efficient\n  coding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.13461v1": {
            "Paper Title": "Active Predictive Coding: A Unified Neural Framework for Learning\n  Hierarchical World Models for Perception and Planning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.06131v2": {
            "Paper Title": "Seeing the forest and the tree: Building representations of both\n  individual and collective dynamics with transformers",
            "Sentences": [
                {
                    "Sentence ID": 67,
                    "Sentence": ", but also\nmight restrict the model\u2019s representational capacity (also discussed in ",
                    "Citation Text": "S. Dutta, T. Gautam, S. Chakrabarti, and T. Chakraborty, \u201cRedesigning the transformer archi-\ntecture with insights from multi-particle dynamical systems,\u201d Advances in Neural Information\nProcessing Systems , vol. 34, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2109.15142",
                        "Citation Paper Title": "Title:Redesigning the Transformer Architecture with Insights from Multi-particle Dynamical Systems",
                        "Citation Paper Abstract": "Abstract:The Transformer and its variants have been proven to be efficient sequence learners in many different domains. Despite their staggering success, a critical issue has been the enormous number of parameters that must be trained (ranging from $10^7$ to $10^{11}$) along with the quadratic complexity of dot-product attention. In this work, we investigate the problem of approximating the two central components of the Transformer -- multi-head self-attention and point-wise feed-forward transformation, with reduced parameter space and computational complexity. We build upon recent developments in analyzing deep neural networks as numerical solvers of ordinary differential equations. Taking advantage of an analogy between Transformer stages and the evolution of a dynamical system of multiple interacting particles, we formulate a temporal evolution scheme, TransEvolve, to bypass costly dot-product attention over multiple stacked layers. We perform exhaustive experiments with TransEvolve on well-known encoder-decoder as well as encoder-only tasks. We observe that the degree of approximation (or inversely, the degree of parameter reduction) has different effects on the performance, depending on the task. While in the encoder-decoder regime, TransEvolve delivers performances comparable to the original Transformer, in encoder-only tasks it consistently outperforms Transformer along with several subsequent variants.",
                        "Citation Paper Authors": "Authors:Subhabrata Dutta, Tanya Gautam, Soumen Chakrabarti, Tanmoy Chakraborty"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": "proposed a manifold alignment method to continuously update a neural decoder over time as neurons\nshift in and out of the recording; ",
                    "Citation Text": "A. Farshchian, J. A. Gallego, J. P. Cohen, Y . Bengio, L. E. Miller, and S. A. Solla, \u201cAdversarial\ndomain adaptation for stable brain-machine interfaces,\u201d 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.00045",
                        "Citation Paper Title": "Title:Adversarial Domain Adaptation for Stable Brain-Machine Interfaces",
                        "Citation Paper Abstract": "Abstract:Brain-Machine Interfaces (BMIs) have recently emerged as a clinically viable option to restore voluntary movements after paralysis. These devices are based on the ability to extract information about movement intent from neural signals recorded using multi-electrode arrays chronically implanted in the motor cortices of the brain. However, the inherent loss and turnover of recorded neurons requires repeated recalibrations of the interface, which can potentially alter the day-to-day user experience. The resulting need for continued user adaptation interferes with the natural, subconscious use of the BMI. Here, we introduce a new computational approach that decodes movement intent from a low-dimensional latent representation of the neural data. We implement various domain adaptation methods to stabilize the interface over significantly long times. This includes Canonical Correlation Analysis used to align the latent variables across days; this method requires prior point-to-point correspondence of the time series across domains. Alternatively, we match the empirical probability distributions of the latent variables across days through the minimization of their Kullback-Leibler divergence. These two methods provide a significant and comparable improvement in the performance of the interface. However, implementation of an Adversarial Domain Adaptation Network trained to match the empirical probability distribution of the residuals of the reconstructed neural signals outperforms the two methods based on latent variables, while requiring remarkably few data points to solve the domain adaptation problem.",
                        "Citation Paper Authors": "Authors:Ali Farshchian, Juan A. Gallego, Joseph P. Cohen, Yoshua Bengio, Lee E. Miller, Sara A. Solla"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": "use a graph neural network to embed interconnected-structures to perform skeleton-based action\nrecognition; ",
                    "Citation Text": "R. Girdhar, J. Carreira, C. Doersch, and A. Zisserman, \u201cVideo action transformer network,\u201d\ninProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\npp. 244\u2013253, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1812.02707",
                        "Citation Paper Title": "Title:Video Action Transformer Network",
                        "Citation Paper Abstract": "Abstract:We introduce the Action Transformer model for recognizing and localizing human actions in video clips. We repurpose a Transformer-style architecture to aggregate features from the spatiotemporal context around the person whose actions we are trying to classify. We show that by using high-resolution, person-specific, class-agnostic queries, the model spontaneously learns to track individual people and to pick up on semantic context from the actions of others. Additionally its attention mechanism learns to emphasize hands and faces, which are often crucial to discriminate an action - all without explicit supervision other than boxes and class labels. We train and test our Action Transformer network on the Atomic Visual Actions (AVA) dataset, outperforming the state-of-the-art by a significant margin using only raw RGB frames as input.",
                        "Citation Paper Authors": "Authors:Rohit Girdhar, Jo\u00e3o Carreira, Carl Doersch, Andrew Zisserman"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "extract embeddings from multivariate physical systems with\nKoopman operators before feeding the resulting representation into a temporal transformer; ",
                    "Citation Text": "C. Plizzari, M. Cannici, and M. Matteucci, \u201cSpatial temporal transformer network for skeleton-\nbased action recognition,\u201d in International Conference on Pattern Recognition , pp. 694\u2013701,\nSpringer, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.06399",
                        "Citation Paper Title": "Title:Spatial Temporal Transformer Network for Skeleton-based Action Recognition",
                        "Citation Paper Abstract": "Abstract:Skeleton-based human action recognition has achieved a great interest in recent years, as skeleton data has been demonstrated to be robust to illumination changes, body scales, dynamic camera views, and complex background. Nevertheless, an effective encoding of the latent information underlying the 3D skeleton is still an open problem. In this work, we propose a novel Spatial-Temporal Transformer network (ST-TR) which models dependencies between joints using the Transformer self-attention operator. In our ST-TR model, a Spatial Self-Attention module (SSA) is used to understand intra-frame interactions between different body parts, and a Temporal Self-Attention module (TSA) to model inter-frame correlations. The two are combined in a two-stream network which outperforms state-of-the-art models using the same input data on both NTU-RGB+D 60 and NTU-RGB+D 120.",
                        "Citation Paper Authors": "Authors:Chiara Plizzari, Marco Cannici, Matteo Matteucci"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": "2.1 Transformers\nSelf-attention. Transformers have revolutionized natural language processing (NLP) through the\nmechanism of self-attention ",
                    "Citation Text": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and\nI. Polosukhin, \u201cAttention is all you need,\u201d in Advances in neural information processing systems ,\npp. 5998\u20136008, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.11953v2": {
            "Paper Title": "TD-BPQBC: A 1.8\u03bcW 5.5mm3 ADC-less Neural Implant SoC utilizing\n  13.2pJ/Sample Time-domain Bi-phasic Quasi-static Brain Communication",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.10492v1": {
            "Paper Title": "Efficient, probabilistic analysis of combinatorial neural codes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.09554v1": {
            "Paper Title": "A novel statistical methodology for quantifying the spatial arrangements\n  of axons in peripheral nerves",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.09446v1": {
            "Paper Title": "Deformably-Scaled Transposed Convolution",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": "and epileptic lesion conspicuity in\nimages from low-\ufb01eld scanners in low-and-middle-incomecountries ",
                    "Citation Text": "Hongxiang Lin, Matteo Figini, Ryutaro Tanno, Stefano B.\nBlumberg, Enrico Kaden, GodwIn: Ogbole, Biobele J.\nBrown, Felice D\u2019Arco, David W. Carmichael, Ikeoluwa\nLagunju, Helen J. Cross, Delmiro Fernandez-Reyes, and\nDaniel C. Alexander. Deep learning for low-\ufb01eld to high-\n\ufb01eld MR: Image quality transfer with probabilistic decima-\ntion simulator. In: Machine Learning In Medical Imaging\nWorkshop (MLMI) for Medical Image Computing and Com-\nputer Assisted Intervention (MICCAI) , 2019. 7",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1909.06763",
                        "Citation Paper Title": "Title:Deep Learning for Low-Field to High-Field MR: Image Quality Transfer with Probabilistic Decimation Simulator",
                        "Citation Paper Abstract": "Abstract:MR images scanned at low magnetic field ($<1$T) have lower resolution in the slice direction and lower contrast, due to a relatively small signal-to-noise ratio (SNR) than those from high field (typically 1.5T and 3T). We adapt the recent idea of Image Quality Transfer (IQT) to enhance very low-field structural images aiming to estimate the resolution, spatial coverage, and contrast of high-field images. Analogous to many learning-based image enhancement techniques, IQT generates training data from high-field scans alone by simulating low-field images through a pre-defined decimation model. However, the ground truth decimation model is not well-known in practice, and lack of its specification can bias the trained model, aggravating performance on the real low-field scans. In this paper we propose a probabilistic decimation simulator to improve robustness of model training. It is used to generate and augment various low-field images whose parameters are random variables and sampled from an empirical distribution related to tissue-specific SNR on a 0.36T scanner. The probabilistic decimation simulator is model-agnostic, that is, it can be used with any super-resolution networks. Furthermore we propose a variant of U-Net architecture to improve its learning performance. We show promising qualitative results from clinical low-field images confirming the strong efficacy of IQT in an important new application area: epilepsy diagnosis in sub-Saharan Africa where only low-field scanners are normally available.",
                        "Citation Paper Authors": "Authors:Hongxiang Lin, Matteo Figini, Ryutaro Tanno, Stefano B. Blumberg, Enrico Kaden, Godwin Ogbole, Biobele J. Brown, Felice D'Arco, David W. Carmichael, Ikeoluwa Lagunju, Helen J. Cross, Delmiro Fernandez-Reyes, Daniel C. Alexander"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": ").\n4.1. Object Detection and Instance Segmentation\nwith Mask-RCNN with FPN\nIn this section, we use the Mask-RCNN ",
                    "Citation Text": "Kaiming He, Georgia Gkioxari, Piotr Doll \u00b4ar, and Ross Gir-\nshick. Mask R-CNN. In: International Conference on Com-\nputer Vision (ICCV) , 2017. 1, 2, 5, 6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.06870",
                        "Citation Paper Title": "Title:Mask R-CNN",
                        "Citation Paper Abstract": "Abstract:We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: this https URL",
                        "Citation Paper Authors": "Authors:Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, Ross Girshick"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": ", which is a pixel-wise shuf\ufb02e, and the Context-\nAware ReAssembly of FEatures (CARAFE) ",
                    "Citation Text": "Jiaqi Wang, Kai Chen, Rui Xu, Ziwei Liu, Chen Change\nLoy, and Dahua Lin. CARAFE: Context-aware reassembly\nof features. In: International Conference on Computer Vi-\nsion (ICCV) , 2019. 2, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.02188",
                        "Citation Paper Title": "Title:CARAFE: Content-Aware ReAssembly of FEatures",
                        "Citation Paper Abstract": "Abstract:Feature upsampling is a key operation in a number of modern convolutional network architectures, e.g. feature pyramids. Its design is critical for dense prediction tasks such as object detection and semantic/instance segmentation. In this work, we propose Content-Aware ReAssembly of FEatures (CARAFE), a universal, lightweight and highly effective operator to fulfill this goal. CARAFE has several appealing properties: (1) Large field of view. Unlike previous works (e.g. bilinear interpolation) that only exploit sub-pixel neighborhood, CARAFE can aggregate contextual information within a large receptive field. (2) Content-aware handling. Instead of using a fixed kernel for all samples (e.g. deconvolution), CARAFE enables instance-specific content-aware handling, which generates adaptive kernels on-the-fly. (3) Lightweight and fast to compute. CARAFE introduces little computational overhead and can be readily integrated into modern network architectures. We conduct comprehensive evaluations on standard benchmarks in object detection, instance/semantic segmentation and inpainting. CARAFE shows consistent and substantial gains across all the tasks (1.2%, 1.3%, 1.8%, 1.1db respectively) with negligible computational overhead. It has great potential to serve as a strong building block for future research. It has great potential to serve as a strong building block for future research. Code and models are available at this https URL.",
                        "Citation Paper Authors": "Authors:Jiaqi Wang, Kai Chen, Rui Xu, Ziwei Liu, Chen Change Loy, Dahua Lin"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "fol-\nlowed by a convolution, v) the Transposed Pixel-Adaptive\nConvolution ",
                    "Citation Text": "Hang Su, Varun Jampani, Deqing Sun, Orazio Gallo, Erik\nLearned-Miller, and Jan Kautz. Pixel-adaptive convolutional\nneural networks. In: Computer Vision and Pattern Recogni-\ntion (CVPR) , 2019. 2, 5",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.05373",
                        "Citation Paper Title": "Title:Pixel-Adaptive Convolutional Neural Networks",
                        "Citation Paper Abstract": "Abstract:Convolutions are the fundamental building block of CNNs. The fact that their weights are spatially shared is one of the main reasons for their widespread use, but it also is a major limitation, as it makes convolutions content agnostic. We propose a pixel-adaptive convolution (PAC) operation, a simple yet effective modification of standard convolutions, in which the filter weights are multiplied with a spatially-varying kernel that depends on learnable, local pixel features. PAC is a generalization of several popular filtering techniques and thus can be used for a wide range of use cases. Specifically, we demonstrate state-of-the-art performance when PAC is used for deep joint image upsampling. PAC also offers an effective alternative to fully-connected CRF (Full-CRF), called PAC-CRF, which performs competitively, while being considerably faster. In addition, we also demonstrate that PAC can be used as a drop-in replacement for convolution layers in pre-trained networks, resulting in consistent performance improvements.",
                        "Citation Paper Authors": "Authors:Hang Su, Varun Jampani, Deqing Sun, Orazio Gallo, Erik Learned-Miller, Jan Kautz"
                    }
                },
                {
                    "Sentence ID": 53,
                    "Sentence": "addressed anti-\naliasing in the context of deep learning, by using simple\nspatial blurs before downsampling operations, to both\nimprove network performance and improve robustness\nto shift-based adversarial attacks. Furthermore, ",
                    "Citation Text": "Xueyan Zou, Fanyi Xiao, Zhiding Yu, and Yong Jae Lee.\nDelving deeper into anti-aliasing in convNets. In: British\nMachine Vision Conference (BMVC) , 2020. 2\n10Supplementary Materials\nAdditional Experimental Details\nMask-RCNN Additional Details We present more de-\ntails of our settings in section 4.1, where we used the\nMask-RCNN, which extends the Faster-RCNN",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.09604",
                        "Citation Paper Title": "Title:Delving Deeper into Anti-aliasing in ConvNets",
                        "Citation Paper Abstract": "Abstract:Aliasing refers to the phenomenon that high frequency signals degenerate into completely different ones after sampling. It arises as a problem in the context of deep learning as downsampling layers are widely adopted in deep architectures to reduce parameters and computation. The standard solution is to apply a low-pass filter (e.g., Gaussian blur) before downsampling. However, it can be suboptimal to apply the same filter across the entire content, as the frequency of feature maps can vary across both spatial locations and feature channels. To tackle this, we propose an adaptive content-aware low-pass filtering layer, which predicts separate filter weights for each spatial location and channel group of the input feature maps. We investigate the effectiveness and generalization of the proposed method across multiple tasks including ImageNet classification, COCO instance segmentation, and Cityscapes semantic segmentation. Qualitative and quantitative results demonstrate that our approach effectively adapts to the different feature frequencies to avoid aliasing while preserving useful information for recognition. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Xueyan Zou, Fanyi Xiao, Zhiding Yu, Yong Jae Lee"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "s\nAdaptive Convolutions The \ufb01rst approach to spatially\nadapt features in deep learning was the Spatial Transformer\nNetworks ",
                    "Citation Text": "Max Jaderberg, Karen Simonyan, Andrew Zisserman, and\nKoray Kavukcuoglu. Spatial transformer networks. In: Neu-\nral Information Processing Systems (NIPS) , 2015. 2, 3",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.02025",
                        "Citation Paper Title": "Title:Spatial Transformer Networks",
                        "Citation Paper Abstract": "Abstract:Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.",
                        "Citation Paper Authors": "Authors:Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.09366v1": {
            "Paper Title": "Bridging the Gap between Artificial Intelligence and Artificial General\n  Intelligence: A Ten Commandment Framework for Human-Like Intelligence",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.09217v1": {
            "Paper Title": "Statistical learning methods for neuroimaging data analysis with\n  applications",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.07935v1": {
            "Paper Title": "Neurobiology and Changing Ecosystems: toward understanding the impact of\n  anthropogenic influences on neurons and circuits",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.09051v2": {
            "Paper Title": "EPOC Emotiv EEG Basics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.06472v1": {
            "Paper Title": "Inner speech recognition through electroencephalographic signals",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.04552v1": {
            "Paper Title": "Intrinsic motivation, Need for cognition, Grit, Growth Mindset and\n  Academic Achievement in High School Students: Latent Profiles and Its\n  Predictive Effects",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.04520v1": {
            "Paper Title": "Continual task learning in natural and artificial agents",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.04172v1": {
            "Paper Title": "A Transformer-based deep neural network model for SSVEP classification",
            "Sentences": [
                {
                    "Sentence ID": 54,
                    "Sentence": ".\nIn recent years, deep learning methods have achieved great succ ess for brain\nsignalsanalysis ",
                    "Citation Text": "Zhang, X., Yao, L., Wang, X., Monaghan, J., Mcalpine, D., Zhang, Y .,\n2021c. A survey on deep learning-based non-invasive brain signals: recent\nadvances and new frontiers. Journal of Neural Engineering 18, 0 31002.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.04149",
                        "Citation Paper Title": "Title:A Survey on Deep Learning-based Non-Invasive Brain Signals:Recent Advances and New Frontiers",
                        "Citation Paper Abstract": "Abstract:Brain-Computer Interface (BCI) bridges the human's neural world and the outer physical world by decoding individuals' brain signals into commands recognizable by computer devices. Deep learning has lifted the performance of brain-computer interface systems significantly in recent years. In this article, we systematically investigate brain signal types for BCI and related deep learning concepts for brain signal analysis. We then present a comprehensive survey of deep learning techniques used for BCI, by summarizing over 230 contributions most published in the past five years. Finally, we discuss the applied areas, opening challenges, and future directions for deep learning-based BCI.",
                        "Citation Paper Authors": "Authors:Xiang Zhang, Lina Yao, Xianzhi Wang, Jessica Monaghan, David Mcalpine, Yu Zhang"
                    }
                },
                {
                    "Sentence ID": 49,
                    "Sentence": ". Some studies have use d convolution,\nMLP or even pooling to replace the attention module and achieved simila r\nresults ",
                    "Citation Text": "Yu, W., Luo, M., Zhou, P., Si, C., Zhou, Y., Wang, X., Feng, J., Yan, S .,\n2022. Metaformer is actually what you need for vision, in: Proceedin gs of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recogn ition,\npp. 10819\u201310829.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.11418",
                        "Citation Paper Title": "Title:MetaFormer Is Actually What You Need for Vision",
                        "Citation Paper Abstract": "Abstract:Transformers have shown great potential in computer vision tasks. A common belief is their attention-based token mixer module contributes most to their competence. However, recent works show the attention-based module in Transformers can be replaced by spatial MLPs and the resulted models still perform quite well. Based on this observation, we hypothesize that the general architecture of the Transformers, instead of the specific token mixer module, is more essential to the model's performance. To verify this, we deliberately replace the attention module in Transformers with an embarrassingly simple spatial pooling operator to conduct only basic token mixing. Surprisingly, we observe that the derived model, termed as PoolFormer, achieves competitive performance on multiple computer vision tasks. For example, on ImageNet-1K, PoolFormer achieves 82.1% top-1 accuracy, surpassing well-tuned Vision Transformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3%/1.1% accuracy with 35%/52% fewer parameters and 50%/62% fewer MACs. The effectiveness of PoolFormer verifies our hypothesis and urges us to initiate the concept of \"MetaFormer\", a general architecture abstracted from Transformers without specifying the token mixer. Based on the extensive experiments, we argue that MetaFormer is the key player in achieving superior results for recent Transformer and MLP-like models on vision tasks. This work calls for more future research dedicated to improving MetaFormer instead of focusing on the token mixer modules. Additionally, our proposed PoolFormer could serve as a starting baseline for future MetaFormer architecture design. Code is available at this https URL.",
                        "Citation Paper Authors": "Authors:Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, Shuicheng Yan"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.03910v1": {
            "Paper Title": "Quantifying constraints determining independent activation on NMDA\n  receptors mediated currents from evoked and spontaneous synaptic transmission\n  at an individual synapse",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.03400v1": {
            "Paper Title": "Computational imaging with the human brain",
            "Sentences": [
                {
                    "Sentence ID": 34,
                    "Sentence": "B. Bai, Y. He, J. Liu, Y. Zhou, H. Zheng, S. Zhang, and\nZ. Xu, Optik 147, 136 (2017).6 ",
                    "Citation Text": "D. Faccio, A. Velten, and G. Wetzstein, Nature Reviews\nPhysics 2, 318 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.08026",
                        "Citation Paper Title": "Title:Non-line-of-sight Imaging",
                        "Citation Paper Abstract": "Abstract:Emerging single-photon-sensitive sensors combined with advanced inverse methods to process picosecond-accurate time-stamped photon counts have given rise to unprecedented imaging capabilities. Rather than imaging photons that travel along direct paths from a source to an object and back to the detector, non-line-of-sight (NLOS) imaging approaches analyse photons {scattered from multiple surfaces that travel} along indirect light paths to estimate 3D images of scenes outside the direct line of sight of a camera, hidden by a wall or other obstacles. Here we review recent advances in the field of NLOS imaging, discussing how to see around corners and future prospects for the field.",
                        "Citation Paper Authors": "Authors:D. Faccio, A. Velten, G. Wetzstein"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.02996v1": {
            "Paper Title": "Synergistic information supports modality integration and flexible\n  learning in neural networks solving multiple tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.14406v2": {
            "Paper Title": "Biological connectomes as a representation for the architecture of\n  artificial neural networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.02349v1": {
            "Paper Title": "Fitting a Directional Microstructure Model to Diffusion-Relaxation MRI\n  Data with Self-Supervised Machine Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.01986v1": {
            "Paper Title": "MAtt: A Manifold Attention Network for EEG Decoding",
            "Sentences": [
                {
                    "Sentence ID": 36,
                    "Sentence": "proposes a Riemannian-embedding-banks method that separates the entire embeddings into multiple\nsub-problems for learning spatial patterns of MI EEG signals based on the features extracted from\nthe SPDNet. ",
                    "Citation Text": "Ce Ju, Dashan Gao, Ravikiran Mane, Ben Tan, Yang Liu, and Cuntai Guan. Federated transfer\nlearning for EEG signal classi\ufb01cation. In 2020 42nd Annual International Conference of the\nIEEE Engineering in Medicine & Biology Society (EMBC) , pages 3040\u20133045. IEEE, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.12321",
                        "Citation Paper Title": "Title:Federated Transfer Learning for EEG Signal Classification",
                        "Citation Paper Abstract": "Abstract:The success of deep learning (DL) methods in the Brain-Computer Interfaces (BCI) field for classification of electroencephalographic (EEG) recordings has been restricted by the lack of large datasets. Privacy concerns associated with EEG signals limit the possibility of constructing a large EEG-BCI dataset by the conglomeration of multiple small ones for jointly training machine learning models. Hence, in this paper, we propose a novel privacy-preserving DL architecture named federated transfer learning (FTL) for EEG classification that is based on the federated learning framework. Working with the single-trial covariance matrix, the proposed architecture extracts common discriminative information from multi-subject EEG data with the help of domain adaptation techniques. We evaluate the performance of the proposed architecture on the PhysioNet dataset for 2-class motor imagery classification. While avoiding the actual data sharing, our FTL approach achieves 2% higher classification accuracy in a subject-adaptive analysis. Also, in the absence of multi-subject data, our architecture provides 6% better accuracy compared to other state-of-the-art DL architectures.",
                        "Citation Paper Authors": "Authors:Ce Ju, Dashan Gao, Ravikiran Mane, Ben Tan, Yang Liu, Cuntai Guan"
                    }
                },
                {
                    "Sentence ID": 33,
                    "Sentence": "offers high performance in\nmedical image classi\ufb01cation with manifold autoencoder. ",
                    "Citation Text": "Jonathan Masci, Davide Boscaini, Michael Bronstein, and Pierre Vandergheynst. Geodesic con-\nvolutional neural networks on Riemannian manifolds. In Proceedings of the IEEE international\nconference on computer vision workshops , pages 37\u201345, 2015.\n12",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1501.06297",
                        "Citation Paper Title": "Title:Geodesic convolutional neural networks on Riemannian manifolds",
                        "Citation Paper Abstract": "Abstract:Feature descriptors play a crucial role in a wide range of geometry analysis and processing applications, including shape correspondence, retrieval, and segmentation. In this paper, we introduce Geodesic Convolutional Neural Networks (GCNN), a generalization of the convolutional networks (CNN) paradigm to non-Euclidean manifolds. Our construction is based on a local geodesic system of polar coordinates to extract \"patches\", which are then passed through a cascade of filters and linear and non-linear operators. The coefficients of the filters and linear combination weights are optimization variables that are learned to minimize a task-specific cost function. We use GCNN to learn invariant shape features, allowing to achieve state-of-the-art performance in problems such as shape description, retrieval, and correspondence.",
                        "Citation Paper Authors": "Authors:Jonathan Masci, Davide Boscaini, Michael M. Bronstein, Pierre Vandergheynst"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "is a Riemannian network\nfor non-linear SPD-based learning on Riemannian manifolds using bi-linear mapping that mimics\nEuclidean convolution for visual classi\ufb01cation tasks. ManifoldNet ",
                    "Citation Text": "Rudrasis Chakraborty, Jose Bouza, Jonathan Manton, and Baba C Vemuri. Manifoldnet: A\ndeep neural network for manifold-valued data with applications. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.06211",
                        "Citation Paper Title": "Title:ManifoldNet: A Deep Network Framework for Manifold-valued Data",
                        "Citation Paper Abstract": "Abstract:Deep neural networks have become the main work horse for many tasks involving learning from data in a variety of applications in Science and Engineering. Traditionally, the input to these networks lie in a vector space and the operations employed within the network are well defined on vector-spaces. In the recent past, due to technological advances in sensing, it has become possible to acquire manifold-valued data sets either directly or indirectly. Examples include but are not limited to data from omnidirectional cameras on automobiles, drones etc., synthetic aperture radar imaging, diffusion magnetic resonance imaging, elastography and conductance imaging in the Medical Imaging domain and others. Thus, there is need to generalize the deep neural networks to cope with input data that reside on curved manifolds where vector space operations are not naturally admissible. In this paper, we present a novel theoretical framework to generalize the widely popular convolutional neural networks (CNNs) to high dimensional manifold-valued data inputs. We call these networks, ManifoldNets.\nIn ManifoldNets, convolution operation on data residing on Riemannian manifolds is achieved via a provably convergent recursive computation of the weighted Fr\u00e9chet Mean (wFM) of the given data, where the weights makeup the convolution mask, to be learned. Further, we prove that the proposed wFM layer achieves a contraction mapping and hence ManifoldNet does not need the non-linear ReLU unit used in standard CNNs. We present experiments, using the ManifoldNet framework, to achieve dimensionality reduction by computing the principal linear subspaces that naturally reside on a Grassmannian. The experimental results demonstrate the efficacy of ManifoldNets in the context of classification and reconstruction accuracy.",
                        "Citation Paper Authors": "Authors:Rudrasis Chakraborty, Jose Bouza, Jonathan Manton, Baba C. Vemuri"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.01691v1": {
            "Paper Title": "Adaptive Synaptic Failure Enables Sampling from Posterior Predictive\n  Distributions in the Brain",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.00506v1": {
            "Paper Title": "Loc-VAE: Learning Structurally Localized Representation from 3D Brain MR\n  Images for Content-Based Image Retrieval",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.00500v1": {
            "Paper Title": "Cognitive modelling with multilayer networks: Insights, advancements and\n  future challenges",
            "Sentences": [
                {
                    "Sentence ID": 49,
                    "Sentence": "). These di\u000berent layers\nmight display a small level of correlation since words sounding similar to each\nother tend also to be recalled together in free association tasks (as measured\nin ",
                    "Citation Text": "Stella, M., Beckage, N.M., Brede, M.: Multiplex lexical networks reveal\npatterns in early word acquisition in children. Scienti\fc reports 7(1), 1{10\n(2017)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.03207",
                        "Citation Paper Title": "Title:Multiplex lexical networks reveal patterns in early word acquisition in children",
                        "Citation Paper Abstract": "Abstract:Network models of language have provided a way of linking cognitive processes to the structure and connectivity of language. However, one shortcoming of current approaches is focusing on only one type of linguistic relationship at a time, missing the complex multi-relational nature of language. In this work, we overcome this limitation by modelling the mental lexicon of English-speaking toddlers as a multiplex lexical network, i.e. a multi-layered network where N=529 words/nodes are connected according to four types of relationships: (i) free associations, (ii) feature sharing, (iii) co-occurrence, and (iv) phonological similarity. We provide analysis of the topology of the resulting multiplex and then proceed to evaluate single layers as well as the full multiplex structure on their ability to predict empirically observed age of acquisition data of English speaking toddlers. We find that the emerging multiplex network topology is an important proxy of the cognitive processes of acquisition, capable of capturing emergent lexicon structure. In fact, we show that the multiplex topology is fundamentally more powerful than individual layers in predicting the ordering with which words are acquired. Furthermore, multiplex analysis allows for a quantification of distinct phases of lexical acquisition in early learners: while initially all the multiplex layers contribute to word learning, after about month 23 free associations take the lead in driving word acquisition.",
                        "Citation Paper Authors": "Authors:Massimo Stella, Nicole M. Beckage, Markus Brede"
                    }
                },
                {
                    "Sentence ID": 67,
                    "Sentence": "to\naccount for simultaneous interactions between more than two entities at once.\nFrom a cognitive perspective, hypergraphs could be structured across multilay-\ner/multiplex structures either by building links through information-theoretic\nmeasures ",
                    "Citation Text": "Marinazzo, D., Van Roozendaal, J., Rosas, F.E., Stella, M., Comolatti,\nR., Colenbier, N., Stramaglia, S., Rosseel, Y.: An information-theoretic\napproach to hypergraph psychometrics. arXiv preprint arXiv:2205.01035\n(2022)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2205.01035",
                        "Citation Paper Title": "Title:An information-theoretic approach to hypergraph psychometrics",
                        "Citation Paper Abstract": "Abstract:Psychological network approaches propose to see symptoms or questionnaire items as interconnected nodes, with links between them reflecting pairwise statistical dependencies evaluated cross-sectional, time-series, or panel data. These networks constitute an established methodology to assess the interactions and relative importance of nodes/indicators, providing an important complement to other approaches such as factor analysis. However, focusing the modelling solely on pairwise relationships can neglect potentially critical information shared by groups of three or more variables in the form of higher-order interdependencies. To overcome this important limitation, here we propose an information-theoretic framework based on hypergraphs as psychometric models. As edges in hypergraphs are capable of encompassing several nodes together, this extension can thus provide a richer representation of the interactions that may exist among sets of psychological variables. Our results show how psychometric hypergraphs can highlight meaningful redundant and synergistic interactions on either simulated or state-of-art, re-analyzed psychometric datasets. Overall, our framework extends current network approaches while leading to new ways of assessing the data that differ at their core from other methods, extending the psychometric toolbox and opening promising avenues for future investigation.",
                        "Citation Paper Authors": "Authors:Daniele Marinazzo, Jan Van Roozendaal, Fernando E. Rosas, Massimo Stella, Renzo Comolatti, Nigel Colenbier, Sebastiano Stramaglia, Yves Rosseel"
                    }
                },
                {
                    "Sentence ID": 88,
                    "Sentence": ").\nThe task of community detection is more complicated in multilayer net-\nworks, because the community detection algorithm must consider the di\u000berent\ntypes of relationships occurring in di\u000berent layers at the same time ",
                    "Citation Text": "Magnani, M., Hanteer, O., Interdonato, R., Rossi, L., Tagarelli, A.:\nCommunity detection in multiplex networks. ACM Computing Surveys\n(CSUR) 54(3), 1{35 (2021)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.07646",
                        "Citation Paper Title": "Title:Community Detection in Multiplex Networks",
                        "Citation Paper Abstract": "Abstract:A multiplex network models different modes of interaction among same-type entities. In this article we provide a taxonomy of community detection algorithms in multiplex networks. We characterize the different algorithms based on various properties and we discuss the type of communities detected by each method. We then provide an extensive experimental evaluation of the reviewed methods to answer three main questions: to what extent the evaluated methods are able to detect ground-truth communities, to what extent different methods produce similar community structures and to what extent the evaluated methods are scalable. One goal of this survey is to help scholars and practitioners to choose the right methods for the data and the task at hand, while also emphasizing when such choice is problematic.",
                        "Citation Paper Authors": "Authors:Matteo Magnani, Obaida Hanteer, Roberto Interdonato, Luca Rossi, Andrea Tagarelli"
                    }
                },
                {
                    "Sentence ID": 83,
                    "Sentence": ". Community detection\nis the task of decomposing a network into well-connected and well-separated\ngroups of nodes, and it is one of the most challenging problems in complex net-\nwork analysis ",
                    "Citation Text": "Fortunato, S., Hric, D.: Community detection in networks: A user guide.\nPhysics reports 659, 1{44 (2016)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1608.00163",
                        "Citation Paper Title": "Title:Community detection in networks: A user guide",
                        "Citation Paper Abstract": "Abstract:Community detection in networks is one of the most popular topics of modern network science. Communities, or clusters, are usually groups of vertices having higher probability of being connected to each other than to members of other groups, though other patterns are possible. Identifying communities is an ill-defined problem. There are no universal protocols on the fundamental ingredients, like the definition of community itself, nor on other crucial issues, like the validation of algorithms and the comparison of their performances. This has generated a number of confusions and misconceptions, which undermine the progress in the field. We offer a guided tour through the main aspects of the problem. We also point out strengths and weaknesses of popular methods, and give directions to their use.",
                        "Citation Paper Authors": "Authors:Santo Fortunato, Darko Hric"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2210.01767v1": {
            "Paper Title": "Supervised Parameter Estimation of Neuron Populations from Multiple\n  Firing Events",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.01769v1": {
            "Paper Title": "Mind Reader: Reconstructing complex images from brain activities",
            "Sentences": [
                {
                    "Sentence ID": 29,
                    "Sentence": "with the visual cortex. In addition, other conditional generative models, such as\ndiffusion models, can be explored. In particular, DALL-E 2 ",
                    "Citation Text": "A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image generation\nwith clip latents. arXiv preprint arXiv:2204.06125 , 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2204.06125",
                        "Citation Paper Title": "Title:Hierarchical Text-Conditional Image Generation with CLIP Latents",
                        "Citation Paper Abstract": "Abstract:Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.",
                        "Citation Paper Authors": "Authors:Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "since\nwe can view the image reconstruction problem as signal-to-signal translation. In particular, we\napplied VQV AE ",
                    "Citation Text": "A. Van Den Oord, O. Vinyals, et al. Neural discrete representation learning. Advances in neural information\nprocessing systems , 30, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.00937",
                        "Citation Paper Title": "Title:Neural Discrete Representation Learning",
                        "Citation Paper Abstract": "Abstract:Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of \"posterior collapse\" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.",
                        "Citation Paper Authors": "Authors:Aaron van den Oord, Oriol Vinyals, Koray Kavukcuoglu"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "), and we believe moving to a multimodal latent space is a crucial step\ntowards better brain signal decodings.\n4 Further discussions\nPrior to our current pipeline design, we experimented with a DALL-E -like structure ",
                    "Citation Text": "A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. V oss, A. Radford, M. Chen, and I. Sutskever. Zero-shot\ntext-to-image generation. In International Conference on Machine Learning , pages 8821\u20138831. PMLR,\n2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.12092",
                        "Citation Paper Title": "Title:Zero-Shot Text-to-Image Generation",
                        "Citation Paper Abstract": "Abstract:Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.",
                        "Citation Paper Authors": "Authors:Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.14218v1": {
            "Paper Title": "DMAP: a Distributed Morphological Attention Policy for Learning to\n  Locomote with a Changing Body",
            "Sentences": [
                {
                    "Sentence ID": 16,
                    "Sentence": "Benchmarks for zero-shot adaptation. The setup of the experiments described in this paper can be\nframed as a zero-shot adaptation problem, as the agents are evaluated starting from the \ufb01rst moment\nin which they interact with an unseen context ",
                    "Citation Text": "Robert Kirk, Amy Zhang, Edward Grefenstette, and Tim Rockt\u00e4schel. A survey of generalisation\nin deep reinforcement learning. arXiv preprint arXiv:2111.09794 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.09794",
                        "Citation Paper Title": "Title:A Survey of Zero-shot Generalisation in Deep Reinforcement Learning",
                        "Citation Paper Abstract": "Abstract:The study of zero-shot generalisation (ZSG) in deep Reinforcement Learning (RL) aims to produce RL algorithms whose policies generalise well to novel unseen situations at deployment time, avoiding overfitting to their training environments. Tackling this is vital if we are to deploy reinforcement learning algorithms in real world scenarios, where the environment will be diverse, dynamic and unpredictable. This survey is an overview of this nascent field. We rely on a unifying formalism and terminology for discussing different ZSG problems, building upon previous works. We go on to categorise existing benchmarks for ZSG, as well as current methods for tackling these problems. Finally, we provide a critical discussion of the current state of the field, including recommendations for future work. Among other conclusions, we argue that taking a purely procedural content generation approach to benchmark design is not conducive to progress in ZSG, we suggest fast online adaptation and tackling RL-specific problems as some areas for future work on methods for ZSG, and we recommend building benchmarks in underexplored problem settings such as offline RL ZSG and reward-function variation.",
                        "Citation Paper Authors": "Authors:Robert Kirk, Amy Zhang, Edward Grefenstette, Tim Rockt\u00e4schel"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": ". Walking with different body shapes could be interpreted as the union of a\nlong-horizon task (locomotion) and a short-horizon one (body shape identi\ufb01cation). Algorithms\nto tackle multi-horizon control problems by separating high-level behavior from motor primitives\ninclude Hierarchical Actor-Critic ",
                    "Citation Text": "Andrew Levy, Robert Platt, and Kate Saenko. Hierarchical actor-critic. arXiv preprint\narXiv:1712.00948 , 12, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1712.00948",
                        "Citation Paper Title": "Title:Learning Multi-Level Hierarchies with Hindsight",
                        "Citation Paper Abstract": "Abstract:Hierarchical agents have the potential to solve sequential decision making tasks with greater sample efficiency than their non-hierarchical counterparts because hierarchical agents can break down tasks into sets of subtasks that only require short sequences of decisions. In order to realize this potential of faster learning, hierarchical agents need to be able to learn their multiple levels of policies in parallel so these simpler subproblems can be solved simultaneously. Yet, learning multiple levels of policies in parallel is hard because it is inherently unstable: changes in a policy at one level of the hierarchy may cause changes in the transition and reward functions at higher levels in the hierarchy, making it difficult to jointly learn multiple levels of policies. In this paper, we introduce a new Hierarchical Reinforcement Learning (HRL) framework, Hierarchical Actor-Critic (HAC), that can overcome the instability issues that arise when agents try to jointly learn multiple levels of policies. The main idea behind HAC is to train each level of the hierarchy independently of the lower levels by training each level as if the lower level policies are already optimal. We demonstrate experimentally in both grid world and simulated robotics domains that our approach can significantly accelerate learning relative to other non-hierarchical and hierarchical methods. Indeed, our framework is the first to successfully learn 3-level hierarchies in parallel in tasks with continuous state and action spaces.",
                        "Citation Paper Authors": "Authors:Andrew Levy, George Konidaris, Robert Platt, Kate Saenko"
                    }
                },
                {
                    "Sentence ID": 42,
                    "Sentence": "employs the same policy for each joint, enabling an agent to deal with variable\nobservation and action space sizes. Further solutions to handle bodies with variable connectivity\ngraphs include transformer-based solutions, such as Amorpheus ",
                    "Citation Text": "Vitaly Kurin, Maximilian Igl, Tim Rockt\u00e4schel, Wendelin Boehmer, and Shimon Whiteson. My\nbody is a cage: the role of morphology in graph-based incompatible control. arXiv preprint\narXiv:2010.01856 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.01856",
                        "Citation Paper Title": "Title:My Body is a Cage: the Role of Morphology in Graph-Based Incompatible Control",
                        "Citation Paper Abstract": "Abstract:Multitask Reinforcement Learning is a promising way to obtain models with better performance, generalisation, data efficiency, and robustness. Most existing work is limited to compatible settings, where the state and action space dimensions are the same across tasks. Graph Neural Networks (GNN) are one way to address incompatible environments, because they can process graphs of arbitrary size. They also allow practitioners to inject biases encoded in the structure of the input graph. Existing work in graph-based continuous control uses the physical morphology of the agent to construct the input graph, i.e., encoding limb features as node labels and using edges to connect the nodes if their corresponded limbs are physically connected. In this work, we present a series of ablations on existing methods that show that morphological information encoded in the graph does not improve their performance. Motivated by the hypothesis that any benefits GNNs extract from the graph structure are outweighed by difficulties they create for message passing, we also propose Amorpheus, a transformer-based approach. Further results show that, while Amorpheus ignores the morphological information that GNNs encode, it nonetheless substantially outperforms GNN-based methods that use the morphological information to define the message-passing scheme.",
                        "Citation Paper Authors": "Authors:Vitaly Kurin, Maximilian Igl, Tim Rockt\u00e4schel, Wendelin Boehmer, Shimon Whiteson"
                    }
                },
                {
                    "Sentence ID": 41,
                    "Sentence": "achieves state-of-the-art performance on standard\nRL benchmarks by leveraging different policies for each joint type. However, it cannot generalize to\ndifferent designs in a zero-shot manner (without \ufb01ne-tuning). On the other hand, Shared Modular\nPolicies (SMP) ",
                    "Citation Text": "Wenlong Huang, Igor Mordatch, and Deepak Pathak. One policy to control them all: Shared\nmodular policies for agent-agnostic control. International Conference on Machine Learning ,\npages 4455\u20134464, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2007.04976",
                        "Citation Paper Title": "Title:One Policy to Control Them All: Shared Modular Policies for Agent-Agnostic Control",
                        "Citation Paper Abstract": "Abstract:Reinforcement learning is typically concerned with learning control policies tailored to a particular agent. We investigate whether there exists a single global policy that can generalize to control a wide variety of agent morphologies -- ones in which even dimensionality of state and action spaces changes. We propose to express this global policy as a collection of identical modular neural networks, dubbed as Shared Modular Policies (SMP), that correspond to each of the agent's actuators. Every module is only responsible for controlling its corresponding actuator and receives information from only its local sensors. In addition, messages are passed between modules, propagating information between distant modules. We show that a single modular policy can successfully generate locomotion behaviors for several planar agents with different skeletal structures such as monopod hoppers, quadrupeds, bipeds, and generalize to variants not seen during training -- a process that would normally require training and manual hyperparameter tuning for each morphology. We observe that a wide variety of drastically diverse locomotion styles across morphologies as well as centralized coordination emerges via message passing between decentralized modules purely from the reinforcement learning objective. Videos and code at this https URL",
                        "Citation Paper Authors": "Authors:Wenlong Huang, Igor Mordatch, Deepak Pathak"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": ".\nDistributed control. An agent can learn to generalize to multiple body shapes by using modular\npolicies to control each joint independently, e.g., by treating each joint ",
                    "Citation Text": "Deepak Pathak, Christopher Lu, Trevor Darrell, Phillip Isola, and Alexei A Efros. Learning to\ncontrol self-assembling morphologies: a study of generalization via modularity. Advances in\nNeural Information Processing Systems , 32, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.05546",
                        "Citation Paper Title": "Title:Learning to Control Self-Assembling Morphologies: A Study of Generalization via Modularity",
                        "Citation Paper Abstract": "Abstract:Contemporary sensorimotor learning approaches typically start with an existing complex agent (e.g., a robotic arm), which they learn to control. In contrast, this paper investigates a modular co-evolution strategy: a collection of primitive agents learns to dynamically self-assemble into composite bodies while also learning to coordinate their behavior to control these bodies. Each primitive agent consists of a limb with a motor attached at one end. Limbs may choose to link up to form collectives. When a limb initiates a link-up action, and there is another limb nearby, the latter is magnetically connected to the 'parent' limb's motor. This forms a new single agent, which may further link with other agents. In this way, complex morphologies can emerge, controlled by a policy whose architecture is in explicit correspondence with the morphology. We evaluate the performance of these dynamic and modular agents in simulated environments. We demonstrate better generalization to test-time changes both in the environment, as well as in the structure of the agent, compared to static and monolithic baselines. Project video and code are available at this https URL",
                        "Citation Paper Authors": "Authors:Deepak Pathak, Chris Lu, Trevor Darrell, Phillip Isola, Alexei A. Efros"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ",\nor with a recurrent neural network (RNN) [ 18,29]. However, while off-policy continuous control\nalgorithms such as TD3 ",
                    "Citation Text": "Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in\nactor-critic methods. International conference on machine learning , pages 1587\u20131596, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.09477",
                        "Citation Paper Title": "Title:Addressing Function Approximation Error in Actor-Critic Methods",
                        "Citation Paper Abstract": "Abstract:In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.",
                        "Citation Paper Authors": "Authors:Scott Fujimoto, Herke van Hoof, David Meger"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": ", assess the ability of an agent to maximize the cumulative reward in the training\nenvironment, more recent ones introduce perturbations in the dynamics [ 19,20], in the observation\nfunction [ 21,22] or in the reward function ",
                    "Citation Text": "Meire Fortunato, Melissa Tan, Ryan Faulkner, Steven Hansen, Adri\u00e0 Puigdom\u00e8nech Badia,\nGavin Buttimore, Charles Deck, Joel Z Leibo, and Charles Blundell. Generalization of re-\ninforcement learners with working and episodic memory. Advances in Neural Information\nProcessing Systems , 32, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1910.13406",
                        "Citation Paper Title": "Title:Generalization of Reinforcement Learners with Working and Episodic Memory",
                        "Citation Paper Abstract": "Abstract:Memory is an important aspect of intelligence and plays a role in many deep reinforcement learning models. However, little progress has been made in understanding when specific memory systems help more than others and how well they generalize. The field also has yet to see a prevalent consistent and rigorous approach for evaluating agent performance on holdout data. In this paper, we aim to develop a comprehensive methodology to test different kinds of memory in an agent and assess how well the agent can apply what it learns in training to a holdout set that differs from the training set along dimensions that we suggest are relevant for evaluating memory-specific generalization. To that end, we first construct a diverse set of memory tasks that allow us to evaluate test-time generalization across multiple dimensions. Second, we develop and perform multiple ablations on an agent architecture that combines multiple memory systems, observe its baseline models, and investigate its performance against the task suite.",
                        "Citation Paper Authors": "Authors:Meire Fortunato, Melissa Tan, Ryan Faulkner, Steven Hansen, Adri\u00e0 Puigdom\u00e8nech Badia, Gavin Buttimore, Charlie Deck, Joel Z Leibo, Charles Blundell"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.11737v1": {
            "Paper Title": "Semantic scene descriptions as an objective of human vision",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.07431v3": {
            "Paper Title": "Compositional generalization through abstract representations in human\n  and artificial neural networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.10992v1": {
            "Paper Title": "Modeling cognitive load as a self-supervised brain rate with\n  electroencephalography and deep learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.09953v1": {
            "Paper Title": "Development of theoretical frameworks in neuroscience: a pressing need\n  in a sea of data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.09696v1": {
            "Paper Title": "Synthesis of realistic fetal MRI with conditional Generative Adversarial\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2210.01772v1": {
            "Paper Title": "Joint Reconstruction and Parcellation of Cortical Surfaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.07583v1": {
            "Paper Title": "Combinatorial geometry of neural codes, neural data analysis, and neural\n  networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.07508v1": {
            "Paper Title": "Information Theoretic Measures of Causal Influences during Transient\n  Neural Events",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.13529v1": {
            "Paper Title": "Deep Cross-Modality and Resolution Graph Integration for Universal Brain\n  Connectivity Mapping and Augmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.05627v1": {
            "Paper Title": "SENDER: SEmi-Nonlinear Deep Efficient Reconstructor for Extraction\n  Canonical, Meta, and Sub Functional Connectivity in the Human Brain",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.05002v1": {
            "Paper Title": "Quantifying the attractor landscape and transition path of distributed\n  working memory from large-scale brain network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.04084v1": {
            "Paper Title": "Polarization effects on fluorescence emission of zebrafish neurons using\n  light-sheet microscopy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.03718v1": {
            "Paper Title": "The neuroconnectionist research programme",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.02876v1": {
            "Paper Title": "Self-supervised multimodal neuroimaging yields predictive\n  representations for a spectrum of Alzheimer's phenotypes",
            "Sentences": [
                {
                    "Sentence ID": 105,
                    "Sentence": ".\nIn our analysis, we do not consider the family of multimodal generative variational models ",
                    "Citation Text": "D. P . Kingma, M. Welling, Auto-encoding variational bayes, arXiv preprint arXiv:1312.6114.\n28",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1312.6114",
                        "Citation Paper Title": "Title:Auto-Encoding Variational Bayes",
                        "Citation Paper Abstract": "Abstract:How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.",
                        "Citation Paper Authors": "Authors:Diederik P Kingma, Max Welling"
                    }
                },
                {
                    "Sentence ID": 91,
                    "Sentence": "we add regularization to InfoNCE objective by penalizing the squared scores\ncomputed by the critic function as \u00b8f(x,y)2with\u00b8\u00c64e\u00a12, and cliping the scores by ctanh(s\nc) with c\u00c620.\nWe perform the training of the models on OASIS-3 dataset with RAdam ",
                    "Citation Text": "L. Liu, H. Jiang, P . He, W. Chen, X. Liu, J. Gao, J. Han, On the variance of the adaptive learning rate and beyond,\narXiv preprint arXiv:1908.03265.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.03265",
                        "Citation Paper Title": "Title:On the Variance of the Adaptive Learning Rate and Beyond",
                        "Citation Paper Abstract": "Abstract:The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Here, we study its mechanism in details. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate (i.e., it has problematically large variance in the early stage), suggest warmup works as a variance reduction technique, and provide both empirical and theoretical evidence to verify our hypothesis. We further propose RAdam, a new variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Extensive experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the effectiveness and robustness of our proposed method. All implementations are available at: this https URL.",
                        "Citation Paper Authors": "Authors:Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, Jiawei Han"
                    }
                },
                {
                    "Sentence ID": 82,
                    "Sentence": "volume, which is generated from T1w and resting-state fMRI (rs-fMRI) images. The purpose of the\n10Figure 4: The \ufb01gures show the learning framework for the CR-based objective with the T1 image. It includes an encoder with DCGAN ",
                    "Citation Text": "A. Radford, L. Metz, S. Chintala, Unsupervised representation learning with deep convolutional generative\nadversarial networks, arXiv preprint arXiv:1511.06434.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06434",
                        "Citation Paper Title": "Title:Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.",
                        "Citation Paper Authors": "Authors:Alec Radford, Luke Metz, Soumith Chintala"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": ". These\nmethods can naturally be extended to modeling multimodal data ",
                    "Citation Text": "A. Fedorov, T. Sylvain, E. Geenjaar, M. Luck, L. Wu, T. P . DeRamus, A. Kirilin, D. Bleklov, V . D. Calhoun, S. M.\nPlis, Self-supervised multimodal domino: in search of biomarkers for alzheimer\u2019s disease, in: 2021 IEEE 9th\nInternational Conference on Healthcare Informatics (ICHI), IEEE, 2021, pp. 23\u201330.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.13623",
                        "Citation Paper Title": "Title:Self-Supervised Multimodal Domino: in Search of Biomarkers for Alzheimer's Disease",
                        "Citation Paper Abstract": "Abstract:Sensory input from multiple sources is crucial for robust and coherent human perception. Different sources contribute complementary explanatory factors. Similarly, research studies often collect multimodal imaging data, each of which can provide shared and unique information. This observation motivated the design of powerful multimodal self-supervised representation-learning algorithms. In this paper, we unify recent work on multimodal self-supervised learning under a single framework. Observing that most self-supervised methods optimize similarity metrics between a set of model components, we propose a taxonomy of all reasonable ways to organize this process. We first evaluate models on toy multimodal MNIST datasets and then apply them to a multimodal neuroimaging dataset with Alzheimer's disease patients. We find that (1) multimodal contrastive learning has significant benefits over its unimodal counterpart, (2) the specific composition of multiple contrastive objectives is critical to performance on a downstream task, (3) maximization of the similarity between representations has a regularizing effect on a neural network, which can sometimes lead to reduced downstream performance but still reveal multimodal relations. Results show that the proposed approach outperforms previous self-supervised encoder-decoder methods based on canonical correlation analysis (CCA) or the mixture-of-experts multimodal variational autoEncoder (MMVAE) on various datasets with a linear evaluation protocol. Importantly, we find a promising solution to uncover connections between modalities through a jointly shared subspace that can help advance work in our search for neuroimaging biomarkers.",
                        "Citation Paper Authors": "Authors:Alex Fedorov, Tristan Sylvain, Eloy Geenjaar, Margaux Luck, Lei Wu, Thomas P. DeRamus, Alex Kirilin, Dmitry Bleklov, Vince D. Calhoun, Sergey M. Plis"
                    }
                },
                {
                    "Sentence ID": 76,
                    "Sentence": ". We chose to use logistic regression, however, due to faster training times.\n2.2.2. Alignment analysis\nTo evaluate the alignment between representations of different modalities, we use central kernel alignment\n(CKA) ",
                    "Citation Text": "S. Kornblith, M. Norouzi, H. Lee, G. Hinton, Similarity of neural network representations revisited, in: Interna-\ntional Conference on Machine Learning, PMLR, 2019, pp. 3519\u20133529.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.00414",
                        "Citation Paper Title": "Title:Similarity of Neural Network Representations Revisited",
                        "Citation Paper Abstract": "Abstract:Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.",
                        "Citation Paper Authors": "Authors:Simon Kornblith, Mohammad Norouzi, Honglak Lee, Geoffrey Hinton"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": ", cross-modal DIM (CM-DIM) [ 50,51],\nand spatio-temporal DIM (ST-DIM) ",
                    "Citation Text": "A. Anand, E. Racah, S. Ozair, Y. Bengio, M. C\u00f4t\u00e9, D. Hjelm, Unsupervised state representation learning in atari,\nin: NeurIPS, 2019.\n24",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.08226",
                        "Citation Paper Title": "Title:Unsupervised State Representation Learning in Atari",
                        "Citation Paper Abstract": "Abstract:State representation learning, or the ability to capture latent generative factors of an environment, is crucial for building intelligent agents that can perform a wide variety of tasks. Learning such representations without supervision from rewards is a challenging open problem. We introduce a method that learns state representations by maximizing mutual information across spatially and temporally distinct features of a neural encoder of the observations. We also introduce a new benchmark based on Atari 2600 games where we evaluate representations based on how well they capture the ground truth state variables. We believe this new framework for evaluating representation learning models will be crucial for future representation learning research. Finally, we compare our technique with other state-of-the-art generative and contrastive representation learning methods. The code associated with this work is available at this https URL",
                        "Citation Paper Authors": "Authors:Ankesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc-Alexandre C\u00f4t\u00e9, R Devon Hjelm"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": ", for example, this is\ndone between the context and a future intermediate state. Both DIM and CPC have been successfully extended and\napplied unimodally for the prediction of Alzheimer\u2019s disease from sMRI ",
                    "Citation Text": "A. Fedorov, R. D. Hjelm, A. Abrol, Z. Fu, Y. Du, S. Plis, V . D. Calhoun, Prediction of progression to alzheimer\u2019s\ndisease with deep infomax, in: 2019 IEEE EMBS International conference on biomedical & health informatics\n(BHI), IEEE, 2019, pp. 1\u20135.\n23",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.10931",
                        "Citation Paper Title": "Title:Prediction of Progression to Alzheimer's disease with Deep InfoMax",
                        "Citation Paper Abstract": "Abstract:Arguably, unsupervised learning plays a crucial role in the majority of algorithms for processing brain imaging. A recently introduced unsupervised approach Deep InfoMax (DIM) is a promising tool for exploring brain structure in a flexible non-linear way. In this paper, we investigate the use of variants of DIM in a setting of progression to Alzheimer's disease in comparison with supervised AlexNet and ResNet inspired convolutional neural networks. As a benchmark, we use a classification task between four groups: patients with stable, and progressive mild cognitive impairment (MCI), with Alzheimer's disease, and healthy controls. Our dataset is comprised of 828 subjects from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. Our experiments highlight encouraging evidence of the high potential utility of DIM in future neuroimaging studies.",
                        "Citation Paper Authors": "Authors:Alex Fedorov, R Devon Hjelm, Anees Abrol, Zening Fu, Yuhui Du, Sergey Plis, Vince D. Calhoun"
                    }
                },
                {
                    "Sentence ID": 40,
                    "Sentence": ", transfer learning with fMRI [ 38,39],\nand brain tumor, pancreas tumor segmentation, and diabetic retinopathy detection ",
                    "Citation Text": "A. Taleb, W. Loetzsch, N. Danz, J. Severin, T. Gaertner, B. Bergner, C. Lippert, 3d self-supervised methods\nfor medical imaging, in: H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, H. Lin (Eds.), Advances in Neural\nInformation Processing Systems, Vol. 33, Curran Associates, Inc., 2020, pp. 18158\u201318172.\nURLhttps://proceedings.neurips.cc/paper/2020/file/d2dc6368837861b42020ee72b0896182-Paper.\npdf",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.03829",
                        "Citation Paper Title": "Title:3D Self-Supervised Methods for Medical Imaging",
                        "Citation Paper Abstract": "Abstract:Self-supervised learning methods have witnessed a recent surge of interest after proving successful in multiple application fields. In this work, we leverage these techniques, and we propose 3D versions for five different self-supervised methods, in the form of proxy tasks. Our methods facilitate neural network feature learning from unlabeled 3D images, aiming to reduce the required cost for expert annotation. The developed algorithms are 3D Contrastive Predictive Coding, 3D Rotation prediction, 3D Jigsaw puzzles, Relative 3D patch location, and 3D Exemplar networks. Our experiments show that pretraining models with our 3D tasks yields more powerful semantic representations, and enables solving downstream tasks more accurately and efficiently, compared to training the models from scratch and to pretraining them on 2D slices. We demonstrate the effectiveness of our methods on three downstream tasks from the medical imaging domain: i) Brain Tumor Segmentation from 3D MRI, ii) Pancreas Tumor Segmentation from 3D CT, and iii) Diabetic Retinopathy Detection from 2D Fundus images. In each task, we assess the gains in data-efficiency, performance, and speed of convergence. Interestingly, we also find gains when transferring the learned representations, by our methods, from a large unlabeled 3D corpus to a small downstream-specific dataset. We achieve results competitive to state-of-the-art solutions at a fraction of the computational expense. We publish our implementations for the developed algorithms (both 3D and 2D versions) as an open-source library, in an effort to allow other researchers to apply and extend our methods on their datasets.",
                        "Citation Paper Authors": "Authors:Aiham Taleb, Winfried Loetzsch, Noel Danz, Julius Severin, Thomas Gaertner, Benjamin Bergner, Christoph Lippert"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": ", and can\noutperform supervised approaches on image recognition tasks ",
                    "Citation Text": "M. Caron, I. Misra, J. Mairal, P . Goyal, P . Bojanowski, A. Joulin, Unsupervised learning of visual features by\ncontrasting cluster assignments, in: H. Larochelle, M. Ranzato, R. Hadsell, M. F . Balcan, H. Lin (Eds.), Advances\nin Neural Information Processing Systems, Vol. 33, Curran Associates, Inc., 2020, pp. 9912\u20139924.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.09882",
                        "Citation Paper Title": "Title:Unsupervised Learning of Visual Features by Contrasting Cluster Assignments",
                        "Citation Paper Abstract": "Abstract:Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or views) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a swapped prediction mechanism where we predict the cluster assignment of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements much. We validate our findings by achieving 75.3% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.",
                        "Citation Paper Authors": "Authors:Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, Armand Joulin"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": "which are commonplace in healthcare [ 14,15].\nFurthermore, supervised methods have also been shown to be data-inef\ufb01cient ",
                    "Citation Text": "O. J. H\u00e9naff, A. Srinivas, J. De Fauw, A. Razavi, C. Doersch, S. Eslami, A. v. d. Oord, Data-ef\ufb01cient image\nrecognition with contrastive predictive coding (2020) 4182\u20134192.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.09272",
                        "Citation Paper Title": "Title:Data-Efficient Image Recognition with Contrastive Predictive Coding",
                        "Citation Paper Abstract": "Abstract:Human observers can learn to recognize new categories of images from a handful of examples, yet doing so with artificial ones remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable. We therefore revisit and improve Contrastive Predictive Coding, an unsupervised objective for learning such representations. This new implementation produces features which support state-of-the-art linear classification accuracy on the ImageNet dataset. When used as input for non-linear classification with deep neural networks, this representation allows us to use 2-5x less labels than classifiers trained directly on image pixels. Finally, this unsupervised representation substantially improves transfer learning to object detection on the PASCAL VOC dataset, surpassing fully supervised pre-trained ImageNet classifiers.",
                        "Citation Paper Authors": "Authors:Olivier J. H\u00e9naff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, S. M. Ali Eslami, Aaron van den Oord"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": ". In SSL, a model trains on a proxy task that does not\n2require externally provided labels. SSL has been shown to improve robustness ",
                    "Citation Text": "D. Hendrycks, M. Mazeika, S. Kadavath, D. Song, Using self-supervised learning can improve model robustness\nand uncertainty, in: H. Wallach, H. Larochelle, A. Beygelzimer, F . d 'Alch\u00e9-Buc, E. Fox, R. Garnett (Eds.),\nAdvances in Neural Information Processing Systems, Vol. 32, Curran Associates, Inc., 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.12340",
                        "Citation Paper Title": "Title:Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty",
                        "Citation Paper Abstract": "Abstract:Self-supervision provides effective representations for downstream tasks without requiring labels. However, existing approaches lag behind fully supervised training and are often not thought beneficial beyond obviating or reducing the need for annotations. We find that self-supervision can benefit robustness in a variety of ways, including robustness to adversarial examples, label corruption, and common input corruptions. Additionally, self-supervision greatly benefits out-of-distribution detection on difficult, near-distribution outliers, so much so that it exceeds the performance of fully supervised methods. These results demonstrate the promise of self-supervision for improving robustness and uncertainty estimation and establish these tasks as new axes of evaluation for future self-supervised learning research.",
                        "Citation Paper Authors": "Authors:Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, Dawn Song"
                    }
                },
                {
                    "Sentence ID": 17,
                    "Sentence": ". Many of these problems can be addressed with unsupervised\nlearning and, more recently, self-supervised learning (SSL) ",
                    "Citation Text": "A. Dosovitskiy, J. T. Springenberg, M. Riedmiller, T. Brox, Discriminative unsupervised feature learning with\nconvolutional neural networks, Advances in neural information processing systems 27.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1406.6909",
                        "Citation Paper Title": "Title:Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks",
                        "Citation Paper Abstract": "Abstract:Deep convolutional networks have proven to be very successful in learning task specific features that allow for unprecedented performance on various computer vision tasks. Training of such networks follows mostly the supervised learning paradigm, where sufficiently many input-output pairs are required for training. Acquisition of large training sets is one of the key challenges, when approaching a new task. In this paper, we aim for generic feature learning and present an approach for training a convolutional network using only unlabeled data. To this end, we train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled 'seed' image patch. In contrast to supervised network training, the resulting feature representation is not class specific. It rather provides robustness to the transformations that have been applied during training. This generic feature representation allows for classification results that outperform the state of the art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101, Caltech-256). While such generic features cannot compete with class specific features from supervised training on a classification task, we show that they are advantageous on geometric matching problems, where they also outperform the SIFT descriptor.",
                        "Citation Paper Authors": "Authors:Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, Thomas Brox"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.02816v1": {
            "Paper Title": "A probabilistic framework for task-aligned intra- and inter-area neural\n  manifold estimation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.02592v1": {
            "Paper Title": "Quasicriticality explains variability of human neural dynamics across\n  life span",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.02582v1": {
            "Paper Title": "Improving the Accuracy and Robustness of CNNs Using a Deep CCA Neural\n  Data Regularizer",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.11560v2": {
            "Paper Title": "Modelling continual learning in humans with Hebbian context gating and\n  exponentially decaying task signals",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.01369v3": {
            "Paper Title": "The Face of Affective Disorders",
            "Sentences": [
                {
                    "Sentence ID": 52,
                    "Sentence": ". Some \ufb01rst application\nscenarios for this can be studied in ",
                    "Citation Text": "A. Kacem, M. Daoudi, B. Amor, S. Berreti, and J. Alvarez-Paiva,\n\u201cA novel geometric framework on gram matrix trajectories for human\nbehavior understanding.\u201d IEEE Transactions on Pattern Analysis and\nMachine Intelligence , vol. 42, no. 1, pp. 1\u201314, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.00676",
                        "Citation Paper Title": "Title:A Novel Geometric Framework on Gram Matrix Trajectories for Human Behavior Understanding",
                        "Citation Paper Abstract": "Abstract:In this paper, we propose a novel space-time geometric representation of human landmark configurations and derive tools for comparison and classification. We model the temporal evolution of landmarks as parametrized trajectories on the Riemannian manifold of positive semidefinite matrices of fixed-rank. Our representation has the benefit to bring naturally a second desirable quantity when comparing shapes, the spatial covariance, in addition to the conventional affine-shape representation. We derived then geometric and computational tools for rate-invariant analysis and adaptive re-sampling of trajectories, grounding on the Riemannian geometry of the underlying manifold. Specifically, our approach involves three steps: (1) landmarks are first mapped into the Riemannian manifold of positive semidefinite matrices of fixed-rank to build time-parameterized trajectories; (2) a temporal warping is performed on the trajectories, providing a geometry-aware (dis-)similarity measure between them; (3) finally, a pairwise proximity function SVM is used to classify them, incorporating the (dis-)similarity measure into the kernel function. We show that such representation and metric achieve competitive results in applications as action recognition and emotion recognition from 3D skeletal data, and facial expression recognition from videos. Experiments have been conducted on several publicly available up-to-date benchmarks.",
                        "Citation Paper Authors": "Authors:Anis Kacem, Mohamed Daoudi, Boulbaba Ben Amor, Stefano Berretti, Juan Carlos Alvarez-Paiva"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2209.01497v1": {
            "Paper Title": "A descriptive analysis of olfactory sensation and memory in Drosophila\n  and its relation to artificial neural networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.01258v1": {
            "Paper Title": "Object-based active inference",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.00627v1": {
            "Paper Title": "Classification of Electroencephalograms during Mathematical Calculations\n  Using Deep Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.14376v1": {
            "Paper Title": "Associative Learning for Network Embedding",
            "Sentences": [
                {
                    "Sentence ID": 31,
                    "Sentence": "to do implicit matrix factorization for different proximity\nmatrices, and proposes different proximity matrices for small and\nlarge window sizes. Recently, ",
                    "Citation Text": "Zhu, J., Lu, X., Heimann, M., and Koutra, D. Node proximity is all you need:\nUnified structural and positional node and graph embedding. In Proceedings\nof the 2021 SIAM International Conference on Data Mining (SDM) (2021), SIAM,\npp. 163\u2013171.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2102.13582",
                        "Citation Paper Title": "Title:Node Proximity Is All You Need: Unified Structural and Positional Node and Graph Embedding",
                        "Citation Paper Abstract": "Abstract:While most network embedding techniques model the relative positions of nodes in a network, recently there has been significant interest in structural embeddings that model node role equivalences, irrespective of their distances to any specific nodes. We present PhUSION, a proximity-based unified framework for computing structural and positional node embeddings, which leverages well-established methods for calculating node proximity scores. Clarifying a point of contention in the literature, we show which step of PhUSION produces the different kinds of embeddings and what steps can be used by both. Moreover, by aggregating the PhUSION node embeddings, we obtain graph-level features that model information lost by previous graph feature learning and kernel methods. In a comprehensive empirical study with over 10 datasets, 4 tasks, and 35 methods, we systematically reveal successful design choices for node and graph-level machine learning with embeddings.",
                        "Citation Paper Authors": "Authors:Jing Zhu, Xingyu Lu, Mark Heimann, Danai Koutra"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": "is used to increase the model efficiency.\nMany subsequent studies try to improve the graph diffusion pro-\ncess in order to better model the representation of the network. For\nexample, Node2vec ",
                    "Citation Text": "Grover, A., and Leskovec, J. node2vec: Scalable feature learning for networks.\nInProceedings of the 22nd ACM SIGKDD international conference on Knowledge\ndiscovery and data mining (2016), pp. 855\u2013864.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.00653",
                        "Citation Paper Title": "Title:node2vec: Scalable Feature Learning for Networks",
                        "Citation Paper Abstract": "Abstract:Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations. We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks.",
                        "Citation Paper Authors": "Authors:Aditya Grover, Jure Leskovec"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "utilizes the SkipGram model to maximize the proba-\nbility of seeing the neighboring nodes in the node sequence, con-\nditioned on the node embedding itself. Hierarchical softmax and\nnegative sampling ",
                    "Citation Text": "Mikolov, T., Sutskever, I., Chen, K., Corrado, G., and Dean, J. Distributed\nrepresentations of words and phrases and their compositionality. arXiv preprint\narXiv:1310.4546 (2013).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1310.4546",
                        "Citation Paper Title": "Title:Distributed Representations of Words and Phrases and their Compositionality",
                        "Citation Paper Abstract": "Abstract:The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
                        "Citation Paper Authors": "Authors:Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "uses low-rank matrix factorization\nover the pointwise mutual information matrix and adds context\ninformation during the factorization. ",
                    "Citation Text": "Qiu, J., Dong, Y., Ma, H., Li, J., Wang, K., and Tang, J. Network embedding as\nmatrix factorization: Unifying deepwalk, line, pte, and node2vec. In Proceedings\nof the eleventh ACM international conference on web search and data mining (2018),\npp. 459\u2013467.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1710.02971",
                        "Citation Paper Title": "Title:Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec",
                        "Citation Paper Abstract": "Abstract:Since the invention of word2vec, the skip-gram model has significantly advanced the research of network embedding, such as the recent emergence of the DeepWalk, LINE, PTE, and node2vec approaches. In this work, we show that all of the aforementioned models with negative sampling can be unified into the matrix factorization framework with closed forms. Our analysis and proofs reveal that: (1) DeepWalk empirically produces a low-rank transformation of a network's normalized Laplacian matrix; (2) LINE, in theory, is a special case of DeepWalk when the size of vertices' context is set to one; (3) As an extension of LINE, PTE can be viewed as the joint factorization of multiple networks' Laplacians; (4) node2vec is factorizing a matrix related to the stationary distribution and transition probability tensor of a 2nd-order random walk. We further provide the theoretical connections between skip-gram based network embedding algorithms and the theory of graph Laplacian. Finally, we present the NetMF method as well as its approximation algorithm for computing network embedding. Our method offers significant improvements over DeepWalk and LINE for conventional network mining tasks. This work lays the theoretical foundation for skip-gram based network embedding methods, leading to a better understanding of latent network representation learning.",
                        "Citation Paper Authors": "Authors:Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, Jie Tang"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2208.13675v2": {
            "Paper Title": "Juggling too many balls at once: Qualitatively different effects when\n  measuring priming and masking in single, dual, and triple tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.08083v1": {
            "Paper Title": "Periodic and non-periodic brainwaves emerging via random syncronization\n  of closed loops of firing neurons",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": "Mazzetti, P., and Carbone, A. \u201cHarmonic spectral com-\nponents in time sequences of Markov correlated events.\u201d\nAIP Advances, 7(7), (2017): 075216. ",
                    "Citation Text": "Centers, J., Tan, X., Hareedy, A., & Calderbank, R.\n(2021). Power spectra of constrained codes with level-\nbased signaling: Overcoming \ufb01nite-length challenges.IEEE Transactions on Communications, 69(8), 4971-\n4986.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.04878",
                        "Citation Paper Title": "Title:Power Spectra of Constrained Codes with Level-Based Signaling: Overcoming Finite-Length Challenges",
                        "Citation Paper Abstract": "Abstract:Constrained codes are used to eliminate error-prone patterns in various practical systems. Recently, we introduced efficient binary symmetric lexicographically-ordered constrained (LOCO) codes and asymmetric LOCO (A-LOCO) codes to increase density in magnetic recording systems and lifetime in Flash systems by eliminating the relevant detrimental patterns. Due to their application, LOCO and A-LOCO codes are associated with level-based signaling. Studying the power spectrum of a random signal with certain properties is principal for any storage or transmission system. In this paper, we first modify a framework from the literature in order to introduce a method to derive the power spectrum of a sequence of constrained data associated with level-based signaling. We apply our method to infinitely long sequences satisfying symmetric and asymmetric constraints. Next, we show how to generalize the method such that it works for a stream of finite-length codewords. We use the generalized method to devise closed forms for the spectra of finite-length LOCO and A-LOCO codes from their transition diagrams. Our LOCO and A-LOCO spectral derivations can be performed for any code length and can be extended to other constrained codes. We plot these power spectra, and discuss various important spectral properties for both LOCO and A-LOCO codes.",
                        "Citation Paper Authors": "Authors:Jessica Centers, Xinyu Tan, Ahmed Hareedy, Robert Calderbank"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.03950v3": {
            "Paper Title": "Transfer learning to decode brain states reflecting the relationship\n  between cognitive tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.02059v2": {
            "Paper Title": "Transformer based Models for Unsupervised Anomaly Segmentation in Brain\n  MR Images",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.13213v1": {
            "Paper Title": "The Role of Valence and Meta-awareness in Mirror Self-recognition Using\n  Hierarchical Active Inference",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.12231v1": {
            "Paper Title": "Controlling collective dynamical states of mesoscale brain networks with\n  local perturbations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.09677v2": {
            "Paper Title": "Net2Brain: A Toolbox to compare artificial vision models with human\n  brain responses",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.11783v1": {
            "Paper Title": "Population recordings of human motor units often display 'onion skin'\n  discharge patterns -- implications for voluntary motor control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.11631v1": {
            "Paper Title": "A structurally informed model for modulating functional connectivity",
            "Sentences": [
                {
                    "Sentence ID": 66,
                    "Sentence": ". From these data, we derived seeds and targets for probabilistic tractography, which we\nran with the FSL probtrackx2 algorithm using 1000 streams initiated from each voxel in a given\nparcel ",
                    "Citation Text": "Andrew C. Murphy, Maxwell A. Bertolero, Lia Papadopoulos, David M. Lydon-Staley, and\nDanielle S. Bassett. Multiscale and multimodal network dynamics underpinning working mem-\nory. Nature Communications , (2020):1{13, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.06552",
                        "Citation Paper Title": "Title:Multiscale and multimodal network dynamics underpinning working memory",
                        "Citation Paper Abstract": "Abstract:Working memory (WM) allows information to be stored and manipulated over short time scales. Performance on WM tasks is thought to be supported by the frontoparietal system (FPS), the default mode system (DMS), and interactions between them. Yet little is known about how these systems and their interactions relate to individual differences in WM performance. We address this gap in knowledge using functional MRI data acquired during the performance of a 2-back WM task, as well as diffusion tensor imaging data collected in the same individuals. We show that the strength of functional interactions between the FPS and DMS during task engagement is inversely correlated with WM performance, and that this strength is modulated by the activation of FPS regions but not DMS regions. Next, we use a clustering algorithm to identify two distinct subnetworks of the FPS, and find that these subnetworks display distinguishable patterns of gene expression. Activity in one subnetwork is positively associated with the strength of FPS-DMS functional interactions, while activity in the second subnetwork is negatively associated. Further, the pattern of structural linkages of these subnetworks explains their differential capacity to influence the strength of FPS-DMS functional interactions. To determine whether these observations could provide a mechanistic account of large-scale neural underpinnings of WM, we build a computational model of the system composed of coupled oscillators. Modulating the amplitude of the subnetworks in the model causes the expected change in the strength of FPS-DMS functional interactions, thereby offering support for a mechanism in which subnetwork activity tunes functional interactions. Broadly, our study presents a holistic account of how regional activity, functional interactions, and structural linkages together support individual differences in WM in humans.",
                        "Citation Paper Authors": "Authors:Andrew C. Murphy, Maxwell A. Bertolero, Lia Papadopoulos, David M. Lydon-Staley, Danielle S. Bassett"
                    }
                },
                {
                    "Sentence ID": 7,
                    "Sentence": ". We collated these estimates into a single\n400\u0002400 connectivity matrix, Cf, which we then treated as a formal network model ",
                    "Citation Text": "Danielle S. Bassett, Perry Zurn, and Joshua I. Gold. On the nature and use of models in\nnetwork neuroscience. Nature Reviews Neuroscience , 19(9):566{578, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.11935",
                        "Citation Paper Title": "Title:Network models in neuroscience",
                        "Citation Paper Abstract": "Abstract:From interacting cellular components to networks of neurons and neural systems, interconnected units comprise a fundamental organizing principle of the nervous system. Understanding how their patterns of connections and interactions give rise to the many functions of the nervous system is a primary goal of neuroscience. Recently, this pursuit has begun to benefit from the development of new mathematical tools that can relate a system's architecture to its dynamics and function. These tools, which are known collectively as network science, have been used with increasing success to build models of neural systems across spatial scales and species. Here we discuss the nature of network models in neuroscience. We begin with a review of model theory from a philosophical perspective to inform our view of networks as models of complex systems in general, and of the brain in particular. We then summarize the types of models that are frequently studied in network neuroscience along three primary dimensions: from data representations to first-principles theory, from biophysical realism to functional phenomenology, and from elementary descriptions to coarse-grained approximations. We then consider ways to validate these models, focusing on approaches that perturb a system to probe its function. We close with a description of important frontiers in the construction of network models and their relevance for understanding increasingly complex functions of neural systems.",
                        "Citation Paper Authors": "Authors:Danielle S. Bassett, Perry Zurn, Joshua I. Gold"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.15158v2": {
            "Paper Title": "Global dynamics of neural mass models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.10601v1": {
            "Paper Title": "Deriving time-averaged active inference from control principles",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.11700v1": {
            "Paper Title": "Low-Level Physiological Implications of End-to-End Learning of Speech\n  Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.08960v1": {
            "Paper Title": "Deploying Enhanced Speech Feature Decreased Audio Complaints at SVT Play\n  VOD Service",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.01939v3": {
            "Paper Title": "Learning Generative Factors of EEG Data with Variational auto-encoders",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.00619v2": {
            "Paper Title": "Maze Learning using a Hyperdimensional Predictive Processing Cognitive\n  Architecture",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.02629v3": {
            "Paper Title": "Backpropagation at the Infinitesimal Inference Limit of Energy-Based\n  Models: Unifying Predictive Coding, Equilibrium Propagation, and Contrastive\n  Hebbian Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.00352v1": {
            "Paper Title": "Neural Correlates of Face Familiarity Perception",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.10093v5": {
            "Paper Title": "Deep reinforcement learning guided graph neural networks for brain\n  network analysis",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "and preliminary knowledge,\nrespectively. Sec. 4 details the implementation of BN-GNN with graph convolutional network (GCN) ",
                    "Citation Text": "T. N. Kipf, M. Welling, Semi-supervised classi\ufb01cation with graph convolutional networks, in: International Conference\non Learning Representations, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1609.02907",
                        "Citation Paper Title": "Title:Semi-Supervised Classification with Graph Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",
                        "Citation Paper Authors": "Authors:Thomas N. Kipf, Max Welling"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "extracts a consistent and prede\ufb01ned number of\nlocal neighbors for each target one. In addition, many GNN variants based on the above methods or frameworks have been\nproposed, and they have made outstanding contributions, such as in clinical medicine ",
                    "Citation Text": "X. Zhang, L. He, K. Chen, Y . Luo, J. Zhou, F. Wang, Multi-view graph convolutional network and its applications\non neuroimage analysis for parkinson\u2019s disease, in: AMIA Annual Symposium Proceedings, volume 2018, American\nMedical Informatics Association, 2018, pp. 1147\u20131156.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.08801",
                        "Citation Paper Title": "Title:Multi-View Graph Convolutional Network and Its Applications on Neuroimage Analysis for Parkinson's Disease",
                        "Citation Paper Abstract": "Abstract:Parkinson's Disease (PD) is one of the most prevalent neurodegenerative diseases that affects tens of millions of Americans. PD is highly progressive and heterogeneous. Quite a few studies have been conducted in recent years on predictive or disease progression modeling of PD using clinical and biomarkers data. Neuroimaging, as another important information source for neurodegenerative disease, has also arisen considerable interests from the PD community. In this paper, we propose a deep learning method based on Graph Convolutional Networks (GCN) for fusing multiple modalities of brain images in relationship prediction which is useful for distinguishing PD cases from controls. On Parkinson's Progression Markers Initiative (PPMI) cohort, our approach achieved $0.9537\\pm 0.0587$ AUC, compared with $0.6443\\pm 0.0223$ AUC achieved by traditional approaches such as PCA.",
                        "Citation Paper Authors": "Authors:Xi Sheryl Zhang, Lifang He, Kun Chen, Yuan Luo, Jiayu Zhou, Fei Wang"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": ".\n2.2. Reinforcement Learning Guided Graph Neural Networks\nRecently, with the advances of RL, many works combine RL with GNNs to further raise the performance boundary of\nGNNs. For example, ",
                    "Citation Text": "Y . Dou, Z. Liu, L. Sun, Y . Deng, H. Peng, P. S. Yu, Enhancing graph neural network-based fraud detectors against\ncamou\ufb02aged fraudsters, in: Proceedings of the ACM International Conference on Information Knowledge Management,\n2020, pp. 315\u2013324.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.08692",
                        "Citation Paper Title": "Title:Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters",
                        "Citation Paper Abstract": "Abstract:Graph Neural Networks (GNNs) have been widely applied to fraud detection problems in recent years, revealing the suspiciousness of nodes by aggregating their neighborhood information via different relations. However, few prior works have noticed the camouflage behavior of fraudsters, which could hamper the performance of GNN-based fraud detectors during the aggregation process. In this paper, we introduce two types of camouflages based on recent empirical studies, i.e., the feature camouflage and the relation camouflage. Existing GNNs have not addressed these two camouflages, which results in their poor performance in fraud detection problems. Alternatively, we propose a new model named CAmouflage-REsistant GNN (CARE-GNN), to enhance the GNN aggregation process with three unique modules against camouflages. Concretely, we first devise a label-aware similarity measure to find informative neighboring nodes. Then, we leverage reinforcement learning (RL) to find the optimal amounts of neighbors to be selected. Finally, the selected neighbors across different relations are aggregated together. Comprehensive experiments on two real-world fraud datasets demonstrate the effectiveness of the RL algorithm. The proposed CARE-GNN also outperforms state-of-the-art GNNs and GNN-based fraud detectors. We integrate all GNN-based fraud detectors as an opensource toolbox: this https URL. The CARE-GNN code and datasets are available at this https URL.",
                        "Citation Paper Authors": "Authors:Yingtong Dou, Zhiwei Liu, Li Sun, Yutong Deng, Hao Peng, Philip S. Yu"
                    }
                },
                {
                    "Sentence ID": 39,
                    "Sentence": "can perform hierarchical embedding of brain networks. In order to improve the accuracy of brain disease analysis,\nHi-GCN considers graph structure induction and also introduces patient group-level structural information. ",
                    "Citation Text": "G. Ma, N. K. Ahmed, T. L. Willke, D. Sengupta, M. W. Cole, N. B. Turk-Browne, P. S. Yu, Deep graph similarity\nlearning for brain data analysis, in: Proceedings of the ACM International Conference on Information and Knowledge\nManagement, 2019, pp. 2743\u20132751.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.02662",
                        "Citation Paper Title": "Title:Similarity Learning with Higher-Order Graph Convolutions for Brain Network Analysis",
                        "Citation Paper Abstract": "Abstract:Learning a similarity metric has gained much attention recently, where the goal is to learn a function that maps input patterns to a target space while preserving the semantic distance in the input space. While most related work focused on images, we focus instead on learning a similarity metric for neuroimages, such as fMRI and DTI images. We propose an end-to-end similarity learning framework called Higher-order Siamese GCN for multi-subject fMRI data analysis. The proposed framework learns the brain network representations via a supervised metric-based approach with siamese neural networks using two graph convolutional networks as the twin networks. Our proposed framework performs higher-order convolutions by incorporating higher-order proximity in graph convolutional networks to characterize and learn the community structure in brain connectivity networks. To the best of our knowledge, this is the first community-preserving similarity learning framework for multi-subject brain network analysis. Experimental results on four real fMRI datasets demonstrate the potential use cases of the proposed framework for multi-subject brain analysis in health and neuropsychiatric disorders. Our proposed approach achieves an average AUC gain of 75% compared to PCA, an average AUC gain of 65.5% compared to Spectral Embedding, and an average AUC gain of 24.3% compared to S-GCN across the four datasets, indicating promising application in clinical investigation and brain disease diagnosis.",
                        "Citation Paper Authors": "Authors:Guixiang Ma, Nesreen K. Ahmed, Ted Willke, Dipanjan Sengupta, Michael W. Cole, Nicholas B. Turk-Browne, Philip S. Yu"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "proposed a virtual network embedding algorithm (i.e., V3C+GCN)\nthat combines DRL with a GCN-based module. ",
                    "Citation Text": "K.-H. Lai, D. Zha, K. Zhou, X. Hu, Policy-gnn: Aggregation optimization for graph neural networks, in: Proceedings\nof the ACM SIGKDD International Conference on Knowledge Discovery Data Mining, 2020, pp. 461\u2013471.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.15097",
                        "Citation Paper Title": "Title:Policy-GNN: Aggregation Optimization for Graph Neural Networks",
                        "Citation Paper Abstract": "Abstract:Graph data are pervasive in many real-world applications. Recently, increasing attention has been paid on graph neural networks (GNNs), which aim to model the local graph structures and capture the hierarchical patterns by aggregating the information from neighbors with stackable network modules. Motivated by the observation that different nodes often require different iterations of aggregation to fully capture the structural information, in this paper, we propose to explicitly sample diverse iterations of aggregation for different nodes to boost the performance of GNNs. It is a challenging task to develop an effective aggregation strategy for each node, given complex graphs and sparse features. Moreover, it is not straightforward to derive an efficient algorithm since we need to feed the sampled nodes into different number of network layers. To address the above challenges, we propose Policy-GNN, a meta-policy framework that models the sampling procedure and message passing of GNNs into a combined learning process. Specifically, Policy-GNN uses a meta-policy to adaptively determine the number of aggregations for each node. The meta-policy is trained with deep reinforcement learning (RL) by exploiting the feedback from the model. We further introduce parameter sharing and a buffer mechanism to boost the training efficiency. Experimental results on three real-world benchmark datasets suggest that Policy-GNN significantly outperforms the state-of-the-art alternatives, showing the promise in aggregation optimization for GNNs.",
                        "Citation Paper Authors": "Authors:Kwei-Herng Lai, Daochen Zha, Kaixiong Zhou, Xia Hu"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.11690v3": {
            "Paper Title": "Brain Structural Saliency Over The Ages",
            "Sentences": []
        },
        "http://arxiv.org/abs/2209.03956v1": {
            "Paper Title": "Technology and Consciousness",
            "Sentences": []
        },
        "http://arxiv.org/abs/2208.04707v1": {
            "Paper Title": "Context sequence theory: a common explanation for multiple types of\n  learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.05058v2": {
            "Paper Title": "Inferring and Conveying Intentionality: Beyond Numerical Rewards to\n  Logical Intentions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.01607v2": {
            "Paper Title": "Modern Views of Machine Learning for Precision Psychiatry",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.00850v2": {
            "Paper Title": "Selective Inhibition and Recruitment of Linear-Threshold Thalamocortical\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.03901v1": {
            "Paper Title": "Reproducing sensory induced hallucinations via neural fields",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.03569v1": {
            "Paper Title": "Enhanced brain structure-function tethering in transmodal cortex\n  revealed by high-frequency eigenmodes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.16374v2": {
            "Paper Title": "The excitatory-inhibitory branching process: a parsimonious view of\n  cortical asynchronous states, excitability, and criticality",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.00583v2": {
            "Paper Title": "Feature-selected Graph Spatial Attention Network for Addictive\n  Brain-Networks Identification",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.00790v1": {
            "Paper Title": "Pavlov Learning Machines",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.00636v1": {
            "Paper Title": "Action-modulated midbrain dopamine activity arises from distributed\n  control policies",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.12508v2": {
            "Paper Title": "The Temporal Evolution of Modality-Independent Representations of\n  Conceptual Categories",
            "Sentences": []
        },
        "http://arxiv.org/abs/2207.00815v1": {
            "Paper Title": "Simulating reaction time for Eureka effect in visual object recognition\n  using artificial neural network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.08734v2": {
            "Paper Title": "Brain wave would include equipotential fluctuations in addition to\n  oscillations as sine waves: \u03c4 and burst",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.14048v1": {
            "Paper Title": "Short-Term Plasticity Neurons Learning to Learn and Forget",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.13465v1": {
            "Paper Title": "Iso-CapsNet: Isomorphic Capsule Network for Brain Graph Representation\n  Learning",
            "Sentences": [
                {
                    "Sentence ID": 41,
                    "Sentence": "can classify graph instances with node features.\nMeanwhile, the DIFFPOOL model ",
                    "Citation Text": "Rex Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L. Hamilton, and\nJure Leskovec. Hierarchical graph representation learning with differentiable\npooling. ArXiv , abs/1806.08804, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.08804",
                        "Citation Paper Title": "Title:Hierarchical Graph Representation Learning with Differentiable Pooling",
                        "Citation Paper Abstract": "Abstract:Recently, graph neural networks (GNNs) have revolutionized the field of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classification and link prediction. However, current GNN methods are inherently flat and do not learn hierarchical representations of graphs---a limitation that is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph. Here we propose DiffPool, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. DiffPool learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a set of clusters, which then form the coarsened input for the next GNN layer. Our experimental results show that combining existing GNN methods with DiffPool yields an average improvement of 5-10% accuracy on graph classification benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of five benchmark data sets.",
                        "Citation Paper Authors": "Authors:Rex Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L. Hamilton, Jure Leskovec"
                    }
                },
                {
                    "Sentence ID": 26,
                    "Sentence": "proposes a self-routing strategy to address the\nhigh time cost and strong cluster assumptions of the input data; ",
                    "Citation Text": "Jathushan Rajasegaran, Vinoj Jayasundara, Sandaru Jayasekara, Hirunima\nJayasekara, Suranga Seneviratne, and Ranga Rodrigo. Deepcaps: Going deeper\nwith capsule networks. CoRR , abs/1904.09546, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1904.09546",
                        "Citation Paper Title": "Title:DeepCaps: Going Deeper with Capsule Networks",
                        "Citation Paper Abstract": "Abstract:Capsule Network is a promising concept in deep learning, yet its true potential is not fully realized thus far, providing sub-par performance on several key benchmark datasets with complex data. Drawing intuition from the success achieved by Convolutional Neural Networks (CNNs) by going deeper, we introduce DeepCaps1, a deep capsule network architecture which uses a novel 3D convolution based dynamic routing algorithm. With DeepCaps, we surpass the state-of-the-art results in the capsule network domain on CIFAR10, SVHN and Fashion MNIST, while achieving a 68% reduction in the number of parameters. Further, we propose a class-independent decoder network, which strengthens the use of reconstruction loss as a regularization term. This leads to an interesting property of the decoder, which allows us to identify and control the physical attributes of the images represented by the instantiation parameters.",
                        "Citation Paper Authors": "Authors:Jathushan Rajasegaran, Vinoj Jayasundara, Sandaru Jayasekara, Hirunima Jayasekara, Suranga Seneviratne, Ranga Rodrigo"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "studies the relation\nextraction with capsule network from textual data; and ",
                    "Citation Text": "Congying Xia, Chenwei Zhang, Xiaohui Yan, Yi Chang, and Philip S. Yu. Zero-\nshot user intent detection via capsule neural networks. CoRR , abs/1809.00385,\n2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1809.00385",
                        "Citation Paper Title": "Title:Zero-shot User Intent Detection via Capsule Neural Networks",
                        "Citation Paper Abstract": "Abstract:User intent detection plays a critical role in question-answering and dialog systems. Most previous works treat intent detection as a classification problem where utterances are labeled with predefined intents. However, it is labor-intensive and time-consuming to label users' utterances as intents are diversely expressed and novel intents will continually be involved. Instead, we study the zero-shot intent detection problem, which aims to detect emerging user intents where no labeled utterances are currently available. We propose two capsule-based architectures: INTENT-CAPSNET that extracts semantic features from utterances and aggregates them to discriminate existing intents, and INTENTCAPSNET-ZSL which gives INTENTCAPSNET the zero-shot learning ability to discriminate emerging intents via knowledge transfer from existing intents. Experiments on two real-world datasets show that our model not only can better discriminate diversely expressed existing intents, but is also able to discriminate emerging intents when no labeled utterances are available.",
                        "Citation Paper Authors": "Authors:Congying Xia, Chenwei Zhang, Xiaohui Yan, Yi Chang, Philip S. Yu"
                    }
                },
                {
                    "Sentence ID": 45,
                    "Sentence": "studies the applica-\ntion of capsule network in various natural language processing\ntasks, e.g., text classification and Q&A; ",
                    "Citation Text": "Xinsong Zhang, Pengshuai Li, Weijia Jia, and Hai Zhao. Multi-labeled relation\nextraction with attentive capsule network. CoRR , abs/1811.04354, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.04354",
                        "Citation Paper Title": "Title:Multi-labeled Relation Extraction with Attentive Capsule Network",
                        "Citation Paper Abstract": "Abstract:To disclose overlapped multiple relations from a sentence still keeps challenging. Most current works in terms of neural models inconveniently assuming that each sentence is explicitly mapped to a relation label, cannot handle multiple relations properly as the overlapped features of the relations are either ignored or very difficult to identify. To tackle with the new issue, we propose a novel approach for multi-labeled relation extraction with capsule network which acts considerably better than current convolutional or recurrent net in identifying the highly overlapped relations within an individual sentence. To better cluster the features and precisely extract the relations, we further devise attention-based routing algorithm and sliding-margin loss function, and embed them into our capsule network. The experimental results show that the proposed approach can indeed extract the highly overlapped features and achieve significant performance improvement for relation extraction comparing to the state-of-the-art works.",
                        "Citation Paper Authors": "Authors:Xinsong Zhang, Pengshuai Li, Weijia Jia, Hai Zhao"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "studies the low-resolution image recognition with\na dual directed capsule network. In NLP, ",
                    "Citation Text": "Wei Zhao, Haiyun Peng, Steffen Eger, Erik Cambria, and Min Yang. Towards\nscalable and reliable capsule networks for challenging NLP applications. CoRR ,\nabs/1906.02829, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.02829",
                        "Citation Paper Title": "Title:Towards Scalable and Reliable Capsule Networks for Challenging NLP Applications",
                        "Citation Paper Abstract": "Abstract:Obstacles hindering the development of capsule networks for challenging NLP applications include poor scalability to large output spaces and less reliable routing processes. In this paper, we introduce: 1) an agreement score to evaluate the performance of routing processes at instance level; 2) an adaptive optimizer to enhance the reliability of routing; 3) capsule compression and partial routing to improve the scalability of capsule networks. We validate our approach on two NLP tasks, namely: multi-label text classification and question answering. Experimental results show that our approach considerably improves over strong competitors on both tasks. In addition, we gain the best results in low-resource settings with few training instances.",
                        "Citation Paper Authors": "Authors:Wei Zhao, Haiyun Peng, Steffen Eger, Erik Cambria, Min Yang"
                    }
                },
                {
                    "Sentence ID": 1,
                    "Sentence": "proposes to\napply capsule network for object segmentation; based on brain raw\nimaging data, ",
                    "Citation Text": "Parnian Afshar, Konstantinos N. Plataniotis, and Arash Mohammadi. Capsule\nnetworks for brain tumor classification based on MRI images and course tumor\nboundaries. CoRR , abs/1811.00597, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1811.00597",
                        "Citation Paper Title": "Title:Capsule Networks for Brain Tumor Classification based on MRI Images and Course Tumor Boundaries",
                        "Citation Paper Abstract": "Abstract:According to official statistics, cancer is considered as the second leading cause of human fatalities. Among different types of cancer, brain tumor is seen as one of the deadliest forms due to its aggressive nature, heterogeneous characteristics, and low relative survival rate. Determining the type of brain tumor has significant impact on the treatment choice and patient's survival. Human-centered diagnosis is typically error-prone and unreliable resulting in a recent surge of interest to automatize this process using convolutional neural networks (CNNs). CNNs, however, fail to fully utilize spatial relations, which is particularly harmful for tumor classification, as the relation between the tumor and its surrounding tissue is a critical indicator of the tumor's type. In our recent work, we have incorporated newly developed CapsNets to overcome this shortcoming. CapsNets are, however, highly sensitive to the miscellaneous image background. The paper addresses this gap. The main contribution is to equip CapsNet with access to the tumor surrounding tissues, without distracting it from the main target. A modified CapsNet architecture is, therefore, proposed for brain tumor classification, which takes the tumor coarse boundaries as extra inputs within its pipeline to increase the CapsNet's focus. The proposed approach noticeably outperforms its counterparts.",
                        "Citation Paper Authors": "Authors:Parnian Afshar, Konstantinos N. Plataniotis, Arash Mohammadi"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.11567v1": {
            "Paper Title": "Restoring speech intelligibility for hearing aid users with deep\n  learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.10677v1": {
            "Paper Title": "ConTraNet: A single end-to-end hybrid network for EEG-based and\n  EMG-based human machine interfaces",
            "Sentences": [
                {
                    "Sentence ID": 43,
                    "Sentence": ". Since  the data scarcity issue \nprevails in EEG and sEMG based HMI paradigms, the use of transformers in this field also \nremains limited. In ",
                    "Citation Text": "Y. Song, X. Jia, L. Yang, and L. Xie, \u201cTransformer -based Spatial -Temporal Feature Learning for \nEEG Decoding,\u201d 2021, doi: 10.48550/ARXIV.2106.11170.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2106.11170",
                        "Citation Paper Title": "Title:Transformer-based Spatial-Temporal Feature Learning for EEG Decoding",
                        "Citation Paper Abstract": "Abstract:At present, people usually use some methods based on convolutional neural networks (CNNs) for Electroencephalograph (EEG) decoding. However, CNNs have limitations in perceiving global dependencies, which is not adequate for common EEG paradigms with a strong overall relationship. Regarding this issue, we propose a novel EEG decoding method that mainly relies on the attention mechanism. The EEG data is firstly preprocessed and spatially filtered. And then, we apply attention transforming on the feature-channel dimension so that the model can enhance more relevant spatial features. The most crucial step is to slice the data in the time dimension for attention transforming, and finally obtain a highly distinguishable representation. At this time, global averaging pooling and a simple fully-connected layer are used to classify different categories of EEG data. Experiments on two public datasets indicate that the strategy of attention transforming effectively utilizes spatial and temporal features. And we have reached the level of the state-of-the-art in multi-classification of EEG, with fewer parameters. As far as we know, it is the first time that a detailed and complete method based on the transformer idea has been proposed in this field. It has good potential to promote the practicality of brain-computer interface (BCI). The source code can be found at: \\textit{this https URL}.",
                        "Citation Paper Authors": "Authors:Yonghao Song, Xueyu Jia, Lie Yang, Longhan Xie"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.09621v1": {
            "Paper Title": "Degeneracy in epilepsy: Multiple Routes to Hyperexcitable Brain Circuits\n  and their Repair",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.11228v1": {
            "Paper Title": "Adversarially trained neural representations may already be as robust as\n  corresponding biological neural representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.08813v1": {
            "Paper Title": "Effects of Neural Heterogeneity on Spiking Neural Network Dynamics",
            "Sentences": [
                {
                    "Sentence ID": 32,
                    "Sentence": "E. Ott and T. M. Antonsen, Chaos: An Interdisciplinary\nJournal of Nonlinear Science 18, 037113 (2008). ",
                    "Citation Text": "C. Bick, M. Goodfellow, C. R. Laing, and E. A. Martens,\nThe Journal of Mathematical Neuroscience 10, 9 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.05307",
                        "Citation Paper Title": "Title:Understanding the dynamics of biological and neural oscillator networks through exact mean-field reductions: a review",
                        "Citation Paper Abstract": "Abstract:Many biological and neural systems can be seen as networks of interacting periodic processes. Importantly, their functionality depends on the emerging collective dynamics of the network. Synchrony of oscillations is one of the most prominent examples of such collective behavior and has been associated both with function and dysfunction. Understanding how network structure and interactions, as well as the microscopic properties of individual units, shape the emerging collective dynamics is critical to find factors that lead to malfunction. However, many biological systems such as the brain consist of a large number of dynamical units. Hence, their analysis has either relied on simplified heuristic models on a coarse scale, or the analysis comes at a huge computational cost. Here we review recently introduced approaches, known as the Ott-Antonsen and Watanabe-Strogatz reductions, allowing one to simplify the analysis by bridging small and large scales. Thus, reduced model equations are obtained that exactly describe the collective dynamics for each subpopulation in the oscillator network via few collective variables only. The resulting equations are next-generation models: Rather than being heuristic, they exactly link microscopic and macroscopic descriptions and therefore accurately capture microscopic properties of the underlying system. At the same time, they are sufficiently simple to analyze without great computational effort. In the last decade, these reduction methods have become instrumental in understanding how network structure and interactions shape the collective dynamics and the emergence of synchrony. We review this progress based on concrete examples and outline possible limitations. Finally, we discuss how linking the reduced models with experimental data can guide the way towards the development of new treatment approaches, for example, for neurological disease.",
                        "Citation Paper Authors": "Authors:Christian Bick, Marc Goodfellow, Carlo R. Laing, Erik A. Martens"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.08666v1": {
            "Paper Title": "The Sensorium competition on predicting large-scale mouse primary visual\n  cortex activity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.11224v1": {
            "Paper Title": "Deep reinforcement learning for fMRI prediction of Autism Spectrum\n  Disorder",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.08492v1": {
            "Paper Title": "TKIL: Tangent Kernel Approach for Class Balanced Incremental Learning",
            "Sentences": [
                {
                    "Sentence ID": 38,
                    "Sentence": "with\nan initial learning rate of 0.01 for 70 epochs (which is notable less than the previous method, like\nPODNet ",
                    "Citation Text": "Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas Robert, and Eduardo Valle. Podnet:\nPooled outputs distillation for small-tasks incremental learning. In European Conference on\nComputer Vision , pages 86\u2013102. Springer, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2004.13513",
                        "Citation Paper Title": "Title:PODNet: Pooled Outputs Distillation for Small-Tasks Incremental Learning",
                        "Citation Paper Abstract": "Abstract:Lifelong learning has attracted much attention, but existing works still struggle to fight catastrophic forgetting and accumulate knowledge over long stretches of incremental learning. In this work, we propose PODNet, a model inspired by representation learning. By carefully balancing the compromise between remembering the old classes and learning new ones, PODNet fights catastrophic forgetting, even over very long runs of small incremental tasks --a setting so far unexplored by current works. PODNet innovates on existing art with an efficient spatial-based distillation-loss applied throughout the model and a representation comprising multiple proxy vectors for each class. We validate those innovations thoroughly, comparing PODNet with three state-of-the-art models on three datasets: CIFAR100, ImageNet100, and ImageNet1000. Our results showcase a significant advantage of PODNet over existing art, with accuracy gains of 12.10, 6.51, and 2.85 percentage points, respectively. Code is available at this https URL",
                        "Citation Paper Authors": "Authors:Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas Robert, Eduardo Valle"
                    }
                },
                {
                    "Sentence ID": 13,
                    "Sentence": "\ufb01netuned the model by\nre-learning samples in the memory after learning a new task. iTAML ",
                    "Citation Text": "Jathushan Rajasegaran, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Mubarak\nShah. itaml: An incremental task-agnostic meta-learning approach. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 13588\u201313597, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2003.11652",
                        "Citation Paper Title": "Title:iTAML: An Incremental Task-Agnostic Meta-learning Approach",
                        "Citation Paper Abstract": "Abstract:Humans can continuously learn new knowledge as their experience grows. In contrast, previous learning in deep neural networks can quickly fade out when they are trained on a new task. In this paper, we hypothesize this problem can be avoided by learning a set of generalized parameters, that are neither specific to old nor new tasks. In this pursuit, we introduce a novel meta-learning approach that seeks to maintain an equilibrium between all the encountered tasks. This is ensured by a new meta-update rule which avoids catastrophic forgetting. In comparison to previous meta-learning techniques, our approach is task-agnostic. When presented with a continuum of data, our model automatically identifies the task and quickly adapts to it with just a single update. We perform extensive experiments on five datasets in a class-incremental setting, leading to significant improvements over the state of the art methods (e.g., a 21.3% boost on CIFAR100 with 10 incremental tasks). Specifically, on large-scale datasets that generally prove difficult cases for incremental learning, our approach delivers absolute gains as high as 19.1% and 7.4% on ImageNet and MS-Celeb datasets, respectively.",
                        "Citation Paper Authors": "Authors:Jathushan Rajasegaran, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Mubarak Shah"
                    }
                },
                {
                    "Sentence ID": 37,
                    "Sentence": "as our base model. For all datasets, we use RAdam ",
                    "Citation Text": "Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao,\nand Jiawei Han. On the variance of the adaptive learning rate and beyond. arXiv preprint\narXiv:1908.03265 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.03265",
                        "Citation Paper Title": "Title:On the Variance of the Adaptive Learning Rate and Beyond",
                        "Citation Paper Abstract": "Abstract:The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Here, we study its mechanism in details. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate (i.e., it has problematically large variance in the early stage), suggest warmup works as a variance reduction technique, and provide both empirical and theoretical evidence to verify our hypothesis. We further propose RAdam, a new variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Extensive experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the effectiveness and robustness of our proposed method. All implementations are available at: this https URL.",
                        "Citation Paper Authors": "Authors:Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, Jiawei Han"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.08094v1": {
            "Paper Title": "Deep Neural Imputation: A Framework for Recovering Incomplete Brain\n  Recordings",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.06972v1": {
            "Paper Title": "Dilating blow-up time: A generalized solution of the NNLIF neuron model\n  and its global well-posedness",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": ", population density model s\nof integrate-and-\ufb01re neurons with jumps [19\u201321], and a mode l derived via a fast conductance limit ",
                    "Citation Text": "Jos\u00b4 e A Carrillo, Xu\u2019an Dou, and Zhennan Zhou. A simpli\ufb01 ed voltage-conductance kinetic model for interacting\nneurons and its asymptotic limit. arXivpreprint arXiv:2203.02746, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2203.02746",
                        "Citation Paper Title": "Title:A simplified voltage-conductance kinetic model for interacting neurons and its asymptotic limit",
                        "Citation Paper Abstract": "Abstract:The voltage-conductance kinetic model for the collective behavior of neurons has been studied by scientists and mathematicians for two decades, but the rigorous analysis of its solution structure has been only partially obtained in spite of plenty of numerical evidence in various scenarios. In this work, we consider a simplified voltage-conductance model in which the velocity field in the voltage variable is in a separable form. The long time behavior of the simplified model is fully investigated leading to the following dichotomy: either the density function converges to the global equilibrium, or the firing rate diverges as time goes to infinity. Besides, the fast conductance asymptotic limit is justified and analyzed, where the solution to the limit model either blows up in finite time, or globally exists leading to time periodic solutions. An important implication of these results is that the non-separable velocity field, or physically the leaky mechanism, is a key element for the emergence of periodic solutions in the original model based on the available numerical evidence.",
                        "Citation Paper Authors": "Authors:Jos\u00e9 A. Carrillo, Xu'an Dou, Zhennan Zhou"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.06477v1": {
            "Paper Title": "Multivariate Information Theory Uncovers Synergistic Subsystems of the\n  Human Cerebral Cortex",
            "Sentences": [
                {
                    "Sentence ID": 62,
                    "Sentence": "T. F. Varley and E. Hoel, Philosophical Transactions of\nthe Royal Society A: Mathematical, Physical and Engi-\nneering Sciences 380, 20210150 (2022). ",
                    "Citation Text": "P. Wollstadt, S. Schmitt, and M. Wibral, arXiv (2021).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2105.04187",
                        "Citation Paper Title": "Title:A Rigorous Information-Theoretic Definition of Redundancy and Relevancy in Feature Selection Based on (Partial) Information Decomposition",
                        "Citation Paper Abstract": "Abstract:Selecting a minimal feature set that is maximally informative about a target variable is a central task in machine learning and statistics. Information theory provides a powerful framework for formulating feature selection algorithms -- yet, a rigorous, information-theoretic definition of feature relevancy, which accounts for feature interactions such as redundant and synergistic contributions, is still missing. We argue that this lack is inherent to classical information theory which does not provide measures to decompose the information a set of variables provides about a target into unique, redundant, and synergistic contributions. Such a decomposition has been introduced only recently by the partial information decomposition (PID) framework. Using PID, we clarify why feature selection is a conceptually difficult problem when approached using information theory and provide a novel definition of feature relevancy and redundancy in PID terms. From this definition, we show that the conditional mutual information (CMI) maximizes relevancy while minimizing redundancy and propose an iterative, CMI-based algorithm for practical feature selection. We demonstrate the power of our CMI-based algorithm in comparison to the unconditional mutual information on benchmark examples and provide corresponding PID estimates to highlight how PID allows to quantify information contribution of features and their interactions in feature-selection problems.",
                        "Citation Paper Authors": "Authors:Patricia Wollstadt, Sebastian Schmitt, Michael Wibral"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "F. Rosas, P. A. M. Mediano, M. Gastpar, and H. J.\nJensen, Physical Review E 100, 032305 (2019). ",
                    "Citation Text": "J. T. Lizier, B. Flecker, and P. L. Williams, arXiv\n(2013), 10.1109/ALIFE.2013.6602430.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1303.3440",
                        "Citation Paper Title": "Title:Towards a Synergy-based Approach to Measuring Information Modification",
                        "Citation Paper Abstract": "Abstract:Distributed computation in artificial life and complex systems is often described in terms of component operations on information: information storage, transfer and modification. Information modification remains poorly described however, with the popularly-understood examples of glider and particle collisions in cellular automata being only quantitatively identified to date using a heuristic (separable information) rather than a proper information-theoretic measure. We outline how a recently-introduced axiomatic framework for measuring information redundancy and synergy, called partial information decomposition, can be applied to a perspective of distributed computation in order to quantify component operations on information. Using this framework, we propose a new measure of information modification that captures the intuitive understanding of information modification events as those involving interactions between two or more information sources. We also consider how the local dynamics of information modification in space and time could be measured, and suggest a new axiom that redundancy measures would need to meet in order to make such local measurements. Finally, we evaluate the potential for existing redundancy measures to meet this localizability axiom.",
                        "Citation Paper Authors": "Authors:Joseph T. Lizier, Benjamin Flecker, Paul L. Williams"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "A. I. Luppi, P. A. M. Mediano, F. E. Rosas, D. J. Har-\nrison, R. L. Carhart-Harris, D. Bor, and E. A. Sta-\nmatakis, Neuroscience of Consciousness 2021 (2021),\n10.1093/nc/niab027. ",
                    "Citation Text": "F. Rosas, P. A. M. Mediano, M. Gastpar, and H. J.\nJensen, Physical Review E 100, 032305 (2019).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1902.11239",
                        "Citation Paper Title": "Title:Quantifying High-order Interdependencies via Multivariate Extensions of the Mutual Information",
                        "Citation Paper Abstract": "Abstract:This article introduces a model-agnostic approach to study statistical synergy, a form of emergence in which patterns at large scales are not traceable from lower scales. Our framework leverages various multivariate extensions of Shannon's mutual information, and introduces the O-information as a metric capable of characterising synergy- and redundancy-dominated systems. We develop key analytical properties of the O-information, and study how it relates to other metrics of high-order interactions from the statistical mechanics and neuroscience literature. Finally, as a proof of concept, we use the proposed framework to explore the relevance of statistical synergy in Baroque music scores.",
                        "Citation Paper Authors": "Authors:Fernando Rosas, Pedro A.M. Mediano, Michael Gastpar, Henrik J. Jensen"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "F. Battiston, G. Cencetti, I. Iacopini, V. Latora, M. Lu-\ncas, A. Patania, J.-G. Young, and G. Petri, Physics Re-\nports Networks beyond pairwise interactions: Structure\nand dynamics, 874, 1 (2020). ",
                    "Citation Text": "F. Battiston, E. Amico, A. Barrat, G. Bianconi, G. Fer-\nraz de Arruda, B. Franceschiello, I. Iacopini, S. K\u0013 e\f,\nV. Latora, Y. Moreno, M. M. Murray, T. P. Peixoto,\nF. Vaccarino, and G. Petri, Nature Physics (2021),\n10.1038/s41567-021-01371-4.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.06023",
                        "Citation Paper Title": "Title:The physics of higher-order interactions in complex systems",
                        "Citation Paper Abstract": "Abstract:Complex networks have become the main paradigm for modelling the dynamics of interacting systems. However, networks are intrinsically limited to describing pairwise interactions, whereas real-world systems are often characterized by higher-order interactions involving groups of three or more units. Higher-order structures, such as hypergraphs and simplicial complexes, are therefore a better tool to map the real organization of many social, biological and man-made systems. Here, we highlight recent evidence of collective behaviours induced by higher-order interactions, and we outline three key challenges for the physics of higher-order systems.",
                        "Citation Paper Authors": "Authors:Federico Battiston, Enrico Amico, Alain Barrat, Ginestra Bianconi, Guilherme Ferraz de Arruda, Benedetta Franceschiello, Iacopo Iacopini, Sonia K\u00e9fi, Vito Latora, Yamir Moreno, Micah M. Murray, Tiago P. Peixoto, Francesco Vaccarino, Giovanni Petri"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.04727v1": {
            "Paper Title": "STNDT: Modeling Neural Population Activity with a Spatiotemporal\n  Transformer",
            "Sentences": [
                {
                    "Sentence ID": 14,
                    "Sentence": ". However,\nmore success was found in approaches that explicitly model neural responses as a dynamical system,\nincluding methods treating the population dynamics as being linear [ 12,13], switched linear ",
                    "Citation Text": "Scott Linderman, Matthew Johnson, Andrew Miller, Ryan Adams, David Blei, and Liam\nPaninski. Bayesian learning and inference in recurrent switching linear dynamical systems. In\nArti\ufb01cial Intelligence and Statistics , pages 914\u2013922. PMLR, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.08466",
                        "Citation Paper Title": "Title:Recurrent switching linear dynamical systems",
                        "Citation Paper Abstract": "Abstract:Many natural systems, such as neurons firing in the brain or basketball teams traversing a court, give rise to time series data with complex, nonlinear dynamics. We can gain insight into these systems by decomposing the data into segments that are each explained by simpler dynamic units. Building on switching linear dynamical systems (SLDS), we present a new model class that not only discovers these dynamical units, but also explains how their switching behavior depends on observations or continuous latent states. These \"recurrent\" switching linear dynamical systems provide further insight by discovering the conditions under which each unit is deployed, something that traditional SLDS models fail to do. We leverage recent algorithmic advances in approximate inference to make Bayesian inference in these models easy, fast, and scalable.",
                        "Citation Paper Authors": "Authors:Scott W. Linderman, Andrew C. Miller, Ryan P. Adams, David M. Blei, Liam Paninski, Matthew J. Johnson"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": ", we train the spatiotemporal transformer in\nan unsupervised way with BERT\u2019s mask modeling objective ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 ,\n2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 19,
                    "Sentence": "was\nproposed as a non-recurrent approach to improve inference speed by leveraging the transformers\narchitecture which learns and predicts momentary inputs in parallel ",
                    "Citation Text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems , 30, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.03762",
                        "Citation Paper Title": "Title:Attention Is All You Need",
                        "Citation Paper Abstract": "Abstract:The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                        "Citation Paper Authors": "Authors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.04603v1": {
            "Paper Title": "In search for an alternative to the computer metaphor of the mind and\n  brain",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.04293v1": {
            "Paper Title": "OptWedge: Cognitive Optimized Guidance toward Off-screen POIs",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.02851v1": {
            "Paper Title": "Synchronization of coupled Kuramoto oscillators competing for resources",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.02716v1": {
            "Paper Title": "Stacked unsupervised learning with a network architecture found by\n  supervised meta-learning",
            "Sentences": [
                {
                    "Sentence ID": 18,
                    "Sentence": "2.1\nOURS (THREE LAYER NET ) 2.1\nINVARIANT INFORMATION CLUSTERING (AVG SUB -HEAD )y ",
                    "Citation Text": "Xu Ji, Joao F Henriques, and Andrea Vedaldi. Invariant information clustering for unsuper-\nvised image classi\ufb01cation and segmentation. In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 9865\u20139874, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1807.06653",
                        "Citation Paper Title": "Title:Invariant Information Clustering for Unsupervised Image Classification and Segmentation",
                        "Citation Paper Abstract": "Abstract:We present a novel clustering objective that learns a neural network classifier from scratch, given only unlabelled data samples. The model discovers clusters that accurately match semantic classes, achieving state-of-the-art results in eight unsupervised clustering benchmarks spanning image classification and segmentation. These include STL10, an unsupervised variant of ImageNet, and CIFAR10, where we significantly beat the accuracy of our closest competitors by 6.6 and 9.5 absolute percentage points respectively. The method is not specialised to computer vision and operates on any paired dataset samples; in our experiments we use random transforms to obtain a pair from each image. The trained network directly outputs semantic labels, rather than high dimensional representations that need external processing to be usable for semantic clustering. The objective is simply to maximise mutual information between the class assignments of each pair. It is easy to implement and rigorously grounded in information theory, meaning we effortlessly avoid degenerate solutions that other clustering methods are susceptible to. In addition to the fully unsupervised mode, we also test two semi-supervised settings. The first achieves 88.8% accuracy on STL10 classification, setting a new global state-of-the-art over all existing methods (whether supervised, semi-supervised or unsupervised). The second shows robustness to 90% reductions in label coverage, of relevance to applications that wish to make use of small amounts of labels. this http URL",
                        "Citation Paper Authors": "Authors:Xu Ji, Jo\u00e3o F. Henriques, Andrea Vedaldi"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "is perhaps the most closely related to\nours. They use a few-shot supervised learning rule to tune an unsupervised learning algorithm that is\na form of randomized backward propagation ",
                    "Citation Text": "Timothy P Lillicrap, Daniel Cownden, Douglas B Tweed, and Colin J Akerman. Random synap-\ntic feedback weights support error backpropagation for deep learning. Nature communications ,\n7(1):1\u201310, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1411.0247",
                        "Citation Paper Title": "Title:Random feedback weights support learning in deep neural networks",
                        "Citation Paper Abstract": "Abstract:The brain processes information through many layers of neurons. This deep architecture is representationally powerful, but it complicates learning by making it hard to identify the responsible neurons when a mistake is made. In machine learning, the backpropagation algorithm assigns blame to a neuron by computing exactly how it contributed to an error. To do this, it multiplies error signals by matrices consisting of all the synaptic weights on the neuron's axon and farther downstream. This operation requires a precisely choreographed transport of synaptic weight information, which is thought to be impossible in the brain. Here we present a surprisingly simple algorithm for deep learning, which assigns blame by multiplying error signals by random synaptic weights. We show that a network can learn to extract useful information from signals sent through these random feedback connections. In essence, the network learns to learn. We demonstrate that this new mechanism performs as quickly and accurately as backpropagation on a variety of problems and describe the principles which underlie its function. Our demonstration provides a plausible basis for how a neuron can be adapted using error signals generated at distal locations in the brain, and thus dispels long-held assumptions about the algorithmic constraints on learning in neural circuits.",
                        "Citation Paper Authors": "Authors:Timothy P. Lillicrap, Daniel Cownden, Douglas B. Tweed, Colin J. Akerman"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.05103v2": {
            "Paper Title": "Transformer-Based Self-Supervised Learning for Emotion Recognition",
            "Sentences": [
                {
                    "Sentence ID": 33,
                    "Sentence": "use a Long Short-Term\nMemory (LSTM) network concurrently with a 1D-CNN. Siddhart h et al. ",
                    "Citation Text": "S. Siddharth, T. Jung, and T. J. Sejnowski. Utilizing De ep Learning Towards Multi-modal Bio-sensing and\nVision-based Affective Computing. IEEE Transactions on Affective Computing , pages 1\u20131, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1905.07039",
                        "Citation Paper Title": "Title:Utilizing Deep Learning Towards Multi-modal Bio-sensing and Vision-based Affective Computing",
                        "Citation Paper Abstract": "Abstract:In recent years, the use of bio-sensing signals such as electroencephalogram (EEG), electrocardiogram (ECG), etc. have garnered interest towards applications in affective computing. The parallel trend of deep-learning has led to a huge leap in performance towards solving various vision-based research problems such as object detection. Yet, these advances in deep-learning have not adequately translated into bio-sensing research. This work applies novel deep-learning-based methods to various bio-sensing and video data of four publicly available multi-modal emotion datasets. For each dataset, we first individually evaluate the emotion-classification performance obtained by each modality. We then evaluate the performance obtained by fusing the features from these modalities. We show that our algorithms outperform the results reported by other studies for emotion/valence/arousal/liking classification on DEAP and MAHNOB-HCI datasets and set up benchmarks for the newer AMIGOS and DREAMER datasets. We also evaluate the performance of our algorithms by combining the datasets and by using transfer learning to show that the proposed method overcomes the inconsistencies between the datasets. Hence, we do a thorough analysis of multi-modal affective data from more than 120 subjects and 2,800 trials. Finally, utilizing a convolution-deconvolution network, we propose a new technique towards identifying salient brain regions corresponding to various affective states.",
                        "Citation Paper Authors": "Authors:Siddharth Siddharth, Tzyy-Ping Jung, Terrence J. Sejnowski"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": ", we use \ufb01xed sinusoidal positional embeddings. We sum\nthe positional embeddings with the features F\u2032:\nZ={CLS+pe0,f1+pe1,...,fs+pes}, (1)\nwherepei\u2208Rdmodelis the positional embedding for time-step i. We then apply layer normalization ",
                    "Citation Text": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. L ayer Normalization. arXiv:1607.06450 [cs, stat] ,\nJuly 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1607.06450",
                        "Citation Paper Title": "Title:Layer Normalization",
                        "Citation Paper Abstract": "Abstract:Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.",
                        "Citation Paper Authors": "Authors:Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton"
                    }
                },
                {
                    "Sentence ID": 44,
                    "Sentence": "to process EEG signals for emo-\ntion recognition, converting the EEG signals into images us ing continuous wavelet transform. Behinaein et al. ",
                    "Citation Text": "Behnam Behinaein, Anubhav Bhatti, Dirk Rodenburg, Pau l Hungler, and Ali Etemad. A Transformer Architec-\nture for Stress Detection from ECG. In 2021 International Symposium on Wearable Computers , pages 132\u2013134,\nVirtual USA, September 2021. ACM.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2108.09737",
                        "Citation Paper Title": "Title:A Transformer Architecture for Stress Detection from ECG",
                        "Citation Paper Abstract": "Abstract:Electrocardiogram (ECG) has been widely used for emotion recognition. This paper presents a deep neural network based on convolutional layers and a transformer mechanism to detect stress using ECG signals. We perform leave-one-subject-out experiments on two publicly available datasets, WESAD and SWELL-KW, to evaluate our method. Our experiments show that the proposed model achieves strong results, comparable or better than the state-of-the-art models for ECG-based stress detection on these two datasets. Moreover, our method is end-to-end, does not require handcrafted features, and can learn robust representations with only a few convolutional blocks and the transformer component.",
                        "Citation Paper Authors": "Authors:Behnam Behinaein, Anubhav Bhatti, Dirk Rodenburg, Paul Hungler, Ali Etemad"
                    }
                },
                {
                    "Sentence ID": 43,
                    "Sentence": "employ a variation of the Transformer, the V ision Transformer ",
                    "Citation Text": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesniko v, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylva in Gelly, Jakob Uszkoreit, and Neil Houlsby. An\nImage is Worth 16x16 Words: Transformers for Image Recognit ion at Scale. arXiv:2010.11929 [cs] , October\n2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2010.11929",
                        "Citation Paper Title": "Title:An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
                        "Citation Paper Abstract": "Abstract:While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",
                        "Citation Paper Authors": "Authors:Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.00436v1": {
            "Paper Title": "Top-down inference in an early visual cortex inspired hierarchical\n  Variational Autoencoder",
            "Sentences": [
                {
                    "Sentence ID": 7,
                    "Sentence": "and has been used as a building block for\nV AEs ",
                    "Citation Text": "Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv preprint\narXiv:1509.00519 , 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1509.00519",
                        "Citation Paper Title": "Title:Importance Weighted Autoencoders",
                        "Citation Paper Abstract": "Abstract:The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.",
                        "Citation Paper Authors": "Authors:Yuri Burda, Roger Grosse, Ruslan Salakhutdinov"
                    }
                },
                {
                    "Sentence ID": 32,
                    "Sentence": ".\nIn these models, instead of learning higher-order dependencies between low-level features, higher\nlevels of hierarchy attempt to reconstruct activations at lower layers. These models have provided\nimportant insights into how learning dynamical representations contribute to neural phenomena ",
                    "Citation Text": "William Lotter, Gabriel Kreiman, and David Cox. A neural network trained for prediction mimics diverse\nfeatures of biological neurons and perception. Nature Machine Intelligence , 2(4):210\u2013219, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1805.10734",
                        "Citation Paper Title": "Title:A neural network trained to predict future video frames mimics critical properties of biological neuronal responses and perception",
                        "Citation Paper Abstract": "Abstract:While deep neural networks take loose inspiration from neuroscience, it is an open question how seriously to take the analogies between artificial deep networks and biological neuronal systems. Interestingly, recent work has shown that deep convolutional neural networks (CNNs) trained on large-scale image recognition tasks can serve as strikingly good models for predicting the responses of neurons in visual cortex to visual stimuli, suggesting that analogies between artificial and biological neural networks may be more than superficial. However, while CNNs capture key properties of the average responses of cortical neurons, they fail to explain other properties of these neurons. For one, CNNs typically require large quantities of labeled input data for training. Our own brains, in contrast, rarely have access to this kind of supervision, so to the extent that representations are similar between CNNs and brains, this similarity must arise via different training paths. In addition, neurons in visual cortex produce complex time-varying responses even to static inputs, and they dynamically tune themselves to temporal regularities in the visual environment. We argue that these differences are clues to fundamental differences between the computations performed in the brain and in deep networks. To begin to close the gap, here we study the emergent properties of a previously-described recurrent generative network that is trained to predict future video frames in a self-supervised manner. Remarkably, the model is able to capture a wide variety of seemingly disparate phenomena observed in visual cortex, ranging from single unit response dynamics to complex perceptual motion illusions. These results suggest potentially deep connections between recurrent predictive neural network models and the brain, providing new leads that can enrich both fields.",
                        "Citation Paper Authors": "Authors:William Lotter, Gabriel Kreiman, David Cox"
                    }
                },
                {
                    "Sentence ID": 23,
                    "Sentence": "but it it remains unclear if similar contribution is achieved in hV AEs. Another\nV AE used strong nonlinearities to understand visual processing in a higher level of hierarchy ",
                    "Citation Text": "Irina Higgins, Le Chang, Victoria Langston, Demis Hassabis, Christopher Summer\ufb01eld, Doris Tsao,\nand Matthew Botvinick. Unsupervised deep learning identi\ufb01es semantic disentanglement in single\ninferotemporal face patch neurons. Nature communications , 12(1):1\u201314, 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.14304",
                        "Citation Paper Title": "Title:Unsupervised deep learning identifies semantic disentanglement in single inferotemporal neurons",
                        "Citation Paper Abstract": "Abstract:Deep supervised neural networks trained to classify objects have emerged as popular models of computation in the primate ventral stream. These models represent information with a high-dimensional distributed population code, implying that inferotemporal (IT) responses are also too complex to interpret at the single-neuron level. We challenge this view by modelling neural responses to faces in the macaque IT with a deep unsupervised generative model, beta-VAE. Unlike deep classifiers, beta-VAE \"disentangles\" sensory data into interpretable latent factors, such as gender or hair length. We found a remarkable correspondence between the generative factors discovered by the model and those coded by single IT neurons. Moreover, we were able to reconstruct face images using the signals from just a handful of cells. This suggests that the ventral visual stream may be optimising the disentangling objective, producing a neural code that is low-dimensional and semantically interpretable at the single-unit level.",
                        "Citation Paper Authors": "Authors:Irina Higgins, Le Chang, Victoria Langston, Demis Hassabis, Christopher Summerfield, Doris Tsao, Matthew Botvinick"
                    }
                },
                {
                    "Sentence ID": 9,
                    "Sentence": "Hierarchical representations and image completion with V AEs Recently there have been signif-\nicant progress in scaling up non-Markovian hV AEs [ 44,10,22] but their learnt representations were\nmuch less studied. In ",
                    "Citation Text": "Yukun Chen, Frederik Tr\u00e4uble, Andrea Dittadi, Stefan Bauer, and Bernhard Sch\u00f6lkopf. Boxhead: A dataset\nfor learning hierarchical representations. arXiv preprint arXiv:2110.03628 , 2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.03628",
                        "Citation Paper Title": "Title:Boxhead: A Dataset for Learning Hierarchical Representations",
                        "Citation Paper Abstract": "Abstract:Disentanglement is hypothesized to be beneficial towards a number of downstream tasks. However, a common assumption in learning disentangled representations is that the data generative factors are statistically independent. As current methods are almost solely evaluated on toy datasets where this ideal assumption holds, we investigate their performance in hierarchical settings, a relevant feature of real-world data. In this work, we introduce Boxhead, a dataset with hierarchically structured ground-truth generative factors. We use this novel dataset to evaluate the performance of state-of-the-art autoencoder-based disentanglement models and observe that hierarchical models generally outperform single-layer VAEs in terms of disentanglement of hierarchically arranged factors.",
                        "Citation Paper Authors": "Authors:Yukun Chen, Andrea Dittadi, Frederik Tr\u00e4uble, Stefan Bauer, Bernhard Sch\u00f6lkopf"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.00293v1": {
            "Paper Title": "Multi-dimensional structure of C. elegans thermal learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.07655v1": {
            "Paper Title": "Classification of EEG Motor Imagery Using Deep Learning for\n  Brain-Computer Interface Systems",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": ". Either trials could in turn improve\nthe model's ability to produce a higher and more consis-\ntent rate of accuracy, ultimately allowing the CNN model\nto be used to control a device or combine with other inputs\nof human ",
                    "Citation Text": "M. D. Phung, Q. V. Tran, K. Hara, H. Inagaki, M. Abe, Easy-\nsetup eye movement recording system for human-computer in-\nteraction, in: 2008 IEEE International Conference on Research,\nInnovation and Vision for the Future in Computing and Com-\nmunication Technologies, 2008, pp. 292{297. doi:10.1109/\nRIVF.2008.4586369 .\n6",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.09427",
                        "Citation Paper Title": "Title:Easy-setup eye movement recording system for human-computer interaction",
                        "Citation Paper Abstract": "Abstract:Tracking the movement of human eyes is expected to yield natural and convenient applications based on human-computer interaction (HCI). To implement an effective eye-tracking system, eye movements must be recorded without placing any restriction on the user's behavior or user discomfort. This paper describes an eye movement recording system that offers free-head, simple configuration. It does not require the user to wear anything on her head, and she can move her head freely. Instead of using a computer, the system uses a visual digital signal processor (DSP) camera to detect the position of eye corner, the center of pupil and then calculate the eye movement. Evaluation tests show that the sampling rate of the system can be 300 Hz and the accuracy is about 1.8 degree/s.",
                        "Citation Paper Authors": "Authors:Manh Duong Phung, Quang Vinh Tran, Kenji Hara, Hirohito Inagaki, Masanobu Abe"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.09576v2": {
            "Paper Title": "Discovering Dynamic Functional Brain Networks via Spatial and\n  Channel-wise Attention",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": ". Thus, FBNs has a\nneuroanatomically meaningful context.\n2.3 Attention Mechanisms\nAttention Mechanisms have been applied to multiple tasks such as machine translation [ 22,51], image\nclassi\ufb01cation [ 25,53], object detection ",
                    "Citation Text": "Dai, J., Qi, H., Xiong, Y ., Li, Y ., Zhang, G., Hu, H., Wei, Y .: Deformable convolutional networks. In:\nProceedings of the IEEE international conference on computer vision. pp. 764\u2013773 (2017)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1703.06211",
                        "Citation Paper Title": "Title:Deformable Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in its building modules. In this work, we introduce two new modules to enhance the transformation modeling capacity of CNNs, namely, deformable convolution and deformable RoI pooling. Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks. Extensive experiments validate the effectiveness of our approach on sophisticated vision tasks of object detection and semantic segmentation. The code would be released.",
                        "Citation Paper Authors": "Authors:Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, Yichen Wei"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2206.00649v1": {
            "Paper Title": "Differentiable programming for functional connectomics",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.12465v2": {
            "Paper Title": "FBNETGEN: Task-aware GNN-based fMRI Analysis via Functional Brain\n  Network Generation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.14663v1": {
            "Paper Title": "Change in structural brain network abnormalities after traumatic brain\n  injury determines post-injury recovery",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.14544v1": {
            "Paper Title": "Temporal support vectors for spiking neuronal networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.13614v1": {
            "Paper Title": "Emergent organization of receptive fields in networks of excitatory and\n  inhibitory neurons",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.03711v2": {
            "Paper Title": "Deconvolution of the Functional Ultrasound Response in the Mouse Visual\n  Pathway Using Block-Term Decomposition",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.11914v1": {
            "Paper Title": "An Adaptive Contrastive Learning Model for Spike Sorting",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.03354v2": {
            "Paper Title": "Predictive coding and stochastic resonance as fundamental principles of\n  auditory perception",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.13198v2": {
            "Paper Title": "Dynamical properties of neuromorphic Josephson junctions",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.10605v1": {
            "Paper Title": "Brain Cortical Functional Gradients Predict Cortical Folding Patterns\n  via Attention Mesh Convolution",
            "Sentences": [
                {
                    "Sentence ID": 26,
                    "Sentence": "provides a solution to the problem. BrainSurfCNN is a\nmethod which uses spherical convolutional kernel ",
                    "Citation Text": "Chiyu Jiang, Jingwei Huang, Karthik Kashinath, Philip Marcus, Matthias Niessner, et al. Spherical cnns\non unstructured grids. arXiv preprint arXiv:1901.02039 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.02039",
                        "Citation Paper Title": "Title:Spherical CNNs on Unstructured Grids",
                        "Citation Paper Abstract": "Abstract:We present an efficient convolution kernel for Convolutional Neural Networks (CNNs) on unstructured grids using parameterized differential operators while focusing on spherical signals such as panorama images or planetary signals. To this end, we replace conventional convolution kernels with linear combinations of differential operators that are weighted by learnable parameters. Differential operators can be efficiently estimated on unstructured grids using one-ring neighbors, and learnable parameters can be optimized through standard back-propagation. As a result, we obtain extremely efficient neural networks that match or outperform state-of-the-art network architectures in terms of performance but with a significantly lower number of network parameters. We evaluate our algorithm in an extensive series of experiments on a variety of computer vision and climate science tasks, including shape classification, climate pattern segmentation, and omnidirectional image semantic segmentation. Overall, we present (1) a novel CNN approach on unstructured grids using parameterized differential operators for spherical signals, and (2) we show that our unique kernel parameterization allows our model to achieve the same or higher accuracy with significantly fewer network parameters.",
                        "Citation Paper Authors": "Authors:Chiyu \"Max\" Jiang, Jingwei Huang, Karthik Kashinath, Prabhat, Philip Marcus, Matthias Niessner"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": ", need geometric features of meshes, such as coordinates of\nvertices, which is not applicable to our task that learns and predicts \"texture\" features on meshes.\nBrainSurfCNN ",
                    "Citation Text": "Gia H Ngo, Meenakshi Khosla, Keith Jamison, Amy Kuceyeski, and Mert R Sabuncu. From connectomic\nto task-evoked \ufb01ngerprints: Individualized prediction of task contrasts from resting-state functional con-\nnectivity. In International Conference on Medical Image Computing and Computer-Assisted Intervention ,\npages 62\u201371. Springer, 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.02961",
                        "Citation Paper Title": "Title:From Connectomic to Task-evoked Fingerprints: Individualized Prediction of Task Contrasts from Resting-state Functional Connectivity",
                        "Citation Paper Abstract": "Abstract:Resting-state functional MRI (rsfMRI) yields functional connectomes that can serve as cognitive fingerprints of individuals. Connectomic fingerprints have proven useful in many machine learning tasks, such as predicting subject-specific behavioral traits or task-evoked activity. In this work, we propose a surface-based convolutional neural network (BrainSurfCNN) model to predict individual task contrasts from their resting-state fingerprints. We introduce a reconstructive-contrastive loss that enforces subject-specificity of model outputs while minimizing predictive error. The proposed approach significantly improves the accuracy of predicted contrasts over a well-established baseline. Furthermore, BrainSurfCNN's prediction also surpasses test-retest benchmark in a subject identification task.",
                        "Citation Paper Authors": "Authors:Gia H. Ngo, Meenakshi Khosla, Keith Jamison, Amy Kuceyeski, Mert R. Sabuncu"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "2.1 Mesh Convolution\nMeshes are composed of three distinct types of geometric primitives: vertices, edges, and faces. The\nclassi\ufb01cation on meshes is a basic research area which has made great progress with the development\nof deep learning. DiffusionNet ",
                    "Citation Text": "Nicholas Sharp, Souhaib Attaiki, Keenan Crane, and Maks Ovsjanikov. Diffusionnet: Discretization\nagnostic learning on surfaces. ACM Transactions on Graphics (TOG) , 41(3):1\u201316, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2012.00888",
                        "Citation Paper Title": "Title:DiffusionNet: Discretization Agnostic Learning on Surfaces",
                        "Citation Paper Abstract": "Abstract:We introduce a new general-purpose approach to deep learning on 3D surfaces, based on the insight that a simple diffusion layer is highly effective for spatial communication. The resulting networks are automatically robust to changes in resolution and sampling of a surface -- a basic property which is crucial for practical applications. Our networks can be discretized on various geometric representations such as triangle meshes or point clouds, and can even be trained on one representation then applied to another. We optimize the spatial support of diffusion as a continuous network parameter ranging from purely local to totally global, removing the burden of manually choosing neighborhood sizes. The only other ingredients in the method are a multi-layer perceptron applied independently at each point, and spatial gradient features to support directional filters. The resulting networks are simple, robust, and efficient. Here, we focus primarily on triangle mesh surfaces, and demonstrate state-of-the-art results for a variety of tasks including surface classification, segmentation, and non-rigid correspondence.",
                        "Citation Paper Authors": "Authors:Nicholas Sharp, Souhaib Attaiki, Keenan Crane, Maks Ovsjanikov"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2205.10144v1": {
            "Paper Title": "The developmental trajectory of object recognition robustness: children\n  are like small adults but unlike big deep neural networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.02461v2": {
            "Paper Title": "A whitening approach for Transfer Entropy permits the application to\n  narrow-band signals",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.10021v1": {
            "Paper Title": "Predicting electrode array impedance after one month from cochlear\n  implantation surgery",
            "Sentences": []
        },
        "http://arxiv.org/abs/2206.01725v1": {
            "Paper Title": "Sentences as connection paths: A neural language architecture of\n  sentence structure in the brain",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.08035v1": {
            "Paper Title": "Global solutions with infinitely many blowups in a mean-field neural\n  network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.13106v4": {
            "Paper Title": "Computational Complexity of Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.11016v2": {
            "Paper Title": "Linking Theories and Methods in Cognitive Sciences via Joint Embedding\n  of the Scientific Literature: The Example of Cognitive Control",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.05112v1": {
            "Paper Title": "Gamma Rhythm Analysis and Simulation Using Neuron Models",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.04670v1": {
            "Paper Title": "Explaining the effectiveness of fear extinction through latent-cause\n  inference",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.04347v1": {
            "Paper Title": "A neural network model for timing control with reinforcement",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.03503v1": {
            "Paper Title": "Pattern dynamics and stochasticity of the brain rhythms",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.01675v2": {
            "Paper Title": "Mesoscopic description of hippocampal replay and metastability in\n  spiking neural networks with short-term plasticity",
            "Sentences": []
        },
        "http://arxiv.org/abs/2205.02110v1": {
            "Paper Title": "Vehicle Noise: Comparison of Loudness Ratings in the Field and the\n  Laboratory",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.12774v1": {
            "Paper Title": "Matter & Mind Matter",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.12555v1": {
            "Paper Title": "Modeling observed gender imbalances in academic citation practices",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.12440v1": {
            "Paper Title": "neuro2vec: Masked Fourier Spectrum Prediction for Neurophysiological\n  Representation Learning",
            "Sentences": [
                {
                    "Sentence ID": 19,
                    "Sentence": "predicts succeeding pix-\nels given a sequence of pixels as input. MAE ",
                    "Citation Text": "Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Gir-\nshick. 2021. Masked autoencoders are scalable vision learners. arXiv preprint\narXiv:2111.06377 (2021).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2111.06377",
                        "Citation Paper Title": "Title:Masked Autoencoders Are Scalable Vision Learners",
                        "Citation Paper Abstract": "Abstract:This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.",
                        "Citation Paper Authors": "Authors:Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, Ross Girshick"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "adopted a larger batch size to replace the memory bank\nmechanism. BYOL ",
                    "Citation Text": "Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H\nRichemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhao-\nhan Daniel Guo, Mohammad Gheshlaghi Azar, et al .2020. Bootstrap your own\nlatent: A new approach to self-supervised learning. In NeurIPS .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.07733",
                        "Citation Paper Title": "Title:Bootstrap your own latent: A new approach to self-supervised Learning",
                        "Citation Paper Abstract": "Abstract:We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches $74.3\\%$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and $79.6\\%$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.",
                        "Citation Paper Authors": "Authors:Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R\u00e9mi Munos, Michal Valko"
                    }
                },
                {
                    "Sentence ID": 10,
                    "Sentence": "Contrastive ConvNet 76.19\u00b11.24 76.83\u00b11.56 83.84\u00b11.35 77.18\u00b11.62 95.95\u00b11.23 98.36\u00b11.43 98.60\u00b11.12 97.82\u00b10.53\nMoCo.V3 ",
                    "Citation Text": "Xinlei Chen, Saining Xie, and Kaiming He. 2021. An empirical study of training\nself-supervised vision transformers. In Proceedings of the IEEE/CVF International\nConference on Computer Vision . 9640\u20139649.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2104.02057",
                        "Citation Paper Title": "Title:An Empirical Study of Training Self-Supervised Vision Transformers",
                        "Citation Paper Abstract": "Abstract:This paper does not describe a novel method. Instead, it studies a straightforward, incremental, yet must-know baseline given the recent progress in computer vision: self-supervised learning for Vision Transformers (ViT). While the training recipes for standard convolutional networks have been highly mature and robust, the recipes for ViT are yet to be built, especially in the self-supervised scenarios where training becomes more challenging. In this work, we go back to basics and investigate the effects of several fundamental components for training self-supervised ViT. We observe that instability is a major issue that degrades accuracy, and it can be hidden by apparently good results. We reveal that these results are indeed partial failure, and they can be improved when training is made more stable. We benchmark ViT results in MoCo v3 and several other self-supervised frameworks, with ablations in various aspects. We discuss the currently positive evidence as well as challenges and open questions. We hope that this work will provide useful data points and experience for future research.",
                        "Citation Paper Authors": "Authors:Xinlei Chen, Saining Xie, Kaiming He"
                    }
                },
                {
                    "Sentence ID": 6,
                    "Sentence": "Contrastive ConvNet 71.65\u00b10.73 75.26\u00b10.81 83.29\u00b10.76 76.68\u00b10.94 94.54\u00b10.48 96.64\u00b10.52 97.94\u00b10.64 96.70\u00b10.87\nSwAV ",
                    "Citation Text": "Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and\nArmand Joulin. 2020. Unsupervised learning of visual features by contrasting\ncluster assignments. Advances in Neural Information Processing Systems 33 (2020),\n9912\u20139924.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2006.09882",
                        "Citation Paper Title": "Title:Unsupervised Learning of Visual Features by Contrasting Cluster Assignments",
                        "Citation Paper Abstract": "Abstract:Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or views) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a swapped prediction mechanism where we predict the cluster assignment of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements much. We validate our findings by achieving 75.3% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.",
                        "Citation Paper Authors": "Authors:Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, Armand Joulin"
                    }
                },
                {
                    "Sentence ID": 8,
                    "Sentence": "adopted a large mem-\nory bank to introduce enough informative negative samples, while\nSimCLR ",
                    "Citation Text": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A\nSimple Framework for Contrastive Learning of Visual Representations. arXiv\npreprint arXiv:2002.05709 (2020).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2002.05709",
                        "Citation Paper Title": "Title:A Simple Framework for Contrastive Learning of Visual Representations",
                        "Citation Paper Abstract": "Abstract:This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.",
                        "Citation Paper Authors": "Authors:Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": "are a family of autoencoders that re-\nconstruct the uncorrupted input signal with a corrupted version\nof the signal as input. Generalizing the denoising autoregressive\nmodeling, masked predictions attracted the attention of both the\nNLP and vision community. BERT ",
                    "Citation Text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding. arXiv\npreprint arXiv:1810.04805 (2018).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "2.1 Contrastive Learning\nContrastive learning methods learn instance-level discriminative\nrepresentations by extracting invariant features over distorted\nviews of the same data point. MoCo ",
                    "Citation Text": "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Momen-\ntum contrast for unsupervised visual representation learning. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition . 9729\u20139738.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.05722",
                        "Citation Paper Title": "Title:Momentum Contrast for Unsupervised Visual Representation Learning",
                        "Citation Paper Abstract": "Abstract:We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.",
                        "Citation Paper Authors": "Authors:Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2204.09147v1": {
            "Paper Title": "Magnetic field effects in biology from the perspective of the radical\n  pair mechanism",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.09564v1": {
            "Paper Title": "Cross-view Brain Decoding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.08086v1": {
            "Paper Title": "Intracranial EEG structure-function coupling predicts surgical outcomes\n  in focal epilepsy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.15415v2": {
            "Paper Title": "Spatiotemporal Patterns in Neurobiology: An Overview for Future\n  Artificial Intelligence",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.07005v1": {
            "Paper Title": "Interpretability of Machine Learning Methods Applied to Neuroimaging",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.04733v1": {
            "Paper Title": "NeuRL: Closed-form Inverse Reinforcement Learning for Neural Decoding",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.03348v1": {
            "Paper Title": "Barcodes distinguish morphology of neuronal tauopathy",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.02169v2": {
            "Paper Title": "Hybrid Predictive Coding: Inferring, Fast and Slow",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.01411v1": {
            "Paper Title": "Computer-Aided Extraction of Select MRI Markers of Cerebral Small Vessel\n  Disease: A Systematic Review",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.01182v1": {
            "Paper Title": "Curiosity as filling, compressing, and reconfiguring knowledge networks",
            "Sentences": [
                {
                    "Sentence ID": 84,
                    "Sentence": "Daniel Maliniak, Ryan Powers, and Barbara F Walter. The gender citation gap in international\nrelations. International Organization , 67(4):889{922, 2013. ",
                    "Citation Text": "Jordan D. Dworkin, Kristin A. Linn, Erin G. Teich, Perry Zurn, Russell T. Shinohara, and\nDanielle S. Bassett. The extent and drivers of gender imbalance in neuroscience reference lists.\nbioRxiv , 2020. doi: 10.1101/2020.01.03.894378. URL https://www.biorxiv.org/content/\nearly/2020/01/11/2020.01.03.894378 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2001.01002",
                        "Citation Paper Title": "Title:The extent and drivers of gender imbalance in neuroscience reference lists",
                        "Citation Paper Abstract": "Abstract:Like many scientific disciplines, neuroscience has increasingly attempted to confront pervasive gender imbalances within the field. While much of the conversation has centered around publishing and conference participation, recent research in other fields has called attention to the prevalence of gender bias in citation practices. Because of the downstream effects that citations can have on visibility and career advancement, understanding and eliminating gender bias in citation practices is vital for addressing inequity in a scientific community. In this study, we sought to determine whether there is evidence of gender bias in the citation practices of neuroscientists. Using data from five top neuroscience journals, we find that reference lists tend to include more papers with men as first and last author than would be expected if gender were not a factor in referencing. Importantly, we show that this overcitation of men and undercitation of women is driven largely by the citation practices of men, and is increasing over time as the field becomes more diverse. We develop a co-authorship network to assess homophily in researchers' social networks, and we find that men tend to overcite men even when their social networks are representative. We discuss possible mechanisms and consider how individual researchers might address these findings in their own practices.",
                        "Citation Paper Authors": "Authors:Jordan D. Dworkin, Kristin A. Linn, Erin G. Teich, Perry Zurn, Russell T. Shinohara, Danielle S. Bassett"
                    }
                },
                {
                    "Sentence ID": 82,
                    "Sentence": "Michelle L Dion, Jane Lawrence Sumner, and Sara McLaughlin Mitchell. Gendered citation\npatterns across political science and social science methodology \felds. Political Analysis , 26\n(3):312{327, 2018. ",
                    "Citation Text": "Neven Caplar, Sandro Tacchella, and Simon Birrer. Quantitative evaluation of gender bias in\nastronomical publications from citation counts. Nature Astronomy , 1(6):0141, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.08984",
                        "Citation Paper Title": "Title:Quantitative Evaluation of Gender Bias in Astronomical Publications from Citation Counts",
                        "Citation Paper Abstract": "Abstract:We analyze the role of first (leading) author gender on the number of citations that a paper receives, on the publishing frequency and on the self-citing tendency. We consider a complete sample of over 200,000 publications from 1950 to 2015 from five major astronomy journals. We determine the gender of the first author for over 70% of all publications. The fraction of papers which have a female first author has increased from less than 5% in the 1960s to about 25% today. We find that the increase of the fraction of papers authored by females is slowest in the most prestigious journals such as Science and Nature. Furthermore, female authors write 19$\\pm$7% fewer papers in seven years following their first paper than their male colleagues. At all times papers with male first authors receive more citations than papers with female first authors. This difference has been decreasing with time and amounts to $\\sim$6% measured over the last 30 years. To account for the fact that the properties of female and male first author papers differ intrinsically, we use a random forest algorithm to control for the non-gender specific properties of these papers which include seniority of the first author, number of references, total number of authors, year of publication, publication journal, field of study and region of the first author's institution. We show that papers authored by females receive 10.4$\\pm$0.9% fewer citations than what would be expected if the papers with the same non-gender specific properties were written by the male authors. Finally, we also find that female authors in our sample tend to self-cite more, but that this effect disappears when controlled for non-gender specific variables.",
                        "Citation Paper Authors": "Authors:Neven Caplar, Sandro Tacchella, Simon Birrer"
                    }
                },
                {
                    "Sentence ID": 77,
                    "Sentence": "Afra Zomorodian and Gunnar Carlsson. Computing persistent homology. Discrete & Com-\nputational Geometry , 33(2):249{274, 2005. doi: 10.1007/s00454-004-1146-y. URL https:\n//doi.org/10.1007/s00454-004-1146-y . ",
                    "Citation Text": "Ann E. Sizemore, Jennifer E. Phillips-Cremins, Robert Ghrist, and Danielle S. Bassett. The\nimportance of the whole: Topological data analysis for the network neuroscientist. Network\n28Neuroscience , 3(3):656{673, July 2019. ISSN 2472-1751. doi: 10.1162/netn a00073. URL\nhttps://doi.org/10.1162/netn_a_00073 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.05167",
                        "Citation Paper Title": "Title:The importance of the whole: topological data analysis for the network neuroscientist",
                        "Citation Paper Abstract": "Abstract:The application of network techniques to the analysis of neural data has greatly improved our ability to quantify and describe these rich interacting systems. Among many important contributions, networks have proven useful in identifying sets of node pairs that are densely connected and that collectively support brain function. Yet the restriction to pairwise interactions prevents us from realizing intrinsic topological features such as cavities within the interconnection structure that may be just as crucial for proper function. To detect and quantify these topological features we must turn to methods from algebraic topology that encode data as a simplicial complex built of sets of interacting nodes called simplices. On this substrate, we can then use the relations between simplices and higher-order connectivity to expose cavities within the complex, thereby summarizing its topological nature. Here we provide an introduction to persistent homology, a fundamental method from applied topology that builds a global descriptor of system structure by chronicling the evolution of cavities as we move through a combinatorial object such as a weighted network. We detail the underlying mathematics and perform demonstrative calculations on the mouse structural connectome, electrical and chemical synapses in \\textit{C. elegans}, and genomic interaction data. Finally we suggest avenues for future work and highlight new advances in mathematics that appear ready for use in revealing the architecture and function of neural systems.",
                        "Citation Paper Authors": "Authors:Ann E. Sizemore, Jennifer Phillips-Cremins, Robert Ghrist, Danielle S. Bassett"
                    }
                },
                {
                    "Sentence ID": 68,
                    "Sentence": "Arthur Aubret, Laetitia Matignon, and Salima Hassas. A survey on intrinsic motivation in\nreinforcement learning. arXiv , 2019. ",
                    "Citation Text": "Chad Giusti, Eva Pastalkova, Carina Curto, and Vladimir Itskov. Clique topology reveals\nintrinsic geometric structure in neural correlations. Proceedings of the National Academy of\nSciences , 112(44):13455{13460, 2015. ISSN 0027-8424. doi: 10.1073/pnas.1506407112. URL\nhttps://www.pnas.org/content/112/44/13455 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1502.06172",
                        "Citation Paper Title": "Title:Clique topology reveals intrinsic geometric structure in neural correlations",
                        "Citation Paper Abstract": "Abstract:Detecting meaningful structure in neural activity and connectivity data is challenging in the presence of hidden nonlinearities, where traditional eigenvalue-based methods may be misleading. We introduce a novel approach to matrix analysis, called clique topology, that extracts features of the data invariant under nonlinear monotone transformations. These features can be used to detect both random and geometric structure, and depend only on the relative ordering of matrix entries. We then analyzed the activity of pyramidal neurons in rat hippocampus, recorded while the animal was exploring a two-dimensional environment, and confirmed that our method is able to detect geometric organization using only the intrinsic pattern of neural correlations. Remarkably, we found similar results during non-spatial behaviors such as wheel running and REM sleep. This suggests that the geometric structure of correlations is shaped by the underlying hippocampal circuits, and is not merely a consequence of position coding. We propose that clique topology is a powerful new tool for matrix analysis in biological settings, where the relationship of observed quantities to more meaningful variables is often nonlinear and unknown.",
                        "Citation Paper Authors": "Authors:Chad Giusti, Eva Pastalkova, Carina Curto, Vladimir Itskov"
                    }
                },
                {
                    "Sentence ID": 67,
                    "Sentence": "Nikolay Savinov, Anton Raichuk, Rapha\u007f el Marinier, Damien Vincent, Marc Pollefeys, Timothy\nLillicrap, and Sylvain Gelly. Episodic curiosity through reachability. arXiv , 2018. ",
                    "Citation Text": "Arthur Aubret, Laetitia Matignon, and Salima Hassas. A survey on intrinsic motivation in\nreinforcement learning. arXiv , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1908.06976",
                        "Citation Paper Title": "Title:A survey on intrinsic motivation in reinforcement learning",
                        "Citation Paper Abstract": "Abstract:The reinforcement learning (RL) research area is very active, with an important number of new contributions; especially considering the emergent field of deep RL (DRL). However a number of scientific and technical challenges still need to be addressed, amongst which we can mention the ability to abstract actions or the difficulty to explore the environment which can be addressed by intrinsic motivation (IM). In this article, we provide a survey on the role of intrinsic motivation in DRL. We categorize the different kinds of intrinsic motivations and detail for each category, its advantages and limitations with respect to the mentioned challenges. Additionnally, we conduct an in-depth investigation of substantial current research questions, that are currently under study or not addressed at all in the considered research area of DRL. We choose to survey these research works, from the perspective of learning how to achieve tasks. We suggest then, that solving current challenges could lead to a larger developmental architecture which may tackle most of the tasks. We describe this developmental architecture on the basis of several building blocks composed of a RL algorithm and an IM module compressing information.",
                        "Citation Paper Authors": "Authors:Arthur Aubret, Laetitia Matignon, Salima Hassas"
                    }
                },
                {
                    "Sentence ID": 66,
                    "Sentence": "Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven explo-\nration by self-supervised prediction. In Proceedings of the 34th International Conference on\nMachine Learning - Volume 70 , ICML'17, page 2778{2787. JMLR.org, 2017. ",
                    "Citation Text": "Nikolay Savinov, Anton Raichuk, Rapha\u007f el Marinier, Damien Vincent, Marc Pollefeys, Timothy\nLillicrap, and Sylvain Gelly. Episodic curiosity through reachability. arXiv , 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.02274",
                        "Citation Paper Title": "Title:Episodic Curiosity through Reachability",
                        "Citation Paper Abstract": "Abstract:Rewards are sparse in the real world and most of today's reinforcement learning algorithms struggle with such sparsity. One solution to this problem is to allow the agent to create rewards for itself - thus making rewards dense and more suitable for learning. In particular, inspired by curious behaviour in animals, observing something novel could be rewarded with a bonus. Such bonus is summed up with the real task reward - making it possible for RL algorithms to learn from the combined reward. We propose a new curiosity method which uses episodic memory to form the novelty bonus. To determine the bonus, the current observation is compared with the observations in memory. Crucially, the comparison is done based on how many environment steps it takes to reach the current observation from those in memory - which incorporates rich information about environment dynamics. This allows us to overcome the known \"couch-potato\" issues of prior work - when the agent finds a way to instantly gratify itself by exploiting actions which lead to hardly predictable consequences. We test our approach in visually rich 3D environments in ViZDoom, DMLab and MuJoCo. In navigational tasks from ViZDoom and DMLab, our agent outperforms the state-of-the-art curiosity method ICM. In MuJoCo, an ant equipped with our curiosity module learns locomotion out of the first-person-view curiosity only.",
                        "Citation Paper Authors": "Authors:Nikolay Savinov, Anton Raichuk, Rapha\u00ebl Marinier, Damien Vincent, Marc Pollefeys, Timothy Lillicrap, Sylvain Gelly"
                    }
                },
                {
                    "Sentence ID": 65,
                    "Sentence": "Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction . The MIT\nPress, 2018. ",
                    "Citation Text": "Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven explo-\nration by self-supervised prediction. In Proceedings of the 34th International Conference on\nMachine Learning - Volume 70 , ICML'17, page 2778{2787. JMLR.org, 2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.05363",
                        "Citation Paper Title": "Title:Curiosity-driven Exploration by Self-supervised Prediction",
                        "Citation Paper Abstract": "Abstract:In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at this https URL",
                        "Citation Paper Authors": "Authors:Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, Trevor Darrell"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.12710v3": {
            "Paper Title": "Brain Principles Programming",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.16414v1": {
            "Paper Title": "Surface Vision Transformers: Attention-Based Modelling applied to\n  Cortical Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/2204.05132v1": {
            "Paper Title": "A Spiking Neural Network based on Neural Manifold for Augmenting\n  Intracortical Brain-Computer Interface Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.10602v1": {
            "Paper Title": "Qualia as physical measurements: a mathematical model of qualia and pure\n  concepts",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.09719v1": {
            "Paper Title": "Evolution as Explanation: The Origins of Neural Codes and their\n  Efficiencies",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.08648v1": {
            "Paper Title": "Artificial Intelligence Enables Real-Time and Intuitive Control of\n  Prostheses via Nerve Interface",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.08341v1": {
            "Paper Title": "Exact mean-field models for spiking neural networks with adaptation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.06448v1": {
            "Paper Title": "Discrete, recurrent, and scalable patterns in human judgement underlie\n  affective picture ratings",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.05191v1": {
            "Paper Title": "Evaluation of Performance for Human In-Vivo Conductivity Estimation from\n  EEG and sEEG Recorded in Simultaneous with Intracerebral Electrical\n  Stimulation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.16444v1": {
            "Paper Title": "Accounting for iron-related off-target binding effects of 18F-AV1451 PET\n  in the evaluation of cognition and microstructure in APOE-e4+ MCI",
            "Sentences": []
        },
        "http://arxiv.org/abs/2203.03481v1": {
            "Paper Title": "EEG to fMRI Synthesis Benefits from Attentional Graphs of Electrode\n  Relationships",
            "Sentences": [
                {
                    "Sentence ID": 41,
                    "Sentence": "Xin Yi, Ekta Walia, and Paul Babyn. Generative adversarial network in medi-\ncal imaging: A review. Medical Image Analysis , 58:101552, 2019. ISSN 1361-\n8415. https://doi.org/https://doi.org/10.1016/j.media.2019.101552. URL http://www.\nsciencedirect.com/science/article/pii/S1361841518308430 . ",
                    "Citation Text": "Francisco Afonso Raposo, David Martins de Matos, and Ricardo Ribeiro. Learning low-\ndimensional semantics for music and language via multi-subject fmri. Neuroinformatics ,\npages 1\u201311, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1906.11759",
                        "Citation Paper Title": "Title:Low-dimensional Embodied Semantics for Music and Language",
                        "Citation Paper Abstract": "Abstract:Embodied cognition states that semantics is encoded in the brain as firing patterns of neural circuits, which are learned according to the statistical structure of human multimodal experience. However, each human brain is idiosyncratically biased, according to its subjective experience history, making this biological semantic machinery noisy with respect to the overall semantics inherent to media artifacts, such as music and language excerpts. We propose to represent shared semantics using low-dimensional vector embeddings by jointly modeling several brains from human subjects. We show these unsupervised efficient representations outperform the original high-dimensional fMRI voxel spaces in proxy music genre and language topic classification tasks. We further show that joint modeling of several subjects increases the semantic richness of the learned latent vector spaces.",
                        "Citation Paper Authors": "Authors:Francisco Afonso Raposo, David Martins de Matos, Ricardo Ribeiro"
                    }
                },
                {
                    "Sentence ID": 30,
                    "Sentence": "Ngoc-Trung Tran, Tuan-Anh Bui, and Ngai-Man Cheung. Dist-gan: An improved gan us-\ning distance constraints. In Proceedings of the European conference on computer vision\n(ECCV) , pages 370\u2013385, 2018. ",
                    "Citation Text": "Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of\nmachine learning algorithms. Advances in neural information processing systems , 25, 2012.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1206.2944",
                        "Citation Paper Title": "Title:Practical Bayesian Optimization of Machine Learning Algorithms",
                        "Citation Paper Abstract": "Abstract:Machine learning algorithms frequently require careful tuning of model hyperparameters, regularization terms, and optimization parameters. Unfortunately, this tuning is often a \"black art\" that requires expert experience, unwritten rules of thumb, or sometimes brute-force search. Much more appealing is the idea of developing automatic approaches which can optimize the performance of a given learning algorithm to the task at hand. In this work, we consider the automatic tuning problem within the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). The tractable posterior distribution induced by the GP leads to efficient use of the information gathered by previous experiments, enabling optimal choices about what parameters to try next. Here we show how the effects of the Gaussian process prior and the associated inference procedure can have a large impact on the success or failure of Bayesian optimization. We show that thoughtful choices can lead to results that exceed expert-level performance in tuning machine learning algorithms. We also describe new algorithms that take into account the variable cost (duration) of learning experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization on a diverse set of contemporary algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.",
                        "Citation Paper Authors": "Authors:Jasper Snoek, Hugo Larochelle, Ryan P. Adams"
                    }
                },
                {
                    "Sentence ID": 29,
                    "Sentence": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep\nconvolutional neural networks. Advances in neural information processing systems , 25:\n1097\u20131105, 2012. ",
                    "Citation Text": "Ngoc-Trung Tran, Tuan-Anh Bui, and Ngai-Man Cheung. Dist-gan: An improved gan us-\ning distance constraints. In Proceedings of the European conference on computer vision\n(ECCV) , pages 370\u2013385, 2018.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.08887",
                        "Citation Paper Title": "Title:Dist-GAN: An Improved GAN using Distance Constraints",
                        "Citation Paper Abstract": "Abstract:We introduce effective training algorithms for Generative Adversarial Networks (GAN) to alleviate mode collapse and gradient vanishing. In our system, we constrain the generator by an Autoencoder (AE). We propose a formulation to consider the reconstructed samples from AE as \"real\" samples for the discriminator. This couples the convergence of the AE with that of the discriminator, effectively slowing down the convergence of discriminator and reducing gradient vanishing. Importantly, we propose two novel distance constraints to improve the generator. First, we propose a latent-data distance constraint to enforce compatibility between the latent sample distances and the corresponding data sample distances. We use this constraint to explicitly prevent the generator from mode collapse. Second, we propose a discriminator-score distance constraint to align the distribution of the generated samples with that of the real samples through the discriminator score. We use this constraint to guide the generator to synthesize samples that resemble the real ones. Our proposed GAN using these distance constraints, namely Dist-GAN, can achieve better results than state-of-the-art methods across benchmark datasets: synthetic, MNIST, MNIST-1K, CelebA, CIFAR-10 and STL-10 datasets. Our code is published here (this https URL) for research.",
                        "Citation Paper Authors": "Authors:Ngoc-Trung Tran, Tuan-Anh Bui, Ngai-Man Cheung"
                    }
                },
                {
                    "Sentence ID": 25,
                    "Sentence": "Matthew Tancik, Pratul P Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Ragha-\nvan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T Barron, and Ren Ng. Fourier features\nlet networks learn high frequency functions in low dimensional domains. arXiv preprint\narXiv:2006.10739 , 2020. ",
                    "Citation Text": "Zhu Li, Jean-Francois Ton, Dino Oglic, and Dino Sejdinovic. Towards a unified analysis of\nrandom fourier features. In International Conference on Machine Learning , pages 3905\u2013\n3914. PMLR, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1806.09178",
                        "Citation Paper Title": "Title:Towards A Unified Analysis of Random Fourier Features",
                        "Citation Paper Abstract": "Abstract:Random Fourier features is a widely used, simple, and effective technique for scaling up kernel methods. The existing theoretical analysis of the approach, however, remains focused on specific learning tasks and typically gives pessimistic bounds which are at odds with the empirical results. We tackle these problems and provide the first unified risk analysis of learning with random Fourier features using the squared error and Lipschitz continuous loss functions. In our bounds, the trade-off between the computational cost and the expected risk convergence rate is problem specific and expressed in terms of the regularization parameter and the \\emph{number of effective degrees of freedom}. We study both the standard random Fourier features method for which we improve the existing bounds on the number of features required to guarantee the corresponding minimax risk convergence rate of kernel ridge regression, as well as a data-dependent modification which samples features proportional to \\emph{ridge leverage scores} and further reduces the required number of features. As ridge leverage scores are expensive to compute, we devise a simple approximation scheme which provably reduces the computational cost without loss of statistical efficiency.",
                        "Citation Paper Authors": "Authors:Zhu Li, Jean-Francois Ton, Dino Oglic, Dino Sejdinovic"
                    }
                },
                {
                    "Sentence ID": 18,
                    "Sentence": "Edwin JR van Beek, Christiane Kuhl, Yoshimi Anzai, Patricia Desmond, Richard L Ehman,\nQiyong Gong, Garry Gold, Vikas Gulani, Margaret Hall-Craggs, Tim Leiner, et al. Value\nof mri in medicine: More than just another test?, 2019. ",
                    "Citation Text": "Jiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt. Stylenerf: A style-based 3d-\naware generator for high-resolution image synthesis. arXiv preprint arXiv:2110.08985 ,\n2021.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2110.08985",
                        "Citation Paper Title": "Title:StyleNeRF: A Style-based 3D-Aware Generator for High-resolution Image Synthesis",
                        "Citation Paper Abstract": "Abstract:We propose StyleNeRF, a 3D-aware generative model for photo-realistic high-resolution image synthesis with high multi-view consistency, which can be trained on unstructured 2D images. Existing approaches either cannot synthesize high-resolution images with fine details or yield noticeable 3D-inconsistent artifacts. In addition, many of them lack control over style attributes and explicit 3D camera poses. StyleNeRF integrates the neural radiance field (NeRF) into a style-based generator to tackle the aforementioned challenges, i.e., improving rendering efficiency and 3D consistency for high-resolution image generation. We perform volume rendering only to produce a low-resolution feature map and progressively apply upsampling in 2D to address the first issue. To mitigate the inconsistencies caused by 2D upsampling, we propose multiple designs, including a better upsampler and a new regularization loss. With these designs, StyleNeRF can synthesize high-resolution images at interactive rates while preserving 3D consistency at high quality. StyleNeRF also enables control of camera poses and different levels of styles, which can generalize to unseen views. It also supports challenging tasks, including zoom-in and-out, style mixing, inversion, and semantic editing.",
                        "Citation Paper Authors": "Authors:Jiatao Gu, Lingjie Liu, Peng Wang, Christian Theobalt"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2203.02151v1": {
            "Paper Title": "Neural mechanisms underlying the temporal organization of naturalistic\n  animal behavior",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.12692v1": {
            "Paper Title": "Reconstruction of Perceived Images from fMRI Patterns and Semantic Brain\n  Exploration using Instance-Conditioned GANs",
            "Sentences": [
                {
                    "Sentence ID": 13,
                    "Sentence": ", etc.) are obtained via a two-stage\noptimization of the IC-GAN \u201cnoise\u201d and \u201cdense\u201d latent vectors\n(inspired by the method of Pividori et al. ",
                    "Citation Text": "M. Pividori, G. L. Grinblat, and L. C. Uzal, \u201cExploiting gan internal ca-\npacity for high-quality reconstruction of natural images,\u201d arXiv preprint\narXiv:1911.05630 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.05630",
                        "Citation Paper Title": "Title:Exploiting GAN Internal Capacity for High-Quality Reconstruction of Natural Images",
                        "Citation Paper Abstract": "Abstract:Generative Adversarial Networks (GAN) have demonstrated impressive results in modeling the distribution of natural images, learning latent representations that capture semantic variations in an unsupervised basis. Beyond the generation of novel samples, it is of special interest to exploit the ability of the GAN generator to model the natural image manifold and hence generate credible changes when manipulating images. However, this line of work is conditioned by the quality of the reconstruction. Until now, only inversion to the latent space has been considered, we propose to exploit the representation in intermediate layers of the generator, and we show that this leads to increased capacity. In particular, we observe that the representation after the first dense layer, present in all state-of-the-art GAN models, is expressive enough to represent natural images with high visual fidelity. It is possible to interpolate around these images obtaining a sequence of new plausible synthetic images that cannot be generated from the latent space. Finally, as an example of potential applications that arise from this inversion mechanism, we show preliminary results in exploiting the learned representation in the attention map of the generator to obtain an unsupervised segmentation of natural images.",
                        "Citation Paper Authors": "Authors:Marcos Pividori, Guillermo L. Grinblat, Lucas C. Uzal"
                    }
                },
                {
                    "Sentence ID": 21,
                    "Sentence": ". IC-GAN can be considered as a generic\nframework rather than a single model, because it can be\napplied to different GAN backbones, e.g. StyleGAN ",
                    "Citation Text": "T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila,\n\u201cAnalyzing and improving the image quality of stylegan,\u201d in Proceedings\nof the IEEE/CVF conference on computer vision and pattern recognition ,\n2020, pp. 8110\u20138119.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.04958",
                        "Citation Paper Title": "Title:Analyzing and Improving the Image Quality of StyleGAN",
                        "Citation Paper Abstract": "Abstract:The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.",
                        "Citation Paper Authors": "Authors:Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.10849v1": {
            "Paper Title": "Brain representation of perceptual stimuli at different levels of\n  awareness",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.10376v1": {
            "Paper Title": "Same Cause; Different Effects in the Brain",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.09689v1": {
            "Paper Title": "Is there an aesthetic component of language?",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.06159v2": {
            "Paper Title": "Robust alignment of cross-session recordings of neural population\n  activity by behaviour via unsupervised domain adaptation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.02001v2": {
            "Paper Title": "Introducing Block-Toeplitz Covariance Matrices to Remaster Linear\n  Discriminant Analysis for Event-related Potential Brain-computer Interfaces",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.08209v1": {
            "Paper Title": "An Extension Of Combinatorial Contextuality For Cognitive Protocols",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.07132v1": {
            "Paper Title": "Memory via Temporal Delays in weightless Spiking Neural Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.05808v1": {
            "Paper Title": "Investigating Power laws in Deep Representation Learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.08103v1": {
            "Paper Title": "Paraphrasing Magritte's Observation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.12716v1": {
            "Paper Title": "Reality: Physics, the Physical Universe Language and Mathematics,\n  Physics Formalisms in Human Brain Machine Language",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.02649v1": {
            "Paper Title": "The Implicit Bias of Gradient Descent on Generalized Gated Linear\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.02228v1": {
            "Paper Title": "Uncertainty in fMRI Functional Networks of Autism Brain Imaging Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.02090v1": {
            "Paper Title": "Relating cognition to both brain structure and function: A systematic\n  review of methods",
            "Sentences": []
        },
        "http://arxiv.org/abs/2202.01933v1": {
            "Paper Title": "Identifying stimulus-driven neural activity patterns in multi-patient\n  intracranial recordings",
            "Sentences": [
                {
                    "Sentence ID": 156,
                    "Sentence": "Westerhuis, J.A., Kourti, T., MacGregor, J.F.: Analysis of multiblock and\nhierarchical PCA and PLS models. Journal of Chemometrics 12, 301\u2013321\n(1998) ",
                    "Citation Text": "Wieting, J., Kiela, D.: No training required: exploring random encoders\nfor sentence classi\ufb01cation. arXiv 1901.10444 (2019)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1901.10444",
                        "Citation Paper Title": "Title:No Training Required: Exploring Random Encoders for Sentence Classification",
                        "Citation Paper Abstract": "Abstract:We explore various methods for computing sentence representations from pre-trained word embeddings without any training, i.e., using nothing but random parameterizations. Our aim is to put sentence embeddings on more solid footing by 1) looking at how much modern sentence embeddings gain over random methods---as it turns out, surprisingly little; and by 2) providing the field with more appropriate baselines going forward---which are, as it turns out, quite strong. We also make important observations about proper experimental protocol for sentence classification evaluation, together with recommendations for future research.",
                        "Citation Paper Authors": "Authors:John Wieting, Douwe Kiela"
                    }
                },
                {
                    "Sentence ID": 116,
                    "Sentence": "Pennington, J., Socher, R., Manning, C.D.: GloVe: global vectors for word\nrepresentation. In: Proceedings of the Conference on Empirical Methods\nin Natural Language Processing (2014)Identifying stimulus-driven neural activity 37 ",
                    "Citation Text": "Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee,\nK., Zettlemoyer, L.: Deep contextualized word representations. arXiv\n1802.05365 (2018)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1802.05365",
                        "Citation Paper Title": "Title:Deep contextualized word representations",
                        "Citation Paper Abstract": "Abstract:We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
                        "Citation Paper Authors": "Authors:Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer"
                    }
                },
                {
                    "Sentence ID": 113,
                    "Sentence": "Pasley, B.N., David, S.V ., Mesgarani, N., Flinker, A., Shamma, S.A., Crone,\nN.E., Knight, R.T., Chang, E.F.: Reconstructing speech from human audi-\ntory cortex. PLoS Biology 10(1), e1001251 (2012) ",
                    "Citation Text": "Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,\nT., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., K \u00a8opf, A., Yang, E.,\nDeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,\nBai, J., Chintala, S.: PyTorch: an imperative style, high-performance deep\nlearning library. Advances in Neural Information Processing Systems pp.\n8026\u20138037 (2019)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1912.01703",
                        "Citation Paper Title": "Title:PyTorch: An Imperative Style, High-Performance Deep Learning Library",
                        "Citation Paper Abstract": "Abstract:Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.\nIn this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.\nWe demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.",
                        "Citation Paper Authors": "Authors:Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala"
                    }
                },
                {
                    "Sentence ID": 99,
                    "Sentence": "Manning, J.R., Zhu, X., Willke, T.L., Ranganath, R., Stachenfeld, K., Has-\nson, U., Blei, D.M., Norman, K.A.: A probabilistic approach to discover-\ning dynamic full-brain functional connectivity patterns. NeuroImage 180,\n243\u2013252 (2018) ",
                    "Citation Text": "Mikolov, T., Chen, K., Corrado, G., Dean, J.: E \u000ecient estimation of word\nrepresentations in vector space. arXiv 1301.3781 (2013)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1301.3781",
                        "Citation Paper Title": "Title:Efficient Estimation of Word Representations in Vector Space",
                        "Citation Paper Abstract": "Abstract:We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.",
                        "Citation Paper Authors": "Authors:Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean"
                    }
                },
                {
                    "Sentence ID": 81,
                    "Sentence": "Ki, J.J., Kelly, S.P ., Parra, L.C.: Attention stronly modulates reliability of\nneural responses to naturalistic narrative stimuli. The Journal of Neuro-\nscience 36(10), 3092\u20133101 (2016) ",
                    "Citation Text": "Kiros, R., Zhu, Y., Salakhutdinov, R., Zemel, R., Torralba, A., Urtasun, R.,\nFidler, S.: Skip-thought vectors. Advances in Neural Information Process-\ning Systems pp. 3294\u20133302 (2015)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1506.06726",
                        "Citation Paper Title": "Title:Skip-Thought Vectors",
                        "Citation Paper Abstract": "Abstract:We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available.",
                        "Citation Paper Authors": "Authors:Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Urtasun, Sanja Fidler"
                    }
                },
                {
                    "Sentence ID": 62,
                    "Sentence": "to a common space. In\nPanels A and B, the high-dimensional stimulus and neural trajectories have been pro-\njected onto three dimensions to facilitate visualization ",
                    "Citation Text": "Heusser, A.C., Ziman, K., Owen, L.L.W., Manning, J.R.: HyperTools: a\nPython toolbox for gaining geometric insights into high-dimensional data.\nJournal of Machine Learning Research 18(152), 1\u20136 (2018)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1701.08290",
                        "Citation Paper Title": "Title:HyperTools: A Python toolbox for visualizing and manipulating high-dimensional data",
                        "Citation Paper Abstract": "Abstract:Data visualizations can reveal trends and patterns that are not otherwise obvious from the raw data or summary statistics. While visualizing low-dimensional data is relatively straightforward (for example, plotting the change in a variable over time as (x,y) coordinates on a graph), it is not always obvious how to visualize high-dimensional datasets in a similarly intuitive way. Here we present HypeTools, a Python toolbox for visualizing and manipulating large, high-dimensional datasets. Our primary approach is to use dimensionality reduction techniques (Pearson, 1901; Tipping & Bishop, 1999) to embed high-dimensional datasets in a lower-dimensional space, and plot the data using a simple (yet powerful) API with many options for data manipulation [e.g. hyperalignment (Haxby et al., 2011), clustering, normalizing, etc.] and plot styling. The toolbox is designed around the notion of data trajectories and point clouds. Just as the position of an object moving through space can be visualized as a 3D trajectory, HyperTools uses dimensionality reduction algorithms to create similar 2D and 3D trajectories for time series of high-dimensional observations. The trajectories may be plotted as interactive static plots or visualized as animations. These same dimensionality reduction and alignment algorithms can also reveal structure in static datasets (e.g. collections of observations or attributes). We present several examples showcasing how using our toolbox to explore data through trajectories and low-dimensional embeddings can reveal deep insights into datasets across a wide variety of domains.",
                        "Citation Paper Authors": "Authors:Andrew C. Heusser, Kirsten Ziman, Lucy L. W. Owen, Jeremy R. Manning"
                    }
                },
                {
                    "Sentence ID": 38,
                    "Sentence": "Deerwester, S., Dumais, S.T., Furnas, G.W., Landauer, T.K., Harshman, R.:\nIndexing by latent semantic analysis. Journal of the American Society for\nInformation Science 41(6), 391\u2013407 (1990) ",
                    "Citation Text": "Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: pre-training of deep\nbidirectional transformers for language understanding. arXiv 1810.04805\n(2018)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1810.04805",
                        "Citation Paper Title": "Title:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "Citation Paper Abstract": "Abstract:We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
                        "Citation Paper Authors": "Authors:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
                    }
                },
                {
                    "Sentence ID": 34,
                    "Sentence": "Comon, P ., Jutten, C., Herault, J.: Blind separation of sources, part II:\nproblems statement. Signal Processing 24(1), 11\u201320 (1991) ",
                    "Citation Text": "Conneau, A., Kiela, D., Schwenk, H., Barrault, L., Bordes, A.: Supervised\nlearning of universal sentence representations from natural language in-\nference data. arXiv 1705.02364 (2018)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1705.02364",
                        "Citation Paper Title": "Title:Supervised Learning of Universal Sentence Representations from Natural Language Inference Data",
                        "Citation Paper Abstract": "Abstract:Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.",
                        "Citation Paper Authors": "Authors:Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, Antoine Bordes"
                    }
                },
                {
                    "Sentence ID": 24,
                    "Sentence": "Carroll, J.D., Chang, J.: Analysis of individual di \u000berences in multidimen-\nsional scaling via an n-way generalization of \u201ceckart-young\u201d decomposi-\ntion. Psychometrika 35(3), 283\u2013319 (1970) ",
                    "Citation Text": "Cer, D., Yang, Y., Kong, S.Y., Hua, N., Limtiaco, N., John, R.S., Constant, N.,\nGuajardo-Cespedes, M., Yuan, S., Tar, C., Sung, Y.H., Strope, B., Kurzweil,\nR.: Universal sentence encoder. arXiv 1803.11175 (2018)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1803.11175",
                        "Citation Paper Title": "Title:Universal Sentence Encoder",
                        "Citation Paper Abstract": "Abstract:We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on TF Hub.",
                        "Citation Paper Authors": "Authors:Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, Ray Kurzweil"
                    }
                },
                {
                    "Sentence ID": 20,
                    "Sentence": "Brown, P .F., deSouza, P .V ., Mercer, R.L.: Class-based n-gram models of\nnatural language. Computational Linguistics 18(4), 467\u2013480 (1992) ",
                    "Citation Text": "Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P .,\nNeelakantan, A., Shyam, P ., Sastry, G., Askell, A., Agarwal, S., Herbert-\nVoss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M.,\nWu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S.,\nChess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever,\nI., Amodei, D.: Language models are few-shot learners. arXiv 2005.14165\n(2020)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.14165",
                        "Citation Paper Title": "Title:Language Models are Few-Shot Learners",
                        "Citation Paper Abstract": "Abstract:Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                        "Citation Paper Authors": "Authors:Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei"
                    }
                },
                {
                    "Sentence ID": 15,
                    "Sentence": "Bengio, Y., Ducharme, R., Vincent, P .: A neural probabilistic language\nmodel. Journal of Machine Learning Research 3, 1137\u20131155 (2003) ",
                    "Citation Text": "Betzel, R.F., Medaglia, J.D., Kahn, A.E., So \u000ber, J., Schonhaut, D.R., Bas-\nsett, D.S.: Inter-regional ECoG correlations predicted by communication\ndynamics, geometry, and correlated gene expression. arXiv 1706.06088\n(2017)",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1706.06088",
                        "Citation Paper Title": "Title:Inter-regional ECoG correlations predicted by communication dynamics, geometry, and correlated gene expression",
                        "Citation Paper Abstract": "Abstract:Electrocorticography (ECoG) provides direct measurements of synchronized postsynaptic potentials at the exposed cortical surface. Patterns of signal covariance across ECoG sensors have been associated with diverse cognitive functions and remain a critical marker of seizure onset, progression, and termination. Yet, a systems level understanding of these patterns (or networks) has remained elusive, in part due to variable electrode placement and sparse cortical coverage. Here, we address these challenges by constructing inter-regional ECoG networks from multi-subject recordings, demonstrate similarities between these networks and those constructed from blood-oxygen-level-dependent signal in functional magnetic resonance imaging, and predict network topology from anatomical connectivity, interregional distance, and correlated gene expression patterns. Our models accurately predict out-of-sample ECoG networks and perform well even when fit to data from individual subjects, suggesting shared organizing principles across persons. In addition, we identify a set of genes whose brain-wide co-expression is highly correlated with ECoG network organization. Using gene ontology analysis, we show that these same genes are enriched for membrane and ion channel maintenance and function, suggesting a molecular underpinning of ECoG connectivity. Our findings provide fundamental understanding of the factors that influence interregional ECoG networks, and open the possibility for predictive modeling of surgical outcomes in disease.",
                        "Citation Paper Authors": "Authors:Richard F. Betzel, John D. Medaglia, Ari E. Kahn, Jonathan Soffer, Daniel R. Schonhaut, Danielle S. Bassett"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2202.00838v2": {
            "Paper Title": "Finding Biological Plausibility for Adversarially Robust Features via\n  Metameric Tasks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.11063v2": {
            "Paper Title": "The BrainScaleS-2 accelerated neuromorphic system with hybrid plasticity",
            "Sentences": [
                {
                    "Sentence ID": 81,
                    "Sentence": ". Similarly, the evolving-\nto-learn framework by Jordan et al. ",
                    "Citation Text": "Jakob Jordan, Maximilian Schmidt, Walter Senn, and\nMihai A Petrovici. Evolving to learn: discovering in-\nterpretable plasticity rules for spiking networks. arXiv\npreprint arXiv:2005.14149 , 2020.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2005.14149",
                        "Citation Paper Title": "Title:Evolving to learn: discovering interpretable plasticity rules for spiking networks",
                        "Citation Paper Abstract": "Abstract:Continuous adaptation allows survival in an ever-changing world. Adjustments in the synaptic coupling strength between neurons are essential for this capability, setting us apart from simpler, hard-wired organisms. How these changes can be mathematically described at the phenomenological level, as so called \"plasticity rules\", is essential both for understanding biological information processing and for developing cognitively performant artificial systems. We suggest an automated approach for discovering biophysically plausible plasticity rules based on the definition of task families, associated performance measures and biophysical constraints. By evolving compact symbolic expressions we ensure the discovered plasticity rules are amenable to intuitive understanding, fundamental for successful communication and human-guided generalization. We successfully apply our approach to typical learning scenarios and discover previously unknown mechanisms for learning efficiently from rewards, recover efficient gradient-descent methods for learning from target signals, and uncover various functionally equivalent STDP-like rules with tuned homeostatic mechanisms.",
                        "Citation Paper Authors": "Authors:Jakob Jordan, Maximilian Schmidt, Walter Senn, Mihai A. Petrovici"
                    }
                },
                {
                    "Sentence ID": 80,
                    "Sentence": "Robert Klassert, Andreas Baumbach, Mihai A. Petrovici,\nand Martin G\u007f artner. Variational learning of quantum\nground states on spiking neuromorphic hardware. 2021. ",
                    "Citation Text": "Stefanie Czischek, Andreas Baumbach, Sebastian Bil-\nlaudelle, Benjamin Cramer, Lukas Kades, Jan M\nPawlowski, Markus Oberthaler, Johannes Schemmel, Mi-\nhai A Petrovici, Thomas Gasenzer, et al. Spiking neuro-\nmorphic chip learns entangled quantum states. SciPost\nPhysics , 12(1):039, 2022.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2008.01039",
                        "Citation Paper Title": "Title:Spiking neuromorphic chip learns entangled quantum states",
                        "Citation Paper Abstract": "Abstract:The approximation of quantum states with artificial neural networks has gained a lot of attention during the last years. Meanwhile, analog neuromorphic chips, inspired by structural and dynamical properties of the biological brain, show a high energy efficiency in running artificial neural-network architectures for the profit of generative applications. This encourages employing such hardware systems as platforms for simulations of quantum systems. Here we report on the realization of a prototype using the latest spike-based BrainScaleS hardware allowing us to represent few-qubit maximally entangled quantum states with high fidelities. Bell correlations of pure and mixed two-qubit states are well captured by the analog hardware, demonstrating an important building block for simulating quantum systems with spiking neuromorphic chips.",
                        "Citation Paper Authors": "Authors:Stefanie Czischek, Andreas Baumbach, Sebastian Billaudelle, Benjamin Cramer, Lukas Kades, Jan M. Pawlowski, Markus K. Oberthaler, Johannes Schemmel, Mihai A. Petrovici, Thomas Gasenzer, Martin G\u00e4rttner"
                    }
                },
                {
                    "Sentence ID": 78,
                    "Sentence": "Mihai A. Petrovici, Johannes Bill, Ilja Bytschok, Jo-\nhannes Schemmel, and Karlheinz Meier. Stochastic in-\nference with spiking neurons in the high-conductance\nstate. Physical Review E , 94(4), October 2016. doi:\n10.1103/PhysRevE.94.042312. URL http://journals.\naps.org/pre/abstract/10.1103/PhysRevE.94.042312 . ",
                    "Citation Text": "Stefanie Czischek, Jan M Pawlowski, Thomas Gasenzer,\nand Martin G\u007f arttner. Sampling scheme for neuromor-\nphic simulation of entangled quantum systems. Physical\nReview B , 100(19):195120, 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1907.12844",
                        "Citation Paper Title": "Title:Sampling scheme for neuromorphic simulation of entangled quantum systems",
                        "Citation Paper Abstract": "Abstract:Due to the complexity of the space of quantum many-body states the computation of expectation values by statistical sampling is, in general, a hard task. Neural network representations of such quantum states which can be physically implemented by neuromorphic hardware could enable efficient sampling. A scheme is proposed which leverages this capability to speed up sampling from so-called neural quantum states encoded by a restricted Boltzmann machine. Due to the complex network parameters a direct hardware implementation is not feasible. We overcome this problem by considering a phase reweighting scheme for sampling expectation values of observables. Applying our method to a set of paradigmatic entangled quantum states we find that, in general, the phase-reweighted sampling is subject to a form of sign problem, which renders the sampling computationally costly. The use of neuromorphic chips could allow reducing computation times and thereby extend the range of tractable system sizes.",
                        "Citation Paper Authors": "Authors:Stefanie Czischek, Jan M. Pawlowski, Thomas Gasenzer, Martin G\u00e4rttner"
                    }
                },
                {
                    "Sentence ID": 74,
                    "Sentence": ".\nConsiderable progress on learning methods for neu-\nromorphic hardware has been made as well. Here we\nonly highlight those most closely related to the presented\nmethods. Esser et al. ",
                    "Citation Text": "Steven K. Esser, Paul A. Merolla, John V. Arthur, An-\ndrew S. Cassidy, Rathinakumar Appuswamy, Alexander\nAndreopoulos, David J. Berg, Je\u000brey L. McKinstry, Tim-\nothy Melano, Davis R. Barch, Carmelo di Nolfo, Pallab\nDatta, Arnon Amir, Brian Taba, Myron D. Flickner, and\nDharmendra S. Modha. Convolutional networks for fast,\nenergy-e\u000ecient neuromorphic computing. Proceedings of\nthe National Academy of Sciences , 113(41):11441{11446,\n2016. ISSN 0027-8424. doi:10.1073/pnas.1604850113.\nURL https://www.pnas.org/content/113/41/11441 .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.08270",
                        "Citation Paper Title": "Title:Convolutional Networks for Fast, Energy-Efficient Neuromorphic Computing",
                        "Citation Paper Abstract": "Abstract:Deep networks are now able to achieve human-level performance on a broad spectrum of recognition tasks. Independently, neuromorphic computing has now demonstrated unprecedented energy-efficiency through a new chip architecture based on spiking neurons, low precision synapses, and a scalable communication network. Here, we demonstrate that neuromorphic computing, despite its novel architectural primitives, can implement deep convolution networks that i) approach state-of-the-art classification accuracy across 8 standard datasets, encompassing vision and speech, ii) perform inference while preserving the hardware's underlying energy-efficiency and high throughput, running on the aforementioned datasets at between 1200 and 2600 frames per second and using between 25 and 275 mW (effectively > 6000 frames / sec / W) and iii) can be specified and trained using backpropagation with the same ease-of-use as contemporary deep learning. For the first time, the algorithmic power of deep learning can be merged with the efficiency of neuromorphic processors, bringing the promise of embedded, intelligent, brain-inspired computing one step closer.",
                        "Citation Paper Authors": "Authors:Steven K. Esser, Paul A. Merolla, John V. Arthur, Andrew S. Cassidy, Rathinakumar Appuswamy, Alexander Andreopoulos, David J. Berg, Jeffrey L. McKinstry, Timothy Melano, Davis R. Barch, Carmelo di Nolfo, Pallab Datta, Arnon Amir, Brian Taba, Myron D. Flickner, Dharmendra S. Modha"
                    }
                },
                {
                    "Sentence ID": 63,
                    "Sentence": "Jing Pei, Lei Deng, Sen Song, Mingguo Zhao, Youhui\nZhang, Shuang Wu, Guanrui Wang, Zhe Zou, Zhenzhi\nWu, Wei He, et al. Towards arti\fcial general intelligence\nwith hybrid tianjic chip architecture. Nature , 572(7767):\n106{111, 2019. ",
                    "Citation Text": "Christian Mayr, Sebastian Hoeppner, and Steve Furber.\nSpinnaker 2: A 10 million core processor system for\nbrain simulation and machine learning. arXiv preprint\narXiv:1911.02385 , 2019.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1911.02385",
                        "Citation Paper Title": "Title:SpiNNaker 2: A 10 Million Core Processor System for Brain Simulation and Machine Learning",
                        "Citation Paper Abstract": "Abstract:SpiNNaker is an ARM-based processor platform optimized for the simulation of spiking neural networks. This brief describes the roadmap in going from the current SPINNaker1 system, a 1 Million core machine in 130nm CMOS, to SpiNNaker2, a 10 Million core machine in 22nm FDSOI. Apart from pure scaling, we will take advantage of specific technology features, such as runtime adaptive body biasing, to deliver cutting-edge power consumption. Power management of the cores allows a wide range of workload adaptivity, i.e. processor power scales with the complexity and activity of the spiking network. Additional numerical accelerators will enhance the utility of SpiNNaker2 for simulation of spiking neural networks as well as for executing conventional deep neural networks. These measures should increase the simulation capacity of the machine by a factor $>$50. The interplay between the two domains, i.e. spiking and rate based, will provide an interesting field for algorithm exploration on SpiNNaker2. Apart from the platforms' traditional usage as a neuroscience exploration tool, the extended functionality opens up new application areas such as automotive AI, tactile internet, industry 4.0 and biomedical processing.",
                        "Citation Paper Authors": "Authors:Christian Mayr, Sebastian Hoeppner, Steve Furber"
                    }
                },
                {
                    "Sentence ID": 52,
                    "Sentence": ". Concurrent work also\nintroduced this technique to the wider machine learning\ncommunity ",
                    "Citation Text": "Ricky T. Q. Chen, Brandon Amos, and Maximilian Nickel.\nLearning neural event functions for ordinary di\u000berential\nequations. In International Conference on Learning Repre-\nsentations , 2021. URL https://openreview.net/forum?\nid=kW_zpEmMLdP .",
                    "Citation": {
                        "Citation Paper ID": "arXiv:2011.03902",
                        "Citation Paper Title": "Title:Learning Neural Event Functions for Ordinary Differential Equations",
                        "Citation Paper Abstract": "Abstract:The existing Neural ODE formulation relies on an explicit knowledge of the termination time. We extend Neural ODEs to implicitly defined termination criteria modeled by neural event functions, which can be chained together and differentiated through. Neural Event ODEs are capable of modeling discrete and instantaneous changes in a continuous-time system, without prior knowledge of when these changes should occur or how many such changes should exist. We test our approach in modeling hybrid discrete- and continuous- systems such as switching dynamical systems and collision in multi-body systems, and we propose simulation-based training of point processes with applications in discrete control.",
                        "Citation Paper Authors": "Authors:Ricky T. Q. Chen, Brandon Amos, Maximilian Nickel"
                    }
                },
                {
                    "Sentence ID": 12,
                    "Sentence": ".\nSpike timing-dependent plasticity (STDP) and related\ncorrelation-based plasticity rules are supported through\nanalog sensor circuits within each synapse ",
                    "Citation Text": "S. Friedmann, J. Schemmel, A. Gr\u007f ubl, A. Hartel, M. Hock,\nand K. Meier. Demonstrating hybrid learning in a \rex-\nible neuromorphic hardware system. In IEEE Trans-\nactions on Biomedical Circuits and Systems Friedmann\net al.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1604.05080",
                        "Citation Paper Title": "Title:Demonstrating Hybrid Learning in a Flexible Neuromorphic Hardware System",
                        "Citation Paper Abstract": "Abstract:We present results from a new approach to learning and plasticity in neuromorphic hardware systems: to enable flexibility in implementable learning mechanisms while keeping high efficiency associated with neuromorphic implementations, we combine a general-purpose processor with full-custom analog elements.\nThis processor is operating in parallel with a fully parallel neuromorphic system consisting of an array of synapses connected to analog, continuous time neuron circuits. Novel analog correlation sensor circuits process spike events for each synapse in parallel and in real-time.\nThe processor uses this pre-processing to compute new weights possibly using additional information following its program.\nTherefore, learning rules can be defined in software giving a large degree of flexibility.\nSynapses realize correlation detection geared towards Spike-Timing Dependent Plasticity (STDP) as central computational primitive in the analog domain.\nOperating at a speed-up factor of 1000 compared to biological time-scale, we measure time-constants from tens to hundreds of micro-seconds.\nWe analyze variability across multiple chips and demonstrate learning using a multiplicative STDP rule.\nWe conclude, that the presented approach will enable flexible and efficient learning as a platform for neuroscientific research and technological applications.",
                        "Citation Paper Authors": "Authors:Simon Friedmann, Johannes Schemmel, Andreas Gruebl, Andreas Hartel, Matthias Hock, Karlheinz Meier"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2201.11915v1": {
            "Paper Title": "The fine line between dead neurons and sparsity in binarized spiking\n  neural networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.11717v1": {
            "Paper Title": "Burst-dependent plasticity and dendritic amplification support\n  target-based learning and hierarchical imitation learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.07867v1": {
            "Paper Title": "The spatial scale dimension of speech processing in the human brain",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.06795v1": {
            "Paper Title": "Retinal processing: insights from mathematical modelling",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.03902v1": {
            "Paper Title": "Where Is My Mind (looking at)? Predicting Visual Attention from Brain\n  Activity",
            "Sentences": [
                {
                    "Sentence ID": 20,
                    "Sentence": ".\nGiven the visual saliency images, a V AE has been trained to\nrepresent them in a lower subspace. The considered network\narchitecture is based on the ResNet proposed by He et al. ",
                    "Citation Text": "K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep Residual Learning for Image\nRecognition,\u201d arXiv:1512.03385 [cs] , Dec. 2015. arXiv: 1512.03385.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1512.03385",
                        "Citation Paper Title": "Title:Deep Residual Learning for Image Recognition",
                        "Citation Paper Abstract": "Abstract:Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
                        "Citation Paper Authors": "Authors:Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/2201.03537v1": {
            "Paper Title": "Data Processing of Functional Optical Microscopy for Neuroscience",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.04065v1": {
            "Paper Title": "ExBrainable: An Open-Source GUI for CNN-based EEG Decoding and Model\n  Interpretation",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.00716v2": {
            "Paper Title": "Modeling Associative Reasoning Processes",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.01796v1": {
            "Paper Title": "Formal models of memory based on temporally-varying representations",
            "Sentences": []
        },
        "http://arxiv.org/abs/2201.00817v1": {
            "Paper Title": "An approach to interfacing the brain with quantum computers: practical\n  steps and caveats",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.03263v4": {
            "Paper Title": "Topological limits to parallel processing capability of network\n  architectures",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.00909v4": {
            "Paper Title": "Machine learning for neural decoding",
            "Sentences": []
        },
        "http://arxiv.org/abs/1707.05649v2": {
            "Paper Title": "Sequence learning in Associative Neuronal-Astrocytic Network",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.00779v2": {
            "Paper Title": "Control of Functional Connectivity in Cerebral Cortex by Basal Ganglia\n  Mediated Synchronization",
            "Sentences": []
        },
        "http://arxiv.org/abs/1704.05344v5": {
            "Paper Title": "Linear response for spiking neuronal networks with unbounded memory",
            "Sentences": []
        },
        "http://arxiv.org/abs/1702.07368v3": {
            "Paper Title": "Reconfiguring motor circuits for a joint manual and BCI task",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.04020v4": {
            "Paper Title": "Disentangling causal webs in the brain using functional Magnetic\n  Resonance Imaging: A review of current approaches",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.08856v3": {
            "Paper Title": "Critical Learning Periods in Deep Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.06969v2": {
            "Paper Title": "Comparing deep neural networks against humans: object recognition when\n  the signal gets weaker",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.02563v2": {
            "Paper Title": "Expectation-induced modulation of metastable activity underlies faster\n  coding of sensory stimuli",
            "Sentences": []
        },
        "http://arxiv.org/abs/1709.03000v3": {
            "Paper Title": "Network constraints on learnability of probabilistic motor sequences",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.02603v2": {
            "Paper Title": "Learning Feedforward and Recurrent Deterministic Spiking Neuron Network\n  Feedback Controllers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.07205v5": {
            "Paper Title": "Decoding of neural data using cohomological feature extraction",
            "Sentences": []
        },
        "http://arxiv.org/abs/1703.04182v7": {
            "Paper Title": "Optimal control of eye-movements during visual search",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.00556v3": {
            "Paper Title": "WAND: A 128-channel, closed-loop, wireless artifact-free neuromodulation\n  device",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.00133v2": {
            "Paper Title": "Cortical Circuits from Scratch: A Metaplastic Architecture for the\n  Emergence of Lognormal Firing Rates and Realistic Topology",
            "Sentences": []
        },
        "http://arxiv.org/abs/1701.04675v2": {
            "Paper Title": "VOCSMAT: a connectionist-inspired treatment proposal for relational\n  traumas",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.08961v3": {
            "Paper Title": "Fast and Scalable Distributed Deep Convolutional Autoencoder for fMRI\n  Big Data Analytics",
            "Sentences": [
                {
                    "Sentence ID": 33,
                    "Sentence": "used \nconvolutional neural networks to cla ssify fMRI -derived \nfunctional brain networks, and Wen et al. ",
                    "Citation Text": "Wen, Haiguang, et al. \"Neural Encoding and Decoding with Deep \nLearning for Dynamic Natural Vi sion.\"  arXiv preprint arXiv:1608.03425  (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1608.03425",
                        "Citation Paper Title": "Title:Neural Encoding and Decoding with Deep Learning for Dynamic Natural Vision",
                        "Citation Paper Abstract": "Abstract:Convolutional neural network (CNN) driven by image recognition has been shown to be able to explain cortical responses to static pictures at ventral-stream areas. Here, we further showed that such CNN could reliably predict and decode functional magnetic resonance imaging data from humans watching natural movies, despite its lack of any mechanism to account for temporal dynamics or feedback processing. Using separate data, encoding and decoding models were developed and evaluated for describing the bi-directional relationships be-tween the CNN and the brain. Through the encoding models, the CNN-predicted areas covered not only the ventral stream, but also the dorsal stream, albe-it to a lesser degree; single-voxel response was visualized as the specific pixel pattern that drove the response, revealing the distinct representation of individual cortical location; cortical activation was synthesized from natural images with high-throughput to map category representation, con-trast, and selectivity. Through the decoding models, fMRI signals were directly decoded to estimate the feature representations in both visual and semantic spaces, for direct visual reconstruction and seman-tic categorization, respectively. These results cor-roborate, generalize, and extend previous findings, and highlight the value of using deep learning, as an all-in-one model of the visual cortex, to understand and decode natural vision.",
                        "Citation Paper Authors": "Authors:Haiguang Wen, Junxing Shi, Yizhen Zhang, Kun-Han Lu, Jiayue Cao, Zhongming Liu"
                    }
                },
                {
                    "Sentence ID": 46,
                    "Sentence": "to efficiently  learn meaningful \nhierarchical abstraction of massive size of fMRI data.  \n2.1 TensorFlow  \nTensorFlow ",
                    "Citation Text": "Abadi, Mart\u00ed n, et al. \"Tensorflow: Large -scale machine learning on \nheterogeneous distributed systems.\"  arXiv preprint arXiv:1603.04467  (2016).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.04467",
                        "Citation Paper Title": "Title:TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems",
                        "Citation Paper Abstract": "Abstract:TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at this http URL.",
                        "Citation Paper Authors": "Authors:Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, Xiaoqiang Zheng"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1709.05939v2": {
            "Paper Title": "AJILE Movement Prediction: Multimodal Deep Learning for Natural Human\n  Neural Recordings and Video",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.07508v3": {
            "Paper Title": "Which spike train distance is most suitable for distinguishing rate and\n  temporal coding?",
            "Sentences": []
        },
        "http://arxiv.org/abs/1709.05022v3": {
            "Paper Title": "Network Controllability in the IFG Relates to Controlled Language\n  Variability and Susceptibility to TMS",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.02653v2": {
            "Paper Title": "Neural system identification for large populations separating \"what\" and\n  \"where\"",
            "Sentences": []
        },
        "http://arxiv.org/abs/1702.08606v3": {
            "Paper Title": "The Active Atlas: Combining 3D Anatomical Models with Texture Detectors",
            "Sentences": []
        },
        "http://arxiv.org/abs/1702.00837v3": {
            "Paper Title": "Eye-Movement behavior identification for AD diagnosis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.09139v3": {
            "Paper Title": "Deep Transfer Learning for Error Decoding from Non-Invasive EEG",
            "Sentences": []
        },
        "http://arxiv.org/abs/1704.04238v4": {
            "Paper Title": "A dynamic connectome supports the emergence of stable computational\n  function of neural circuits through reward-based learning",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.02448v2": {
            "Paper Title": "Cortical microcircuits as gated-recurrent neural networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.00062v1": {
            "Paper Title": "Dendritic error backpropagation in deep cortical microcircuits",
            "Sentences": []
        },
        "http://arxiv.org/abs/1712.09776v1": {
            "Paper Title": "Deep Architectures for Automated Seizure Detection in Scalp EEGs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1712.09771v1": {
            "Paper Title": "Automatic Analysis of EEGs Using Big Data and Hybrid Deep Learning\n  Architectures",
            "Sentences": []
        },
        "http://arxiv.org/abs/1712.09479v1": {
            "Paper Title": "Analysis of BOLD fMRI Signal Preprocessing Pipeline on Different\n  Datasets while Reducing False Positive Rates",
            "Sentences": []
        },
        "http://arxiv.org/abs/1712.10280v1": {
            "Paper Title": "First Draft on the xInf Model for Universal Physical Computation and\n  Reverse Engineering of Natural Intelligence",
            "Sentences": []
        },
        "http://arxiv.org/abs/1801.06452v1": {
            "Paper Title": "Collision Selective Visual Neural Network Inspired by LGMD2 Neurons in\n  Juvenile Locusts",
            "Sentences": []
        },
        "http://arxiv.org/abs/1712.07443v1": {
            "Paper Title": "Tensor-driven extraction of developmental features from varying\n  paediatric EEG datasets",
            "Sentences": []
        },
        "http://arxiv.org/abs/1712.06353v1": {
            "Paper Title": "Modules for Automated Validation and Comparison of Models of\n  Neurophysiological and Neurocognitive Biomarkers of Psychiatric Disorders:\n  ASSRUnit - A Case Study",
            "Sentences": [
                {
                    "Sentence ID": 47,
                    "Sentence": "Timm Rosburg, Nash N Boutros, and Judith M Ford. Reduced auditory\nevoked potential component n100 in schizophrenia|a critical review. Psy-\nchiatry research , 161(3):259{274, 2008. ",
                    "Citation Text": "Gopal P Sarma, Travis W Jacobs, Mark D Watts, S Vahid Ghayoomie,\nStephen D Larson, and Richard C Gerkin. Unit testing, model validation,\nand biological simulation. F1000Research , 5, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1508.04635",
                        "Citation Paper Title": "Title:Unit Testing, Model Validation, and Biological Simulation",
                        "Citation Paper Abstract": "Abstract:The growth of the software industry has gone hand in hand with the development of tools and cultural practices for ensuring the reliability of complex pieces of software. These tools and practices are now acknowledged to be essential to the management of modern software. As computational models and methods have become increasingly common in the biological sciences, it is important to examine how these practices can accelerate biological software development and improve research quality. In this article, we give a focused case study of our experience with the practices of unit testing and test-driven development in OpenWorm, an open-science project aimed at modeling Caenorhabditis elegans. We identify and discuss the challenges of incorporating test-driven development into a heterogeneous, data-driven project, as well as the role of model validation tests, a category of tests unique to software which expresses scientific models.",
                        "Citation Paper Authors": "Authors:Gopal P. Sarma, Travis W. Jacobs, Mark D. Watts, Vahid Ghayoomi, Richard C. Gerkin, Stephen D. Larson"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1712.05784v1": {
            "Paper Title": "Measuring behavior across scales",
            "Sentences": []
        },
        "http://arxiv.org/abs/1712.05197v2": {
            "Paper Title": "Towards Deep Modeling of Music Semantics using EEG Regularizers",
            "Sentences": [
                {
                    "Sentence ID": 24,
                    "Sentence": "and has previously been applied to learn a correlated\nspace in music between audio and lyrics views in order to\nperform cross-modal retrieval ",
                    "Citation Text": "Y . Yu, S. Tang, and F. Raposo, \u201cDeep Cross-modal Correlation Learning\nfor Audio and Lyrics in Music Retrieval,\u201d CoRR , vol. arXiv:1711.08976,\n2017.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1711.08976",
                        "Citation Paper Title": "Title:Deep Cross-Modal Correlation Learning for Audio and Lyrics in Music Retrieval",
                        "Citation Paper Abstract": "Abstract:Little research focuses on cross-modal correlation learning where temporal structures of different data modalities such as audio and lyrics are taken into account. Stemming from the characteristic of temporal structures of music in nature, we are motivated to learn the deep sequential correlation between audio and lyrics. In this work, we propose a deep cross-modal correlation learning architecture involving two-branch deep neural networks for audio modality and text modality (lyrics). Different modality data are converted to the same canonical space where inter modal canonical correlation analysis is utilized as an objective function to calculate the similarity of temporal structures. This is the first study on understanding the correlation between language and music audio through deep architectures for learning the paired temporal correlation of audio and lyrics. Pre-trained Doc2vec model followed by fully-connected layers (fully-connected deep neural network) is used to represent lyrics. Two significant contributions are made in the audio branch, as follows: i) pre-trained CNN followed by fully-connected layers is investigated for representing music audio. ii) We further suggest an end-to-end architecture that simultaneously trains convolutional layers and fully-connected layers to better learn temporal structures of music audio. Particularly, our end-to-end deep architecture contains two properties: simultaneously implementing feature learning and cross-modal correlation learning, and learning joint representation by considering temporal structures. Experimental results, using audio to retrieve lyrics or using lyrics to retrieve audio, verify the effectiveness of the proposed deep correlation learning architectures in cross-modal music retrieval.",
                        "Citation Paper Authors": "Authors:Yi Yu, Suhua Tang, Francisco Raposo, Lei Chen"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1711.07425v2": {
            "Paper Title": "Modular Continual Learning in a Unified Visual Environment",
            "Sentences": []
        },
        "http://arxiv.org/abs/1712.02142v1": {
            "Paper Title": "Maps of Visual Importance",
            "Sentences": []
        },
        "http://arxiv.org/abs/1712.01277v1": {
            "Paper Title": "A coherence study on EEG and EMG signals",
            "Sentences": []
        },
        "http://arxiv.org/abs/1712.00190v1": {
            "Paper Title": "Modeling the Multiple Sclerosis Brain Disease Using Agents: What Works\n  and What Doesn't?",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.10991v2": {
            "Paper Title": "Early processing of consonance and dissonance in human auditory cortex",
            "Sentences": []
        },
        "http://arxiv.org/abs/1707.03321v2": {
            "Paper Title": "A deep learning architecture for temporal sleep stage classification\n  using multivariate and multimodal time series",
            "Sentences": [
                {
                    "Sentence ID": 11,
                    "Sentence": "trained on hand-crafted features and\ntwo convolutional networks trained on raw univariate time\nseries following the approach of ",
                    "Citation Text": "O. Tsinalis, P. M. Matthews, Y . Guo, and S. Zafeiriou, \u201cAutomatic Sleep\nStage Scoring with Single-Channel EEG Using Convolutional Neural\nNetworks,\u201d arXiv:1610.01683 , pp. 1\u201310, 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1610.01683",
                        "Citation Paper Title": "Title:Automatic Sleep Stage Scoring with Single-Channel EEG Using Convolutional Neural Networks",
                        "Citation Paper Abstract": "Abstract:We used convolutional neural networks (CNNs) for automatic sleep stage scoring based on single-channel electroencephalography (EEG) to learn task-specific filters for classification without using prior domain knowledge. We used an openly available dataset from 20 healthy young adults for evaluation and applied 20-fold cross-validation. We used class-balanced random sampling within the stochastic gradient descent (SGD) optimization of the CNN to avoid skewed performance in favor of the most represented sleep stages. We achieved high mean F1-score (81%, range 79-83%), mean accuracy across individual sleep stages (82%, range 80-84%) and overall accuracy (74%, range 71-76%) over all subjects. By analyzing and visualizing the filters that our CNN learns, we found that rules learned by the filters correspond to sleep scoring criteria in the American Academy of Sleep Medicine (AASM) manual that human experts follow. Our method's performance is balanced across classes and our results are comparable to state-of-the-art methods with hand-engineered features. We show that, without using prior domain knowledge, a CNN can automatically learn to distinguish among different normal sleep stages.",
                        "Citation Paper Authors": "Authors:Orestis Tsinalis, Paul M. Matthews, Yike Guo, Stefanos Zafeiriou"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1711.06967v4": {
            "Paper Title": "Neural correlates of flow using auditory evoked potential suppression",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.08533v1": {
            "Paper Title": "Universality of macroscopic neuronal dynamics in Caenorhabditis elegans",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.08309v1": {
            "Paper Title": "Subthreshold signal encoding in coupled FitzHugh-Nagumo neurons",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.08063v1": {
            "Paper Title": "Clonal analysis of newborn hippocampal dentate granule cell\n  proliferation and development in temporal lobe epilepsy",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.08032v1": {
            "Paper Title": "Efficient low-dimensional approximation of continuous attractor networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.07894v1": {
            "Paper Title": "Quantifying Performance of Bipedal Standing with Multi-channel EMG",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.03058v2": {
            "Paper Title": "Matrix-normal models for fMRI analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1703.07633v2": {
            "Paper Title": "Deep brain fluorescence imaging with minimally invasive ultra-thin\n  optical fibers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.02301v3": {
            "Paper Title": "Potential role of a ventral nerve cord central pattern generator in\n  forward and backward locomotion in Caenorhabditis elegans",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.01846v1": {
            "Paper Title": "Fast amortized inference of neural activity from calcium imaging data\n  with variational autoencoders",
            "Sentences": []
        },
        "http://arxiv.org/abs/1711.01487v1": {
            "Paper Title": "How well do reduced models capture the dynamics in models of interacting\n  neurons ?",
            "Sentences": []
        },
        "http://arxiv.org/abs/1704.02019v2": {
            "Paper Title": "Associative content-addressable networks with exponentially many robust\n  stable states",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.04405v2": {
            "Paper Title": "Practical Bayesian Optimization for Model Fitting with Bayesian Adaptive\n  Direct Search",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.04568v2": {
            "Paper Title": "SideEye: A Generative Neural Network Based Simulator of Human Peripheral\n  Vision",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.07659v1": {
            "Paper Title": "Point Neurons with Conductance-Based Synapses in the Neural Engineering\n  Framework",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.05183v2": {
            "Paper Title": "Inferring Mesoscale Models of Neural Computation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.05945v1": {
            "Paper Title": "Rats optimally accumulate and discount evidence in a dynamic environment",
            "Sentences": []
        },
        "http://arxiv.org/abs/1701.07138v5": {
            "Paper Title": "Learning Mid-Level Auditory Codes from Natural Sound Statistics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.04947v1": {
            "Paper Title": "A combinatorial framework to quantify peak/pit asymmetries in complex\n  dynamics",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.04462v1": {
            "Paper Title": "Effects of Images with Different Levels of Familiarity on EEG",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.03999v1": {
            "Paper Title": "Gap junction plasticity can lead to spindle oscillations",
            "Sentences": []
        },
        "http://arxiv.org/abs/1709.08166v3": {
            "Paper Title": "Spiking neurons with short-term synaptic plasticity form superior\n  generative networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.03667v1": {
            "Paper Title": "High-dimensional dynamics of generalization error in neural networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.03613v1": {
            "Paper Title": "Auditory Brainstem Response in Infants and Children with Autism: A\n  Meta-Analysis",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.02316v1": {
            "Paper Title": "A Multiscale Patch Based Convolutional Network for Brain Tumor\n  Segmentation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.02113v1": {
            "Paper Title": "Anatomical Pattern Analysis for decoding visual stimuli in human brains",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.00559v1": {
            "Paper Title": "Crude EEG parameter provides sleep medicine with well-defined continuous\n  hypnograms",
            "Sentences": []
        },
        "http://arxiv.org/abs/1710.02387v1": {
            "Paper Title": "Simulating and Reconstructing Neurodynamics with Epsilon-Automata\n  Applied to Electroencephalography (EEG) Microstate Sequences",
            "Sentences": []
        },
        "http://arxiv.org/abs/1709.04090v2": {
            "Paper Title": "A Constrained, Weighted-L1 Minimization Approach for Joint Discovery of\n  Heterogeneous Neural Connectivity Graphs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1709.06134v1": {
            "Paper Title": "Discrete Dynamic Causal Modeling and Its Relationship with Directed\n  Information",
            "Sentences": []
        },
        "http://arxiv.org/abs/1703.06091v2": {
            "Paper Title": "Frequency-based brain networks: From a multiplex framework to a full\n  multilayer description",
            "Sentences": []
        },
        "http://arxiv.org/abs/1709.04654v1": {
            "Paper Title": "Deep Predictive Learning: A Comprehensive Model of Three Visual Streams",
            "Sentences": []
        },
        "http://arxiv.org/abs/1709.04550v1": {
            "Paper Title": "A Computational Model of Afterimages based on Simultaneous and\n  Successive Contrasts",
            "Sentences": []
        },
        "http://arxiv.org/abs/1709.08578v1": {
            "Paper Title": "Heritability estimates on resting state fMRI data using the ENIGMA\n  analysis pipeline",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.08755v2": {
            "Paper Title": "Multi-task Neural Networks for Personalized Pain Recognition from\n  Physiological Signals",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.09932v3": {
            "Paper Title": "The placement of the head that maximizes predictability. An information\n  theoretic approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/1709.00583v1": {
            "Paper Title": "Training Spiking Neural Networks for Cognitive Tasks: A Versatile\n  Framework Compatible to Various Temporal Codes",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.09072v1": {
            "Paper Title": "Continual One-Shot Learning of Hidden Spike-Patterns with Neural Network\n  Simulation Expansion and STDP Convergence Predictions",
            "Sentences": []
        },
        "http://arxiv.org/abs/1704.03568v2": {
            "Paper Title": "Beyond Planar Symmetry: Modeling human perception of reflection and\n  rotation symmetries in the wild",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.08006v1": {
            "Paper Title": "Effects of mindfulness on perceived stress levels and heart rate\n  variability",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.01382v2": {
            "Paper Title": "Neuro-RAM Unit with Applications to Similarity Testing and Compression\n  in Spiking Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1704.01350v4": {
            "Paper Title": "Resting-state functional connectivity-based biomarkers and functional\n  MRI-based neurofeedback for psychiatric disorders: a challenge for developing\n  theranostic biomarkers",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.04168v1": {
            "Paper Title": "Robust transformations of firing patterns for neural networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1701.00838v2": {
            "Paper Title": "Encoding Sensory and Motor Patterns as Time-Invariant Trajectories in\n  Recurrent Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.03687v1": {
            "Paper Title": "Effects of pitch and timing expectancy on musical emotion",
            "Sentences": []
        },
        "http://arxiv.org/abs/1708.03666v1": {
            "Paper Title": "Attention but not musical training affects auditory streaming",
            "Sentences": []
        },
        "http://arxiv.org/abs/1703.05917v3": {
            "Paper Title": "Autocatalytic networks in cognition and the origin of culture",
            "Sentences": []
        },
        "http://arxiv.org/abs/1707.06314v1": {
            "Paper Title": "Fast, Simple Calcium Imaging Segmentation with Fully Convolutional\n  Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1707.05182v1": {
            "Paper Title": "A probabilistic model for learning in cortical microcircuit motifs with\n  data-based divisive inhibition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1707.03591v1": {
            "Paper Title": "Multiscale Granger causality analysis by \u00e0 trous wavelet transform",
            "Sentences": []
        },
        "http://arxiv.org/abs/1704.07575v3": {
            "Paper Title": "Sharing deep generative representation for perceived image\n  reconstruction from human brain activity",
            "Sentences": []
        },
        "http://arxiv.org/abs/1707.01746v1": {
            "Paper Title": "Spike-based probabilistic inference with correlated noise",
            "Sentences": []
        },
        "http://arxiv.org/abs/1707.00375v1": {
            "Paper Title": "Adaptive Stimulus Selection in ERP-Based Brain-Computer Interfaces by\n  Maximizing Expected Discrimination Gain",
            "Sentences": []
        },
        "http://arxiv.org/abs/1707.00142v1": {
            "Paper Title": "EEG and ECG changes during deep-sea manned submersible operation",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.09570v1": {
            "Paper Title": "Simulation Study of Two Measures of Integrated Information",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.09089v1": {
            "Paper Title": "Continuous use of ERP-based BCIs with different visual angles in ALS\n  patients",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.08202v1": {
            "Paper Title": "Parsing spatiotemporal dynamical stability in ECoG during seizure onset,\n  propagation, and termination",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.07147v1": {
            "Paper Title": "A Useful Motif for Flexible Task Learning in an Embodied Two-Dimensional\n  Visual Environment",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.06495v1": {
            "Paper Title": "Wireless Optogenetic Nanonetworks: Device Model and Charging Protocols",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.04698v2": {
            "Paper Title": "Gradient Descent for Spiking Neural Networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.06088v1": {
            "Paper Title": "Inter-regional ECoG correlations predicted by communication dynamics,\n  geometry, and correlated gene expression",
            "Sentences": [
                {
                    "Sentence ID": 101,
                    "Sentence": "M. Bazzi, M. A. Porter, S. Williams, M. McDonald,\nD. J. Fenn, and S. D. Howison, Multiscale Modeling &\nSimulation 14, 1 (2016). ",
                    "Citation Text": "V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and\nE. Lefebvre, Journal of statistical mechanics: theory\nand experiment 2008 , P10008 (2008).",
                    "Citation": {
                        "Citation Paper ID": "arXiv:0803.0476",
                        "Citation Paper Title": "Title:Fast unfolding of communities in large networks",
                        "Citation Paper Abstract": "Abstract:  We propose a simple method to extract the community structure of large networks. Our method is a heuristic method that is based on modularity optimization. It is shown to outperform all other known community detection method in terms of computation time. Moreover, the quality of the communities detected is very good, as measured by the so-called modularity. This is shown first by identifying language communities in a Belgian mobile phone network of 2.6 million customers and by analyzing a web graph of 118 million nodes and more than one billion links. The accuracy of our algorithm is also verified on ad-hoc modular networks. .",
                        "Citation Paper Authors": "Authors:Vincent D. Blondel, Jean-Loup Guillaume, Renaud Lambiotte, Etienne Lefebvre"
                    }
                },
                {
                    "Sentence ID": 59,
                    "Sentence": ". Modules are thought to be critical for both devel-\nopment and evolution by compartmentalizing brain areas\nthat perform similar functions ",
                    "Citation Text": "J. Clune, J.-B. Mouret, and H. Lipson, in Proc. R. Soc.\nB, Vol. 280 (The Royal Society, 2013) p. 20122863.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1207.2743",
                        "Citation Paper Title": "Title:The evolutionary origins of modularity",
                        "Citation Paper Abstract": "Abstract:A central biological question is how natural organisms are so evolvable (capable of quickly adapting to new environments). A key driver of evolvability is the widespread modularity of biological networks--their organization as functional, sparsely connected subunits--but there is no consensus regarding why modularity itself evolved. While most hypotheses assume indirect selection for evolvability, here we demonstrate that the ubiquitous, direct selection pressure to reduce the cost of connections between network nodes causes the emergence of modular networks. Experiments with selection pressures to maximize network performance and minimize connection costs yield networks that are significantly more modular and more evolvable than control experiments that only select for performance. These results will catalyze research in numerous disciplines, including neuroscience, genetics and harnessing evolution for engineering purposes.",
                        "Citation Paper Authors": "Authors:Jeff Clune, Jean-Baptiste Mouret, Hod Lipson"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1705.07109v3": {
            "Paper Title": "Deep adversarial neural decoding",
            "Sentences": [
                {
                    "Sentence ID": 37,
                    "Sentence": ". Model parameters were initialized as follows: biases were set to zero, the scaling parameters\nwere drawn fromN(1;2\u000110\u00002I), the shifting parameters were set to zero and the weights were drawn\nfromN(1;10\u00002I) ",
                    "Citation Text": "A. Radford, L. Metz, and S. Chintala, \u201cUnsupervised representation learning with deep convolutional\ngenerative adversarial networks,\u201d CoRR , vol. abs/1511.06434, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1511.06434",
                        "Citation Paper Title": "Title:Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
                        "Citation Paper Abstract": "Abstract:In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.",
                        "Citation Paper Authors": "Authors:Alec Radford, Luke Metz, Soumith Chintala"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1705.02205v2": {
            "Paper Title": "Towards a realistic NNLIF model: Analysis and numerical solver for\n  excitatory-inhibitory networks with delay and refractory periods",
            "Sentences": [
                {
                    "Sentence ID": 8,
                    "Sentence": ".\nWe start the analysis of the blow-up phenomenon by consideri ng only one average-excitatory popu-\nlation (we recall that there is global existence for one aver age-inhibitory population, see ",
                    "Citation Text": "J. A. Carrillo, M. d. M. Gonz \u00b4alez, M. P. Gualdani, and M. E. Schonbek ,Classical\nsolutions for a nonlinear fokker-planck equation arising i n computational neuroscience , Comm.\nin Partial Di\ufb00erential Equations, 38 (2013), pp. 385\u2013409.\n27",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1109.1298",
                        "Citation Paper Title": "Title:Classical Solutions for a nonlinear Fokker-Planck equation arising in Computational Neuroscience",
                        "Citation Paper Abstract": "Abstract:In this paper we analyze the global existence of classical solutions to the initial boundary-value problem for a nonlinear parabolic equation describing the collective behavior of an ensemble of neurons. These equations were obtained as a diffusive approximation of the mean-field limit of a stochastic differential equation system. The resulting Fokker-Planck equation presents a nonlinearity in the coefficients depending on the probability flux through the boundary. We show by an appropriate change of variables that this parabolic equation with nonlinear boundary conditions can be transformed into a non standard Stefan-like free boundary problem with a source term given by a delta function. We prove that there are global classical solutions for inhibitory neural networks, while for excitatory networks we give local well-posedness of classical solutions together with a blow up criterium. Finally, we will also study the spectrum for the linear problem corresponding to uncoupled networks and its relation to Poincar\u00e9 inequalities for studying their asymptotic behavior.",
                        "Citation Paper Authors": "Authors:Jos\u00e9 A. Carrillo, Mar\u00eda d. M. Gonz\u00e1lez, Maria P. Gualdani, Maria E. Schonbek"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1703.04200v3": {
            "Paper Title": "Continual Learning Through Synaptic Intelligence",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.02764v1": {
            "Paper Title": "What modern vision science reveals about the awareness puzzle:\n  Summary-statistic encoding plus decision limits underlie the richness of\n  visual perception and its quirky failures",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.02451v1": {
            "Paper Title": "Differential Covariance: A New Class of Methods to Estimate Sparse\n  Connectivity from Neural Recordings",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.01728v1": {
            "Paper Title": "The Impact of Flow in an EEG-based Brain Computer Interface",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.01521v1": {
            "Paper Title": "Will Break for Productivity: Generalized Symptoms of Cognitive Depletion",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.00780v1": {
            "Paper Title": "Unsupervised Learning of Spike Patterns for Seizure Detection and\n  Wavefront Estimation of High Resolution Micro Electrocorticographic\n  (\u03bcECoG) Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.00603v1": {
            "Paper Title": "Classification of meaningful and meaningless visual objects: a graph\n  similarity approach",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.10882v1": {
            "Paper Title": "Morphological Error Detection in 3D Segmentations",
            "Sentences": [
                {
                    "Sentence ID": 1,
                    "Sentence": ", demonstrating that our algorithm can be integrated into a scalable connectomics\npipeline. Note that our experiments were performed using TensorFlow ",
                    "Citation Text": "Mart\u0013 \u0010n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S\nCorrado, Andy Davis, Je\u000brey Dean, Matthieu Devin, et al. Tensor\row: Large-scale machine learning\non heterogeneous distributed systems. arXiv preprint arXiv:1603.04467 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1603.04467",
                        "Citation Paper Title": "Title:TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems",
                        "Citation Paper Abstract": "Abstract:TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at this http URL.",
                        "Citation Paper Authors": "Authors:Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, Xiaoqiang Zheng"
                    }
                },
                {
                    "Sentence ID": 16,
                    "Sentence": ". Various techniques have been proposed to work with anisotropic\ndata, including 2D ConvNets feeding into 3D ConvNets ",
                    "Citation Text": "Kisuk Lee, Aleksandar Zlateski, Vishwanathan Ashwin, and H Sebastian Seung. Recursive training of\n2d-3d convolutional networks for neuronal boundary prediction. In Advances in Neural Information\nProcessing Systems (NIPS) , pages 3573{3581, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1508.04843",
                        "Citation Paper Title": "Title:Recursive Training of 2D-3D Convolutional Networks for Neuronal Boundary Detection",
                        "Citation Paper Abstract": "Abstract:Efforts to automate the reconstruction of neural circuits from 3D electron microscopic (EM) brain images are critical for the field of connectomics. An important computation for reconstruction is the detection of neuronal boundaries. Images acquired by serial section EM, a leading 3D EM technique, are highly anisotropic, with inferior quality along the third dimension. For such images, the 2D max-pooling convolutional network has set the standard for performance at boundary detection. Here we achieve a substantial gain in accuracy through three innovations. Following the trend towards deeper networks for object recognition, we use a much deeper network than previously employed for boundary detection. Second, we incorporate 3D as well as 2D filters, to enable computations that use 3D context. Finally, we adopt a recursively trained architecture in which a first network generates a preliminary boundary map that is provided as input along with the original image to a second network that generates a final boundary map. Backpropagation training is accelerated by ZNN, a new implementation of 3D convolutional networks that uses multicore CPU parallelism for speed. Our hybrid 2D-3D architecture could be more generally applicable to other types of anisotropic 3D images, including video, and our recursive framework for any image labeling problem.",
                        "Citation Paper Authors": "Authors:Kisuk Lee, Aleksandar Zlateski, Ashwin Vishwanathan, H. Sebastian Seung"
                    }
                },
                {
                    "Sentence ID": 11,
                    "Sentence": "describe CELIS, a neural network\napproach for optimizing local features of a segmented image. Januszewski et al. ",
                    "Citation Text": "Micha l Januszewski, Jeremy Maitin-Shepard, Peter Li, J\u007f orgen Kornfeld, Winfried Denk, and Viren\nJain. Flood-\flling networks. arXiv preprint arXiv:1611.00421 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.00421",
                        "Citation Paper Title": "Title:Flood-Filling Networks",
                        "Citation Paper Abstract": "Abstract:State-of-the-art image segmentation algorithms generally consist of at least two successive and distinct computations: a boundary detection process that uses local image information to classify image locations as boundaries between objects, followed by a pixel grouping step such as watershed or connected components that clusters pixels into segments. Prior work has varied the complexity and approach employed in these two steps, including the incorporation of multi-layer neural networks to perform boundary prediction, and the use of global optimizations during pixel clustering. We propose a unified and end-to-end trainable machine learning approach, flood-filling networks, in which a recurrent 3d convolutional network directly produces individual segments from a raw image. The proposed approach robustly segments images with an unknown and variable number of objects as well as highly variable object sizes. We demonstrate the approach on a challenging 3d image segmentation task, connectomic reconstruction from volume electron microscopy data, on which flood-filling neural networks substantially improve accuracy over other state-of-the-art methods. The proposed approach can replace complex multi-step segmentation pipelines with a single neural network that is learned end-to-end.",
                        "Citation Paper Authors": "Authors:Micha\u0142 Januszewski, Jeremy Maitin-Shepard, Peter Li, J\u00f6rgen Kornfeld, Winfried Denk, Viren Jain"
                    }
                },
                {
                    "Sentence ID": 4,
                    "Sentence": ", combined 2D-3D\napproaches [6, 16], and purely 3D networks [20, 36]. Accelerated implementation techniques for 3D\nnetworks have been introduced by Budden, et al. ",
                    "Citation Text": "David Budden, Alexander Matveev, Shibani Santurkar, Shraman Ray Chaudhuri, and Nir Shavit.\nDeep tensor convolution on multicores. arXiv preprint arXiv:1611.06565 , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1611.06565",
                        "Citation Paper Title": "Title:Deep Tensor Convolution on Multicores",
                        "Citation Paper Abstract": "Abstract:Deep convolutional neural networks (ConvNets) of 3-dimensional kernels allow joint modeling of spatiotemporal features. These networks have improved performance of video and volumetric image analysis, but have been limited in size due to the low memory ceiling of GPU hardware. Existing CPU implementations overcome this constraint but are impractically slow. Here we extend and optimize the faster Winograd-class of convolutional algorithms to the $N$-dimensional case and specifically for CPU hardware. First, we remove the need to manually hand-craft algorithms by exploiting the relaxed constraints and cheap sparse access of CPU memory. Second, we maximize CPU utilization and multicore scalability by transforming data matrices to be cache-aware, integer multiples of AVX vector widths. Treating 2-dimensional ConvNets as a special (and the least beneficial) case of our approach, we demonstrate a 5 to 25-fold improvement in throughput compared to previous state-of-the-art.",
                        "Citation Paper Authors": "Authors:David Budden, Alexander Matveev, Shibani Santurkar, Shraman Ray Chaudhuri, Nir Shavit"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1705.10854v1": {
            "Paper Title": "A Tale of Two Animats: What does it take to have goals?",
            "Sentences": []
        },
        "http://arxiv.org/abs/1702.05394v2": {
            "Paper Title": "Measures of spike train synchrony for data with multiple time-scales",
            "Sentences": []
        },
        "http://arxiv.org/abs/1706.05932v1": {
            "Paper Title": "A deep learning-inspired model of the hippocampus as storage device of\n  the brain extended dataset",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.07614v1": {
            "Paper Title": "Feedback inhibition shapes emergent computational properties of cortical\n  microcircuit motifs",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.07887v1": {
            "Paper Title": "Using Branch Predictors to Monitor Brain Activity",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.02176v2": {
            "Paper Title": "Discrete Modeling of Multi-Transmitter Neural Networks with Neuron\n  Competition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.01208v1": {
            "Paper Title": "A Rule-Based Computational Model of Cognitive Arithmetic",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.02019v1": {
            "Paper Title": "Complex tensor factorisation with PARAFAC2 for the estimation of brain\n  connectivity from the EEG",
            "Sentences": []
        },
        "http://arxiv.org/abs/1705.00550v1": {
            "Paper Title": "Topography of the nuclei and distribution of Acetylcholinesterase\n  activity in the septum of the telencephalon in man",
            "Sentences": []
        },
        "http://arxiv.org/abs/1704.07526v1": {
            "Paper Title": "Neurogenesis and multiple plasticity mechanisms enhance associative\n  memory retrieval in a spiking network model of the hippocampus",
            "Sentences": []
        },
        "http://arxiv.org/abs/1704.05826v1": {
            "Paper Title": "Multi-scale detection of hierarchical community architecture in\n  structural and functional brain networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1704.05694v1": {
            "Paper Title": "NEURAL: quantitative features for newborn EEG using Matlab",
            "Sentences": []
        },
        "http://arxiv.org/abs/1704.03941v2": {
            "Paper Title": "Analysis of pacemaker activity in a two-component model of some\n  brainstem neurons",
            "Sentences": []
        },
        "http://arxiv.org/abs/1702.02510v3": {
            "Paper Title": "AMIGOS: A Dataset for Affect, Personality and Mood Research on\n  Individuals and Groups",
            "Sentences": []
        },
        "http://arxiv.org/abs/1704.02342v1": {
            "Paper Title": "Sleep Paralysis: phenomenology, neurophysiology and treatment",
            "Sentences": []
        },
        "http://arxiv.org/abs/1704.00793v1": {
            "Paper Title": "Seeds Cleansing CNMF for Spatiotemporal Neural Signals Extraction of\n  Miniscope Imaging Data",
            "Sentences": [
                {
                    "Sentence ID": 10,
                    "Sentence": "Previous algorithms\nfor automatic extraction of calcium sig-\nnals from imaging videos can be divided\ninto three categories: linear unsupervised\nbasis learning methods [ 5\u20137], nonlin-\near unsupervised basis learning meth-\nods [ 8,9], and the supervised learning\nmethod ",
                    "Citation Text": "N. Apthorpe, A. Riordan, R. Aguilar, J. Homann, Y . Gu, D. Tank, and S. Seung. Automatic neuron\ndetection in calcium imaging data using convolutional networks. In Advances In Neural Information\nProcessing Systems , 2016.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1606.07372",
                        "Citation Paper Title": "Title:Automatic Neuron Detection in Calcium Imaging Data Using Convolutional Networks",
                        "Citation Paper Abstract": "Abstract:Calcium imaging is an important technique for monitoring the activity of thousands of neurons simultaneously. As calcium imaging datasets grow in size, automated detection of individual neurons is becoming important. Here we apply a supervised learning approach to this problem and show that convolutional networks can achieve near-human accuracy and superhuman speed. Accuracy is superior to the popular PCA/ICA method based on precision and recall relative to ground truth annotation by a human expert. These results suggest that convolutional networks are an efficient and flexible tool for the analysis of large-scale calcium imaging data.",
                        "Citation Paper Authors": "Authors:Noah J. Apthorpe, Alexander J. Riordan, Rob E. Aguilar, Jan Homann, Yi Gu, David W. Tank, H. Sebastian Seung"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1703.10627v1": {
            "Paper Title": "Analysis and Modelling of Subthreshold Neural Multi-electrode Array Data\n  by Statistical Field Theory",
            "Sentences": []
        },
        "http://arxiv.org/abs/1703.06946v1": {
            "Paper Title": "SCALPEL: Extracting Neurons from Calcium Imaging Data",
            "Sentences": []
        },
        "http://arxiv.org/abs/1703.04056v1": {
            "Paper Title": "Quantifying the strength of structural connectivity underlying\n  functional brain networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1703.03132v1": {
            "Paper Title": "From the statistics of connectivity to the statistics of spike times in\n  neuronal networks",
            "Sentences": []
        },
        "http://arxiv.org/abs/1702.08509v2": {
            "Paper Title": "The Main Cognitive Model of Visual Recognition: Contour Recognition",
            "Sentences": []
        },
        "http://arxiv.org/abs/1701.07083v2": {
            "Paper Title": "Harnessing the Web for Population-Scale Physiological Sensing: A Case\n  Study of Sleep and Performance",
            "Sentences": []
        },
        "http://arxiv.org/abs/1702.07426v1": {
            "Paper Title": "Control of the Correlation of Spontaneous Neuron Activity in Biological\n  and Noise-activated CMOS Artificial Neural Microcircuits",
            "Sentences": []
        },
        "http://arxiv.org/abs/1702.06538v1": {
            "Paper Title": "Applications of Discrete Mathematics for Understanding Dynamics of\n  Synapses and Networks in Neuroscience",
            "Sentences": []
        },
        "http://arxiv.org/abs/1702.01591v2": {
            "Paper Title": "The Partial Entropy Decomposition: Decomposing multivariate entropy and\n  mutual information via pointwise common surprisal",
            "Sentences": []
        },
        "http://arxiv.org/abs/1702.03993v1": {
            "Paper Title": "The Causal Role of Astrocytes in Slow-Wave Rhythmogenesis: A\n  Computational Modelling Study",
            "Sentences": []
        },
        "http://arxiv.org/abs/1702.03579v1": {
            "Paper Title": "Autonomous line follower robot controlled by cell culture",
            "Sentences": []
        },
        "http://arxiv.org/abs/1702.03418v1": {
            "Paper Title": "Exponential distance distribution of connected neurons in simulations of\n  two-dimensional in vitro neural network development",
            "Sentences": []
        },
        "http://arxiv.org/abs/1702.03409v1": {
            "Paper Title": "Disruptive Behavior Disorder (DBD) Rating Scale for Georgian Population",
            "Sentences": []
        },
        "http://arxiv.org/abs/1702.00687v1": {
            "Paper Title": "Heterogeneous gain distributions in neural networks I:The stationary\n  case",
            "Sentences": []
        },
        "http://arxiv.org/abs/1702.00101v1": {
            "Paper Title": "Process reveals structure: How a network is traversed mediates\n  expectations about its architecture",
            "Sentences": []
        },
        "http://arxiv.org/abs/1701.08663v1": {
            "Paper Title": "A computational study on synaptic and extrasynaptic effects of astrocyte\n  glutamate uptake on orientation tuning in V1",
            "Sentences": []
        },
        "http://arxiv.org/abs/1701.07646v1": {
            "Paper Title": "Brain State Flexibility Accompanies Motor-Skill Acquisition",
            "Sentences": [
                {
                    "Sentence ID": 98,
                    "Sentence": ", the temporal dynamics of an activity time trace do not necessarily map onto patterns of\nfunctional connectivity ",
                    "Citation Text": "F. Siebenhuhner, S. A. Weiss, R. Coppola, D. R. Weinberger, and D. S. Bassett. Intra- and inter-\nfrequency brain network structure in health and schizophrenia. PLoS One , 8(8):e72351, 2013.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1209.0729",
                        "Citation Paper Title": "Title:Intra- and Inter-Frequency Brain Network Structure in Health and Schizophrenia",
                        "Citation Paper Abstract": "Abstract:Empirical studies over the past two decades have supported the hypothesis that schizophrenia is characterized by altered connectivity patterns in functional brain networks. These alterations have been proposed as genetically-mediated diagnostic biomarkers and are thought to underlie altered cognitive functions such as working memory. In this study, we perform an extensive analysis of functional connectivity patterns extracted from MEG data in 14 subjects with schizophrenia and 14 healthy controls during a 2-back working memory task. We investigate uni-, bi- and multivariate properties of sensor time series by computing wavelet entropy of and correlation between time series, and by constructing binary networks of functional connectivity both within and between classical frequency bands (gamma, beta, alpha, and theta). Networks are based on the mutual information between wavelet time series, and estimated for 66 separate time windows. We observed decreases in entropy in prefrontal and lateral sensor time series and increases in connectivity strength in the schizophrenia group in comparison to the healthy controls. We identified an inverse relationship between entropy and strength across both subjects and sensors that varied over frequency bands and was more pronounced in controls than in patients. Brain network topology was altered in schizophrenia specifically in high frequency gamma and beta band networks as well as in the gamma-beta cross-frequency networks. Network topology varied over trials to a greater extent in patients than in controls, suggesting disease-associated alterations in dynamic network properties of brain function. Our results identify signatures of aberrant neurophysiological behavior in schizophrenia across uni-, bi- and multivariate scales and identify cross-frequency network architecture and network dynamics as candidate intermediate phenotypes.",
                        "Citation Paper Authors": "Authors:Felix Siebenhuhner, Shennan A. Weiss, Richard Coppola, Daniel R. Weinberger, Danielle S. Bassett"
                    }
                },
                {
                    "Sentence ID": 14,
                    "Sentence": ", and we report results summarized over those iterations by building what are known as\nconsensus partitions ",
                    "Citation Text": "D. S. Bassett, M. Yang, N. F. Wymbs, and S. T. Grafton. Learning-induced autonomy of sensorimotor\nsystems. Nature neuroscience , 18(5):744{751, 2015.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:1403.6034",
                        "Citation Paper Title": "Title:Learning-Induced Autonomy of Sensorimotor Systems",
                        "Citation Paper Abstract": "Abstract:Distributed networks of brain areas interact with one another in a time-varying fashion to enable complex cognitive and sensorimotor functions. Here we use novel network analysis algorithms to test the recruitment and integration of large-scale functional neural circuitry during learning. Using functional magnetic resonance imaging (fMRI) data acquired from healthy human participants, from initial training through mastery of a simple motor skill, we investigate changes in the architecture of functional connectivity patterns that promote learning. Our results reveal that learning induces an autonomy of sensorimotor systems and that the release of cognitive control hubs in frontal and cingulate cortices predicts individual differences in the rate of learning on other days of practice. Our general statistical approach is applicable across other cognitive domains and provides a key to understanding time-resolved interactions between distributed neural circuits that enable task performance.",
                        "Citation Paper Authors": "Authors:Danielle S. Bassett, Muzhi Yang, Nicholas F. Wymbs, Scott T. Grafton"
                    }
                },
                {
                    "Sentence ID": 50,
                    "Sentence": ":\nPij=kikj\n2m\nwhereki=P\njAijis the strength of region iandm=1\n2P\nijAij. Importantly, the algorithm we use is a\nheuristic that implements a non-deterministic optimization ",
                    "Citation Text": "B. H. Good, Y. A. de Montjoye, and A. Clauset. Performance of modularity maximization in practical\ncontexts. Phys Rev E Stat Nonlin Soft Matter Phys , 81(4 Pt 2):046106, 2010.",
                    "Citation": {
                        "Citation Paper ID": "arXiv:0910.0165",
                        "Citation Paper Title": "Title:The performance of modularity maximization in practical contexts",
                        "Citation Paper Abstract": "Abstract:Although widely used in practice, the behavior and accuracy of the popular module identification technique called modularity maximization is not well understood in practical contexts. Here, we present a broad characterization of its performance in such situations. First, we revisit and clarify the resolution limit phenomenon for modularity maximization. Second, we show that the modularity function Q exhibits extreme degeneracies: it typically admits an exponential number of distinct high-scoring solutions and typically lacks a clear global maximum. Third, we derive the limiting behavior of the maximum modularity Q_max for one model of infinitely modular networks, showing that it depends strongly both on the size of the network and on the number of modules it contains. Finally, using three real-world metabolic networks as examples, we show that the degenerate solutions can fundamentally disagree on many, but not all, partition properties such as the composition of the largest modules and the distribution of module sizes. These results imply that the output of any modularity maximization procedure should be interpreted cautiously in scientific contexts. They also explain why many heuristics are often successful at finding high-scoring partitions in practice and why different heuristics can disagree on the modular structure of the same network. We conclude by discussing avenues for mitigating some of these behaviors, such as combining information from many degenerate solutions or using generative models.",
                        "Citation Paper Authors": "Authors:Benjamin H. Good, Yves-Alexandre de Montjoye, Aaron Clauset"
                    }
                }
            ]
        },
        "http://arxiv.org/abs/1701.04893v1": {
            "Paper Title": "Grid cells show field-to-field variability and this explains the\n  aperiodic response of inhibitory interneurons",
            "Sentences": []
        },
        "http://arxiv.org/abs/1701.04782v1": {
            "Paper Title": "Organization and hierarchy of the human functional brain network lead to\n  a chain-like core",
            "Sentences": []
        },
        "http://arxiv.org/abs/1701.04674v1": {
            "Paper Title": "Human perception in computer vision",
            "Sentences": []
        },
        "http://arxiv.org/abs/1701.01101v1": {
            "Paper Title": "Network Analyses and Nervous System Disorders",
            "Sentences": []
        }
    }
}